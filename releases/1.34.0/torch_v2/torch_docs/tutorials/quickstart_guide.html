<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quickstart Guide &mdash; AI Model Efficiency Toolkit Documentation: ver 1.34.0</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/style.css" />
      <link rel="stylesheet" href="../../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Post-Training Quantization" href="../examples/ptq.html" />
    <link rel="prev" title="AIMET Installation in Docker" href="../../install/install_docker.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="../index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        1.34.0
      </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantized_modules.html">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../encoding_analyzer.html">Encoding Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/nn.fake_quantization_mixin.html">FakeQuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/nn.quantization_mixin.html">QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/quantization/affine/index.html">quantization.affine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/quantization/float/index.html">quantization.float</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quickstart Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/torch_docs/tutorials/quickstart_guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="../../install/install_docker.html" class="btn btn-neutral float-left" title="AIMET Installation in Docker" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/ptq.html" class="btn btn-neutral float-right" title="Post-Training Quantization" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quickstart-guide">
<span id="tutorials-quickstart-guide"></span><h1>Quickstart Guide<a class="headerlink" href="#quickstart-guide" title="Permalink to this heading"></a></h1>
<p>In this tutorial, we will go through the end-to-end process of using AIMET and PyTorch to create, calibrate, and export
a simple quantized model. Note that this is intended to show the most basic workflow in AIMET. It is <em>not</em> meant to
demonstrate the most state-of-the-art techniques available in AIMET.</p>
<section id="overall-flow">
<h2>Overall flow<a class="headerlink" href="#overall-flow" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Define the basic floating-point PyTorch model, training, and eval loops</p></li>
<li><p>Prepare the trained model for quantization</p></li>
<li><p>Create quantization simulation (quantsim) model in AIMET to simulate the effects of quantization</p></li>
<li><p>Calibrate the quantsim model on training data and evaluate the quantized accuracy</p></li>
<li><p>Fine-tune the quantized model to improve the quantized accuracy</p></li>
<li><p>Export the quantized model</p></li>
</ol>
</section>
<section id="pytorch-prerequisites">
<h2>PyTorch prerequisites<a class="headerlink" href="#pytorch-prerequisites" title="Permalink to this heading"></a></h2>
<p>To see clearly what happens inside AIMET, let’s first start with some simple PyTorch code for defining, training, and
evaluating a model. The code below is adapted from PyTorch’s
<a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">basic optimization tutorial</a>.
Note that AIMET does not have any special requirement on what these training/eval loops look like.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

<span class="c1"># 1) Start with some data loaders to train, evaluate, and calibrate the model</span>

<span class="n">cifar10_train_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s1">&#39;/tmp/cifar10&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">cifar10_test_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s1">&#39;/tmp/cifar10&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">cifar10_train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">cifar10_train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 2) Define a simple model to train on this dataset</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># 3) Define an evaluation loop for the model</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span> <span class="o">*</span> <span class="mf">100.</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>Now, let’s instantiate a network and train for a few epochs on our dataset to establish a baseline floating-point model</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>

<span class="c1"># Send the model to the desired device (optional)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Define some loss function and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># Train for 4 epochs</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Evaluate the floating-point model</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">fp_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Floating point accuracy: </span><span class="si">{</span><span class="n">fp_accuracy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Floating point accuracy: 91.70999908447266
</pre></div>
</div>
</section>
<section id="prepare-the-floating-point-model-for-quantization">
<h2>Prepare the floating point model for quantization<a class="headerlink" href="#prepare-the-floating-point-model-for-quantization" title="Permalink to this heading"></a></h2>
<p>Before we can (accurately) simulate quantization, there are a couple important steps to take care of:</p>
<section id="model-preparation">
<h3>1) Model preparation<a class="headerlink" href="#model-preparation" title="Permalink to this heading"></a></h3>
<p>AIMET’s quantization simulation tool (<code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code>) expects the floating point model to conform to some
specific guidelines. For example, <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> is only able to quantize math operations performed by
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> objects, whereas <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> calls will be (incorrectly) ignored.</p>
<p>If we look back at our previous model definition, we see it calls <code class="xref py py-func docutils literal notranslate"><span class="pre">F.relu()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">F.softmax()</span></code> in the forward
function. Does this mean we need to completely redefine our model to use AIMET? Thankfully, no. AIMET provides the
<code class="xref py py-mod docutils literal notranslate"><span class="pre">model_preparer</span></code> API to transform our incompatible model into a new fully-compatible model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aimet_torch</span> <span class="kn">import</span> <span class="n">model_preparer</span>

<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">model_preparer</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>

<span class="c1"># Note: This transformation should not change the model&#39;s forward function at all</span>
<span class="n">fp_accuracy_prepared</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">fp_accuracy_prepared</span> <span class="o">==</span> <span class="n">fp_accuracy</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>2024-05-07 14:39:22,747 - root - INFO - AIMET
2024-05-07 14:39:22,806 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_relu}
2024-05-07 14:39:22,806 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_relu_1}
2024-05-07 14:39:22,806 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_softmax}
GraphModule(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=12544, out_features=10, bias=True)
  (module_relu): ReLU()
  (module_relu_1): ReLU()
  (module_softmax): Softmax(dim=-1)
)



def forward(self, x):
    conv1 = self.conv1(x);  x = None
    bn_1 = self.bn_1(conv1);  conv1 = None
    module_relu = self.module_relu(bn_1);  bn_1 = None
    conv2 = self.conv2(module_relu);  module_relu = None
    bn_2 = self.bn_2(conv2);  conv2 = None
    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None
    getattr_1 = module_relu_1.shape
    getitem = getattr_1[0];  getattr_1 = None
    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None
    linear = self.linear(view);  view = None
    module_softmax = self.module_softmax(linear);  linear = None
    return module_softmax

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>Note how the prepared model now contains distinct modules for the <code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">softmax()</span></code> operations.</p>
</section>
<section id="batchnorm-fold">
<h3>2) BatchNorm fold<a class="headerlink" href="#batchnorm-fold" title="Permalink to this heading"></a></h3>
<p>When models are executed in a quantized runtime, batchnorm layers are typically folded into the weight and bias of
an adjacent convolution layer whenever possible in order to remove unnecessary computations. To accurately simulate
inference in these runtimes, it is generally a good idea to perform this batchnorm folding on the floating point model
before applying quantization. AIMET provides the <code class="xref py py-mod docutils literal notranslate"><span class="pre">batch_norm_fold</span></code> tool to do this.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aimet_torch</span> <span class="kn">import</span> <span class="n">batch_norm_fold</span>

<span class="n">sample_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">batch_norm_fold</span><span class="o">.</span><span class="n">fold_all_batch_norms</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">input_shapes</span><span class="o">=</span><span class="n">sample_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn_1): Identity()
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (bn_2): Identity()
  (linear): Linear(in_features=12544, out_features=10, bias=True)
  (module_relu): ReLU()
  (module_relu_1): ReLU()
  (module_softmax): Softmax(dim=-1)
)



def forward(self, x):
    conv1 = self.conv1(x);  x = None
    bn_1 = self.bn_1(conv1);  conv1 = None
    module_relu = self.module_relu(bn_1);  bn_1 = None
    conv2 = self.conv2(module_relu);  module_relu = None
    bn_2 = self.bn_2(conv2);  conv2 = None
    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None
    getattr_1 = module_relu_1.shape
    getitem = getattr_1[0];  getattr_1 = None
    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None
    linear = self.linear(view);  view = None
    module_softmax = self.module_softmax(linear);  linear = None
    return module_softmax

# To see more debug info, please use `graph_module.print_readable()`
</pre></div>
</div>
<p>Note that the model now has <code class="xref py py-class docutils literal notranslate"><span class="pre">Identity</span></code> (passthrough) layers where it previously had <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code> layers. Like the
<code class="xref py py-mod docutils literal notranslate"><span class="pre">model_preparer</span></code> step, this operation should not impact the model’s accuracy.</p>
</section>
</section>
<section id="quantize-the-model">
<h2>Quantize the model<a class="headerlink" href="#quantize-the-model" title="Permalink to this heading"></a></h2>
<p>Now, we are ready to use AIMET’s <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> to simulate quantizing the floating point model. This
involves two steps:</p>
<ol class="arabic simple">
<li><p>Add quantizers to simulate quantization noise during the model’s forward pass</p></li>
<li><p>Calibrate the quantizer encodings (e.g., min/max ranges) on some sample inputs</p></li>
</ol>
<p>Calibration is necessary to determine the range of values each activation quantizer is likely to encounter in the
model’s forward pass, and should therefore be able to represent. Theoretically, we could pass the entire training
dataset through the model for calibration, but in practice we usually only need about 500-1000 representative samples
to accurately estimate the ranges.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">aimet_torch.v2</span> <span class="k">as</span> <span class="nn">aimet</span>
<span class="kn">from</span> <span class="nn">aimet_torch.v2</span> <span class="kn">import</span> <span class="n">quantsim</span>

<span class="c1"># QuantizationSimModel will convert each nn.Module in prepared_model into a quantized equivalent module and configure the module&#39;s quantizers</span>
<span class="c1"># In this case, we will quantize all parameters to 4 bits and all activations to 8 bits.</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">quantsim</span><span class="o">.</span><span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span>
                                    <span class="n">dummy_input</span><span class="o">=</span><span class="n">sample_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                    <span class="n">default_output_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>                                <span class="c1"># Simulate 8-bit activations</span>
                                    <span class="n">default_param_bw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>                                 <span class="c1"># Simulate 4-bit weights</span>

<span class="c1"># Inside the compute_encodings context, quantizers will observe the statistics of the activations passing through them. These statistics will be used</span>
<span class="c1"># to compute properly calibrated encodings upon exiting the context.</span>
<span class="k">with</span> <span class="n">aimet</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">break</span>

<span class="c1"># Compare the accuracy before and after quantization:</span>
<span class="n">quantized_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Floating point model accuracy: </span><span class="si">{</span><span class="n">fp_accuracy</span><span class="si">}</span><span class="s2"> %</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="sa">f</span><span class="s2">&quot;Quantized model accuracy: </span><span class="si">{</span><span class="n">quantized_accuracy</span><span class="si">}</span><span class="s2"> %&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>GraphModule(
  (conv1): QuantizedConv2d(
    1, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
    (param_quantizers): ModuleDict(
      (weight): QuantizeDequantize(shape=[1], bitwidth=4, symmetric=True)
      (bias): None
    )
    (input_quantizers): ModuleList(
      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
    (output_quantizers): ModuleList(
      (0): None
    )
  )
  (bn_1): Identity()
  (module_relu): FakeQuantizedReLU(
    (param_quantizers): ModuleDict()
    (input_quantizers): ModuleList(
      (0): None
    )
    (output_quantizers): ModuleList(
      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
  )
  (conv2): QuantizedConv2d(
    128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
    (param_quantizers): ModuleDict(
      (weight): QuantizeDequantize(shape=[1], bitwidth=4, symmetric=True)
      (bias): None
    )
    (input_quantizers): ModuleList(
      (0): None
    )
    (output_quantizers): ModuleList(
      (0): None
    )
  )
  (bn_2): Identity()
  (module_relu_1): FakeQuantizedReLU(
    (param_quantizers): ModuleDict()
    (input_quantizers): ModuleList(
      (0): None
    )
    (output_quantizers): ModuleList(
      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
  )
  (linear): QuantizedLinear(
    in_features=12544, out_features=10, bias=True
    (param_quantizers): ModuleDict(
      (weight): QuantizeDequantize(shape=[1], bitwidth=4, symmetric=True)
      (bias): None
    )
    (input_quantizers): ModuleList(
      (0): None
    )
    (output_quantizers): ModuleList(
      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
  )
  (module_softmax): QuantizedSoftmax(
    dim=-1
    (param_quantizers): ModuleDict()
    (input_quantizers): ModuleList(
      (0): None
    )
    (output_quantizers): ModuleList(
      (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
  )
)



def forward(self, x):
    conv1 = self.conv1(x);  x = None
    bn_1 = self.bn_1(conv1);  conv1 = None
    module_relu = self.module_relu(bn_1);  bn_1 = None
    conv2 = self.conv2(module_relu);  module_relu = None
    bn_2 = self.bn_2(conv2);  conv2 = None
    module_relu_1 = self.module_relu_1(bn_2);  bn_2 = None
    getattr_1 = module_relu_1.shape
    getitem = getattr_1[0];  getattr_1 = None
    view = module_relu_1.view(getitem, -1);  module_relu_1 = getitem = None
    linear = self.linear(view);  view = None
    module_softmax = self.module_softmax(linear);  linear = None
    return module_softmax

# To see more debug info, please use `graph_module.print_readable()`
Floating point model accuracy: 91.70999908447266 %
Quantized model accuracy: 91.1500015258789 %
</pre></div>
</div>
<p>Here, we can see that <code class="docutils literal notranslate"><span class="pre">sim.model</span></code> is nothing more than the <code class="docutils literal notranslate"><span class="pre">prepared_model</span></code> with every layer replaced with a
quantized version of the layer. The quantization behavior of each module is determined by the configuration of its
held quantizers.</p>
<p>For example, we can see that <code class="docutils literal notranslate"><span class="pre">sim.model.conv2</span></code> has a 4-bit weight quantizer and an 8-bit output quantizer as specified
during construction. We will discuss more advanced ways to configure these quantizers to optimize performance and
accuracy in a later tutorial.</p>
</section>
<section id="fine-tune-the-model-with-quantization-aware-training">
<h2>Fine-tune the model with quantization aware training<a class="headerlink" href="#fine-tune-the-model-with-quantization-aware-training" title="Permalink to this heading"></a></h2>
<p>If we’re not satisfied with our accuracy after applying quantization, there are some steps we can take to further
optimize the quantized accuracy. One such step is quantization aware training (QAT), during which the model is trained
with the fake-quantization ops present.</p>
<p>Let’s repeat our floating-point training loop for one more epoch, but this time use the quantized model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define some loss function and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># Train for one more epoch on the quantsim model</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


<span class="c1"># Compare the accuracy before and after QAT:</span>
<span class="n">post_QAT_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original quantized model accuracy: </span><span class="si">{</span><span class="n">quantized_accuracy</span><span class="si">}</span><span class="s2"> %</span><span class="se">\n</span><span class="s2">&quot;</span>
      <span class="sa">f</span><span class="s2">&quot;Post-QAT model accuracy: </span><span class="si">{</span><span class="n">post_QAT_accuracy</span><span class="si">}</span><span class="s2"> %&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Original quantized model accuracy: 91.1500015258789 %
Post-QAT model accuracy: 92.05333709716797 %
</pre></div>
</div>
</section>
<section id="export-the-quantsim-model">
<h2>Export the quantsim model<a class="headerlink" href="#export-the-quantsim-model" title="Permalink to this heading"></a></h2>
<p>Now that we are happy with our quantized model’s accuracy, we are ready to export the model with its quantization parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">export_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/&quot;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;fashion_mnist_model&quot;</span>
<span class="n">sample_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>

<span class="n">sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">export_path</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">dummy_input</span><span class="o">=</span><span class="n">sample_input</span><span class="p">)</span>
</pre></div>
</div>
<p>This export method will save the model with quantization nodes removed, along with an encodings file containing
quantization parameters for each activation and weight tensor in the model. These artifacts can then be sent to a
quantized runtime such as Qualcomm® Neural Processing SDK.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../install/install_docker.html" class="btn btn-neutral float-left" title="AIMET Installation in Docker" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/ptq.html" class="btn btn-neutral float-right" title="Post-Training Quantization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>