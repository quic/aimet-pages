<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AIMET Quantization Simulation &mdash; AI Model Efficiency Toolkit Documentation: ver 1.34.0</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="../torch_docs/index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        1.34.0
      </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/tutorials/quickstart_guide.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantized_modules.html">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/encoding_analyzer.html">Encoding Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.fake_quantization_mixin.html">FakeQuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.quantization_mixin.html">QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/affine/index.html">quantization.affine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/float/index.html">quantization.float</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../torch_docs/index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../torch_docs/index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AIMET Quantization Simulation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/quantization_sim.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="aimet-quantization-simulation">
<span id="ug-quantsim"></span><h1>AIMET Quantization Simulation<a class="headerlink" href="#aimet-quantization-simulation" title="Permalink to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>AIMET’s Quantization Simulation feature provides functionality to simulate the effects of quantized hardware. This
allows the user to then apply post-training and/or fine-tuning techniques in AIMET to recover the loss in accuracy, and
ultimately deploy the model on the target device.</p>
<p>When applying QuantSim by itself, optimal quantization scale/offset parameters for each quantizer are found, but no
techniques for mitigating accuracy loss from quantization are applied. Users can either pass their original model
directly to QuantSim to simulate quantization noise on the starting model, or apply Post-Training Quantization
techniques to obtain an updated model to then pass into QuantSim to observe a difference in quantization accuracy as a
result of applying the techniques.</p>
<p>Once a QuantSim object has been created, users can fine-tune the model within the QuantSim object using their
existing pipeline. This method is described in the <a class="reference internal" href="quantization_aware_training.html#ug-quantization-aware-training"><span class="std std-ref">Quantization Aware Training</span></a> page.</p>
<p>The quantization nodes used in QuantSim are custom quantizers defined in AIMET, and are not recognized by targets.
QuantSim provides an export functionality that will save a copy of the model with quantization nodes removed, as well as
generate an encodings file containing quantization scale/offset parameters for each activation and weight tensor in
the model.</p>
<p>A hardware runtime can ingest the encodings file and match it with the exported model to find what scale/offset values
to apply on each tensor in the model.</p>
</section>
<section id="quantsim-workflow">
<h2>QuantSim Workflow<a class="headerlink" href="#quantsim-workflow" title="Permalink to this heading"></a></h2>
<p>A typical workflow for using AIMET quantization simulation to simulate on-target quantized accuracy is described below.</p>
<ol class="arabic simple">
<li><p>The user starts with a pretrained floating-point FP32 model.</p></li>
<li><p>AIMET creates a simulation model by inserting quantization simulation ops into the model graph as explained in the
sub-section below.</p></li>
<li><p>AIMET also configures the inserted simulation ops. The  configuration of these ops can be controlled via a
configuration file as discussed in sub-section below.</p></li>
<li><p>AIMET finds optimal quantization parameters, such as scale/offsets, for the inserted quantization simulation ops. To
do this, AIMET requires the user to provide a callback method that feeds a few representative data samples through
the model. These samples can either be from the training or calibration datasets. Generally, samples in the order of
1,000-2,000 have been sufficient for AIMET to find optimal quantization parameters.</p></li>
<li><p>AIMET returns a quantization simulation model that can be used as a drop-in replacement for the original model in
their evaluation pipeline. Running this simulation model through the evaluation pipeline yields a quantized accuracy
metric that closely simulates on-target accuracy.</p></li>
<li><p>The user can call .export() on the sim object to save a copy of the model with quantization nodes removed, along with
an encodings file containing quantization scale/offset parameters for each activation and weight tensor in the model.</p></li>
</ol>
</section>
<section id="simulating-quantization-noise">
<h2>Simulating Quantization Noise<a class="headerlink" href="#simulating-quantization-noise" title="Permalink to this heading"></a></h2>
<p>The diagram below explains how quantization noise is introduced to a model when its input, output or parameters are
quantized and dequantized.</p>
<blockquote>
<div><img alt="../_images/quant_3.png" src="../_images/quant_3.png" />
</div></blockquote>
<p>Since dequantizated value may not be exactly the same as quantized value, the difference between the two values is the
quantization noise.</p>
<p>In order to simulate quantization noise, AIMET QuantSim adds quantizer ops to the PyTorch/TensorFlow/Keras model graph.
The resulting model graph can be used as is in the user’s evaluation or training pipeline.</p>
</section>
<section id="determining-quantization-parameters-encodings">
<h2>Determining Quantization Parameters (Encodings)<a class="headerlink" href="#determining-quantization-parameters-encodings" title="Permalink to this heading"></a></h2>
<p>Using a QuantSim model, AIMET analyzes and determines the optimal quantization encodings (scale and offset parameters)
for each quantizer op.</p>
<p>To do this, AIMET passes some calibration samples through the model. Using hooks, tensor data is intercepted while
flowing through the model. A histogram is created to model the distribution of the floating point numbers in the output
tensor for each layer.</p>
<img alt="../_images/quant_2.png" src="../_images/quant_2.png" />
<p>Using the distribution of the floating point numbers in the output tensor for each layer, quantization encodings are
computed using the specified quantization calibration technique. An encoding for a layer consists of four numbers:</p>
<ul class="simple">
<li><p>Min (q<sub>min</sub>):     Numbers below these are clamped</p></li>
<li><p>Max (q<sub>max</sub>):    Numbers above these are clamped</p></li>
<li><p>Delta:   Granularity of the fixed point numbers (is a function of the bit-width selected)</p></li>
<li><p>Offset:  Offset from zero</p></li>
</ul>
<dl class="simple">
<dt>The Delta and Offset can be calculated using Min and Max and vice versa using the equations:</dt><dd><p><span class="math notranslate nohighlight">\(\textrm{Delta} = \dfrac{\textrm{Max} - \textrm{Min}}{{2}^{\textrm{bitwidth}} - 1} \quad \textrm{Offset} = \dfrac{-\textrm{Min}}{\textrm{Delta}}\)</span></p>
</dd>
</dl>
</section>
<section id="quantization-schemes">
<h2>Quantization Schemes<a class="headerlink" href="#quantization-schemes" title="Permalink to this heading"></a></h2>
<p>AIMET supports various techniques for coming up with min and max values for encodings, also called quantization schemes:</p>
<ul class="simple">
<li><p>Min-Max: Also referred to as “TF” in AIMET (The name TF represents the origin of this technique and
has no relation to what framework the user is using). To cover the whole dynamic range of the tensor, we can define
the quantization parameters Min and Max to be the observed Min and Max during the calibration process. This leads to
no clipping error. However, this approach is sensitive to outliers, as strong outliers may cause excessive rounding
errors.</p></li>
<li><p>Signal-to-Quantization-Noise (SQNR): Also referred to as “TF Enhanced” in AIMET (The name TF
represents the origin of this technique and has no relation to what framework the user is using). The SQNR approach is
similar to the Mean Square Error (MSE) minimization approach. In the SQNR range setting method, we find qmin and qmax
that minimize the total MSE between the original and the quantized tensor. Quantization noise and saturation noise are
different types of erros which are weighted differently.</p></li>
</ul>
<p>For each quantization scheme, there are “post training” and “training range learning” variants. The “post training”
variants are used during regular QuantSim inference as well as QAT without Range Learning, to come up with initial
encoding values for each quantization node. In QAT without Range Learning, encoding values for activation quantizers
will remain static (encoding values for parameter quantizers will change in accordance with changing parameter values
during training).</p>
<p>The “training range learning” variants are used during QAT with Range Learning. The schemes define how to come up with
initial encoding values for each quantization node, but also allow encoding values for activations to be learned
alongside parameter quantizer encodings during training.</p>
<p>For more details on QAT, refer to <a class="reference internal" href="quantization_aware_training.html#ug-quantization-aware-training"><span class="std std-ref">Quantization Aware Training</span></a>.</p>
</section>
<section id="configuring-quantization-simulation-ops">
<h2>Configuring Quantization Simulation Ops<a class="headerlink" href="#configuring-quantization-simulation-ops" title="Permalink to this heading"></a></h2>
<p>Different hardware and on-device runtimes may support different quantization choices for neural network inference. For
example, some runtimes may support asymmetric quantization for both activations and weights, whereas other ones may
support asymmetric quantization just for weights.</p>
<p>As a result, we need to make quantization choices during simulation that best reflect our target runtime and hardware.
AIMET provides a default configuration file, which can be modified. This file is used during quantization simulation if
no other configuration file is specified.  By default, following configuration is used for quantization simulation:</p>
<ul class="simple">
<li><p>Weight quantization: Per-channel, symmetric quantization, INT8</p></li>
<li><p>Activation or layer output quantization: Per-tensor, asymmetric quantization, INT8</p></li>
</ul>
<p>Quantization options that can be controlled via the configuration file include the following:</p>
<ul class="simple">
<li><p>Enabling/disabling of input and output quantizer ops</p></li>
<li><p>Enabling/disabling of parameter quantizer ops</p></li>
<li><p>Enabling/disabling of model input quantizer</p></li>
<li><p>Enabling/disabling of model output quantizer</p></li>
<li><p>Symmetric/Asymmetric quantization</p></li>
<li><p>Unsigned/signed symmetric quantization</p></li>
<li><p>Strict/non strict symmetric quantization</p></li>
<li><p>Per channel/per tensor quantization</p></li>
<li><p>Defining groups of layers to be fused (no quantization done on intermediate tensors within fused layers)</p></li>
</ul>
<p>Please see the <a class="reference internal" href="quantization_configuration.html#ug-quantsim-config"><span class="std std-ref">Quantization Simulation Configuration</span></a> page which describes the configuration
options in detail.</p>
</section>
<section id="quantization-simulation-apis">
<h2>Quantization Simulation APIs<a class="headerlink" href="#quantization-simulation-apis" title="Permalink to this heading"></a></h2>
<p>Please refer to the links below to view the Quantization Simulation API for each AIMET variant:</p>
<ul class="simple">
<li><p><span class="xref std std-ref">Quantization Simulation for PyTorch</span></p></li>
<li><p><span class="xref std std-ref">Quantization Simulation for Keras</span></p></li>
<li><p><span class="xref std std-ref">Quantization Simulation for ONNX</span></p></li>
</ul>
</section>
<section id="frequently-asked-questions">
<h2>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>Q: How many samples are needed in the calibration step (compute encodings)?</dt><dd><p>A: 1,000 - 2,000 unlabeled representative data samples are sufficient.</p>
</dd>
</dl>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>