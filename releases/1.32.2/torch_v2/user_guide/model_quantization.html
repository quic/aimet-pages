<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AIMET Model Quantization &mdash; AI Model Efficiency Toolkit Documentation: ver 1.32.2</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="../torch_docs/index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        1.32.2
      </div>

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      AIMET Variant: <span class="rst-current-version-name"> PyTorch </span>
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Documentation Versions</dt>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/user_guide/index.html">Universal</a></dd>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/torch_v2/torch_docs/index.html">PyTorch</a></dd>
        
      </dl>
    </div>
  </div>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/tutorials/quickstart_guide.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantized_modules.html">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/encoding_analyzer.html">Encoding Analyzers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.MinMaxEncodingAnalyzer.html">MinMaxEncodingAnalyzer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.SqnrEncodingAnalyzer.html">SqnrEncodingAnalyzer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.PercentileEncodingAnalyzer.html">PercentileEncodingAnalyzer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.fake_quantization_mixin.html">FakeQuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.quantization_mixin.html">nn.QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/affine/index.html">quantization.affine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.Quantize.html">Quantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.QuantizeDequantize.html">QuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.quantize_.html">quantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.quantize_dequantize.html">quantize_dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.dequantize.html">dequantize</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/float/index.html">quantization.float</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../torch_docs/index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../torch_docs/index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AIMET Model Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/model_quantization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="aimet-model-quantization">
<span id="ug-model-quantization"></span><h1>AIMET Model Quantization<a class="headerlink" href="#aimet-model-quantization" title="Permalink to this heading"></a></h1>
<p>Models are generally trained on floating-point hardware like CPUs and GPUs. However, when these trained models are run
on quantized hardware that support fixed-precision operations, model parameters are converted from floating-point
precision to fixed precision. As an example, when running on hardware that supports 8-bit integer operations, the
floating point parameters in the trained model need to be converted to 8-bit integers. It is observed that for some
models, running on an 8-bit fixed-precision runtime introduces a loss in accuracy due to noise added from the use
of fixed precision parameters and fixed precision operations.</p>
<p>AIMET provides multiple techniques and tools which help to create quantized models with a minimal loss in accuracy
relative to floating-point models.</p>
<p>This section provides information on typical use cases and AIMET’s quantization features.</p>
<div class="section" id="use-cases">
<h2>Use Cases<a class="headerlink" href="#use-cases" title="Permalink to this heading"></a></h2>
<p>1. <strong>Predict on-target accuracy</strong>: AIMET enables a user to simulate the effects of quantization to get a first order
estimate of the model’s accuracy when run on quantized targets. This is useful to get an estimate of on-target accuracy
without needing an actual target platform. Note that to create a simulation model, AIMET uses representative data
samples to compute per-layer quantization encodings.</p>
<blockquote>
<div><img alt="../_images/quant_use_case_1.PNG" src="../_images/quant_use_case_1.PNG" />
</div></blockquote>
<p>2. <strong>Post-Training Quantization (PTQ)</strong>: PTQ techniques attempt to make a model more quantization friendly without
requiring model re-training/fine-tuning. PTQ (as opposed to fine-tuning) is recommended as a first step in a
quantization workflow due to the following advantages:</p>
<ul>
<li><p>No need for the original training pipeline; an evaluation pipeline is sufficient</p></li>
<li><p>Only requires a small unlabeled dataset for calibration (can even be data-free in some scenarios)</p></li>
<li><p>Fast, simple, and easy to use</p>
<blockquote>
<div><img alt="../_images/quant_use_case_3.PNG" src="../_images/quant_use_case_3.PNG" />
</div></blockquote>
</li>
</ul>
<p>Note that with PTQ techniques, the quantized model accuracy may still have a gap relative to the floating-point model.
In such a scenario, or to even further improve the model accuracy, fine-tuning is recommended.</p>
<p>3. <strong>Quantization-Aware Training (QAT)/Fine-Tuning</strong>: QAT enables a user to fine-tune a model with quantization
operations inserted in network graph, which in effect adapts the model parameters to be robust to quantization noise.
While QAT requires access to a training pipeline and dataset, and takes longer to run due to needing a few epochs of
fine-tuning, it can provide better accuracy especially at low bitwidths. A typical QAT workflow is illustrated below.</p>
<blockquote>
<div><img alt="../_images/quant_use_case_2.PNG" src="../_images/quant_use_case_2.PNG" />
</div></blockquote>
</div>
<div class="section" id="aimet-quantization-features">
<h2>AIMET Quantization Features<a class="headerlink" href="#aimet-quantization-features" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
</div>
<ul>
<li><dl class="simple">
<dt><a class="reference internal" href="quantization_sim.html"><span class="doc">Quantization Simulation</span></a>:</dt><dd><p>QuantSim enables a user to modify a model by adding quantization simulation ops. When an evaluation is run on a
model with these quantization simulation ops, the user can observe a first-order simulation of expected accuracy on
quantized hardware.</p>
</dd>
</dl>
</li>
<li><dl>
<dt><a class="reference internal" href="quantization_aware_training.html#ug-quantization-aware-training"><span class="std std-ref">Quantization-Aware Training (QAT)</span></a>:</dt><dd><p>QAT allows users to take a QuantSim model and further fine-tune the model parameters by taking quantization into
account.</p>
<p>Two modes of QAT are supported:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Regular QAT:</dt><dd><p>Fine-tuning of model parameters. Trainable parameters such as module weights, biases, etc. can be
updated. The scale and offset quantization parameters for activation quantizers remain constant. Scale and
offset parameters for weight quantizers will update to reflect new weight values after each training step.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>QAT with Range Learning:</dt><dd><p>In addition to trainable module weights and scale/offset parameters for weight quantizers, scale/offset
parameters for activation quantizers are also updated during each training step.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="section" id="post-training-quantization">
<h3><span class="hideitem">Post-Training Quantization</span><a class="headerlink" href="#post-training-quantization" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Post-Training Quantization (PTQ) Techniques:</p>
<blockquote>
<div><p>Post-training quantization techniques help a model improve quantized accuracy without needing to re-train.</p>
<div class="toctree-wrapper compound">
</div>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference internal" href="auto_quant.html#ug-auto-quant"><span class="std std-ref">AutoQuant</span></a>:</dt><dd><p>AIMET provides an API that integrates the post-training quantization techniques described below. AutoQuant is
recommended for PTQ. If desired, individual techniques can be invoked using standalone feature specific APIs.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="adaround.html#ug-adaround"><span class="std std-ref">Adaptive Rounding (AdaRound)</span></a>:</dt><dd><p>Determines optimal rounding for weight tensors to improve quantized performance.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="post_training_quant_techniques.html#ug-post-training-quantization"><span class="std std-ref">Cross-Layer Equalization</span></a>:</dt><dd><p>Equalizes weight ranges in consecutive layers.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="bn_reestimation.html#ug-bn-reestimation"><span class="std std-ref">BN Re-estimation</span></a>:</dt><dd><p>Re-estimates Batch Norm layer statistics before folding the Batch Norm layers.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="post_training_quant_techniques.html#ug-post-training-quantization"><span class="std std-ref">Bias Correction</span></a> [Deprecated]:</dt><dd><p>Bias Correction is considered deprecated. It is advised to use AdaRound instead.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="debugging-analysis-tools">
<h3><span class="hideitem">Debugging/Analysis Tools</span><a class="headerlink" href="#debugging-analysis-tools" title="Permalink to this heading"></a></h3>
<div class="toctree-wrapper compound">
</div>
<ul class="simple">
<li><dl class="simple">
<dt>Debugging/Analysis Tools</dt><dd><ul>
<li><dl class="simple">
<dt><a class="reference internal" href="quant_analyzer.html#ug-quant-analyzer"><span class="std std-ref">QuantAnalyzer</span></a>:</dt><dd><p>Automated debugging of the model to understand sensitivity to weight and/or activation quantization, individual
layer sensitivity, etc.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="visualization_quant.html#ug-quantization-visualization"><span class="std std-ref">Visualizations</span></a>:</dt><dd><p>Visualizations and histograms of weight and activation ranges.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="aimet-quantization-workflow">
<h2>AIMET Quantization Workflow<a class="headerlink" href="#aimet-quantization-workflow" title="Permalink to this heading"></a></h2>
<p>This section describes the recommended workflow for quantizing a neural network.</p>
<blockquote>
<div><img alt="../_images/quantization_workflow.PNG" src="../_images/quantization_workflow.PNG" />
</div></blockquote>
<p><strong>1. Model prep and validation</strong></p>
<p>Before attempting quantization, ensure that models have been defined in accordance to model guidelines. These guidelines
depend on the ML framework the model is written in.</p>
<div class="section" id="pytorch">
<h3><span class="hideitem">PyTorch</span><a class="headerlink" href="#pytorch" title="Permalink to this heading"></a></h3>
<div class="toctree-wrapper compound">
</div>
<ul>
<li><p>Pytorch:</p>
<blockquote>
<div><p><span class="xref std std-doc">PyTorch Model Guidelines</span></p>
<blockquote>
<div><p>In the case of PyTorch, there exists the Model Validator utility, to automate the checking of certain PyTorch model
requirements, as well as the Model Preparer utility, to automate the updating of the model definition to align with
certain requirements.</p>
<p>In this model prep and validation phase, we advise the following flow:</p>
<img alt="../_images/pytorch_model_prep_and_validate.PNG" src="../_images/pytorch_model_prep_and_validate.PNG" />
<p>Users can use the model validator utility first to check if the model can be run with AIMET. If validator checks
fail, users can first try using model preparer in their pipeline, an automated feature for updating models, and
retry the model validator to see if checks now pass. If the validator continues to print warnings, users will need
to update the model definition by hand prior to using AIMET features.</p>
<p>For more information on model validator and preparer, refer to the corresponding sections in
<span class="xref std std-doc">AIMET PyTorch Quantization APIs</span>.</p>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="tensorflow">
<h3><span class="hideitem">Tensorflow</span><a class="headerlink" href="#tensorflow" title="Permalink to this heading"></a></h3>
<div class="toctree-wrapper compound">
</div>
<ul class="simple">
<li><dl class="simple">
<dt>Tensorflow:</dt><dd><p><span class="xref std std-doc">TensorFlow Model Guidelines</span></p>
</dd>
</dl>
</li>
</ul>
<p><strong>2. PTQ/AutoQuant</strong></p>
<p>The user can apply various PTQ techniques to the model to adjust model parameters and make the model more robust to
quantization. We recommend trying AutoQuant first, a PTQ feature which internally tries various other PTQ methods and
finds the best combination of methods to apply. Refer to the
AIMET Quantization Features section for more details on PTQ/AutoQuant.</p>
<p><strong>3. QAT</strong></p>
<p>If model accuracy is still not satisfactory after PTQ/AutoQuant, the user can use QAT to fine-tune the model. Refer to
the AIMET Quantization Features section for more details on QAT.</p>
<p><strong>4. Exporting models</strong></p>
<p>In order to bring the model onto the target, users will need two things:</p>
<ul class="simple">
<li><p>a model with updated weights</p></li>
<li><p>an encodings file containing quantization parameters associated with each quantization op</p></li>
</ul>
<p>AIMET QuantSim provides export functionality to generate both items. The exported model type will differ based on the ML
framework used:</p>
<ul class="simple">
<li><p>.onnx for PyTorch</p></li>
<li><p>meta/checkpoint for TensorFlow</p></li>
<li><p>.h5 and .pb for Keras</p></li>
</ul>
<p>Depending on which AIMET Quantization features were used, the user may need to take different steps to export the model
and encodings file. For example, calling AutoQuant will automatically export the model and encodings file as part of its
processing. If QAT is used, users will need to call .export() on the QuantSim object. If lower level PTQ techniques like
CLE are used, users will need to first create a QuantSim object from the modified model, and then call .export() on the
QuantSim object.</p>
</div>
</div>
<div class="section" id="debugging-guidelines">
<h2>Debugging Guidelines<a class="headerlink" href="#debugging-guidelines" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
</div>
<p>Applying AIMET Quantization features may involve some trial and error in order to find the best optimizations to apply
on a particular model. We have included some debugging steps in the <a class="reference internal" href="quantization_feature_guidebook.html#ug-quant-guidebook"><span class="std std-ref">Quantization Guidebook</span></a>
that can be tried when quantization accuracy does not seem to improve right off the bat.</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>