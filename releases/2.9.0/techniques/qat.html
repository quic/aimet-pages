<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Per-block quantization" href="blockwise.html" /><link rel="prev" title="Calibration" href="ptq.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Quantization-aware training - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/aimet-furo.css?v=22b0637d" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.9.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../overview/index.html">Overview</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Overview</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../overview/install/quick-start.html">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/install/index.html">Install</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials/index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Tutorials</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/quantization_workflow.html">Quantization Workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tutorials/quantsim.html">Quantization Simulation</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Quantization Simulation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tutorials/notebooks.html">Example Notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Example Notebooks</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/on_target_inference.html">Running Quantized Models on-device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/debugging_guidelines.html">Debugging Guide</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Techniques</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Techniques</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ptq.html">Post Training Quantization</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Quantization Aware Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="blockwise.html">Blockwise Quantization</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="mixed%20precision/index.html">Mixed precision</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Mixed precision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="mixed%20precision/mmp.html">Manual mixed precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed%20precision/amp.html">Automatic mixed precision</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="analysis%20tools/index.html">Analysis tools</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of Analysis tools</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="analysis%20tools/interactive_visualization.html">Interactive visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="analysis%20tools/quant_analyzer.html">Quantization analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="analysis%20tools/layer_output_generation.html">Layer output generation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="compression/index.html">Compression</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Compression</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="compression/feature_guidebook.html">Compression guidebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="compression/greedy_compression_ratio_selection.html">Greedy compression ratio selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="compression/visualization_compression.html">Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="compression/weight_svd.html">Weight SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="compression/spatial_svd.html">Spatial SVD</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="compression/channel_pruning.html">Channel pruning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of Channel pruning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="compression/winnowing.html">Winnowing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ptq_techniques/index.html">PTQ Techniques</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of PTQ Techniques</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/adascale.html">AdaScale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ptq_techniques/quantized%20LoRa/index.html">Quantized LoRa</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of Quantized LoRa</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../ptq_techniques/quantized%20LoRa/qw_lora.html">QW-LoRa</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ptq_techniques/quantized%20LoRa/qwa_lora.html">QWA-LoRa</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/omniquant.html">OmniQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/autoquant.html">Automatic quantization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../apiref/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../external/index.html">External Resources</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of External Resources</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="http://www.qualcomm.com/developer/artificial-intelligence#overview">Qualcomm AI Stack</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-models/">Qualcomm Hub Models</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-apps/">Qualcomm Hub Apps</a></li>
<li class="toctree-l2"><a class="reference external" href="https://aihub.qualcomm.com/">Qualcomm AI Hub</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="quantization-aware-training">
<span id="techniques-qat"></span><h1>Quantization-aware training<a class="headerlink" href="#quantization-aware-training" title="Link to this heading">¶</a></h1>
<p>Quantization-aware training (QAT) finds better optimized solutions than post-training quantization (PTQ)
by fine-tuning the model parameters in the presence of quantization noise. This higher accuracy comes with
the usual costs of neural network training, including longer training times and the need for labeled data
and hyperparameter search.</p>
<section id="qat-modes">
<h2>QAT modes<a class="headerlink" href="#qat-modes" title="Link to this heading">¶</a></h2>
<p>There are two versions of QAT: without range learning and with range learning.</p>
<dl class="simple">
<dt>Without range learning</dt><dd><p>In QAT without range Learning, encoding values for activation quantizers are found once during calibration and are not updated again.</p>
</dd>
<dt>With range learning</dt><dd><p>In QAT with range Learning, encoding values for activation quantizers are set during calibration and are updated during training, yielding better scale and offset quantization parameters.</p>
</dd>
</dl>
<p>In both versions, parameter quantizer encoding values are updated in sync with the parameters during training.</p>
</section>
<section id="qat-recommendations">
<h2>QAT recommendations<a class="headerlink" href="#qat-recommendations" title="Link to this heading">¶</a></h2>
<p>These guidelines can improve performance and speed convergence with QAT.</p>
<dl class="simple">
<dt>Initialization</dt><dd><ul class="simple">
<li><p>Apply PTQ techniques before applying QAT, especially if there is large drop in INT8 performance from the FP32 baseline.</p></li>
</ul>
</dd>
<dt>Hyper-parameters</dt><dd><ul class="simple">
<li><p>Number of epochs: 15-20 epochs are usually sufficient for convergence.</p></li>
<li><p>Learning rate: Comparable (or one order higher) to FP32 model’s final learning rate at convergence.
Results in AIMET are with learning of the order 1e-6.</p></li>
<li><p>Learning rate schedule: Divide learning rate by 10 every 5-10 epochs.</p></li>
</ul>
</dd>
</dl>
</section>
<section id="workflow">
<h2>Workflow<a class="headerlink" href="#workflow" title="Link to this heading">¶</a></h2>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h3>
<p>You need a PyTorch or TensorFlow model. ONNX does not support QAT.</p>
</section>
<section id="step-1-setup">
<span id="techniques-qat-setup"></span><h3>Step 1: Setup<a class="headerlink" href="#step-1-setup" title="Link to this heading">¶</a></h3>
<p>Set up the model, data loader, and training callback.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-0">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.batch_norm_fold</span><span class="w"> </span><span class="kn">import</span> <span class="n">fold_all_batch_norms</span>

<span class="c1"># General setup that can be changed as needed</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">PATH_TO_IMAGENET</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">ImageNet</span><span class="p">(</span><span class="n">PATH_TO_IMAGENET</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">fold_all_batch_norms</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Callback function to pass calibration data through the model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pass_calibration_data</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">batches</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The User of the QuantizationSimModel API is expected to write this callback based on their dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
            <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">batch</span> <span class="o">&gt;=</span> <span class="n">batches</span><span class="p">:</span>
                <span class="k">break</span>

<span class="c1"># Basic ImageNet evaluation function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>

</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-1">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_common.defs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantScheme</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_tensorflow.keras.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">applications</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">preprocessing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.applications</span><span class="w"> </span><span class="kn">import</span> <span class="n">mobilenet_v2</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">applications</span><span class="o">.</span><span class="n">MobileNetV2</span><span class="p">()</span>

<span class="c1"># Set up dataset</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">imagenet_dataset</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">image_dataset_from_directory</span><span class="p">(</span>
    <span class="n">directory</span><span class="o">=</span><span class="s1">&#39;&lt;your_imagenet_validation_data_path&gt;&#39;</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="s1">&#39;inferred&#39;</span><span class="p">,</span>
    <span class="n">label_mode</span><span class="o">=</span><span class="s1">&#39;categorical&#39;</span><span class="p">,</span>
    <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">imagenet_dataset</span> <span class="o">=</span> <span class="n">imagenet_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">mobilenet_v2</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">NUM_CALIBRATION_SAMPLES</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">calibration_dataset</span> <span class="o">=</span> <span class="n">imagenet_dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">NUM_CALIBRATION_SAMPLES</span> <span class="o">//</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">imagenet_dataset</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">NUM_CALIBRATION_SAMPLES</span> <span class="o">//</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pass_calibration_data</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The User of the QuantizationSimModel API is expected to write this callback based on their dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">calibration_dataset</span><span class="p">:</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-2">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
<section id="step-2-computing-the-initial-quantization-parameters">
<span id="techniques-qat-encodings"></span><h3>Step 2: Computing the initial quantization parameters<a class="headerlink" href="#step-2-computing-the-initial-quantization-parameters" title="Link to this heading">¶</a></h3>
<p>Compute the quantization parameters and calculate quantized accuracy.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-3">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_common.defs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantScheme</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="n">quant_scheme</span><span class="o">=</span><span class="n">QuantScheme</span><span class="o">.</span><span class="n">training_range_learning_with_tf_init</span><span class="p">)</span>

<span class="n">calibration_batches</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">pass_calibration_data</span><span class="p">,</span> <span class="n">calibration_batches</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantized accuracy (W8A8): </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Quantized accuracy (W8A8): 0.68016
</pre></div>
</div>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-4">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PARAM_BITWIDTH</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">ACTIVATION_BITWIDTH</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">QUANT_SCHEME</span> <span class="o">=</span> <span class="n">QuantScheme</span><span class="o">.</span><span class="n">training_range_learning_with_tf_init</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">quant_scheme</span><span class="o">=</span><span class="n">QUANT_SCHEME</span><span class="p">,</span>
    <span class="n">default_param_bw</span><span class="o">=</span><span class="n">PARAM_BITWIDTH</span><span class="p">,</span>
    <span class="n">default_output_bw</span><span class="o">=</span><span class="n">ACTIVATION_BITWIDTH</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">pass_calibration_data</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">()],</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">CategoricalAccuracy</span><span class="p">()],</span>
<span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Quantized accuracy (W8A8): </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Quantized accuracy (W8A8): 0.6583
</pre></div>
</div>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-5">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
<section id="step-3-calibrate-the-quantized-model">
<span id="techniques-qat-calibrate"></span><h3>Step 3: Calibrate the quantized model<a class="headerlink" href="#step-3-calibrate-the-quantized-model" title="Link to this heading">¶</a></h3>
<p>Train the model to fine-tune the parameters.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-6">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training loop can be replaced with any custom training loop</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-7">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">calibration_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-8" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-8">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
<section id="step-4-evaluating-the-model">
<h3>Step 4: Evaluating the model<a class="headerlink" href="#step-4-evaluating-the-model" title="Link to this heading">¶</a></h3>
<p>Evaluate the <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> to determine the improvement in accuracy.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-9" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-9">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model accuracy after QAT: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Model accuracy after QAT: 0.70838
</pre></div>
</div>
</div>
<input id="sd-tab-item-10" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-10">
TensorFlow</label><div class="sd-tab-content docutils">
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model accuracy after QAT: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Model accuracy after QAT: 0.6910
</pre></div>
</div>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-11">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
<section id="step-5-exporting-the-model">
<h3>Step 5: Exporting the model<a class="headerlink" href="#step-5-exporting-the-model" title="Link to this heading">¶</a></h3>
<p>Export the calibrated model to remove quantization operations and create the JSON encodings file containing quantization scale and offset parameters for the model’s activation and weight tensors.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-12">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="s2">&quot;quantized_mobilenetv2&quot;</span><span class="p">,</span> <span class="n">dummy_input</span><span class="o">=</span><span class="n">dummy_input</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-13" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-13">
TensorFlow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/tmp&#39;</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="s1">&#39;quantized_mobilenet_v2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-14" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-14">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
</section>
<section id="multi-gpu-support">
<h2>Multi-GPU support<a class="headerlink" href="#multi-gpu-support" title="Link to this heading">¶</a></h2>
<p>To use QAT with multi-GPU support, do the following. The instructions are the same as above except:</p>
<ul class="simple">
<li><p>Multi-GPU is supported only in PyTorch.</p></li>
<li><p>There is an additional step to parallelize the model.</p></li>
<li><p>It is important not to parallelize the model until after computing encodings.</p></li>
</ul>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-15" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-15">
PyTorch</label><div class="sd-tab-content docutils">
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Do not invoke DataParallel or multi-GPU mode until after you compute the encodings (quantization parameters).</p>
</div>
<p><strong>Step 1: Setup</strong></p>
<p>Create a <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> for your pre-trained PyTorch model per <a class="reference internal" href="#techniques-qat-setup"><span class="std std-ref">Step 1</span></a>. Do not use DataParallel mode.</p>
<p><strong>Step 2: Compute encodings</strong></p>
<p>Compute quantization encodings for the model per <a class="reference internal" href="#techniques-qat-encodings"><span class="std std-ref">Step 2</span></a>. Do not use a forward function that moves the model to multi-gpu and back.</p>
<p><strong>Step 2.5 (additional step)</strong></p>
<p>Move <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> to DataParallel as follows.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># &quot;sim&quot; here refers to the QuantizationSimModel object.</span>
<span class="n">sim</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p><strong>Steps 3 - 5</strong></p>
<p>Evaluate, train, and export the model per <a class="reference internal" href="#techniques-qat-calibrate"><span class="std std-ref">steps 3 - 5</span></a>.</p>
</div>
<input id="sd-tab-item-16" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-16">
TensorFlow</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
<input id="sd-tab-item-17" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-17">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="Link to this heading">¶</a></h2>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-18" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-18">
PyTorch</label><div class="sd-tab-content docutils">
<p><strong>Top level APIs</strong></p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_torch.quantsim.</span></span><span class="sig-name descname"><span class="pre">QuantizationSimModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dummy_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_scheme</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_output_bw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_param_bw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_place</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_data_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QuantizationDataType.int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/quantsim.html#QuantizationSimModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Class that simulates the quantized model execution on a target hardware backend.</p>
<p>QuantizationSimModel simulates quantization of a given model by converting
all PyTorch modules into <span class="xref std std-ref">quantized modules</span>
with input/output/parameter <span class="xref std std-ref">quantizers</span> as necessary.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="go">ResNet(</span>
<span class="go">  (conv1): Conv2d(</span>
<span class="go">    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False</span>
<span class="go">  )</span>
<span class="go">  ...</span>
<span class="go">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="go">ResNet(</span>
<span class="go">  (conv1): QuantizedConv2d(</span>
<span class="go">    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False</span>
<span class="go">    (param_quantizers): ModuleDict(</span>
<span class="go">      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)</span>
<span class="go">    )</span>
<span class="go">    (input_quantizers): ModuleList(</span>
<span class="go">      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)</span>
<span class="go">    )</span>
<span class="go">    (output_quantizers): ModuleList(</span>
<span class="go">      (0): None</span>
<span class="go">    )</span>
<span class="go">  )</span>
<span class="go">  ...</span>
<span class="go">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><cite>rounding_mode</cite> parameter is deprecated.
Passing <cite>rounding_mode</cite> will throw runtime error in &gt;=1.35.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The default value of <cite>quant_scheme</cite> has changed
from <cite>QuantScheme.post_training_tf_enhanced</cite> to <cite>QuantScheme.training_range_learning_with_tf_init</cite>
since 2.0.0, and will be deprecated in the longer term.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – Model to simulate the quantized execution of</p></li>
<li><p><strong>dummy_input</strong> (<em>Tensor</em><em> | </em><em>Sequence</em><em>[</em><em>Tensor</em><em>]</em>) – Dummy input to be used to capture
the computational graph of the model. All input tensors are expected to be
already placed on the appropriate devices to run forward pass of the model.</p></li>
<li><p><strong>quant_scheme</strong> (<a class="reference internal" href="../apiref/torch/v1/quantsim.html#aimet_common.defs.QuantScheme" title="aimet_common.defs.QuantScheme"><em>QuantScheme</em></a><em>, </em><em>optional</em>) – Quantization scheme that indicates
how to observe and calibrate the quantization encodings (Default: <cite>QuantScheme.post_training_tf_enhanced</cite>)</p></li>
<li><p><strong>rounding_mode</strong> – Deprecated</p></li>
<li><p><strong>default_output_bw</strong> (<em>int</em><em>, </em><em>optional</em>) – Default bitwidth (4-31) to use for quantizing all layer inputs and outputs
unless otherwise specified in the config file. (Default: 8)</p></li>
<li><p><strong>default_param_bw</strong> (<em>int</em><em>, </em><em>optional</em>) – Default bitwidth (4-31) to use for quantizing all layer parameters
unless otherwise specified in the config file. (Default: 8)</p></li>
<li><p><strong>in_place</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, then the given model is modified in-place into a quantized model. (Default: <cite>False</cite>)</p></li>
<li><p><strong>config_file</strong> (<em>str</em><em>, </em><em>optional</em>) – File path or alias of the configuration file.
Alias can be one of { default, htp_v66, htp_v68, htp_v69, htp_v73, htp_v75, htp_v79, htp_v81 } (Default: <cite>“default”</cite>)</p></li>
<li><p><strong>default_data_type</strong> (<em>QuantizationDataType</em><em>, </em><em>optional</em>) – Default data type to use for quantizing all
inputs, outputs and parameters unless otherwise specified in the config file.
Possible options are QuantizationDataType.int and QuantizationDataType.float.
Note that the mode default_data_type=QuantizationDataType.float is only supported with
default_output_bw=16 or 32 and default_param_bw=16 or 32. (Default: <cite>QuantizationDataType.int</cite>)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_pass_callback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_pass_callback_args=&lt;class</span> <span class="pre">'aimet_torch.v2.quantsim.quantsim._NOT_SPECIFIED'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/quantsim.html#QuantizationSimModel.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes encodings for all quantizers in the model.</p>
<p>This API will invoke <cite>forward_pass_callback</cite>, a function written by the user that runs
forward pass(es) of the quantized model with a small, representative subset of the training dataset.
By doing so, the quantizers in the quantized model will observe the inputs and initialize
their quantization encodings according to the observed input statistics.</p>
<p>This function is overloaded with the following signatures:</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_pass_callback</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/quantsim.html#QuantizationSimModel.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>forward_pass_callback</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.nn.Module</em><em>]</em><em>, </em><em>Any</em><em>]</em>) – A function that takes a quantized model and runs forward passes
with a small, representative subset of training dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_pass_callback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_pass_callback_args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/quantsim.html#QuantizationSimModel.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_pass_callback</strong> (<em>Callable</em><em>[</em><em>[</em><em>torch.nn.Module</em><em>, </em><em>T</em><em>]</em><em>, </em><em>Any</em><em>]</em>) – A function that takes a quantized model and runs forward passes
with a small, representative subset of training dataset</p></li>
<li><p><strong>forward_pass_callback_args</strong> (<em>T</em>) – The second argument to <cite>forward_pass_callback</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># Can&#39;t run forward until quantizer encodings are initialized</span>
<span class="go">RuntimeError: Failed to run QuantizeDequantize since quantization parameters are not initialized.</span>
<span class="go">Please initialize the quantization parameters using `compute_encodings()`.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">run_forward_pass</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>            <span class="n">_</span> <span class="o">=</span> <span class="n">quantized_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">run_forward_pass</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># Now runs successfully!</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dummy_input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/quantsim.html#QuantizationSimModel.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>This method exports out the quant-sim model so it is ready to be run on-target.</p>
<p>Specifically, the following are saved:</p>
<ol class="arabic simple">
<li><p>The sim-model is exported to a regular PyTorch model without any simulation ops</p></li>
<li><p>The quantization encodings are exported to a separate JSON-formatted file that can
then be imported by the on-target runtime (if desired)</p></li>
<li><p>Optionally, An equivalent model in ONNX format is exported. In addition, nodes in the ONNX model are named
the same as the corresponding PyTorch module names. This helps with matching ONNX node to their quant
encoding from #2.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – path where to store model pth and encodings</p></li>
<li><p><strong>filename_prefix</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – Prefix to use for filenames of the model pth and encodings files</p></li>
<li><p><strong>dummy_input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]</span>) – Dummy input to the model. Used to parse model graph. It is required for the dummy_input to
be placed on CPU.</p></li>
<li><p><strong>onnx_export_args</strong> – Optional export argument with onnx specific overrides provided as a dictionary or
OnnxExportApiArgs object. If not provided, defaults to “opset_version” = None, “input_names” = None,
“output_names” = None, and for torch version &lt; 1.10.0, “enable_onnx_checker” = False.</p></li>
<li><p><strong>propagate_encodings</strong> – If True, encoding entries for intermediate ops (when one PyTorch ops results in
multiple ONNX nodes) are filled with the same BW and data_type as the output tensor for that series of
ops. Defaults to False.</p></li>
<li><p><strong>export_to_torchscript</strong> – If True, export to torchscript. Export to onnx otherwise. Defaults to False.</p></li>
<li><p><strong>use_embedded_encodings</strong> – If True, another onnx model embedded with fakequant nodes will be exported</p></li>
<li><p><strong>export_model</strong> – If True, then ONNX model is exported. When False, only encodings are exported. User should
disable (False) this flag only if the corresponding ONNX model already exists in the path
specified</p></li>
<li><p><strong>filename_prefix_encodings</strong> – File name prefix to be used when saving encodings.
If None, then user defaults to filename_prefix value</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">load_encodings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encodings</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encodings</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">PathLike</span></code>]</span>) – Encoding dictionary or path to the encoding dictionary json file.</p></li>
<li><p><strong>strict</strong> (<em>bool</em>) – If True, an error will be thrown if the model doesn’t
have a quantizer corresponding to the specified encodings.</p></li>
<li><p><strong>partial</strong> (<em>bool</em>) – If True, the encoding will be interpreted as a partial encoding,
and the dangling quantizers with no corresponding encoding will be kept untouched.
Otherwise, the dangling quantizers will be removed from the model.</p></li>
<li><p><strong>requires_grad</strong> (<em>bool</em>) – Whether or not the quantization parameters loaded from the
encodings require gradient computation during training.
If None, <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> flag of the quantization parameters will be kept unchanged.</p></li>
<li><p><strong>allow_overwrite</strong> (<em>bool</em>) – Whether or not the quantization parameters loaded from the
encodings can be overwriiten by <span class="xref std std-ref">compute_encodings</span> or another <span class="xref std std-ref">load_encodings</span>.
If None, whether the quantizer is overwrieable will be kept unchanged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p><strong>Quant Scheme Enum</strong></p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_common.defs.</span></span><span class="sig-name descname"><span class="pre">QuantScheme</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_common/defs.html#QuantScheme"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Quantization schemes</p>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alias</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_common/defs.html#QuantScheme.from_str"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns QuantScheme object from string alias</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="../apiref/torch/v1/quantsim.html#aimet_common.defs.QuantScheme" title="aimet_common.defs.QuantScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantScheme</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<input id="sd-tab-item-19" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="tf" for="sd-tab-item-19">
TensorFlow</label><div class="sd-tab-content docutils">
<p><strong>Top level APIs</strong></p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_tensorflow.keras.quantsim.</span></span><span class="sig-name descname"><span class="pre">QuantizationSimModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_scheme</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tf_enhanced'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_output_bw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_param_bw</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_place</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_data_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QuantizationDataType.int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_tensorflow/keras/quantsim.html#QuantizationSimModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Implements mechanism to add quantization simulations ops to a model. This allows for off-target simulation of
inference accuracy. Also allows the model to be fine-tuned to counter the effects of quantization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – Model to quantize</p></li>
<li><p><strong>quant_scheme</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<a class="reference internal" href="../apiref/torch/v1/quantsim.html#aimet_common.defs.QuantScheme" title="aimet_common.defs.QuantScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantScheme</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]</span>) – Quantization Scheme, currently supported schemes are post_training_tf and
post_training_tf_enhanced, defaults to post_training_tf_enhanced</p></li>
<li><p><strong>rounding_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – The round scheme to used. One of: ‘nearest’ or ‘stochastic’, defaults to ‘nearest’.</p></li>
<li><p><strong>default_output_bw</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – bitwidth to use for activation tensors, defaults to 8</p></li>
<li><p><strong>default_param_bw</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – bitwidth to use for parameter tensors, defaults to 8</p></li>
<li><p><strong>in_place</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If True, then the given ‘model’ is modified in-place to add quant-sim nodes.
Only suggested use of this option is when the user wants to avoid creating a copy of the model</p></li>
<li><p><strong>config_file</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]</span>) – Path to a config file to use to specify rules for placing quant ops in the model</p></li>
<li><p><strong>default_data_type</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationDataType</span></code></span>) – Default data type to use for quantizing all layer parameters.
Possible options are QuantizationDataType.int and QuantizationDataType.float.
Note that the mode default_data_type=QuantizationDataType.float is only supported with
default_output_bw=16 and default_param_bw=16</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_pass_callback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_pass_callback_args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_tensorflow/keras/quantsim.html#QuantizationSimModel.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes encodings for all quantization sim nodes in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_pass_callback</strong> – A callback function that is expected to run forward passes on a model.
This callback function should use representative data for the forward pass, so the calculated encodings work for all data samples.</p></li>
<li><p><strong>forward_pass_callback_args</strong> – These argument(s) are passed to the forward_pass_callback as-is.
Up to the user to determine the type of this parameter. E.g. could be simply an integer representing the number of data samples to use.
Or could be a tuple of parameters or an object representing something more complex.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">export</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_prefix</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_objects</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_to_pb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_tensorflow/keras/quantsim.html#QuantizationSimModel.export"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>This method exports out the quant-sim model so it is ready to be run on-target. Specifically, the following are saved</p>
<ol class="arabic simple">
<li><p>The sim-model is exported to a regular Keras model without any simulation ops</p></li>
<li><p>The quantization encodings are exported to a separate JSON-formatted file that can then be imported by the on-target runtime (if desired)</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – path where to store model pth and encodings</p></li>
<li><p><strong>filename_prefix</strong> – Prefix to use for filenames of the model pth and encodings files</p></li>
<li><p><strong>custom_objects</strong> – If there are custom objects to load, Keras needs a dict of them to map them</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">load_encodings_to_sim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoding_file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_tensorflow/keras/quantsim.html#QuantizationSimModel.load_encodings_to_sim"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Loads the saved encodings to quant sim model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>encoding_file_path</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – path from where to load encodings file</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p><strong>Quant Scheme Enum</strong></p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_common.defs.</span></span><span class="sig-name descname"><span class="pre">QuantScheme</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_common/defs.html#QuantScheme"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Quantization schemes</p>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alias</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_common/defs.html#QuantScheme.from_str"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Returns QuantScheme object from string alias</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="../apiref/torch/v1/quantsim.html#aimet_common.defs.QuantScheme" title="aimet_common.defs.QuantScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantScheme</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<input id="sd-tab-item-20" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="ONNX" for="sd-tab-item-20">
ONNX</label><div class="sd-tab-content docutils">
<p>Not supported.</p>
</div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="blockwise.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Per-block quantization</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="ptq.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Calibration</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Quantization-aware training</a><ul>
<li><a class="reference internal" href="#qat-modes">QAT modes</a></li>
<li><a class="reference internal" href="#qat-recommendations">QAT recommendations</a></li>
<li><a class="reference internal" href="#workflow">Workflow</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#step-1-setup">Step 1: Setup</a></li>
<li><a class="reference internal" href="#step-2-computing-the-initial-quantization-parameters">Step 2: Computing the initial quantization parameters</a></li>
<li><a class="reference internal" href="#step-3-calibrate-the-quantized-model">Step 3: Calibrate the quantized model</a></li>
<li><a class="reference internal" href="#step-4-evaluating-the-model">Step 4: Evaluating the model</a></li>
<li><a class="reference internal" href="#step-5-exporting-the-model">Step 5: Exporting the model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multi-gpu-support">Multi-GPU support</a></li>
<li><a class="reference internal" href="#api">API</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>