Search.setIndex({"alltitles": {"1. Creating a Keras model with subclass layers": [[246, "1.-Creating-a-Keras-model-with-subclass-layers"]], "1. Define Constants and Helper functions": [[257, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[249, "1.-Example-evaluation-and-training-pipeline"], [250, "1.-Example-evaluation-and-training-pipeline"], [258, "1.-Example-evaluation-and-training-pipeline"], [262, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[237, "1.-Example-evaluation-pipeline"], [255, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence checks": [[234, "fp32-confidence-checks"]], "1. Instantiate the example evaluation and training datasets": [[247, "1.-Instantiate-the-example-evaluation-and-training-datasets"], [248, "1.-Instantiate-the-example-evaluation-and-training-datasets"]], "1. Instantiate the example evaluation and training pipeline": [[244, "1.-Instantiate-the-example-evaluation-and-training-pipeline"]], "1. Instantiate the example evaluation method": [[241, "1.-Instantiate-the-example-evaluation-method"]], "1. Instantiate the example training and validation pipeline": [[238, "1.-Instantiate-the-example-training-and-validation-pipeline"], [239, "1.-Instantiate-the-example-training-and-validation-pipeline"], [240, "1.-Instantiate-the-example-training-and-validation-pipeline"], [242, "1.-Instantiate-the-example-training-and-validation-pipeline"], [243, "1.-Instantiate-the-example-training-and-validation-pipeline"], [251, "1.-Instantiate-the-example-training-and-validation-pipeline"], [252, "1.-Instantiate-the-example-training-and-validation-pipeline"], [253, "1.-Instantiate-the-example-training-and-validation-pipeline"], [254, "1.-Instantiate-the-example-training-and-validation-pipeline"], [256, "1.-Instantiate-the-example-training-and-validation-pipeline"], [259, "1.-Instantiate-the-example-training-and-validation-pipeline"], [260, "1.-Instantiate-the-example-training-and-validation-pipeline"], [261, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Run the notebook server": [[236, "run-the-notebook-server"]], "1. Sensitivity to weight and activation quantization": [[214, "sensitivity-to-weight-and-activation-quantization"]], "1. Versioning": [[224, "versioning"]], "1. defaults": [[231, "defaults"]], "1.13.0": [[210, "id44"]], "1.16.0": [[210, "id43"]], "1.16.1": [[210, "id42"]], "1.16.2": [[210, "id41"]], "1.17.0": [[210, "id40"]], "1.18.0": [[210, "id39"]], "1.19.1": [[210, "id38"]], "1.20.0": [[210, "id37"]], "1.21.0": [[210, "id36"]], "1.22.0": [[210, "id35"]], "1.22.1": [[210, "id34"]], "1.22.2": [[210, "id33"]], "1.23.0": [[210, "id32"]], "1.24.0": [[210, "id31"]], "1.25.0": [[210, "id30"]], "1.26.0": [[210, "id29"]], "1.27.0": [[210, "id28"]], "1.28.0": [[210, "id27"]], "1.29.0": [[210, "id26"]], "1.30.0": [[210, "id25"]], "1.31.0": [[210, "id24"]], "1.32.0": [[210, "id23"]], "1.33.0": [[210, "id22"]], "1.33.5": [[210, "id21"]], "1.34.0": [[210, "id20"]], "1.35.0": [[210, "id19"]], "1.35.1": [[210, "id18"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[237, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [238, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [239, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [240, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Converting the Keras model with subclass layers to a Keras model with functional layers": [[246, "2.-Converting-the-Keras-model-with-subclass-layers-to-a-Keras-model-with-functional-layers"]], "2. Create the model in Keras": [[244, "2.-Create-the-model-in-Keras"]], "2. Define Constants and Datasets Prepare": [[244, "2.-Define-Constants-and-Datasets-Prepare"]], "2. Download the example notebooks and related code": [[236, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[258, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[243, "2.-Load-a-pretrained-FP32-model"], [249, "2.-Load-a-pretrained-FP32-model"], [257, "2.-Load-a-pretrained-FP32-model"]], "2. Load the FP32 model and evaluate the model to find the baseline FP32 accuracy": [[241, "2.-Load-the-FP32-model-and-evaluate-the-model-to-find-the-baseline-FP32-accuracy"]], "2. Load the model": [[262, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[242, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [247, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [248, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [250, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [251, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [252, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [253, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [254, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [255, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [256, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [259, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [260, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [261, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement": [[214, "per-layer-quantizer-enablement"]], "2. Version 0.6.1": [[224, "version-0-6-1"]], "2. Weights or activations quantization": [[234, "weights-or-activations-quantization"]], "2. params": [[231, "params"]], "2.0.0": [[210, "id17"]], "2.1. Encoding specification": [[224, "encoding-specification"]], "2.1.0": [[210, "id16"]], "2.10.0": [[210, "id3"]], "2.11.0": [[210, "id1"]], "2.2.0": [[210, "id15"]], "2.3.0": [[210, "id14"]], "2.4.0": [[210, "id13"]], "2.5.0": [[210, "id12"]], "2.6.0": [[210, "id11"]], "2.7.0": [[210, "id10"]], "2.8.0": [[210, "id6"]], "2.9.0": [[210, "id4"]], "3. Apply QuantAnalyzer to the model": [[249, "3.-Apply-QuantAnalyzer-to-the-model"], [262, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[252, "3.-Compress-the-model-and-fine-tune"], [253, "3.-Compress-the-model-and-fine-tune"], [254, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[237, "3.-Create-a-quantization-simulation-model"], [255, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[258, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[238, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [239, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [240, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [242, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [247, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [248, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [250, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [251, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [256, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [259, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [260, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [261, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Determine the baseline FP32 accuracy": [[243, "3.-Determine-the-baseline-FP32-accuracy"]], "3. Fixing weight quantization": [[234, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range": [[214, "per-layer-encodings-min-max-range"]], "3. Run AutoQuant": [[257, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[236, "run-the-notebooks"]], "3. Showing similarities and differences between the original and converted models": [[246, "3.-Showing-similarities-and-differences-between-the-original-and-converted-models"]], "3. Train and evaluate the model": [[244, "3.-Train-and-evaluate-the-model"]], "3. Version 1.0.0": [[224, "version-1-0-0"]], "3. supergroups": [[231, "supergroups"]], "3.1. Encoding specification": [[224, "id1"]], "3.Create a quantization simulation model (with fake quantization ops inserted)": [[241, "3.Create-a-quantization-simulation-model-(with-fake-quantization-ops-inserted)"]], "4. Apply AdaRound": [[238, "4.-Apply-AdaRound"]], "4. Apply Adaround": [[242, "4.-Apply-Adaround"], [250, "4.-Apply-Adaround"], [256, "4.-Apply-Adaround"]], "4. Apply CLE": [[239, "4.-Apply-CLE"], [251, "4.-Apply-CLE"], [259, "4.-Apply-CLE"]], "4. Create a QuantizationSim Model": [[244, "4.-Create-a-QuantizationSim-Model"]], "4. Define constants and helper functions": [[243, "4.-Define-constants-and-helper-functions"]], "4. Discussing the limitations of the Keras Model Preparer": [[246, "4.-Discussing-the-limitations-of-the-Keras-Model-Preparer"]], "4. Fixing activation quantization": [[234, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[214, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[258, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[247, "4.-Perform-QAT"], [248, "4.-Perform-QAT"], [260, "4.-Perform-QAT"], [261, "4.-Perform-QAT"]], "4. Run AMP algorithm on the quantized model": [[237, "4.-Run-AMP-algorithm-on-the-quantized-model"], [241, "4.-Run-AMP-algorithm-on-the-quantized-model"], [255, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. model_input": [[231, "model-input"]], "5. Apply AutoQuant": [[243, "5.-Apply-AutoQuant"]], "5. Export Model": [[244, "5.-Export-Model"], [258, "5.-Export-Model"]], "5. Per-layer mean-square-error loss": [[214, "per-layer-mean-square-error-loss"]], "5. Perform QAT": [[244, "5.-Perform-QAT"]], "5. Performing per-layer analysis": [[234, "performing-per-layer-analysis"]], "5. model_output": [[231, "model-output"]], "6. Visualizing sensitive layers": [[234, "visualizing-sensitive-layers"]], "7. Fixing individual quantizers": [[234, "fixing-individual-quantizers"]], "8. Quantize the model": [[234, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[192, null]], "AIMET documentation versions": [[269, null]], "AIMET features": [[266, "aimet-features"], [267, null]], "AIMET visualization": [[221, null]], "API": [[20, "api"], [172, "api"], [173, "api"], [178, "api"], [197, "api"], [198, "api"], [199, "api"], [200, "api"], [201, "api"], [202, "api"], [204, "api"], [208, "api"], [209, "api"], [212, "api"], [213, "api"], [214, "api"], [215, "api"], [216, "api"], [220, "api"], [222, "api"], [226, "api"], [228, "api"], [229, "api"], [230, "api"]], "API Call for Regular AMP": [[241, "API-Call-for-Regular-AMP"]], "API Reference": [[192, "api-reference"]], "API reference": [[175, "api-reference"], [180, "api-reference"]], "Accuracy improvement tools": [[267, "accuracy-improvement-tools"]], "Accuracy-vs-Performance Tradeoff": [[264, "accuracy-vs-performance-tradeoff"]], "AdaScale": [[198, null], [203, "adascale"]], "Adaptive Rounding (AdaRound)": [[238, null], [242, null], [256, null]], "Adaptive rounding": [[197, null], [203, "adaptive-rounding"]], "Affine quantizers": [[180, "affine-quantizers"]], "Alternative packages": [[195, "alternative-packages"]], "Analysis descriptions": [[214, "analysis-descriptions"]], "Analysis tools": [[211, null], [225, "analysis-tools"]], "AutoQuant": [[243, null], [257, null]], "Automatic Mixed-Precision (AMP)": [[237, null], [241, null], [255, null]], "Automatic mixed precision": [[226, null], [227, "automatic-mixed-precision"]], "Automatic quantization": [[199, null], [203, "automatic-quantization"]], "Batch norm folding": [[201, null], [203, "batch-norm-folding"]], "Batch norm re-estimation": [[200, null], [203, "batch-norm-re-estimation"]], "Blockwise Quantization": [[225, "blockwise-quantization"]], "Blockwise quantization": [[215, null]], "Browse the notebooks": [[236, "browse-the-notebooks"]], "Build AIMET documentation": [[194, "build-aimet-documentation"]], "Build AIMET wheel and run unit tests": [[194, "build-aimet-wheel-and-run-unit-tests"], [194, "id2"]], "Build and run docker container locally": [[194, "build-and-run-docker-container-locally"]], "Building from source": [[194, null], [195, "building-from-source"]], "CLE": [[259, "CLE"]], "Calibration Callback": [[207, "calibration-callback"]], "Call AMP API": [[237, "Call-AMP-API"], [255, "Call-AMP-API"]], "Channel pruning": [[216, null]], "Channel pruning (CP)": [[219, "channel-pruning-cp"]], "Channel selection": [[216, "channel-selection"]], "Choose and install a package": [[195, "choose-and-install-a-package"]], "Code Examples": [[20, "code-examples"], [171, "code-examples"], [172, "code-examples"]], "Code example": [[216, "code-example"], [220, "code-example"], [222, "code-example"]], "Compilation": [[263, "compilation"]], "Compile and install pip package dependencies": [[194, "compile-and-install-pip-package-dependencies"]], "Complementary techniques": [[197, "complementary-techniques"]], "Compressing using Spatial SVD": [[220, "compressing-using-spatial-svd"]], "Compression": [[219, null], [221, "compression"], [225, "compression"]], "Compression features Guidebook": [[217, null]], "Compression ratio selection": [[218, "compression-ratio-selection"], [219, "compression-ratio-selection"]], "Compression using Channel Pruning": [[216, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[222, "compression-using-weight-svd"]], "Compute Encodings": [[237, "Compute-Encodings"], [241, "Compute-Encodings"], [255, "Compute-Encodings"]], "Computing encodings": [[175, "computing-encodings"]], "Conda environment": [[194, "conda-environment"]], "Configuration": [[175, "configuration"]], "Configuration file structure": [[231, "configuration-file-structure"]], "Context": [[197, "context"], [198, "context"], [199, "context"], [200, "context"], [201, "context"], [202, "context"], [204, "context"], [206, "context"], [207, "context"], [208, "context"], [209, "context"], [212, "context"], [213, "context"], [214, "context"], [216, "context"], [220, "context"], [222, "context"], [226, "context"], [228, "context"]], "Conversion": [[263, "conversion"]], "Create Quantization Sim Model": [[237, "Create-Quantization-Sim-Model"], [255, "Create-Quantization-Sim-Model"], [258, "Create-Quantization-Sim-Model"]], "Create QuantizationSimModel": [[207, "create-quantizationsimmodel"]], "Create a new conda environment with Python 3.10": [[194, "create-a-new-conda-environment-with-python-3-10"]], "Create the Quantization Sim Model": [[239, "Create-the-Quantization-Sim-Model"], [240, "Create-the-Quantization-Sim-Model"], [242, "Create-the-Quantization-Sim-Model"], [247, "Create-the-Quantization-Sim-Model"], [248, "Create-the-Quantization-Sim-Model"], [251, "Create-the-Quantization-Sim-Model"], [256, "Create-the-Quantization-Sim-Model"], [259, "Create-the-Quantization-Sim-Model"], [260, "Create-the-Quantization-Sim-Model"], [261, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[239, null], [259, null]], "Cross-Layer Equalization with QuantSim": [[251, null]], "Cross-layer equalization": [[202, null], [203, "cross-layer-equalization"]], "Data type": [[224, "id5"]], "Dataset": [[237, "Dataset"], [238, "Dataset"], [239, "Dataset"], [240, "Dataset"], [241, "Dataset"], [242, "Dataset"], [243, "Dataset"], [244, "Dataset"], [247, "Dataset"], [248, "Dataset"], [249, "Dataset"], [250, "Dataset"], [251, "Dataset"], [252, "Dataset"], [253, "Dataset"], [254, "Dataset"], [255, "Dataset"], [256, "Dataset"], [257, "Dataset"], [258, "Dataset"], [259, "Dataset"], [260, "Dataset"], [261, "Dataset"], [262, "Dataset"]], "Debugging guidelines": [[266, "debugging-guidelines"]], "Debugging workflow": [[234, "debugging-workflow"]], "Define callback functions for AMP": [[237, "Define-callback-functions-for-AMP"], [255, "Define-callback-functions-for-AMP"]], "Deployment paths": [[267, "deployment-paths"]], "DequantizedTensor": [[158, null]], "Design": [[221, "design"]], "Detailed Workflow": [[264, "detailed-workflow"]], "Determine quantization parameters (encodings)": [[265, "determine-quantization-parameters-encodings"]], "Docker environment": [[194, "docker-environment"]], "Encoding Format Specification": [[224, null]], "Encoding dictionary structure": [[224, "id3"]], "Encoding min/max ranges": [[249, "Encoding-min/max-ranges"], [262, "Encoding-min/max-ranges"]], "Encoding type": [[224, "id4"]], "Enum Definition": [[22, "enum-definition"]], "Example Notebooks": [[192, "example-notebooks"], [236, null]], "Executing blockwise quantization": [[215, "executing-blockwise-quantization"]], "Execution": [[200, "execution"], [202, "execution"], [263, "execution"]], "Export API": [[267, "export-api"]], "Export tools": [[267, "export-tools"]], "Exported Encodings": [[265, "exported-encodings"]], "Exporting blockwise-quantized models": [[215, "exporting-blockwise-quantized-models"]], "External resources": [[190, null]], "FAQs": [[219, "faqs"]], "Fast AMP (AMP 2.0)": [[241, "Fast-AMP-(AMP-2.0)"]], "FloatQuantizeDequantize": [[166, null]], "Fold Batch Norm layers": [[239, "Fold-Batch-Norm-layers"], [259, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[237, "Fold-Batch-Normalization-layers"], [240, "Fold-Batch-Normalization-layers"], [242, "Fold-Batch-Normalization-layers"], [247, "Fold-Batch-Normalization-layers"], [248, "Fold-Batch-Normalization-layers"], [250, "Fold-Batch-Normalization-layers"], [251, "Fold-Batch-Normalization-layers"], [255, "Fold-Batch-Normalization-layers"], [256, "Fold-Batch-Normalization-layers"], [260, "Fold-Batch-Normalization-layers"], [261, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[244, "Fold-BatchNorm-Layers"], [258, "Fold-BatchNorm-Layers"]], "For more information": [[238, "For-more-information"], [239, "For-more-information"], [240, "For-more-information"], [242, "For-more-information"], [243, "For-more-information"], [247, "For-more-information"], [248, "For-more-information"], [251, "For-more-information"], [252, "For-more-information"], [253, "For-more-information"], [254, "For-more-information"], [256, "For-more-information"], [257, "For-more-information"], [259, "For-more-information"], [260, "For-more-information"], [261, "For-more-information"]], "General guidelines": [[268, "general-guidelines"]], "Get Started": [[193, "get-started"]], "Glossary": [[191, null], [192, "glossary"]], "Greedy compression ratio selection": [[218, null]], "How it works": [[218, "how-it-works"]], "How quantization simulation works": [[265, "how-quantization-simulation-works"]], "How to modify configuration file": [[231, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[171, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[223, "how-winnowing-works"]], "Hyper parameters": [[197, "hyper-parameters"]], "Installation": [[195, null]], "Installing AIMET": [[196, "installing-aimet"]], "Interactive visualization": [[211, "interactive-visualization"], [212, null]], "Keras Model Preparer": [[246, null]], "Layer output generation": [[211, "layer-output-generation"], [213, null]], "Limitations": [[20, "limitations"], [172, "limitations"]], "LoRa Training": [[206, "lora-training"]], "Low-power blockwise quantization": [[215, "low-power-blockwise-quantization"]], "Manual mixed precision": [[227, "manual-mixed-precision"], [228, null]], "Migration Process": [[171, "migration-process"]], "Migration guide": [[171, null]], "Min-Max (also called \u201cTF\u201d in AIMET)": [[265, "min-max-also-called-tf-in-aimet"]], "Mixed Precision": [[225, "mixed-precision"]], "Mixed Precision Algorithm": [[226, "mixed-precision-algorithm"]], "Mixed precision": [[227, null], [267, "mixed-precision"]], "Model compression": [[219, "model-compression"]], "Model compression using channel pruning": [[252, null]], "Model compression using spatial SVD": [[253, null]], "Model compression using spatial SVD and channel pruning": [[254, null]], "Model guidelines": [[233, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[171, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[171, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "NOTE": [[219, null]], "NVIDIA CUDA support": [[194, "nvidia-cuda-support"]], "Next steps": [[238, "Next-steps"], [242, "Next-steps"], [247, "Next-steps"], [248, "Next-steps"], [251, "Next-steps"], [252, "Next-steps"], [253, "Next-steps"], [254, "Next-steps"], [256, "Next-steps"], [257, "Next-steps"], [259, "Next-steps"], [260, "Next-steps"], [261, "Next-steps"]], "Next: Deploying the model": [[264, "next-deploying-the-model"]], "Next: deploying the model": [[268, "next-deploying-the-model"]], "Old versions": [[195, "old-versions"]], "OmniQuant": [[203, "omniquant"], [204, null]], "On-target inference": [[263, null], [266, "on-target-inference"]], "Optional techniques": [[219, "optional-techniques"]], "Overall flow": [[237, "Overall-flow"], [238, "Overall-flow"], [239, "Overall-flow"], [240, "Overall-flow"], [241, "Overall-flow"], [242, "Overall-flow"], [243, "Overall-flow"], [244, "Overall-flow"], [245, "Overall-flow"], [246, "Overall-flow"], [247, "Overall-flow"], [248, "Overall-flow"], [249, "Overall-flow"], [250, "Overall-flow"], [251, "Overall-flow"], [252, "Overall-flow"], [253, "Overall-flow"], [254, "Overall-flow"], [255, "Overall-flow"], [256, "Overall-flow"], [257, "Overall-flow"], [258, "Overall-flow"], [259, "Overall-flow"], [260, "Overall-flow"], [261, "Overall-flow"], [262, "Overall-flow"]], "Overview": [[192, "overview"], [218, "overview"], [219, "overview"], [221, "overview"], [223, "overview"], [231, "overview"], [265, "overview"]], "PDF of statistics": [[249, "PDF-of-statistics"], [262, "PDF-of-statistics"]], "PTQ": [[206, "ptq"]], "Parameters for AMP algorithm": [[237, "Parameters-for-AMP-algorithm"], [241, "Parameters-for-AMP-algorithm"], [255, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[180, "per-block-quantization"]], "Per-channel quantization": [[180, "per-channel-quantization"]], "Per-layer MSE loss": [[249, "Per-layer-MSE-loss"], [262, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[249, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [262, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[218, "per-layer-exploration"]], "Per-layer fine-tuning": [[219, "per-layer-fine-tuning"]], "Perform QAT": [[258, "Perform-QAT"]], "Phase 0: Find quantizer groups": [[226, "phase-0-find-quantizer-groups"]], "Phase 1: Perform sensitivity analysis": [[226, "phase-1-perform-sensitivity-analysis"]], "Phase 2: Create a Pareto-front list": [[226, "phase-2-create-a-pareto-front-list"]], "Phase 3: Reduce Convert overhead": [[226, "phase-3-reduce-convert-overhead"]], "Post Training Quantization": [[225, "post-training-quantization"], [229, null]], "Post Training Quantization Techniques": [[192, "post-training-quantization-techniques"], [203, null]], "Post-training quantization": [[267, "post-training-quantization"]], "Prepare the evaluation callback function": [[244, "Prepare-the-evaluation-callback-function"]], "Prerequisites": [[195, "prerequisites"], [197, "prerequisites"], [198, "prerequisites"], [199, "prerequisites"], [200, "prerequisites"], [204, "prerequisites"], [208, "prerequisites"], [209, "prerequisites"], [214, "prerequisites"], [228, "prerequisites"], [229, "prerequisites"]], "Procedure": [[198, "procedure"], [199, "procedure"], [201, "procedure"], [204, "procedure"], [208, "procedure"], [209, "procedure"], [216, "procedure"], [226, "procedure"], [268, "procedure"]], "PyPI": [[195, "pypi"]], "PyTorch model guidelines": [[233, null]], "QW-LoRa": [[205, "qw-lora"], [206, null]], "QWA-LoRa": [[205, "qwa-lora"], [207, null]], "Qualcomm\u00ae AI Engine Direct SDK": [[263, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[263, "qualcommreg-ai-hub"]], "Quant Analyzer": [[249, null], [262, null]], "QuantSim workflow": [[265, "quantsim-workflow"]], "Quantization": [[263, "quantization"]], "Quantization Aware Training": [[225, "quantization-aware-training"]], "Quantization analyzer": [[211, "quantization-analyzer"], [214, null]], "Quantization debugging guidelines": [[234, null]], "Quantization granularity": [[265, "quantization-granularity"]], "Quantization schemes": [[265, "quantization-schemes"]], "Quantization simulation": [[240, null], [267, "quantization-simulation"]], "Quantization simulation guide": [[265, null]], "Quantization user guide": [[266, null]], "Quantization workflow": [[264, null], [266, "quantization-workflow"], [268, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[244, null], [258, null]], "Quantization-Aware Training with a Keras Transformer Model": [[245, null]], "Quantization-Aware training with range learning": [[248, null]], "Quantization-aware training": [[230, null], [247, null], [260, null], [267, "quantization-aware-training"]], "Quantization-aware training with range learning": [[261, null]], "QuantizationMixin": [[30, null]], "Quantize": [[161, null]], "Quantize a small model quickly with AIMET": [[196, "quantize-a-small-model-quickly-with-aimet"]], "Quantize and Update Base Model Weights": [[206, "quantize-and-update-base-model-weights"]], "QuantizeDequantize": [[162, null]], "Quantized LoRa": [[203, "quantized-lora"], [205, null]], "Quantized modules": [[175, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[31, null]], "QuantizedAdaptiveAvgPool2d": [[32, null]], "QuantizedAdaptiveAvgPool3d": [[33, null]], "QuantizedAdaptiveMaxPool1d": [[34, null]], "QuantizedAdaptiveMaxPool2d": [[35, null]], "QuantizedAdaptiveMaxPool3d": [[36, null]], "QuantizedAlphaDropout": [[37, null]], "QuantizedAvgPool1d": [[38, null]], "QuantizedAvgPool2d": [[39, null]], "QuantizedAvgPool3d": [[40, null]], "QuantizedBCELoss": [[41, null]], "QuantizedBCEWithLogitsLoss": [[42, null]], "QuantizedBatchNorm1d": [[43, null]], "QuantizedBatchNorm2d": [[44, null]], "QuantizedBatchNorm3d": [[45, null]], "QuantizedBilinear": [[46, null]], "QuantizedCELU": [[47, null]], "QuantizedCTCLoss": [[48, null]], "QuantizedChannelShuffle": [[49, null]], "QuantizedCircularPad1d": [[50, null]], "QuantizedCircularPad2d": [[51, null]], "QuantizedCircularPad3d": [[52, null]], "QuantizedConstantPad1d": [[53, null]], "QuantizedConstantPad2d": [[54, null]], "QuantizedConstantPad3d": [[55, null]], "QuantizedConv1d": [[56, null]], "QuantizedConv2d": [[57, null]], "QuantizedConv3d": [[58, null]], "QuantizedConvTranspose1d": [[59, null]], "QuantizedConvTranspose2d": [[60, null]], "QuantizedConvTranspose3d": [[61, null]], "QuantizedCosineEmbeddingLoss": [[62, null]], "QuantizedCosineSimilarity": [[63, null]], "QuantizedCrossEntropyLoss": [[64, null]], "QuantizedDropout": [[65, null]], "QuantizedDropout1d": [[66, null]], "QuantizedDropout2d": [[67, null]], "QuantizedDropout3d": [[68, null]], "QuantizedELU": [[69, null]], "QuantizedEmbedding": [[70, null]], "QuantizedEmbeddingBag": [[71, null]], "QuantizedFeatureAlphaDropout": [[72, null]], "QuantizedFlatten": [[73, null]], "QuantizedFold": [[74, null]], "QuantizedFractionalMaxPool2d": [[75, null]], "QuantizedFractionalMaxPool3d": [[76, null]], "QuantizedGELU": [[77, null]], "QuantizedGLU": [[78, null]], "QuantizedGRU": [[79, null]], "QuantizedGRUCell": [[80, null]], "QuantizedGaussianNLLLoss": [[81, null]], "QuantizedGroupNorm": [[82, null]], "QuantizedHardshrink": [[83, null]], "QuantizedHardsigmoid": [[84, null]], "QuantizedHardswish": [[85, null]], "QuantizedHardtanh": [[86, null]], "QuantizedHingeEmbeddingLoss": [[87, null]], "QuantizedHuberLoss": [[88, null]], "QuantizedInstanceNorm1d": [[89, null]], "QuantizedInstanceNorm2d": [[90, null]], "QuantizedInstanceNorm3d": [[91, null]], "QuantizedKLDivLoss": [[92, null]], "QuantizedL1Loss": [[93, null]], "QuantizedLPPool1d": [[94, null]], "QuantizedLPPool2d": [[95, null]], "QuantizedLSTM": [[96, null]], "QuantizedLSTMCell": [[97, null]], "QuantizedLayerNorm": [[98, null]], "QuantizedLeakyReLU": [[99, null]], "QuantizedLinear": [[100, null]], "QuantizedLocalResponseNorm": [[101, null]], "QuantizedLogSigmoid": [[102, null]], "QuantizedLogSoftmax": [[103, null]], "QuantizedMSELoss": [[104, null]], "QuantizedMarginRankingLoss": [[105, null]], "QuantizedMaxPool1d": [[106, null]], "QuantizedMaxPool2d": [[107, null]], "QuantizedMaxPool3d": [[108, null]], "QuantizedMaxUnpool1d": [[109, null]], "QuantizedMaxUnpool2d": [[110, null]], "QuantizedMaxUnpool3d": [[111, null]], "QuantizedMish": [[112, null]], "QuantizedMultiLabelMarginLoss": [[113, null]], "QuantizedMultiLabelSoftMarginLoss": [[114, null]], "QuantizedMultiMarginLoss": [[115, null]], "QuantizedNLLLoss": [[116, null]], "QuantizedNLLLoss2d": [[117, null]], "QuantizedPReLU": [[118, null]], "QuantizedPairwiseDistance": [[119, null]], "QuantizedPixelShuffle": [[120, null]], "QuantizedPixelUnshuffle": [[121, null]], "QuantizedPoissonNLLLoss": [[122, null]], "QuantizedRNN": [[123, null]], "QuantizedRNNCell": [[124, null]], "QuantizedRReLU": [[125, null]], "QuantizedReLU": [[126, null]], "QuantizedReLU6": [[127, null]], "QuantizedReflectionPad1d": [[128, null]], "QuantizedReflectionPad2d": [[129, null]], "QuantizedReflectionPad3d": [[130, null]], "QuantizedReplicationPad1d": [[131, null]], "QuantizedReplicationPad2d": [[132, null]], "QuantizedReplicationPad3d": [[133, null]], "QuantizedSELU": [[134, null]], "QuantizedSiLU": [[135, null]], "QuantizedSigmoid": [[136, null]], "QuantizedSmoothL1Loss": [[137, null]], "QuantizedSoftMarginLoss": [[138, null]], "QuantizedSoftmax": [[139, null]], "QuantizedSoftmax2d": [[140, null]], "QuantizedSoftmin": [[141, null]], "QuantizedSoftplus": [[142, null]], "QuantizedSoftshrink": [[143, null]], "QuantizedSoftsign": [[144, null]], "QuantizedTanh": [[145, null]], "QuantizedTanhshrink": [[146, null]], "QuantizedTensor": [[159, null]], "QuantizedTensorBase": [[160, null]], "QuantizedThreshold": [[147, null]], "QuantizedTripletMarginLoss": [[148, null]], "QuantizedTripletMarginWithDistanceLoss": [[149, null]], "QuantizedUnflatten": [[150, null]], "QuantizedUnfold": [[151, null]], "QuantizedUpsample": [[152, null]], "QuantizedUpsamplingBilinear2d": [[153, null]], "QuantizedUpsamplingNearest2d": [[154, null]], "QuantizedZeroPad1d": [[155, null]], "QuantizedZeroPad2d": [[156, null]], "QuantizedZeroPad3d": [[157, null]], "Quantizer Args structure": [[224, "id6"]], "Quantizers": [[180, "quantizers"]], "Quantsim and Adaround - Per Channel Quantization (PCQ)": [[250, null]], "Quick Start": [[196, null]], "Rank Rounding": [[219, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[258, "Re-estimate-BatchNorm-Statistics"]], "References": [[219, "references"]], "Regular AMP": [[241, "Regular-AMP"]], "Release Notes": [[192, "release-notes"]], "Release notes": [[210, null]], "Run QWA-LoRa": [[207, "run-qwa-lora"]], "Running the notebooks": [[236, "running-the-notebooks"]], "Runtime configuration": [[231, null], [265, "runtime-configuration"]], "Sequential MSE": [[203, "sequential-mse"], [208, null]], "Set environment variables to build desired AIMET wheel": [[194, "set-environment-variables-to-build-desired-aimet-wheel"], [194, "id1"]], "Set model input precision": [[228, "set-model-input-precision"]], "Set model output precision": [[228, "set-model-output-precision"]], "Set precision based on layer type": [[228, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[228, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[228, "set-precision-of-a-non-leaf-layer"]], "Setup": [[197, "setup"], [198, "setup"], [200, "setup"], [202, "setup"], [204, "setup"], [206, "setup"], [207, "setup"], [208, "setup"], [209, "setup"], [216, "setup"], [220, "setup"], [222, "setup"], [228, "setup"]], "Signal-to-Quantization-Noise": [[265, "signal-to-quantization-noise"]], "Simulate quantization noise": [[265, "simulate-quantization-noise"]], "Spatial SVD": [[219, "spatial-svd"], [220, null]], "SpinQuant": [[203, "spinquant"], [209, null]], "Starting a Bokeh server session": [[221, "starting-a-bokeh-server-session"]], "Step 1": [[197, "step-1"], [198, "step-1"], [199, "step-1"], [200, "step-1"], [201, "step-1"], [204, "step-1"], [208, "step-1"], [209, "step-1"], [226, "step-1"]], "Step 1 Importing libraries": [[214, "step-1-importing-libraries"]], "Step 1: Applying MMP API options": [[228, "step-1-applying-mmp-api-options"]], "Step 1: Creating a QuantSim model": [[229, "step-1-creating-a-quantsim-model"]], "Step 1: Find baseline precision": [[264, "step-1-find-baseline-precision"]], "Step 1: Importing the API": [[213, "step-1-importing-the-api"]], "Step 1: Setup": [[230, "step-1-setup"]], "Step 1: Trying FP16 precision (no quantization)": [[268, "step-1-trying-fp16-precision-no-quantization"]], "Step 2": [[197, "step-2"], [198, "step-2"], [199, "step-2"], [200, "step-2"], [201, "step-2"], [204, "step-2"], [208, "step-2"], [209, "step-2"], [226, "step-2"]], "Step 2 Preparing calibration callback": [[214, "step-2-preparing-calibration-callback"]], "Step 2: Applying the profile": [[228, "step-2-applying-the-profile"]], "Step 2: Compute initial quantization parameters": [[230, "step-2-compute-initial-quantization-parameters"]], "Step 2: Creating a calibration callback": [[229, "step-2-creating-a-calibration-callback"]], "Step 2: Loading a model": [[213, "step-2-loading-a-model"]], "Step 2: Use lite mixed precision": [[264, "step-2-use-lite-mixed-precision"]], "Step 2: Verifying W16A16 quantization": [[268, "step-2-verifying-w16a16-quantization"]], "Step 3": [[197, "step-3"], [198, "step-3"], [199, "step-3"], [200, "step-3"], [201, "step-3"], [204, "step-3"], [208, "step-3"], [209, "step-3"]], "Step 3 Preparing evaluation callback": [[214, "step-3-preparing-evaluation-callback"]], "Step 3. Reducing precision": [[268, "step-3-reducing-precision"]], "Step 3: Computing encodings": [[229, "step-3-computing-encodings"]], "Step 3: Obtaining inputs": [[213, "step-3-obtaining-inputs"]], "Step 3: Run quantization-aware training": [[230, "step-3-run-quantization-aware-training"]], "Step 3: Use Automatic Mixed Precision (AMP)": [[264, "step-3-use-automatic-mixed-precision-amp"]], "Step 4": [[197, "step-4"], [198, "step-4"], [199, "step-4"], [200, "step-4"], [204, "step-4"], [208, "step-4"], [209, "step-4"]], "Step 4 Preparing model": [[214, "step-4-preparing-model"]], "Step 4. Restoring accuracy": [[268, "step-4-restoring-accuracy"]], "Step 4: Evaluation": [[229, "step-4-evaluation"]], "Step 4: Generating layer outputs": [[213, "step-4-generating-layer-outputs"]], "Step 4: Use advanced Post-Training Quantization (PTQ) techniques": [[264, "step-4-use-advanced-post-training-quantization-ptq-techniques"]], "Step 5": [[198, "step-5"], [199, "step-5"], [204, "step-5"], [208, "step-5"], [209, "step-5"]], "Step 5 Creating QuantAnalyzer": [[214, "step-5-creating-quantanalyzer"]], "Step 5: Exporting the model": [[229, "step-5-exporting-the-model"]], "Step 5: Use Quantization-Aware Training (QAT)": [[264, "step-5-use-quantization-aware-training-qat"]], "Step 6": [[199, "step-6"]], "Step 6 Running the analysis": [[214, "step-6-running-the-analysis"]], "Step 7": [[199, "step-7"]], "Summary": [[241, "Summary"], [244, "Summary"], [246, "Summary"], [250, "Summary"], [258, "Summary"]], "Supported platform": [[193, "supported-platform"]], "Supported precisions for on-target inference": [[267, "supported-precisions-for-on-target-inference"]], "Techniques": [[192, "techniques"], [225, null], [259, "Techniques"]], "TensorFlow model guidelines": [[232, null]], "Terminology": [[178, "terminology"]], "Tested platform": [[196, "tested-platform"]], "Top level structure": [[224, "id2"]], "Training Callback": [[207, "training-callback"]], "Tutorials": [[192, "tutorials"], [235, null]], "Typical recommendations": [[230, "typical-recommendations"]], "Use Case": [[219, "use-case"]], "Use Cases": [[226, "use-cases"]], "User flow": [[178, "user-flow"]], "Variants of QAT": [[230, "variants-of-qat"]], "Verifying the installation": [[195, "verifying-the-installation"], [196, "verifying-the-installation"]], "Visualization Tools": [[168, "visualization-tools"]], "Visualizing compression ratios": [[221, "visualizing-compression-ratios"]], "Weight SVD": [[219, "weight-svd"], [222, null]], "Weight reconstruction": [[216, "weight-reconstruction"]], "What is AIMET?": [[193, null]], "What this notebook is not": [[237, "What-this-notebook-is-not"], [241, "What-this-notebook-is-not"], [249, "What-this-notebook-is-not"], [250, "What-this-notebook-is-not"], [255, "What-this-notebook-is-not"], [258, "What-this-notebook-is-not"], [262, "What-this-notebook-is-not"]], "Winnowing": [[216, "winnowing"], [223, null], [223, "id1"]], "Workflow": [[197, "workflow"], [197, "id2"], [198, "workflow"], [199, "workflow"], [200, "workflow"], [201, "workflow"], [202, "workflow"], [204, "workflow"], [206, "workflow"], [207, "workflow"], [208, "workflow"], [209, "workflow"], [212, "workflow"], [213, "workflow"], [214, "workflow"], [216, "workflow"], [220, "workflow"], [222, "workflow"], [226, "workflow"], [228, "workflow"], [229, "workflow"], [230, "workflow"]], "aimet_onnx API": [[5, null]], "aimet_onnx.apply_adaround": [[1, null]], "aimet_onnx.apply_seq_mse": [[10, null]], "aimet_onnx.batch_norm_fold": [[3, null]], "aimet_onnx.cross_layer_equalization": [[4, null]], "aimet_onnx.layer_output_utils": [[6, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[8, null]], "aimet_onnx.quantsim": [[9, null]], "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights": [[7, null]], "aimet_tensorflow API": [[18, null]], "aimet_tensorflow.adaround": [[11, null]], "aimet_tensorflow.auto_quant_v2": [[13, null]], "aimet_tensorflow.batch_norm_fold": [[15, null]], "aimet_tensorflow.compress": [[17, null]], "aimet_tensorflow.cross_layer_equalization": [[16, null]], "aimet_tensorflow.keras.bn_reestimation": [[14, null]], "aimet_tensorflow.layer_output_utils": [[19, null]], "aimet_tensorflow.mixed_precision": [[12, null]], "aimet_tensorflow.model_preparer": [[20, null]], "aimet_tensorflow.quant_analyzer": [[21, null]], "aimet_tensorflow.quantsim": [[22, null]], "aimet_torch": [[167, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[171, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[167, null]], "aimet_torch.adaround": [[23, null]], "aimet_torch.auto_quant": [[25, null]], "aimet_torch.batch_norm_fold": [[27, null]], "aimet_torch.bn_reestimation": [[26, null]], "aimet_torch.compress": [[29, null]], "aimet_torch.cross_layer_equalization": [[28, null]], "aimet_torch.experimental.adascale": [[24, null]], "aimet_torch.experimental.omniquant": [[176, null]], "aimet_torch.experimental.spinquant": [[183, null]], "aimet_torch.layer_output_utils": [[169, null]], "aimet_torch.mixed_precision": [[174, null]], "aimet_torch.model_preparer": [[172, null]], "aimet_torch.model_validator": [[173, null]], "aimet_torch.nn": [[175, null]], "aimet_torch.onnx.export (beta)": [[177, null]], "aimet_torch.peft": [[178, null]], "aimet_torch.quant_analyzer": [[179, null]], "aimet_torch.quantization": [[180, null]], "aimet_torch.quantsim": [[181, null]], "aimet_torch.quantsim.config_utils": [[170, null]], "aimet_torch.seq_mse": [[182, null]], "aimet_torch.v1": [[167, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[184, null]], "aimet_torch.v1.auto_quant": [[186, null]], "aimet_torch.v1.mixed_precision": [[185, null]], "aimet_torch.v1.quant_analyzer": [[187, null]], "aimet_torch.v1.quantsim": [[188, null]], "aimet_torch.v1.seq_mse": [[189, null]], "aimet_torch.visualization_tools": [[168, null]], "dequantize": [[163, null]], "quantize": [[164, null]], "quantize_dequantize": [[165, null]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/tensorflow/adaround", "apiref/tensorflow/amp", "apiref/tensorflow/autoquant", "apiref/tensorflow/bn", "apiref/tensorflow/bnf", "apiref/tensorflow/cle", "apiref/tensorflow/compress", "apiref/tensorflow/index", "apiref/tensorflow/layer_output_generation", "apiref/tensorflow/model_preparer", "apiref/tensorflow/quant_analyzer", "apiref/tensorflow/quantsim", "apiref/torch/adaround", "apiref/torch/adascale", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/omniquant", "apiref/torch/onnx", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/spinquant", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "external/index", "glossary", "index", "overview/index", "overview/install/build_from_source", "overview/install/index", "overview/install/quick-start", "ptq_techniques/adaround", "ptq_techniques/adascale", "ptq_techniques/autoquant", "ptq_techniques/bn", "ptq_techniques/bnf", "ptq_techniques/cle", "ptq_techniques/index", "ptq_techniques/omniquant", "ptq_techniques/quantized_LoRa/index", "ptq_techniques/quantized_LoRa/qw_lora", "ptq_techniques/quantized_LoRa/qwa_lora", "ptq_techniques/seq_mse", "ptq_techniques/spinquant", "release_notes", "techniques/analysis_tools/index", "techniques/analysis_tools/interactive_visualization", "techniques/analysis_tools/layer_output_generation", "techniques/analysis_tools/quant_analyzer", "techniques/blockwise", "techniques/compression/channel_pruning", "techniques/compression/feature_guidebook", "techniques/compression/greedy_compression_ratio_selection", "techniques/compression/index", "techniques/compression/spatial_svd", "techniques/compression/visualization_compression", "techniques/compression/weight_svd", "techniques/compression/winnowing", "techniques/encoding_spec", "techniques/index", "techniques/mixed_precision/amp", "techniques/mixed_precision/index", "techniques/mixed_precision/mmp", "techniques/ptq", "techniques/qat", "techniques/runtime_config", "techniques/tensorflow/model_guidelines", "techniques/torch/model_guidelines", "tutorials/debugging_guidelines", "tutorials/index", "tutorials/notebooks", "tutorials/notebooks/onnx/quantization/AMP", "tutorials/notebooks/onnx/quantization/adaround", "tutorials/notebooks/onnx/quantization/cle", "tutorials/notebooks/onnx/quantization/quantsim", "tutorials/notebooks/tensorflow/quantization/keras/KerasAMP", "tutorials/notebooks/tensorflow/quantization/keras/adaround", "tutorials/notebooks/tensorflow/quantization/keras/autoquant", "tutorials/notebooks/tensorflow/quantization/keras/bn_reestimation", "tutorials/notebooks/tensorflow/quantization/keras/keras_transformer_qat", "tutorials/notebooks/tensorflow/quantization/keras/model_preparer", "tutorials/notebooks/tensorflow/quantization/keras/qat", "tutorials/notebooks/tensorflow/quantization/keras/qat_range_learning", "tutorials/notebooks/tensorflow/quantization/keras/quant_analyzer", "tutorials/notebooks/tensorflow/quantization/keras/quantsim_adaround_pcq", "tutorials/notebooks/tensorflow/quantization/keras/quantsim_cle", "tutorials/notebooks/torch/compression/channel_pruning", "tutorials/notebooks/torch/compression/spatial_svd", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning", "tutorials/notebooks/torch/quantization/AMP", "tutorials/notebooks/torch/quantization/adaround", "tutorials/notebooks/torch/quantization/autoquant", "tutorials/notebooks/torch/quantization/bn_reestimation", "tutorials/notebooks/torch/quantization/cle", "tutorials/notebooks/torch/quantization/qat", "tutorials/notebooks/torch/quantization/qat_range_learning", "tutorials/notebooks/torch/quantization/quant_analyzer", "tutorials/on_target_inference", "tutorials/quantization_workflow", "tutorials/quantsim", "userguide/index", "userguide/quantization_tools", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/tensorflow/adaround.rst", "apiref/tensorflow/amp.rst", "apiref/tensorflow/autoquant.rst", "apiref/tensorflow/bn.rst", "apiref/tensorflow/bnf.rst", "apiref/tensorflow/cle.rst", "apiref/tensorflow/compress.rst", "apiref/tensorflow/index.rst", "apiref/tensorflow/layer_output_generation.rst", "apiref/tensorflow/model_preparer.rst", "apiref/tensorflow/quant_analyzer.rst", "apiref/tensorflow/quantsim.rst", "apiref/torch/adaround.rst", "apiref/torch/adascale.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/omniquant.rst", "apiref/torch/onnx.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/spinquant.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "external/index.rst", "glossary.rst", "index.rst", "overview/index.rst", "overview/install/build_from_source.rst", "overview/install/index.rst", "overview/install/quick-start.rst", "ptq_techniques/adaround.rst", "ptq_techniques/adascale.rst", "ptq_techniques/autoquant.rst", "ptq_techniques/bn.rst", "ptq_techniques/bnf.rst", "ptq_techniques/cle.rst", "ptq_techniques/index.rst", "ptq_techniques/omniquant.rst", "ptq_techniques/quantized_LoRa/index.rst", "ptq_techniques/quantized_LoRa/qw_lora.rst", "ptq_techniques/quantized_LoRa/qwa_lora.rst", "ptq_techniques/seq_mse.rst", "ptq_techniques/spinquant.rst", "release_notes.rst", "techniques/analysis_tools/index.rst", "techniques/analysis_tools/interactive_visualization.rst", "techniques/analysis_tools/layer_output_generation.rst", "techniques/analysis_tools/quant_analyzer.rst", "techniques/blockwise.rst", "techniques/compression/channel_pruning.rst", "techniques/compression/feature_guidebook.rst", "techniques/compression/greedy_compression_ratio_selection.rst", "techniques/compression/index.rst", "techniques/compression/spatial_svd.rst", "techniques/compression/visualization_compression.rst", "techniques/compression/weight_svd.rst", "techniques/compression/winnowing.rst", "techniques/encoding_spec.rst", "techniques/index.rst", "techniques/mixed_precision/amp.rst", "techniques/mixed_precision/index.rst", "techniques/mixed_precision/mmp.rst", "techniques/ptq.rst", "techniques/qat.rst", "techniques/runtime_config.rst", "techniques/tensorflow/model_guidelines.rst", "techniques/torch/model_guidelines.rst", "tutorials/debugging_guidelines.rst", "tutorials/index.rst", "tutorials/notebooks.rst", "tutorials/notebooks/onnx/quantization/AMP.ipynb", "tutorials/notebooks/onnx/quantization/adaround.ipynb", "tutorials/notebooks/onnx/quantization/cle.ipynb", "tutorials/notebooks/onnx/quantization/quantsim.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/KerasAMP.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/adaround.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/autoquant.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/bn_reestimation.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/keras_transformer_qat.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/model_preparer.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/qat.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/qat_range_learning.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/quant_analyzer.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/quantsim_adaround_pcq.ipynb", "tutorials/notebooks/tensorflow/quantization/keras/quantsim_cle.ipynb", "tutorials/notebooks/torch/compression/channel_pruning.ipynb", "tutorials/notebooks/torch/compression/spatial_svd.ipynb", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning.ipynb", "tutorials/notebooks/torch/quantization/AMP.ipynb", "tutorials/notebooks/torch/quantization/adaround.ipynb", "tutorials/notebooks/torch/quantization/autoquant.ipynb", "tutorials/notebooks/torch/quantization/bn_reestimation.ipynb", "tutorials/notebooks/torch/quantization/cle.ipynb", "tutorials/notebooks/torch/quantization/qat.ipynb", "tutorials/notebooks/torch/quantization/qat_range_learning.ipynb", "tutorials/notebooks/torch/quantization/quant_analyzer.ipynb", "tutorials/on_target_inference.rst", "tutorials/quantization_workflow.rst", "tutorials/quantsim.rst", "userguide/index.rst", "userguide/quantization_tools.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[191, "term-Accelerator", true]], "accuracy": [[191, "term-Accuracy", true]], "activation": [[191, "term-Activation", true]], "activation quantization": [[191, "term-Activation-Quantization", true]], "adaround": [[191, "term-AdaRound", true]], "adaroundparameters (class in aimet_tensorflow.keras.adaround_weight)": [[11, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false], [197, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.adaround.adaround_weight)": [[23, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false], [197, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[184, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[191, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[191, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [214, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [228, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_onnx)": [[1, "aimet_onnx.apply_adaround", false], [197, "aimet_onnx.apply_adaround", false]], "apply_adaround() (in module aimet_tensorflow.keras.adaround_weight.adaround)": [[11, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false], [197, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.adaround.adaround_weight.adaround)": [[23, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false], [197, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[184, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adascale() (in module aimet_torch.experimental.adascale)": [[24, "aimet_torch.experimental.adascale.apply_adascale", false], [198, "aimet_torch.experimental.adascale.apply_adascale", false]], "apply_omniquant() (in module aimet_torch.experimental.omniquant)": [[176, "aimet_torch.experimental.omniquant.apply_omniquant", false], [204, "aimet_torch.experimental.omniquant.apply_omniquant", false]], "apply_seq_mse() (in module aimet_onnx)": [[10, "aimet_onnx.apply_seq_mse", false], [208, "aimet_onnx.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[182, "aimet_torch.seq_mse.apply_seq_mse", false], [208, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[189, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "apply_spinquant() (in module aimet_torch.experimental.spinquant)": [[183, "aimet_torch.experimental.spinquant.apply_spinquant", false], [209, "aimet_torch.experimental.spinquant.apply_spinquant", false]], "auto (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false]], "autoquant": [[191, "term-AutoQuant", true]], "autoquantwithautomixedprecision (class in aimet_tensorflow.keras.auto_quant_v2)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false]], "batch normalization": [[191, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[191, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[191, "term-BN", true]], "callbackfunc (class in aimet_common.defs)": [[2, "aimet_common.defs.CallbackFunc", false], [12, "aimet_common.defs.CallbackFunc", false], [174, "aimet_common.defs.CallbackFunc", false], [185, "aimet_common.defs.CallbackFunc", false], [226, "aimet_common.defs.CallbackFunc", false], [226, "id0", false], [226, "id1", false]], "callbackfunc (class in aimet_common.utils)": [[179, "aimet_common.utils.CallbackFunc", false], [187, "aimet_common.utils.CallbackFunc", false], [214, "aimet_common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_fast_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[12, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false], [226, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [226, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[12, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false], [226, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[174, "aimet_torch.mixed_precision.choose_mixed_precision", false], [226, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[185, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[191, "term-CNN", true]], "compress_model() (aimet_tensorflow.keras.compress.modelcompressor static method)": [[17, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false], [220, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false]], "compression": [[191, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantizationsimmodel method)": [[9, "aimet_onnx.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantizationsimmodel method)": [[181, "aimet_torch.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[188, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (in module aimet_onnx)": [[9, "aimet_onnx.compute_encodings", false]], "convolutional layer": [[191, "term-Convolutional-Layer", true]], "convolutional neural network": [[191, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[163, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[158, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[191, "term-Device", true]], "dlf": [[191, "term-DLF", true]], "dynamic layer fusion": [[191, "term-Dynamic-Layer-Fusion", true]], "edge device": [[191, "term-Edge-device", true]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [214, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[191, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[4, "aimet_onnx.cross_layer_equalization.equalize_model", false], [202, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_tensorflow.keras.cross_layer_equalization)": [[16, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false], [202, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[28, "aimet_torch.cross_layer_equalization.equalize_model", false], [202, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [226, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [185, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [226, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "exponent_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.exponent_bits", false]], "export() (aimet_onnx.quantizationsimmodel method)": [[9, "aimet_onnx.QuantizationSimModel.export", false]], "export() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.export", false]], "export() (aimet_torch.quantizationsimmodel method)": [[181, "aimet_torch.QuantizationSimModel.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[188, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export() (in module aimet_torch.onnx)": [[177, "aimet_torch.onnx.export", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_tensorflow.keras.batch_norm_fold)": [[15, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false], [201, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[27, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [201, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_scale() (in module aimet_tensorflow.keras.batch_norm_fold)": [[14, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false], [200, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[3, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [201, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "fold_param_quantizers() (aimet_torch.quantizationsimmodel method)": [[181, "aimet_torch.QuantizationSimModel.fold_param_quantizers", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[100, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[161, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[162, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[182, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [208, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[189, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[191, "term-FP32", true]], "from_encodings() (aimet_torch.quantization.float.floatquantizedequantize class method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.from_encodings", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.from_module", false]], "from_str() (aimet_common.defs.quantscheme class method)": [[9, "aimet_common.defs.QuantScheme.from_str", false], [22, "aimet_common.defs.QuantScheme.from_str", false], [181, "aimet_common.defs.QuantScheme.from_str", false], [188, "aimet_common.defs.QuantScheme.from_str", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [213, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_tensorflow.keras.layer_output_utils.layeroutpututil method)": [[19, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [213, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[169, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [213, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_param_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_loss_fn() (aimet_torch.seq_mse.seqmseparams method)": [[182, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false], [208, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false]], "get_loss_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[189, "aimet_torch.v1.seq_mse.SeqMseParams.get_loss_fn", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "get_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false]], "greedyselectionparameters (class in aimet_common.defs)": [[17, "aimet_common.defs.GreedySelectionParameters", false], [220, "aimet_common.defs.GreedySelectionParameters", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[191, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[191, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[191, "term-KL-Divergence", true]], "layer": [[191, "term-Layer", true]], "layer-wise quantization": [[191, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [213, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_tensorflow.keras.layer_output_utils)": [[19, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false], [213, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[169, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [213, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[188, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lookup_quantizer() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup static method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false]], "lora mobilenet": [[191, "term-LoRA-MobileNet", true]], "mantissa_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.mantissa_bits", false]], "manual (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [228, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[191, "term-Model", true]], "modelcompressor (class in aimet_tensorflow.keras.compress)": [[17, "aimet_tensorflow.keras.compress.ModelCompressor", false], [220, "aimet_tensorflow.keras.compress.ModelCompressor", false]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[169, "aimet_torch.layer_output_utils.NamingScheme", false], [213, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[191, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[191, "term-NNCF", true]], "node": [[191, "term-Node", true]], "normalization": [[191, "term-Normalization", true]], "onnx": [[191, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [213, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "open neural network exchange": [[191, "term-Open-Neural-Network-Exchange", true]], "optimize() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "per-channel quantization": [[191, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[191, "term-Post-Training-Quantization", true]], "prepare_model() (in module aimet_tensorflow.keras.model_preparer)": [[20, "aimet_tensorflow.keras.model_preparer.prepare_model", false]], "prepare_model() (in module aimet_torch.model_preparer)": [[172, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[191, "term-Pruning", true]], "ptq": [[191, "term-PTQ", true]], "pytorch": [[191, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [213, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[191, "term-QAT", true]], "qdo": [[191, "term-QDO", true]], "qualcomm innovation center": [[191, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [214, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[179, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [214, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[187, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[191, "term-Quantization", true]], "quantization simulation": [[191, "term-Quantization-Simulation", true]], "quantization-aware training": [[191, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx)": [[9, "aimet_onnx.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_tensorflow.keras.quantsim)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch)": [[181, "aimet_torch.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[188, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantize (class in aimet_torch.quantization.affine)": [[161, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[164, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[165, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[162, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[159, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[160, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[147, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[148, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[149, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[150, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[151, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[152, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[153, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[154, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[155, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[156, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[157, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_tensorflow.keras.amp.quantizer_groups)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantscheme (class in aimet_common.defs)": [[9, "aimet_common.defs.QuantScheme", false], [22, "aimet_common.defs.QuantScheme", false], [181, "aimet_common.defs.QuantScheme", false], [188, "aimet_common.defs.QuantScheme", false]], "quantsim": [[191, "term-QuantSim", true]], "quic": [[191, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_tensorflow.keras.bn_reestimation)": [[14, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false], [200, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[26, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [200, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "run_inference() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[188, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[182, "aimet_torch.seq_mse.SeqMseParams", false], [208, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[189, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_adaround_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_onnx.quantsim)": [[7, "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_mixed_precision_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [228, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [228, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [228, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false], [199, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "spatialsvdparameters (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false]], "spatialsvdparameters.automodeparams (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false]], "spatialsvdparameters.manualmodeparams (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false]], "spatialsvdparameters.mode (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false], [220, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [226, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [185, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [226, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[191, "term-Target-Hardware-Accelerator", true]], "target runtime": [[191, "term-Target-Runtime", true]], "tensorflow": [[191, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [226, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false], [226, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [185, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [226, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_onnx_qdq() (aimet_onnx.quantizationsimmodel method)": [[9, "aimet_onnx.QuantizationSimModel.to_onnx_qdq", false]], "torchscript": [[191, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [213, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[191, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[168, "aimet_torch.v2.visualization_tools.visualize_stats", false], [212, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[191, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_common.defs": [[226, 0, 1, "id1", "CallbackFunc"], [220, 0, 1, "", "GreedySelectionParameters"], [188, 0, 1, "", "QuantScheme"]], "aimet_common.defs.QuantScheme": [[188, 1, 1, "", "from_str"]], "aimet_common.utils": [[214, 0, 1, "", "CallbackFunc"]], "aimet_onnx": [[9, 0, 1, "", "QuantizationSimModel"], [197, 2, 1, "", "apply_adaround"], [208, 2, 1, "", "apply_seq_mse"], [9, 2, 1, "", "compute_encodings"]], "aimet_onnx.QuantizationSimModel": [[9, 1, 1, "", "compute_encodings"], [9, 1, 1, "", "export"], [9, 1, 1, "", "to_onnx_qdq"]], "aimet_onnx.amp.mixed_precision_algo": [[226, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[226, 1, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[226, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[226, 1, 1, "", "get_activation_quantizers"], [226, 1, 1, "", "get_active_quantizers"], [226, 1, 1, "", "get_candidate"], [226, 1, 1, "", "get_param_quantizers"], [226, 1, 1, "", "set_quantizers_to_candidate"], [226, 1, 1, "", "to_list"]], "aimet_onnx.batch_norm_fold": [[201, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.cross_layer_equalization": [[202, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[213, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[213, 1, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[226, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[214, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[214, 1, 1, "", "analyze"], [214, 1, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[7, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_tensorflow.keras.adaround_weight": [[197, 0, 1, "", "AdaroundParameters"]], "aimet_tensorflow.keras.adaround_weight.Adaround": [[197, 2, 1, "", "apply_adaround"]], "aimet_tensorflow.keras.amp.quantizer_groups": [[226, 0, 1, "", "QuantizerGroup"]], "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup": [[226, 1, 1, "", "get_active_param_quantizers"], [226, 1, 1, "", "get_active_quantizers"], [226, 1, 1, "", "get_candidate"], [226, 1, 1, "", "lookup_quantizer"], [226, 1, 1, "", "set_quantizers_to_candidate"], [226, 1, 1, "", "to_list"]], "aimet_tensorflow.keras.auto_quant_v2": [[199, 0, 1, "", "AutoQuantWithAutoMixedPrecision"]], "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision": [[199, 1, 1, "", "get_quant_scheme_candidates"], [199, 1, 1, "", "optimize"], [199, 1, 1, "", "run_inference"], [199, 1, 1, "", "set_adaround_params"], [199, 1, 1, "", "set_mixed_precision_params"], [199, 1, 1, "", "set_quant_scheme_candidates"]], "aimet_tensorflow.keras.batch_norm_fold": [[201, 2, 1, "", "fold_all_batch_norms"], [200, 2, 1, "", "fold_all_batch_norms_to_scale"]], "aimet_tensorflow.keras.bn_reestimation": [[200, 2, 1, "", "reestimate_bn_stats"]], "aimet_tensorflow.keras.compress": [[220, 0, 1, "", "ModelCompressor"]], "aimet_tensorflow.keras.compress.ModelCompressor": [[220, 1, 1, "", "compress_model"]], "aimet_tensorflow.keras.cross_layer_equalization": [[202, 2, 1, "", "equalize_model"]], "aimet_tensorflow.keras.defs": [[220, 0, 1, "", "SpatialSvdParameters"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters": [[220, 0, 1, "", "AutoModeParams"], [220, 0, 1, "", "ManualModeParams"], [220, 0, 1, "", "Mode"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode": [[220, 3, 1, "", "auto"], [220, 3, 1, "", "manual"]], "aimet_tensorflow.keras.layer_output_utils": [[213, 0, 1, "", "LayerOutputUtil"]], "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil": [[213, 1, 1, "", "generate_layer_outputs"]], "aimet_tensorflow.keras.mixed_precision": [[226, 2, 1, "", "choose_fast_mixed_precision"], [226, 2, 1, "", "choose_mixed_precision"]], "aimet_tensorflow.keras.model_preparer": [[20, 2, 1, "", "prepare_model"]], "aimet_tensorflow.keras.quantsim": [[22, 0, 1, "", "QuantizationSimModel"]], "aimet_tensorflow.keras.quantsim.QuantizationSimModel": [[22, 1, 1, "", "compute_encodings"], [22, 1, 1, "", "export"]], "aimet_torch": [[181, 0, 1, "", "QuantizationSimModel"]], "aimet_torch.QuantizationSimModel": [[181, 1, 1, "", "compute_encodings"], [181, 1, 1, "", "export"], [181, 1, 1, "", "fold_param_quantizers"]], "aimet_torch.adaround.adaround_weight": [[197, 0, 1, "", "AdaroundParameters"]], "aimet_torch.adaround.adaround_weight.Adaround": [[197, 2, 1, "", "apply_adaround"]], "aimet_torch.amp.mixed_precision_algo": [[226, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[226, 1, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[226, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[226, 1, 1, "", "get_active_quantizers"], [226, 1, 1, "", "get_candidate"], [226, 1, 1, "", "get_input_quantizer_modules"], [226, 1, 1, "", "set_quantizers_to_candidate"], [226, 1, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[201, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[200, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.cross_layer_equalization": [[202, 2, 1, "", "equalize_model"]], "aimet_torch.experimental.adascale": [[198, 2, 1, "", "apply_adascale"]], "aimet_torch.experimental.omniquant": [[204, 2, 1, "", "apply_omniquant"]], "aimet_torch.experimental.spinquant": [[209, 2, 1, "", "apply_spinquant"]], "aimet_torch.layer_output_utils": [[213, 0, 1, "", "LayerOutputUtil"], [213, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[213, 1, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[213, 3, 1, "", "ONNX"], [213, 3, 1, "", "PYTORCH"], [213, 3, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[226, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[172, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[173, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[173, 1, 1, "", "add_check"], [173, 1, 1, "", "validate_model"]], "aimet_torch.nn": [[30, 0, 1, "", "QuantizationMixin"], [31, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [32, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [33, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [34, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [35, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [36, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [37, 0, 1, "", "QuantizedAlphaDropout"], [38, 0, 1, "", "QuantizedAvgPool1d"], [39, 0, 1, "", "QuantizedAvgPool2d"], [40, 0, 1, "", "QuantizedAvgPool3d"], [41, 0, 1, "", "QuantizedBCELoss"], [42, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [43, 0, 1, "", "QuantizedBatchNorm1d"], [44, 0, 1, "", "QuantizedBatchNorm2d"], [45, 0, 1, "", "QuantizedBatchNorm3d"], [46, 0, 1, "", "QuantizedBilinear"], [47, 0, 1, "", "QuantizedCELU"], [48, 0, 1, "", "QuantizedCTCLoss"], [49, 0, 1, "", "QuantizedChannelShuffle"], [50, 0, 1, "", "QuantizedCircularPad1d"], [51, 0, 1, "", "QuantizedCircularPad2d"], [52, 0, 1, "", "QuantizedCircularPad3d"], [53, 0, 1, "", "QuantizedConstantPad1d"], [54, 0, 1, "", "QuantizedConstantPad2d"], [55, 0, 1, "", "QuantizedConstantPad3d"], [56, 0, 1, "", "QuantizedConv1d"], [57, 0, 1, "", "QuantizedConv2d"], [58, 0, 1, "", "QuantizedConv3d"], [59, 0, 1, "", "QuantizedConvTranspose1d"], [60, 0, 1, "", "QuantizedConvTranspose2d"], [61, 0, 1, "", "QuantizedConvTranspose3d"], [62, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [63, 0, 1, "", "QuantizedCosineSimilarity"], [64, 0, 1, "", "QuantizedCrossEntropyLoss"], [65, 0, 1, "", "QuantizedDropout"], [66, 0, 1, "", "QuantizedDropout1d"], [67, 0, 1, "", "QuantizedDropout2d"], [68, 0, 1, "", "QuantizedDropout3d"], [69, 0, 1, "", "QuantizedELU"], [70, 0, 1, "", "QuantizedEmbedding"], [71, 0, 1, "", "QuantizedEmbeddingBag"], [72, 0, 1, "", "QuantizedFeatureAlphaDropout"], [73, 0, 1, "", "QuantizedFlatten"], [74, 0, 1, "", "QuantizedFold"], [75, 0, 1, "", "QuantizedFractionalMaxPool2d"], [76, 0, 1, "", "QuantizedFractionalMaxPool3d"], [77, 0, 1, "", "QuantizedGELU"], [78, 0, 1, "", "QuantizedGLU"], [79, 0, 1, "", "QuantizedGRU"], [80, 0, 1, "", "QuantizedGRUCell"], [81, 0, 1, "", "QuantizedGaussianNLLLoss"], [82, 0, 1, "", "QuantizedGroupNorm"], [83, 0, 1, "", "QuantizedHardshrink"], [84, 0, 1, "", "QuantizedHardsigmoid"], [85, 0, 1, "", "QuantizedHardswish"], [86, 0, 1, "", "QuantizedHardtanh"], [87, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [88, 0, 1, "", "QuantizedHuberLoss"], [89, 0, 1, "", "QuantizedInstanceNorm1d"], [90, 0, 1, "", "QuantizedInstanceNorm2d"], [91, 0, 1, "", "QuantizedInstanceNorm3d"], [92, 0, 1, "", "QuantizedKLDivLoss"], [93, 0, 1, "", "QuantizedL1Loss"], [94, 0, 1, "", "QuantizedLPPool1d"], [95, 0, 1, "", "QuantizedLPPool2d"], [96, 0, 1, "", "QuantizedLSTM"], [97, 0, 1, "", "QuantizedLSTMCell"], [98, 0, 1, "", "QuantizedLayerNorm"], [99, 0, 1, "", "QuantizedLeakyReLU"], [100, 0, 1, "", "QuantizedLinear"], [101, 0, 1, "", "QuantizedLocalResponseNorm"], [102, 0, 1, "", "QuantizedLogSigmoid"], [103, 0, 1, "", "QuantizedLogSoftmax"], [104, 0, 1, "", "QuantizedMSELoss"], [105, 0, 1, "", "QuantizedMarginRankingLoss"], [106, 0, 1, "", "QuantizedMaxPool1d"], [107, 0, 1, "", "QuantizedMaxPool2d"], [108, 0, 1, "", "QuantizedMaxPool3d"], [109, 0, 1, "", "QuantizedMaxUnpool1d"], [110, 0, 1, "", "QuantizedMaxUnpool2d"], [111, 0, 1, "", "QuantizedMaxUnpool3d"], [112, 0, 1, "", "QuantizedMish"], [113, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [114, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [115, 0, 1, "", "QuantizedMultiMarginLoss"], [116, 0, 1, "", "QuantizedNLLLoss"], [117, 0, 1, "", "QuantizedNLLLoss2d"], [118, 0, 1, "", "QuantizedPReLU"], [119, 0, 1, "", "QuantizedPairwiseDistance"], [120, 0, 1, "", "QuantizedPixelShuffle"], [121, 0, 1, "", "QuantizedPixelUnshuffle"], [122, 0, 1, "", "QuantizedPoissonNLLLoss"], [123, 0, 1, "", "QuantizedRNN"], [124, 0, 1, "", "QuantizedRNNCell"], [125, 0, 1, "", "QuantizedRReLU"], [126, 0, 1, "", "QuantizedReLU"], [127, 0, 1, "", "QuantizedReLU6"], [128, 0, 1, "", "QuantizedReflectionPad1d"], [129, 0, 1, "", "QuantizedReflectionPad2d"], [130, 0, 1, "", "QuantizedReflectionPad3d"], [131, 0, 1, "", "QuantizedReplicationPad1d"], [132, 0, 1, "", "QuantizedReplicationPad2d"], [133, 0, 1, "", "QuantizedReplicationPad3d"], [134, 0, 1, "", "QuantizedSELU"], [135, 0, 1, "", "QuantizedSiLU"], [136, 0, 1, "", "QuantizedSigmoid"], [137, 0, 1, "", "QuantizedSmoothL1Loss"], [138, 0, 1, "", "QuantizedSoftMarginLoss"], [139, 0, 1, "", "QuantizedSoftmax"], [140, 0, 1, "", "QuantizedSoftmax2d"], [141, 0, 1, "", "QuantizedSoftmin"], [142, 0, 1, "", "QuantizedSoftplus"], [143, 0, 1, "", "QuantizedSoftshrink"], [144, 0, 1, "", "QuantizedSoftsign"], [145, 0, 1, "", "QuantizedTanh"], [146, 0, 1, "", "QuantizedTanhshrink"], [147, 0, 1, "", "QuantizedThreshold"], [148, 0, 1, "", "QuantizedTripletMarginLoss"], [149, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [150, 0, 1, "", "QuantizedUnflatten"], [151, 0, 1, "", "QuantizedUnfold"], [152, 0, 1, "", "QuantizedUpsample"], [153, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [154, 0, 1, "", "QuantizedUpsamplingNearest2d"], [155, 0, 1, "", "QuantizedZeroPad1d"], [156, 0, 1, "", "QuantizedZeroPad2d"], [157, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[30, 1, 1, "", "compute_encodings"], [30, 1, 1, "", "forward"], [30, 1, 1, "", "from_module"], [30, 1, 1, "", "get_default_kernel"], [30, 1, 1, "", "get_kernel"], [30, 1, 1, "", "implements"], [30, 3, 1, "", "input_quantizers"], [30, 3, 1, "", "output_quantizers"], [30, 3, 1, "", "param_quantizers"], [30, 1, 1, "", "set_default_kernel"], [30, 1, 1, "", "set_kernel"], [30, 1, 1, "", "wrap"]], "aimet_torch.nn.QuantizedLinear": [[100, 1, 1, "", "forward"]], "aimet_torch.onnx": [[177, 2, 1, "", "export"]], "aimet_torch.quant_analyzer": [[214, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[214, 1, 1, "", "analyze"], [214, 1, 1, "", "check_model_sensitivity_to_quantization"], [214, 1, 1, "", "export_per_layer_encoding_min_max_range"], [214, 1, 1, "", "export_per_layer_mse_loss"], [214, 1, 1, "", "export_per_layer_stats_histogram"], [214, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [214, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[158, 0, 1, "", "DequantizedTensor"], [159, 0, 1, "", "QuantizedTensor"], [160, 0, 1, "", "QuantizedTensorBase"]], "aimet_torch.quantization.DequantizedTensor": [[158, 1, 1, "", "dequantize"], [158, 1, 1, "", "quantize"], [158, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[159, 1, 1, "", "dequantize"], [159, 1, 1, "", "quantize"], [159, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[160, 1, 1, "", "clone"], [160, 1, 1, "", "dequantize"], [160, 1, 1, "", "detach"], [160, 1, 1, "", "new_empty"], [160, 1, 1, "", "quantize"], [160, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[161, 0, 1, "", "Quantize"], [162, 0, 1, "", "QuantizeDequantize"], [163, 2, 1, "", "dequantize"], [164, 2, 1, "", "quantize"], [165, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[161, 1, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[162, 1, 1, "", "forward"]], "aimet_torch.quantization.float": [[166, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[166, 4, 1, "", "bitwidth"], [166, 1, 1, "", "compute_encodings"], [166, 4, 1, "", "exponent_bits"], [166, 1, 1, "", "forward"], [166, 1, 1, "", "from_encodings"], [166, 1, 1, "", "get_encodings"], [166, 1, 1, "", "get_extra_state"], [166, 1, 1, "", "is_bfloat16"], [166, 1, 1, "", "is_float16"], [166, 1, 1, "", "load_state_dict"], [166, 4, 1, "", "mantissa_bits"], [166, 1, 1, "", "set_extra_state"]], "aimet_torch.seq_mse": [[208, 0, 1, "", "SeqMseParams"], [208, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[208, 1, 1, "", "forward_fn"], [208, 1, 1, "", "get_loss_fn"]], "aimet_torch.v1.adaround.adaround_weight": [[184, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[184, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[185, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[187, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[187, 1, 1, "", "analyze"], [187, 1, 1, "", "check_model_sensitivity_to_quantization"], [187, 1, 1, "", "export_per_layer_encoding_min_max_range"], [187, 1, 1, "", "export_per_layer_mse_loss"], [187, 1, 1, "", "export_per_layer_stats_histogram"], [187, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [187, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[188, 0, 1, "", "QuantizationSimModel"], [188, 1, 1, "", "load_checkpoint"], [188, 1, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[188, 1, 1, "", "compute_encodings"], [188, 1, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[189, 0, 1, "", "SeqMseParams"], [189, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[189, 1, 1, "", "forward_fn"], [189, 1, 1, "", "get_loss_fn"]], "aimet_torch.v2.mixed_precision": [[228, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[228, 1, 1, "", "apply"], [228, 1, 1, "", "set_model_input_precision"], [228, 1, 1, "", "set_model_output_precision"], [228, 1, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[170, 2, 1, "", "set_activation_quantizers_to_float"], [170, 2, 1, "", "set_blockwise_quantization_for_weights"], [170, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[212, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:attribute", "4": "py:property"}, "terms": {"": [2, 7, 8, 10, 12, 20, 21, 22, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 166, 170, 172, 174, 175, 179, 182, 185, 186, 187, 188, 189, 191, 194, 196, 197, 198, 199, 202, 204, 208, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 228, 229, 230, 232, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264, 265, 267, 268], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269], "00": [164, 165, 201, 202, 252, 253, 254], "000": [197, 265], "0000": [158, 165, 177, 180], "0000e": [164, 165], "001": [200, 237, 255], "0014807": 195, "00183112e": 201, "00215936e": 201, "0022e": 202, "0030": 180, "00317": 198, "0032": 180, "0034": 180, "00347728e": 202, "0035": 180, "0036": [162, 180], "0037": 180, "0038": 180, "0039": [162, 180], "0059": 180, "0063": 180, "0064": 180, "0068": 180, "0069": 180, "0073": 180, "0074": 180, "0078": 180, "0086": 196, "00_224": [197, 201, 202], "01": [11, 23, 164, 165, 184, 197, 199, 201, 202, 241, 243, 257], "0115": 162, "0117": 180, "01392324": 196, "0142": 160, "01457286e": [201, 202], "0156": 180, "0158": 162, "01675645": 196, "0176": 162, "0195": 180, "02": [164, 165, 201, 202], "02078857e": [201, 202], "0234": [180, 196], "0235": 199, "0244e": 202, "0252": 196, "026354755942277083": 195, "02635476": 195, "0271e": 202, "0273": 180, "0278355": 195, "02887694e": 202, "0293162": 195, "0295": 162, "03": [201, 202], "0303": 196, "0312": 180, "03513189": 196, "0352": 180, "03658897": 196, "0375e": 202, "03798249e": 201, "0386": 162, "0391": 180, "04": [193, 195, 196, 201, 202], "04025269e": 201, "0406616e": 202, "0422": 196, "0424": 162, "0428": 200, "0430": 180, "0469": 180, "0471": 162, "04721": 251, "05": [164, 165, 201, 202], "0500e": [164, 165], "0508": 180, "05270951": 195, "0541903": 195, "0549": 162, "05549544e": [201, 202], "0564": 162, "0597e": 202, "0600": 196, "0639": 162, "0667": 165, "0680": 162, "0680e": 202, "0784": 162, "07906426": 195, "08": [164, 165], "080545": 195, "0819": 162, "0820258": 195, "0859": 166, "0870": 196, "08742931e": [201, 202], "0882": 160, "0889": 166, "0891": 166, "0897725": 196, "08c17b8": 210, "09": 226, "0901e": 202, "09111059e": 201, "0932e": 202, "0947": 166, "09685047e": [201, 202], "0973e": 202, "0x7f127685a598": 173, "0x7f9dd9bd90d0": 173, "0x7ff5703eff28": 173, "0x7ff577373598": 173, "1": [0, 2, 12, 13, 17, 20, 21, 23, 24, 25, 29, 30, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 194, 195, 196, 202, 205, 206, 207, 215, 216, 218, 219, 220, 222, 223, 245, 265, 269], "10": [9, 17, 20, 29, 30, 158, 161, 162, 164, 172, 173, 175, 180, 181, 188, 193, 195, 196, 197, 200, 215, 216, 218, 219, 220, 222, 226, 229, 230, 232, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 258, 260, 261, 269], "100": [2, 12, 14, 26, 171, 185, 191, 195, 200, 202, 226, 244, 258], "1000": [8, 21, 160, 179, 187, 197, 214, 216, 220, 222, 229, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "10000": [1, 11, 197, 237, 238, 239, 240, 242, 250, 255, 256], "1000e": [164, 165], "102": 180, "1024": [197, 226, 229, 245], "103": [161, 180], "10541902": 195, "10569119e": [201, 202], "106": 161, "1060": 162, "1068997": 195, "107": 180, "10708203e": [201, 202], "108": 180, "1080": 196, "109": 161, "10984787": 196, "10k": [1, 23, 184, 197], "11": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 254, 263, 264, 265, 266, 267, 268, 269], "110": [161, 180], "1108": 196, "111": [161, 180], "11176670e": 202, "112": [197, 201, 202], "1128": 162, "11446196": 196, "1176": 162, "118": 180, "11899511": 196, "119": 180, "12": [164, 180, 194, 195, 210, 215], "12039044e": 202, "122": 161, "1228e": 202, "1232": 162, "127": [159, 160, 180, 181, 229, 230], "1279": 196, "128": [2, 13, 25, 159, 160, 170, 172, 174, 180, 181, 185, 186, 195, 197, 199, 201, 202, 208, 215, 226, 229, 230, 245, 247, 248], "129": 161, "13": [164, 180, 269], "1307": 162, "131": 161, "13137": [198, 204, 210], "1316e": 202, "13177378": 195, "1333": 165, "1377e": 202, "1398": 219, "14": [164, 180, 210, 269], "1406": 219, "141": 161, "143": 161, "144": 161, "145": 161, "1450607": 196, "146": 161, "1489e": 202, "1493fe1": 210, "15": [164, 165, 180, 200, 210, 219, 244, 245, 247, 248, 258, 260, 261], "150": 161, "1500": [24, 198], "15000": 197, "1500e": [164, 165], "152": 161, "15259957e": 202, "153": 159, "155": 161, "15717569e": 201, "15812853": 195, "15e": [252, 254], "15k": [1, 23, 184, 197], "16": [2, 12, 22, 30, 161, 166, 172, 174, 175, 177, 178, 180, 181, 185, 188, 196, 197, 198, 199, 202, 204, 206, 207, 209, 215, 220, 226, 229, 230, 237, 241, 244, 255, 264, 268, 269], "1609": 196, "162": 161, "16245179e": [201, 202], "16406": [183, 209], "1647": 162, "1676": 196, "16839484e": 202, "16966406e": 202, "17": [161, 180, 202, 269], "1706e": 202, "1709": 162, "172": 161, "1727": 166, "1729": 166, "1741": 162, "178": 161, "17871511e": 202, "179": 161, "18": [180, 237, 238, 239, 240, 269], "181": 161, "18136823e": 201, "18448329": 195, "186": 161, "18673885e": 201, "187": 161, "188": 161, "1889": 162, "19": [161, 180, 195, 201, 202, 269], "1906": 251, "19186290e": [201, 202], "192": 161, "1921e": [164, 165], "194": 161, "1943": 219, "1945": 160, "1955": 219, "1977": 162, "19778645e": 201, "1b": [204, 209], "1e": [20, 200, 201, 202, 229, 230, 245, 246], "1k": [197, 199, 200, 226], "1m": [238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 202, 203, 205, 206, 207, 211, 212, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 227, 232, 233, 235, 245, 263, 265, 266, 267, 269], "20": [10, 11, 20, 23, 160, 177, 182, 184, 189, 197, 200, 201, 202, 206, 207, 208, 220, 243, 244, 245, 246, 247, 248, 258, 260, 261, 269], "200": [20, 171, 245, 246, 257], "2000": [23, 159, 165, 184, 197, 242, 243, 250, 256, 257], "20000": [20, 245, 246], "2000e": [164, 165], "2012": [229, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "2014": 219, "2016": 219, "2017": 219, "2018e": 202, "2019": [239, 251, 259], "2029": 196, "203": 161, "20433941e": [201, 202], "2048": [197, 199, 200, 204, 209, 230], "205": 159, "2050e": 202, "207": 161, "20722957": 196, "20k": 245, "21": [160, 177, 201, 202, 269], "21066449e": 201, "21083805": 195, "2118": 162, "2123188": 195, "21250950e": 201, "2137995": 195, "216": 161, "218": 161, "2196": 162, "22": [180, 193, 194, 195, 196, 201, 202, 269], "2205": 160, "2212": 162, "22219264e": [201, 202], "224": [181, 196, 197, 199, 200, 201, 202, 208, 213, 214, 220, 226, 228, 229, 230, 237, 238, 239, 240, 241, 244, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "225": [197, 199, 208, 226, 229, 238, 257], "22583652e": 201, "2259": 196, "226": 161, "2260": 160, "2265": 162, "22884297": 196, "229": [197, 199, 208, 226, 229, 238, 257], "2298e": 202, "22b5c94": 210, "23": [159, 160, 269], "23011987e": [201, 202], "2306": 198, "2308": [198, 204, 210], "23156421e": [201, 202], "2353": 162, "2355": 162, "2363": 162, "237": 161, "23719281": 195, "23799022": 196, "238": 161, "24": 269, "240": 180, "2401543": 195, "2405": [183, 209], "241": 180, "242": 180, "24257803e": 201, "243": 180, "244": 180, "2443e": 202, "245": 180, "2458": 162, "246": 180, "24665177": 196, "247": 180, "248": 180, "249": 180, "2494": 196, "25": [180, 269], "250": 180, "2500e": [164, 165], "251": 180, "252": 180, "253": 180, "254": 180, "2546": 162, "255": [158, 161, 180, 181, 229, 230], "256": [8, 180, 197, 199, 208, 214, 226, 229, 238, 241, 255], "2568": 162, "2592": 173, "26": [159, 180, 269], "26179108e": 201, "26354757": 195, "2650282": 195, "2667": 165, "27": 269, "27045077": 196, "27415752e": 202, "2771": 162, "28": [159, 160, 216, 220, 222, 250, 269], "28065038": 196, "28238320e": 202, "288": [197, 202], "28990233": 195, "29": [180, 269], "291383": 195, "2921": 160, "2930528e": 202, "29590677e": [201, 202], "2998e": 202, "2b": 217, "2c8ae88": 210, "2d": [215, 216, 219, 223, 250], "2d4e0eb": 210, "2ed8305": 210, "2f05175": 210, "2x": 210, "3": [2, 12, 13, 25, 29, 158, 160, 164, 165, 169, 171, 172, 173, 174, 178, 180, 181, 182, 185, 186, 189, 193, 195, 196, 202, 206, 215, 216, 217, 220, 222, 223, 228, 245, 269], "30": [161, 180, 269], "300": 255, "3000": [159, 245], "30122258e": [201, 202], "3038": 162, "30402938e": 202, "31": [8, 11, 21, 23, 179, 181, 184, 187, 188, 197, 214, 229, 230, 269], "31080866e": 201, "312": 159, "3137": 162, "31625706": 195, "3178": 196, "31ca7fd": 210, "32": [20, 161, 172, 173, 180, 181, 188, 191, 197, 198, 199, 200, 201, 202, 208, 214, 224, 226, 229, 230, 232, 234, 237, 238, 239, 241, 242, 243, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 269], "3209": 196, "32141271e": 201, "3216": 162, "3258": 196, "33": [161, 252, 253, 254, 269], "3333": 165, "33731920e": [201, 202], "339a225": 210, "34": 269, "34215236e": 202, "34261182": 195, "3434e": 202, "3435e": 202, "3451": 162, "3467390e": 202, "34694423e": 202, "347054": 195, "3470540046691895": 195, "3479e": 202, "35": [181, 229, 230, 269], "350m": [206, 207], "35107604": 196, "35139937e": 202, "35536635": 196, "3576329": 196, "3587": 196, "35ad990": 210, "36": 161, "3603": 196, "3657e": 202, "36678016": 196, "3687": 196, "36896658": 195, "37": [160, 161, 180], "3706": 196, "3724": 196, "3734087606114667": 195, "3750526": 210, "37757687e": 202, "3792e": 202, "38": [159, 160, 219], "38100997e": [201, 202], "3851556": 196, "39": 158, "3938e": 202, "39502389e": [201, 202], "3992": 162, "3c92bb7": 210, "3d": 215, "4": [2, 7, 8, 11, 13, 21, 23, 25, 159, 160, 164, 165, 170, 171, 172, 177, 178, 179, 180, 181, 184, 186, 187, 188, 196, 201, 202, 206, 207, 215, 216, 218, 220, 222, 223, 224, 226, 230, 240, 245, 249, 252, 253, 254, 257, 262, 269], "40": [159, 160, 204, 209], "4000": [158, 165], "406": [197, 199, 208, 226, 229, 238, 257], "4082": 196, "4094e": 202, "41": 158, "41059163e": 202, "4130": 196, "4132449": 196, "4157": 162, "41715762e": [201, 202], "4186": 196, "42": 180, "42083430e": 201, "4216761": 195, "4231569": 195, "4236": 196, "4246376": 195, "42477691e": 201, "43": 180, "43178225": 196, "43477735": 196, "4392": 162, "4404": 196, "44408584e": [201, 202], "44632760e": 202, "4475": 162, "44803086": 195, "4495116": 195, "44993666e": 202, "4503196": 196, "45040053": 196, "4549": 162, "455": [193, 195, 196], "456": [197, 199, 208, 226, 229, 238, 257], "4578e": 202, "4585028e": 202, "4599525": 196, "45c2a65": 210, "46642041e": 202, "4667": 165, "46723792": 196, "4677236": 196, "4686e": 202, "4694": 162, "4706": 162, "47438562": 195, "4758663": 195, "4784": 162, "48": 180, "48045555e": 202, "48399768e": [201, 202], "485": [197, 199, 208, 226, 229, 238, 257], "4863": 162, "4881": 196, "49": [161, 180], "49024737e": [201, 202], "4933": 196, "4943e": 202, "499df9f": 210, "4ad0703": 210, "4b94ca9": 210, "4d": 215, "4f": [199, 200, 229, 230], "4febdd4": 210, "5": [160, 161, 162, 164, 165, 166, 171, 172, 175, 178, 180, 193, 195, 196, 200, 201, 202, 215, 216, 217, 220, 222, 226, 230, 239, 240, 241, 242, 245, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261, 269], "50": [217, 220, 226, 237, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "500": [197, 214, 216, 226, 229, 238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 256, 257, 259, 260, 261, 262, 265], "5000": [243, 257], "50000": [197, 220, 226, 238, 241], "5000e": [164, 165], "5006": 221, "50074035": 195, "5022211": 195, "50366503": 196, "5091695": 196, "50f35dd": 210, "51": 158, "512": [206, 207], "51446089e": [201, 202], "51547194e": [201, 202], "5176e": 202, "51876003e": 201, "52": 158, "5203": 196, "521": 158, "5220": 162, "5255": 162, "52709514": 195, "52912678e": [201, 202], "52974629e": 201, "5315115": 196, "5317543": 196, "5333": [162, 165], "53403008e": [201, 202], "53976866e": [201, 202], "54": 180, "5430": 196, "54620111e": [201, 202], "55": [161, 180], "55344987": 195, "5540": 162, "5549306": 195, "55578518e": 202, "56632766e": 201, "56810045": 196, "5695": 162, "57": [159, 160, 180], "57021021e": 202, "57980466": 195, "57984132e": [201, 202], "5803": 196, "5825": 196, "5856506e": 202, "5876": 200, "5897": 162, "59": [161, 180], "59643137e": 201, "5985": 196, "5996e": 202, "5aac9c5": 210, "5e": [253, 254, 258, 260, 261], "6": [20, 160, 164, 165, 172, 180, 201, 202, 215, 226, 229, 244, 245, 246, 247, 248, 254, 256, 259, 260, 261, 269], "60": 180, "6000": [158, 159, 160, 165], "6014": 196, "6039": 162, "6054": 162, "6061594": 195, "60713138e": [201, 202], "6079": 162, "60808927": 196, "61": 180, "61087702e": 201, "6169e": 202, "6177": 162, "6196": 162, "62": 180, "6208e": 202, "6213797e": 202, "62274104": 196, "6247": 162, "62498877e": [201, 202], "63": 161, "63172388e": 201, "6325141": 195, "64": [7, 8, 20, 161, 170, 180, 181, 193, 195, 196, 197, 198, 199, 214, 215, 226, 229, 230, 232, 250], "6431": 162, "64579314e": [201, 202], "6463e": 202, "65": 217, "654f4b1": 210, "6583": 230, "6588689": 195, "66": [161, 217, 252, 253, 254], "6603496": 195, "6618e": 202, "6636515": 210, "6667": 165, "6695": 162, "67278278e": [201, 202], "67416465e": 202, "67510852e": [201, 202], "67677957e": [201, 202], "68": 158, "68016": 230, "68522364": 195, "6867044": 195, "6910": 230, "69716495e": 202, "6998855": 196, "6c92a97": 210, "6c9f584": 210, "6d1664c": 210, "6d3aa97": 210, "6f670a4": 210, "7": [160, 164, 165, 166, 180, 181, 201, 202, 223, 229, 230, 253, 254, 256, 258, 259, 260, 261, 269], "70": 180, "70029c5": 210, "7013": 229, "70130579e": 202, "70838": 230, "71": 160, "7115784": 195, "7119e": 202, "7159e": 202, "7164": 199, "71659231e": [201, 202], "7173": 229, "72468403e": 202, "73242160e": 201, "7333": 165, "7336ead": 210, "7364b37": 210, "73793316": 195, "74": 161, "74478185e": 201, "75": 217, "75162792e": 202, "75700430e": [201, 202], "76": 180, "76428795": 195, "77": 180, "77213307e": 201, "7741": 162, "7765": 162, "7894": 162, "7932": 162, "7978e": 202, "8": [1, 2, 7, 8, 12, 13, 21, 22, 23, 25, 30, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 179, 180, 181, 184, 185, 186, 187, 188, 191, 194, 195, 196, 197, 199, 200, 201, 202, 208, 214, 215, 216, 220, 222, 223, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258, 259, 260, 261, 262, 269], "80": [161, 220], "800": [176, 204], "8000": [162, 165], "80053532e": 201, "8009871": 210, "8078": 162, "80cd141": 210, "81699747": 195, "81760912e": 201, "81884710e": 201, "8229": 162, "83": [161, 180], "8347e": 202, "83640555e": 202, "8365e": 202, "836ab1": 210, "83861901e": [201, 202], "84": 180, "8433522": 195, "84edcf5": 210, "86": 161, "864": [197, 201, 202], "8667": 165, "86945379e": 201, "8706": 162, "8711877": 195, "87471542e": [201, 202], "87630311e": [201, 202], "87656835e": 201, "8789e": 202, "8796": 162, "88260233e": [201, 202], "8836": 162, "88373709e": 202, "88706ef": 210, "89": 161, "89074164e": 202, "89249423e": [201, 202], "89348674e": 201, "8945e": 202, "8984": 166, "8994": 166, "8998": 166, "8bit": 210, "9": [158, 164, 165, 172, 180, 200, 201, 202, 220, 226, 229, 252, 253, 254, 255, 256, 260, 261, 262, 269], "90": 226, "90229788e": [201, 202], "9086e": 202, "91": 180, "911af75": 210, "9157": 162, "9176": 162, "92": 180, "9216": 172, "92511864e": [201, 202], "92f63f5": 210, "93": 180, "93232973e": [201, 202], "9333": 165, "93787616e": 201, "94": [161, 180], "9487": 162, "94877124": 195, "9490": 162, "95": 180, "95088911e": [201, 202], "95260113e": 201, "9570": 180, "9585e": 202, "95997976e": [201, 202], "96": [180, 198], "9609": 180, "96155685e": 201, "9648": 180, "9688": 180, "97": 158, "9700": 162, "9727": 180, "97294299e": [201, 202], "9766": 180, "9805": 180, "9844": 180, "9883": 180, "9922": 180, "9961": [162, 180], "9999": 177, "9a2a407": 210, "9b8c655": 210, "A": [2, 3, 8, 9, 12, 13, 14, 15, 17, 21, 22, 23, 25, 27, 28, 29, 170, 174, 178, 179, 181, 184, 185, 186, 187, 188, 190, 191, 197, 199, 200, 201, 202, 206, 208, 209, 214, 215, 216, 218, 219, 220, 222, 224, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267], "And": [237, 250, 255, 267], "As": [30, 197, 199, 209, 214, 215, 217, 218, 233, 237, 239, 241, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 258, 259, 260, 261], "At": [209, 217, 221, 264], "But": [172, 219, 237, 244, 245, 249, 250, 255, 258, 262], "By": [9, 13, 17, 20, 29, 175, 178, 181, 197, 199, 216, 219, 220, 222, 229, 230, 244, 255, 256, 258, 259, 260, 261, 265], "For": [8, 20, 21, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 172, 173, 175, 177, 178, 179, 180, 187, 192, 193, 195, 196, 197, 199, 202, 209, 210, 213, 214, 215, 216, 217, 218, 219, 224, 226, 228, 229, 230, 231, 232, 233, 236, 237, 241, 244, 245, 246, 249, 250, 255, 258, 262, 264, 265, 267, 268], "If": [1, 2, 8, 9, 12, 13, 17, 20, 21, 22, 23, 25, 29, 30, 100, 161, 162, 163, 164, 165, 166, 171, 172, 173, 174, 175, 177, 179, 180, 181, 184, 185, 186, 187, 188, 197, 198, 199, 200, 204, 206, 208, 210, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "In": [20, 30, 160, 171, 172, 173, 175, 181, 185, 188, 196, 197, 199, 200, 201, 206, 207, 209, 214, 215, 217, 218, 219, 221, 223, 226, 227, 228, 229, 230, 231, 232, 233, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267], "It": [1, 2, 6, 7, 8, 9, 20, 171, 172, 181, 188, 197, 198, 199, 204, 208, 210, 211, 213, 214, 216, 218, 223, 224, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262], "Its": 223, "NOT": [13, 25, 186, 199, 250], "No": [173, 201, 267], "Not": [8, 21, 179, 187, 198, 200, 204, 208, 214, 215, 218, 228, 230, 236, 237, 244, 249, 250, 258, 262], "ON": 194, "ONE": 195, "OR": 171, "Of": [247, 248, 250, 252, 253, 254, 260, 261], "On": [237, 250, 255], "One": [21, 22, 214, 229, 230, 239, 240, 241, 249, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 265], "Or": [22, 172, 188, 213, 214, 219, 229, 230], "Such": 172, "That": [223, 238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "The": [1, 2, 4, 6, 7, 8, 9, 10, 12, 14, 17, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 177, 178, 180, 181, 182, 183, 184, 185, 186, 188, 189, 191, 192, 195, 197, 198, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "Then": [8, 179, 187, 202, 214, 219, 260, 261, 264], "There": [169, 171, 173, 188, 199, 213, 214, 230, 237, 238, 241, 242, 244, 245, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 267, 268], "These": [13, 22, 25, 171, 173, 175, 186, 188, 197, 199, 204, 206, 207, 214, 216, 217, 229, 230, 231, 234, 237, 239, 243, 245, 250, 255, 257, 259, 264, 265], "To": [2, 12, 20, 30, 167, 171, 174, 175, 178, 185, 192, 195, 197, 198, 200, 201, 202, 204, 208, 209, 214, 215, 218, 220, 221, 224, 226, 228, 229, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268], "With": [158, 193, 195, 230, 246, 249, 262], "_": [8, 9, 20, 21, 30, 161, 162, 165, 168, 175, 179, 180, 181, 187, 192, 195, 197, 198, 199, 200, 201, 202, 206, 208, 210, 212, 213, 214, 215, 224, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267, 268], "__________________________________________________________________________________________________": [197, 201, 202], "__getitem__": 257, "__init__": [13, 20, 25, 172, 173, 175, 186, 199, 204, 209, 210, 226, 232, 233, 246, 257], "__iter__": [199, 204, 209, 226], "__len__": [8, 204, 209, 214, 257], "__next__": [199, 209, 226], "__quant_init__": [30, 175], "__torch_function__": 172, "__version__": 196, "_batch_index": 199, "_create_sampled_data_load": 257, "_current_iter": 226, "_data": [199, 226], "_dataset": 257, "_default_forward_fn": [13, 25, 186, 199], "_default_r1_fusion_func": 209, "_default_rmsnorm_linear_pairs_func": 209, "_encodingmismatchinfo": 229, "_get_unlabled_data_load": 214, "_infer_activation_dtyp": 210, "_int": 210, "_is_encoding_frozen": 171, "_max": 166, "_module_to_wrap": 171, "_not_specifi": [181, 229, 230], "_q": 210, "_quantizationsimmodelinterfac": [179, 185, 187, 188, 214], "_quantschemepair": [13, 25, 186, 199], "_remove_input_quant": 171, "_remove_output_quant": 171, "_remove_param_quant": 171, "_step": [164, 165], "_tie_quant": 210, "_unlabel": 226, "a2adae2": 210, "a967b8f": 210, "ab": [198, 204, 251], "abe8782": 210, "abil": [210, 268], "abl": [8, 20, 25, 158, 159, 160, 172, 173, 186, 199, 214, 239, 240, 242, 243, 244, 246, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 264], "about": [158, 190, 192, 197, 202, 224, 238, 239, 240, 241, 242, 243, 246, 247, 248, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 268], "abov": [172, 174, 214, 215, 216, 219, 220, 221, 222, 223, 228, 237, 241, 249, 250, 252, 253, 254, 255, 262, 264, 265], "absolut": [161, 162, 216, 220, 222, 226], "absolute_path_to_workspac": 236, "absorpt": [239, 251, 259], "abstract": [30, 160, 175], "ac05d10": 210, "acc": [199, 214, 243, 244, 245, 249], "acc_top1": 199, "acccuraci": 241, "acceler": [181, 191, 210, 216, 219, 220, 222, 229, 240, 245, 247, 248, 260, 261], "accept": [210, 218, 226, 229, 234, 264, 267, 268], "access": [171, 209, 210, 245, 255, 256, 259, 260, 261], "accord": [9, 169, 181, 213, 229, 230], "accordingli": 210, "account": [24, 198, 210, 218, 234], "accumul": [217, 219, 220, 222, 225], "accur": [197, 201, 245], "accuraci": [2, 10, 12, 13, 17, 22, 23, 25, 29, 178, 184, 185, 186, 191, 192, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 225, 226, 227, 229, 230, 234, 235, 244, 245, 249, 257, 258, 262, 265], "achiev": [180, 191, 193, 217, 226, 227, 228, 238, 242, 250, 256, 264, 267, 268], "acronym": 192, "across": [172, 191, 195, 202, 203, 214, 264, 265], "act": [2, 8, 12, 21, 174, 179, 185, 187, 214, 226, 237], "action": [214, 223], "activ": [2, 8, 9, 12, 15, 20, 21, 22, 24, 168, 170, 172, 174, 175, 178, 179, 182, 185, 187, 189, 191, 193, 194, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264, 265, 267, 268], "activation_bitwidth": [197, 224, 229, 230], "activation_encod": 224, "activation_quant": [2, 226], "activation_typ": [9, 196, 197, 208, 229, 237, 238, 239, 240], "activations_pdf": [214, 249, 262], "actual": [211, 214, 237, 241, 244, 246, 255, 258], "acuraci": [226, 241], "ad": [173, 178, 205, 209, 210, 224, 231, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261], "ada": 238, "ada_model": [242, 250, 256], "ada_round_data": [242, 250], "ada_rounded_model": 197, "adam": [214, 243, 244, 245, 247, 248, 249], "adamw": [206, 207], "adapt": [23, 26, 178, 182, 184, 189, 191, 199, 200, 204, 205, 206, 207, 208, 210, 214, 236, 244, 250, 258, 264], "adapter1": 178, "adaptiveavgpool1d": 31, "adaptiveavgpool2d": 32, "adaptiveavgpool3d": 33, "adaptivemaxpool1d": 34, "adaptivemaxpool2d": 35, "adaptivemaxpool3d": 36, "adaptiveround": 210, "adaptor": [176, 204], "adaround": [0, 1, 13, 18, 25, 167, 171, 186, 191, 197, 199, 210, 214, 234, 236, 243, 257, 264], "adaround_data_load": [199, 257], "adaround_dataset": 243, "adaround_dataset_s": [199, 243, 257], "adaround_param": [13, 25, 186, 199, 243, 257], "adaround_weight": [11, 23, 171, 184, 197, 199, 242, 243, 250, 256, 257], "adarounded_model": 197, "adaroundparamet": [11, 13, 23, 25, 184, 186, 197, 199, 242, 243, 250, 256, 257], "adascal": [0, 167, 210], "adascale_model_config_dict": 198, "adascale_optim": 198, "adascalemodelconfig": 198, "add": [20, 22, 172, 173, 175, 178, 188, 210, 223, 224, 229, 230, 231, 232, 233, 240, 244, 247, 248, 252, 253, 254, 260, 261, 265, 267, 268], "add_adapt": 178, "add_check": 173, "add_lora_to_r": 207, "addit": [30, 180, 181, 188, 200, 210, 215, 224, 229, 230, 231, 237, 244, 245, 246, 249, 250, 255, 258, 262, 263, 264], "address": [202, 221, 234], "adequ": 267, "adher": 268, "adjac": [191, 201, 202, 203, 209, 231, 237, 238, 239, 240, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261], "adjust": [168, 197, 203, 206, 207, 212, 215, 216, 217, 230, 234, 251, 264], "admin": 236, "advanc": [180, 191, 193, 210, 235], "advantag": [230, 264], "ae981f7": 210, "affect": [23, 184, 191, 197, 215, 231, 238, 268], "affin": [7, 30, 158, 159, 160, 161, 162, 163, 164, 165, 170, 175, 201, 202, 206, 215], "affine_q": 171, "affine_qdq": 171, "affine_quant": 171, "affinequant": [166, 171], "after": [1, 8, 21, 166, 172, 173, 175, 179, 180, 187, 188, 191, 197, 199, 200, 201, 202, 209, 210, 214, 217, 219, 221, 229, 230, 234, 237, 238, 239, 241, 242, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264], "afterward": [24, 198], "again": [239, 250, 251, 252, 253, 254, 259, 268], "against": [8, 13, 21, 179, 187, 199, 214, 216, 247, 248, 250, 258, 260, 261], "aggress": 191, "agre": 268, "ahead": 235, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 264, 265, 266, 267, 268, 269], "aim": [226, 244, 252, 253, 254, 258, 264, 268], "aimet": [5, 17, 18, 20, 29, 30, 167, 171, 172, 173, 175, 178, 180, 191, 195, 197, 198, 199, 200, 201, 202, 203, 204, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 225, 226, 227, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 268], "aimet_cl": 251, "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269], "aimet_common_def": 220, "aimet_export_artifact": 213, "aimet_exported_model": 263, "aimet_exported_model_path": 263, "aimet_omniquant_artifact": [176, 204], "aimet_onnx": [0, 195, 196, 197, 201, 202, 208, 210, 213, 214, 224, 226, 229, 237, 238, 239, 240], "aimet_spatial_svd": 220, "aimet_tensorflow": [0, 195, 197, 199, 200, 201, 202, 213, 214, 220, 224, 226, 229, 230, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "aimet_tensorflow_def": 220, "aimet_torch": [0, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 220, 222, 224, 226, 228, 229, 230, 237, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "algo": [185, 226], "algorithm": [2, 12, 17, 29, 185, 216, 217, 218, 219, 220, 222, 223, 224, 234, 264], "alia": [9, 22, 181, 188, 229, 230], "aliasbackward0": [158, 159, 160, 161, 162, 177, 180, 196], "align": [210, 229, 231], "all": [0, 1, 2, 3, 7, 8, 9, 12, 14, 15, 20, 21, 22, 27, 30, 100, 167, 168, 170, 172, 173, 174, 175, 179, 180, 181, 182, 185, 187, 188, 189, 194, 195, 196, 197, 198, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 218, 219, 220, 222, 224, 226, 228, 229, 230, 231, 232, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264, 265, 267], "all_q_modul": 171, "all_quant_wrapp": 171, "allclos": 172, "allow": [2, 12, 13, 17, 20, 22, 25, 29, 158, 159, 160, 170, 172, 177, 185, 186, 199, 210, 211, 215, 216, 219, 220, 221, 222, 224, 226, 229, 230, 233, 234, 237, 241, 243, 245, 255, 257, 264], "allow_custom_downsample_op": [29, 216, 252, 254], "allow_overwrit": [171, 197, 207, 229], "allowed_accuracy_drop": [2, 12, 13, 25, 185, 186, 199, 226, 237, 241, 243, 255, 257], "alon": [178, 265], "along": [159, 178, 180, 226, 264, 265], "alpha": [202, 226], "alphadropout": 37, "alreadi": [28, 30, 180, 181, 188, 202, 206, 209, 210, 226, 229, 230, 234, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264], "also": [11, 13, 22, 23, 160, 172, 180, 184, 185, 186, 187, 188, 189, 194, 195, 197, 198, 199, 201, 204, 210, 211, 214, 215, 216, 219, 223, 224, 226, 228, 229, 230, 231, 233, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 267], "alter": [242, 250, 256], "altern": [8, 179, 187, 214, 215, 237, 241, 252, 253, 254, 255], "alwai": [206, 207, 210, 218, 226], "am": [8, 21, 179, 187, 214], "among": [13, 23, 25, 184, 186, 197, 199, 267], "amount": [208, 226, 249, 262], "amp": [0, 2, 12, 13, 25, 167, 174, 185, 186, 199, 210, 226, 227, 236], "amp_search_algo": [2, 12, 185, 226, 237, 255], "ampsearchalgo": [2, 12, 185, 226, 237, 255], "an": [8, 12, 13, 17, 20, 22, 25, 29, 30, 158, 159, 160, 166, 168, 170, 171, 172, 173, 175, 180, 181, 186, 188, 191, 194, 197, 199, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268], "analys": [214, 249, 262], "analysi": [2, 8, 12, 17, 21, 29, 179, 185, 187, 210, 216, 219, 220, 222, 237, 241, 245, 252, 253, 254, 255], "analyt": 221, "analyz": [8, 17, 21, 29, 161, 162, 175, 179, 187, 195, 199, 203, 210, 216, 220, 221, 222, 228, 236, 243, 257], "analyze_per_layer_sensit": 210, "anchor": [148, 149], "andrea": 219, "andrei": 219, "andrew": 219, "ani": [2, 8, 9, 12, 17, 20, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 172, 173, 174, 179, 181, 184, 185, 186, 187, 188, 195, 197, 198, 199, 200, 206, 210, 214, 215, 216, 220, 222, 226, 228, 229, 230, 231, 232, 233, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 267], "anneal": [11, 23, 184, 197], "anoth": [178, 181, 188, 191, 222, 223, 224, 226, 229, 230, 231, 245], "anyth": [2, 12, 185, 226, 237, 241, 249, 255, 262], "api": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 168, 169, 170, 171, 174, 176, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 206, 207, 210, 211, 221, 224, 232, 233, 236, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263], "app": 190, "appear": [17, 29, 172, 173, 216, 220, 222], "appli": [1, 2, 7, 8, 13, 16, 25, 30, 100, 161, 162, 163, 164, 165, 170, 173, 174, 175, 182, 183, 186, 189, 191, 192, 195, 197, 198, 199, 201, 202, 203, 204, 206, 208, 209, 210, 214, 215, 219, 221, 225, 226, 230, 231, 234, 236, 237, 244, 245, 252, 253, 254, 255, 257, 258, 264, 265, 267, 268], "applic": [2, 12, 185, 197, 199, 200, 201, 202, 214, 220, 226, 229, 230, 237, 238, 241, 242, 243, 247, 248, 249, 250, 251, 255, 256, 259, 263, 268], "apply_adaround": [0, 5, 11, 23, 184, 197, 210, 238, 242, 250, 256], "apply_adascal": [24, 198], "apply_omniqu": [176, 204], "apply_seq_ms": [0, 5, 171, 182, 189, 208, 210], "apply_spinqu": [183, 209], "approach": [174, 206, 208, 228, 230, 264, 265], "appropri": [23, 175, 181, 184, 185, 188, 195, 197, 206, 207, 218, 226, 229, 230, 234, 237, 241, 244, 245, 249, 250, 255, 258, 262, 264, 265], "approx": 265, "approxim": [8, 208, 214, 217, 265, 268], "apt": 195, "ar": [2, 7, 8, 9, 11, 12, 17, 20, 21, 22, 23, 25, 29, 30, 100, 161, 162, 166, 169, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 193, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "arang": [164, 165, 172, 180], "arbitrari": 215, "architectur": [191, 195, 209, 210, 246], "area": [214, 225], "aren": 194, "arg": [9, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 170, 174, 175, 177, 181, 194, 210, 215, 226, 228, 229, 230, 246], "argmax": [197, 199, 208, 226, 229, 230, 238], "argument": [2, 9, 12, 13, 22, 23, 24, 25, 30, 166, 170, 172, 173, 174, 176, 177, 179, 181, 182, 184, 185, 186, 187, 188, 189, 194, 197, 198, 199, 204, 208, 213, 214, 215, 224, 226, 229, 230, 243, 244, 249, 262, 263], "around": [193, 194, 210, 214, 245, 250], "arrai": [170, 196, 215, 224], "arrang": 221, "art": [190, 193, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262], "arthmet": [20, 246], "artifact": [9, 174, 178, 210, 227, 228, 229, 245, 263, 267], "arxiv": [183, 198, 204, 209, 210, 251], "asic": 191, "ask": [30, 237, 241, 244, 245, 249, 255, 258, 262], "assert": [171, 172, 180, 229, 246, 255], "assert_array_equ": 246, "assess": 218, "assign": [30, 161, 162, 166, 175, 215, 247, 248, 264], "associ": [2, 12, 13, 15, 25, 30, 160, 173, 174, 175, 180, 181, 185, 186, 199, 201, 226, 249, 262, 263], "assum": [7, 13, 25, 170, 186, 199, 215, 218], "astyp": 214, "asym": [182, 189, 208], "asymmetr": [161, 162, 180, 231, 265], "att": [20, 246], "attent": [20, 245, 246], "attention_mask": [204, 206, 207, 209], "attn_output": [20, 246], "attribut": [12, 20, 30, 171, 172, 174, 175, 180, 185, 209, 210, 214, 226, 250], "attributeerror": 210, "augment": 221, "auto": [17, 29, 216, 220, 222, 224, 237, 241, 252, 253, 254, 255], "auto_param": [216, 220, 222, 252, 253, 254], "auto_qu": [199, 243, 257], "auto_quant_v2": [0, 18, 199], "autoconfig": [204, 206, 207, 209], "autograd": [158, 159, 160], "autom": [172, 191, 233, 255, 256, 260, 261, 262], "automat": [13, 17, 25, 29, 174, 186, 191, 210, 214, 215, 216, 217, 219, 220, 222, 225, 236, 243, 257], "automodelforcausallm": [206, 207], "automodeparam": [17, 29, 216, 220, 222, 252, 253, 254], "autoqu": [0, 5, 13, 25, 167, 186, 191, 199, 210, 236], "autoquantwithautomixedprecis": [13, 25, 186, 199], "autotoken": [204, 206, 207, 209], "avail": [172, 177, 182, 184, 185, 186, 187, 188, 189, 194, 195, 208, 210, 214, 231, 234, 237, 238, 239, 240, 241, 244, 249, 250, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 266], "avgpool1d": 38, "avgpool2d": [39, 228], "avgpool3d": 40, "avoid": [8, 22, 179, 187, 188, 191, 197, 210, 214, 217, 229, 230, 232, 233, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262], "awai": [230, 250], "awar": [30, 191, 193, 197, 200, 203, 219, 234, 236, 265, 268], "axi": [180, 197, 199, 208, 210, 214, 224, 226, 229, 238, 249, 262, 265], "b": [30, 161, 162, 163, 164, 165, 180, 215, 220, 226, 241], "b1415bd": 210, "b47a97": 210, "b5521f3": 210, "b_": [161, 162, 163, 164, 165], "b_0": [161, 162, 163, 164, 165], "b_1": [161, 162, 163, 164, 165, 215], "b_2": 215, "b_d": [161, 162, 163, 164, 165], "b_n": 215, "back": [2, 12, 158, 159, 174, 179, 185, 187, 214, 226, 231, 265], "backend": [9, 181, 188, 229, 230, 244, 263, 267], "backpropag": [158, 159, 160], "backslash": 210, "backward": [0, 167, 200, 201, 206, 207, 210, 229, 230], "bad": 20, "balanc": [191, 202, 219, 225, 264, 268], "bandwidth": 217, "bar": 210, "base": [2, 8, 21, 30, 160, 161, 162, 164, 165, 166, 171, 175, 177, 178, 179, 180, 185, 187, 195, 198, 204, 205, 207, 208, 210, 214, 215, 216, 217, 218, 221, 226, 227, 229, 230, 237, 241, 242, 243, 244, 250, 255, 256, 257, 258, 263, 264, 267], "baselin": [2, 12, 185, 218, 219, 226, 230, 244, 245, 257], "basi": [210, 250], "basic": [210, 226, 230, 237, 241, 244, 245, 255, 258, 263, 264], "batch": [2, 8, 11, 13, 14, 15, 21, 23, 25, 26, 166, 174, 182, 184, 185, 186, 189, 191, 197, 198, 199, 202, 204, 206, 207, 208, 209, 210, 214, 220, 226, 229, 230, 241, 243, 244, 252, 253, 254, 258], "batch_cntr": [199, 237, 239, 240, 242, 247, 248, 250, 251, 256, 258, 259, 260, 261, 262], "batch_data": 226, "batch_id": [204, 206, 207], "batch_idx": 208, "batch_norm": [3, 14, 15, 27, 200, 201], "batch_norm_fold": [0, 5, 14, 18, 167, 200, 201, 226, 230, 237, 238, 239, 240, 241, 242, 244, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261], "batch_siz": [23, 184, 197, 198, 199, 200, 201, 204, 206, 207, 208, 209, 214, 220, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "batchnorm": [13, 15, 20, 25, 26, 186, 197, 199, 200, 201, 202, 210, 223, 236, 237, 238, 239, 240, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261], "batchnorm1d": [27, 43, 201], "batchnorm2d": [27, 44, 173, 201, 202], "batchnorm3d": 45, "batchnrom": 258, "bb93c76": 210, "bc": [239, 259], "bceloss": 41, "bcewithlogitsloss": 42, "becaus": [20, 172, 215, 226, 246, 258, 267], "becom": [210, 227], "becuas": [20, 213, 246], "been": [0, 20, 30, 158, 167, 168, 173, 191, 196, 206, 210, 212, 223, 224, 243, 244, 245, 250, 257, 263, 264, 265], "befor": [1, 2, 8, 9, 20, 24, 30, 168, 171, 175, 180, 197, 198, 199, 200, 201, 202, 205, 206, 207, 209, 210, 212, 213, 214, 219, 226, 229, 230, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 265, 267], "begin": [20, 161, 162, 163, 164, 165, 172, 173, 192, 246], "behav": [30, 100, 175, 210, 234], "behavior": [171, 172, 175, 201, 210, 231, 237, 250, 255, 256, 259, 260, 261, 265, 267], "behind": 267, "being": [12, 29, 171, 172, 173, 179, 187, 200, 210, 214, 215, 216, 220, 222, 224, 226, 232, 233], "below": [20, 30, 161, 162, 164, 165, 171, 174, 175, 178, 180, 194, 195, 196, 206, 207, 209, 213, 215, 216, 218, 219, 221, 224, 226, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "benefici": [237, 241, 244, 245, 249, 250, 255, 262], "benefit": [208, 215, 224, 230, 264, 265], "bert": 234, "besid": 215, "bespok": 206, "best": [13, 25, 174, 186, 193, 199, 203, 217, 219, 228, 235, 243, 250, 257, 265], "beta": [0, 11, 23, 167, 184, 197, 198, 210], "better": [199, 219, 226, 230, 238, 242, 247, 248, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 267], "between": [2, 8, 11, 17, 21, 23, 29, 169, 171, 174, 175, 179, 184, 185, 187, 191, 197, 202, 203, 208, 211, 213, 214, 215, 216, 220, 222, 226, 229, 231, 237, 250, 255, 264, 265, 268], "bfloat16": 166, "bia": [30, 166, 171, 172, 173, 175, 177, 178, 181, 201, 202, 209, 210, 216, 229, 230, 231, 239, 244, 251, 259], "bias": [201, 237, 239, 251, 255, 259], "bilinear": 46, "billion": [267, 268], "bin": [194, 263], "binari": [2, 12, 185, 226, 237, 255, 263], "binary_classifi": 20, "binary_fil": 263, "binary_file_nam": 263, "bit": [1, 2, 8, 12, 21, 23, 166, 170, 177, 179, 180, 184, 185, 187, 191, 193, 195, 196, 197, 206, 210, 214, 215, 224, 225, 226, 227, 230, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 265, 267, 268], "bitop": [2, 12, 185, 226], "bitwidth": [1, 2, 7, 8, 11, 12, 13, 21, 22, 23, 25, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 174, 175, 179, 180, 181, 184, 185, 186, 187, 188, 191, 195, 197, 199, 202, 206, 210, 214, 215, 224, 226, 229, 230, 237, 241, 242, 250, 255, 256, 265, 267], "biwidth": [242, 250, 256], "bkd": [24, 198, 203, 204], "blankevoort": 219, "block": [7, 25, 161, 162, 163, 164, 165, 170, 171, 176, 186, 198, 199, 204, 206, 207, 210, 215, 224, 225, 245, 265], "block_group": [170, 215], "block_siz": [7, 161, 162, 163, 164, 165, 170, 180, 215, 224], "blockwis": [7, 170, 180, 198, 203, 204, 210, 224, 265], "bn": [3, 14, 15, 26, 27, 191, 200, 201, 203, 210, 214, 237, 238, 239, 240, 242, 244, 247, 248, 250, 251, 255, 256, 259, 260, 261], "bn1": [20, 173], "bn2": 20, "bn_conv": 210, "bn_conv1": [197, 201, 202], "bn_num_batch": [14, 200], "bn_re_estimation_dataset": [14, 200], "bn_reestim": [0, 167, 200, 244, 258], "bnf": [197, 200, 201, 202], "bokeh": 226, "bool": [2, 9, 12, 13, 17, 22, 25, 29, 161, 162, 164, 165, 166, 170, 172, 173, 174, 177, 181, 185, 186, 188, 199, 215, 216, 220, 222, 224, 226, 228, 229, 230, 252, 253, 254, 256, 258, 259, 260, 261, 262], "boolean": [30, 174, 209, 228], "both": [20, 21, 164, 165, 171, 172, 175, 179, 187, 193, 195, 196, 201, 205, 214, 219, 220, 221, 223, 226, 229, 230, 231, 234, 237, 241, 248, 254, 255, 260, 261, 263, 264, 265, 268], "bottleneck": 234, "box": [210, 235], "bq": [170, 210, 215], "branch": [172, 231, 236], "break": [199, 204, 206, 207, 208, 210, 220, 226, 229, 237, 239, 240, 241, 242, 247, 248, 250, 251, 256, 258, 259, 260, 261, 262], "bridg": 227, "brief": 225, "british": 219, "broad": 264, "broken": [205, 224], "browser": 236, "bruteforc": [12, 226, 237, 255], "buffer": 166, "bug": [210, 224], "bugfix": 210, "build": [9, 20, 171, 210, 229, 246], "build_sess": 210, "buildx": 194, "built": [2, 30, 174, 175, 185, 194, 195, 226, 244, 245], "bw": [2, 7, 12, 25, 170, 174, 181, 185, 186, 188, 199, 210, 215, 224, 226, 229, 230, 233], "bw_output": 233, "c": [180, 194, 217], "c96894f": 210, "c_": 180, "cach": [2, 12, 13, 25, 176, 185, 186, 199, 204, 210, 226, 237, 241, 255], "cache_id": [13, 25, 186, 199], "calcul": [2, 12, 21, 22, 25, 175, 185, 186, 188, 199, 205, 206, 210, 214, 218, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 262, 265], "calculate_and_fuse_encodings_into_weight": 206, "calibr": [8, 9, 21, 161, 162, 168, 175, 178, 179, 181, 187, 188, 191, 196, 206, 210, 211, 212, 225, 226, 227, 230, 234, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 265, 267], "calibration_batch": 226, "calibration_callback": [206, 207], "calibration_data": 196, "calibration_data_load": [175, 229], "calibration_dataset": [197, 200, 229, 230], "calibration_dataset_s": [199, 243, 257], "calibration_wrapp": [204, 209], "call": [2, 8, 12, 13, 14, 20, 24, 25, 30, 100, 158, 159, 160, 166, 172, 174, 175, 179, 180, 185, 186, 187, 198, 199, 200, 210, 214, 215, 216, 220, 222, 226, 228, 229, 232, 238, 239, 240, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 259, 260, 261, 262, 263, 267, 268], "call_funct": 172, "callabl": [2, 8, 9, 12, 13, 17, 23, 24, 25, 26, 29, 30, 170, 172, 173, 174, 176, 179, 181, 182, 184, 185, 186, 187, 189, 197, 198, 199, 200, 204, 208, 214, 215, 216, 220, 222, 226, 229, 230], "callal": [2, 185, 226], "callback": [2, 8, 12, 13, 17, 21, 22, 29, 174, 179, 185, 187, 188, 199, 206, 216, 220, 222, 226, 230, 241, 243, 245, 247, 248, 249, 252, 253, 254, 262, 265], "callbackfunc": [2, 12, 21, 174, 179, 185, 187, 214, 226, 237, 241, 249, 255, 262], "callbak": [237, 255], "can": [2, 6, 7, 8, 9, 10, 11, 12, 19, 20, 22, 23, 27, 28, 158, 160, 161, 162, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 184, 185, 187, 188, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 204, 205, 206, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "candid": [2, 10, 12, 13, 17, 25, 29, 174, 182, 185, 186, 189, 198, 199, 204, 208, 210, 216, 218, 219, 220, 222, 226, 237, 241, 255, 264], "cannot": [161, 162, 172, 173, 237, 238, 239, 240, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261], "capabl": [175, 193, 195, 196, 221], "captur": [6, 19, 169, 172, 181, 211, 213, 229, 230, 238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "card": [193, 195, 196], "care": 219, "carefulli": 191, "carri": [158, 159, 160], "case": [164, 165, 170, 171, 172, 173, 175, 196, 197, 205, 209, 210, 214, 215, 218, 228, 231, 233, 235, 238, 245, 246, 249, 262, 268], "cast": 166, "cat": 180, "catch": 210, "categor": [197, 199, 200, 220, 226, 229, 230, 241, 242, 247, 248, 250], "categorical_crossentropi": [247, 248], "categoricalaccuraci": [197, 199, 200, 214, 229, 230, 243, 244, 249], "categoricalcrossentropi": [197, 199, 200, 214, 229, 230, 243, 244, 249], "caus": [20, 210, 219, 226, 231, 234, 252, 253, 254], "caution": 197, "cbe67a": 210, "cd": [194, 236], "cdot": [161, 162, 163, 164, 165], "ce68e75": 210, "ceil": [23, 184, 197, 199, 226], "cell": [237, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "celu": 47, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "center_crop": [226, 241], "centercrop": [197, 199, 208, 226, 229, 238, 255, 257], "certain": [172, 179, 187, 210, 214, 219, 224, 226, 228, 231, 233, 255, 256, 259, 260, 261, 262], "chain": [204, 206, 207, 209], "challeng": [237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "chang": [2, 12, 24, 160, 168, 171, 172, 178, 180, 181, 185, 197, 198, 200, 201, 202, 204, 208, 209, 210, 212, 214, 219, 223, 224, 226, 227, 228, 229, 230, 231, 233, 237, 238, 239, 240, 241, 244, 245, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 267], "channel": [7, 8, 17, 21, 29, 170, 175, 179, 187, 191, 194, 200, 201, 202, 203, 210, 214, 215, 217, 218, 220, 222, 223, 224, 225, 231, 234, 236, 237, 238, 239, 240, 251, 253, 255, 256, 258, 259, 260, 261, 262, 265], "channel_index": [8, 21, 179, 187, 214], "channel_index_0": [249, 262], "channel_index_1": [249, 262], "channel_index_n": [249, 262], "channel_prun": [29, 216, 220, 222, 252, 254], "channel_pruning_auto_mod": 216, "channel_pruning_manual_mod": 216, "channelpruningparamet": [29, 216, 220, 222, 252, 254], "channelshuffl": 49, "characterist": [237, 255, 258, 262], "chart": 234, "check": [20, 25, 172, 173, 180, 186, 192, 196, 199, 210, 211, 213, 233, 237, 241, 244, 245, 249, 255, 258, 262], "check_model_sensitivity_to_quant": [8, 21, 179, 187, 214], "checker": [173, 210], "checkpoint": [182, 188, 189, 208], "checkpoints_config": [182, 189, 208], "chipset": 267, "choic": [195, 214, 219, 224, 249, 250, 262, 265], "choos": [188, 191, 197, 198, 204, 208, 210, 213, 216, 217, 219, 226, 231, 237, 241, 252, 253, 254, 255], "choose_fast_mixed_precis": [12, 226, 241], "choose_mixed_precis": [2, 12, 174, 185, 226, 237, 241, 255], "chose": 178, "chosen": [236, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262], "chunk": 265, "ci": 194, "cin": 180, "circularpad1d": 50, "circularpad2d": 51, "circularpad3d": 52, "cl": [30, 210, 239, 251, 259], "clamp": [161, 162, 164, 165, 166, 265], "class": [2, 6, 8, 9, 11, 12, 13, 17, 19, 20, 21, 22, 23, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 166, 169, 171, 172, 173, 174, 175, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 197, 199, 201, 202, 204, 206, 207, 208, 209, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 231, 232, 233, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "class_nam": [220, 226, 241, 246], "classif": [20, 191, 197, 219, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "classifi": 20, "classmethod": [9, 22, 30, 166, 181, 188, 229, 230], "cle": [4, 16, 28, 197, 199, 202, 210, 214, 234, 236], "cle_applied_model": [202, 251], "clean": [210, 226], "clean_start": [2, 12, 185, 226, 237, 241, 255], "clear_sess": 244, "clearli": 210, "clip": [198, 201, 204, 231, 244, 265], "clone": [160, 236, 245], "clone_lay": 245, "close": [217, 265], "closer": [238, 242, 250, 256], "cloud": [191, 267], "cmake": 194, "cmake_arg": 194, "cmp_re": 226, "cnn": 191, "cnt": [210, 220, 226, 241], "coars": 202, "code": [195, 197, 199, 202, 209, 213, 214, 215, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 258, 259, 260, 261, 262], "codebas": 195, "collate_fn": [204, 206, 207, 209], "collect": [1, 8, 10, 21, 25, 168, 179, 186, 187, 190, 191, 197, 199, 208, 211, 212, 214, 218, 229, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 259, 260, 261], "color": 223, "column": 218, "com": [195, 236], "combin": [2, 12, 172, 185, 191, 199, 210, 215, 217, 219, 226, 237, 241, 255], "come": [219, 227, 230, 264], "command": [195, 221, 236, 263], "common": [171, 180, 210, 215, 234, 237, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "commonli": [23, 180, 184, 191, 197, 229], "comp": [17, 29, 216, 220, 222], "comp_stat": [252, 253], "compar": [2, 12, 172, 185, 191, 198, 204, 214, 215, 226, 227, 230, 237, 241, 244, 245, 246, 249, 255, 258, 262, 264, 267], "comparison": [211, 213, 239, 240, 256, 259, 260, 261], "compat": [0, 167, 170, 193, 195, 196, 210, 215, 226, 229, 233, 238, 241, 244, 245], "compil": [9, 193, 197, 199, 200, 210, 214, 229, 230, 243, 244, 245, 247, 248, 249, 268], "complet": [2, 12, 185, 220, 226, 234, 237, 241, 252, 253, 254, 255, 263, 264], "complex": [22, 188, 214, 228, 229, 230, 245], "compli": [255, 256, 260, 261, 262], "complic": 210, "compon": [171, 180, 204, 210, 229], "compos": [174, 197, 199, 208, 226, 228, 229, 238, 255, 257], "comprehens": 264, "compress": [0, 18, 167, 191, 192, 210, 223, 224, 236], "compress_model": [17, 29, 216, 220, 221, 222, 252, 253, 254], "compress_schem": [17, 29, 216, 220, 222, 252, 253, 254], "compressed_bw": 224, "compressed_model": [216, 220, 222, 252, 253], "compressionschem": [17, 29, 216, 220, 222, 252, 253, 254], "compressionstat": [17, 29, 216, 220, 222], "compressor": [17, 29, 216, 220, 222], "compris": [204, 218, 237, 241, 255], "compromis": [252, 253, 254], "compuat": [13, 25, 186, 199], "comput": [2, 8, 9, 12, 13, 21, 22, 24, 25, 29, 30, 100, 166, 178, 179, 181, 182, 185, 186, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 199, 201, 202, 204, 206, 207, 208, 209, 210, 213, 214, 216, 219, 220, 221, 222, 223, 224, 226, 238, 239, 240, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 259, 260, 261, 262, 265, 267, 268], "computation": 221, "compute_encod": [9, 22, 24, 30, 159, 160, 161, 162, 166, 168, 171, 174, 175, 180, 181, 182, 188, 189, 196, 197, 198, 200, 204, 206, 207, 208, 209, 210, 212, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261], "computeencod": 195, "concat": 210, "concatenated_exampl": [204, 206, 207, 209], "concept": [171, 215], "concis": 224, "conclus": 264, "concret": 172, "concrete_arg": [25, 172, 186, 199], "concurr": 178, "conda_env_nam": 194, "conda_install_dir": 194, "condit": [172, 173, 200], "confer": 219, "config": [2, 8, 21, 22, 174, 178, 179, 181, 182, 185, 187, 188, 189, 200, 204, 206, 207, 208, 209, 210, 214, 226, 228, 229, 230, 231, 237, 250, 255], "config_fil": [8, 9, 11, 13, 21, 22, 25, 179, 181, 186, 187, 188, 197, 199, 200, 214, 228, 229, 230, 244, 249, 250, 262], "config_util": [0, 167, 215], "configur": [8, 9, 11, 13, 17, 21, 23, 25, 29, 166, 174, 179, 180, 181, 184, 186, 187, 188, 197, 199, 206, 207, 208, 210, 214, 215, 216, 220, 222, 224, 225, 226, 227, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 251, 255, 256, 258, 259, 260, 261, 264], "conflict": [174, 228], "conform": [229, 268], "conjunct": 265, "connect": [191, 197, 201, 202, 216, 219, 222], "connectedgraph": [173, 210], "consecut": [202, 226, 239, 251, 259], "consid": [24, 198, 220, 222, 226, 234, 245, 252, 254, 268], "consist": [169, 170, 171, 209, 210, 213, 215, 218, 237, 255, 265], "consol": 211, "constant": [172, 218, 240, 247, 260, 261], "constantpad2d": [53, 54], "constantpad3d": 55, "constrain": [191, 215], "constraint": [194, 224], "constrast": 230, "construct": [210, 213], "constructor": [6, 19, 169, 172, 213, 233], "consum": [199, 219, 224, 227, 243, 257], "consumpt": 264, "contain": [1, 2, 8, 12, 21, 29, 30, 158, 166, 168, 172, 173, 174, 175, 179, 185, 187, 191, 196, 197, 202, 212, 214, 216, 220, 222, 224, 226, 229, 231, 238, 239, 240, 242, 243, 247, 248, 249, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267], "content": 191, "context": [9, 30, 175, 263, 268], "contigu": [198, 204, 206, 207, 210], "continu": [2, 20, 173, 185, 188, 206, 210, 226, 234, 237, 255, 264], "contrast": [20, 171, 250], "contribut": [195, 214, 234], "control": [25, 172, 175, 180, 186, 199, 265], "conv": [3, 7, 11, 14, 15, 23, 27, 170, 172, 184, 197, 200, 201, 202, 210, 215, 220, 222, 223, 228, 231, 239, 244, 251, 259], "conv1": [7, 20, 172, 173, 181, 197, 201, 202, 216, 220, 222, 229, 230, 233, 244, 252, 253, 254], "conv1_relu": [197, 201, 202], "conv1d": [27, 56, 201, 210], "conv2": [20, 170, 172, 202, 215, 216, 220, 222, 233, 244], "conv2d": [15, 20, 27, 30, 57, 170, 171, 172, 173, 175, 181, 197, 201, 202, 210, 215, 216, 219, 223, 229, 230, 244, 245, 250], "conv2d_lay": 250, "conv2dnormactiv": [201, 202], "conv2dtranspos": [15, 201], "conv3d": 58, "conv_1": [20, 215, 246], "conv_2": [20, 246], "conv_weight": 201, "conv_weight_arrai": 201, "conv_weight_nam": 201, "conveni": 267, "convent": [210, 231], "converg": [191, 230, 264], "convers": [210, 229, 233, 264], "convert": [2, 12, 20, 158, 171, 172, 174, 181, 185, 191, 197, 199, 201, 202, 229, 230, 232, 241, 255, 263, 264, 268], "convert_to_pb": [22, 229, 230, 245], "convolut": [20, 170, 191, 200, 201, 202, 203, 215, 216, 217, 219, 220, 222, 223, 226, 234, 237, 238, 239, 240, 242, 246, 247, 248, 250, 251, 252, 254, 255, 256, 259, 260, 261], "convtranspos": 210, "convtranspose1d": [59, 210], "convtranspose2d": [27, 60, 201], "convtranspose3d": 61, "copi": [9, 20, 22, 160, 166, 188, 204, 206, 207, 209, 229, 230, 236, 238, 255, 256, 259, 260, 261, 265], "copy_": 171, "corp": 226, "correct": [13, 25, 186, 191, 199, 226, 228, 229, 230, 239, 250, 251, 257, 259, 262, 268], "correct_predict": [197, 208, 226, 229, 238], "correctli": 210, "correl": [226, 237, 255], "correspond": [2, 3, 6, 8, 11, 12, 14, 15, 19, 21, 23, 27, 166, 169, 173, 174, 175, 179, 181, 182, 184, 185, 187, 188, 189, 195, 197, 200, 201, 203, 208, 213, 214, 215, 216, 223, 226, 229, 230, 243, 250, 257, 264, 265], "cosine_similar": 177, "cosineembeddingloss": 62, "cosinesimilar": 63, "cost": [17, 29, 216, 218, 219, 220, 222, 225, 227, 230, 252, 254, 265], "cost_metr": [17, 29, 216, 220, 222, 252, 253, 254], "costmetr": [17, 29, 216, 220, 222, 252, 253, 254], "could": [22, 171, 188, 214, 223, 224, 229, 230, 233, 237, 238, 239, 240, 242, 244, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264], "count_param": 246, "counter": [22, 229, 230, 244, 258], "counterpart": [175, 264], "cours": [247, 248, 250, 252, 253, 254, 260, 261], "cout": 180, "cover": [237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 265], "cp": [216, 217, 218, 223], "cp310": 195, "cp_comp_stat": 254, "cpu": [28, 172, 181, 188, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 208, 209, 210, 213, 226, 228, 229, 230, 237, 238, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "cpuexecutionprovid": [9, 197, 208, 210, 229, 237, 238, 239, 240], "creat": [2, 8, 9, 12, 13, 17, 20, 22, 24, 25, 29, 30, 166, 168, 172, 174, 175, 178, 180, 185, 186, 188, 191, 193, 196, 197, 198, 199, 200, 204, 206, 208, 209, 210, 212, 213, 216, 219, 220, 222, 227, 228, 230, 236, 243, 245, 249, 252, 253, 254, 257, 262, 263, 265, 267, 268], "create_quantsim_and_encod": [8, 214], "creation": 210, "critic": 241, "crop_length": [226, 241], "cropped_imag": [226, 241], "cross": [4, 13, 16, 25, 28, 186, 197, 199, 214, 234, 236], "cross_layer_equ": [0, 5, 18, 167, 202, 239, 251, 259], "crossentropyloss": [64, 200, 206, 207, 230], "crosslayerequ": 210, "ctcloss": 48, "ctivations_pdf": [8, 21, 179, 187, 214], "cu118": 195, "cu121": 195, "cubla": 210, "cuda": [6, 193, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 213, 214, 216, 226, 228, 229, 230, 237, 238, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "cudaexecutionprovid": [197, 208, 229, 237, 238, 239, 240], "cudnn": [193, 194, 195, 196], "cudnn_conv_algo_search": [237, 238, 239, 240], "cumul": [243, 257], "curat": 192, "current": [12, 17, 20, 22, 24, 29, 30, 160, 166, 173, 174, 183, 185, 198, 204, 209, 210, 216, 220, 222, 226, 229, 230, 244, 245, 246, 255, 262, 269], "current_batch": 209, "curv": [2, 12, 185, 218, 226, 237, 241, 255], "custom": [9, 20, 22, 30, 172, 175, 191, 204, 209, 210, 224, 229, 230, 234, 237, 243, 245, 257, 265], "custom_function_not_to_be_trac": 172, "custom_object": [22, 229, 230], "customdataload": [199, 226], "customdataset": 198, "custommodel": 172, "custommodul": 172, "customsilu": 210, "cycl": 267, "d": [161, 162, 163, 164, 165, 194, 204, 209, 224], "d33e98c": 210, "dangl": 229, "dark": [237, 238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262], "data": [2, 8, 11, 12, 17, 21, 22, 23, 24, 25, 26, 29, 158, 159, 160, 168, 171, 172, 174, 176, 179, 181, 182, 184, 185, 186, 187, 188, 189, 191, 197, 198, 199, 200, 203, 204, 206, 207, 208, 210, 212, 214, 215, 216, 220, 222, 225, 226, 229, 230, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "data_load": [2, 23, 24, 25, 29, 168, 174, 182, 184, 185, 186, 189, 197, 198, 199, 200, 204, 208, 212, 216, 226, 229, 230, 237, 239, 240, 242, 243, 244, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262], "data_loader_wrapp": [12, 226, 241], "data_set": [11, 24, 197, 198, 242, 250], "data_typ": [25, 171, 181, 186, 188, 199, 210, 229, 230], "dataload": [2, 12, 23, 24, 25, 26, 174, 176, 182, 184, 185, 186, 189, 197, 198, 199, 200, 204, 206, 207, 208, 209, 214, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "dataloader_wrapp": [226, 241], "dataloader_wrapper_len": 204, "dataloadermnist": 216, "dataset": [2, 8, 9, 12, 13, 14, 21, 24, 25, 26, 179, 181, 185, 186, 187, 196, 197, 198, 199, 200, 206, 207, 208, 214, 220, 226, 229, 230, 236, 245, 265, 267], "dataset_dir": [220, 226, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "dataset_path": 229, "dataset_root": [197, 208], "dataset_train": [247, 248], "dataset_valid": [247, 248], "datasetv2": [11, 13, 14, 21, 197, 199, 200, 214], "datatyp": [13, 25, 186, 199, 224], "db99447": 210, "dc34fa4": 210, "de": [180, 265], "debian": 195, "debug": [171, 210, 211, 213, 224, 235, 241], "decai": 219, "decid": [182, 189, 198, 204, 208, 221, 226, 237, 238, 239, 240, 241, 242, 250, 252, 253, 254, 255, 256, 259, 260, 261, 262], "decim": [216, 220, 222, 252, 253, 254], "decis": 268, "declar": 30, "decode_predict": [220, 226, 241], "decompos": [219, 220, 222], "decomposit": [219, 220, 222, 253, 254], "decompress": [7, 170, 215], "decompressed_bw": [7, 170, 215], "decor": 30, "decreas": [10, 191, 208], "dedic": 191, "deem": [226, 267], "deep": [191, 192, 193, 201, 219], "deepcopi": 238, "deeper": 236, "deepseek": 204, "deepspe": 210, "def": [2, 9, 12, 17, 20, 22, 29, 30, 171, 172, 173, 174, 181, 185, 188, 196, 197, 198, 199, 200, 204, 206, 207, 208, 209, 214, 216, 220, 222, 226, 229, 230, 232, 233, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "default": [2, 8, 9, 11, 12, 13, 17, 20, 21, 22, 23, 25, 29, 30, 160, 161, 162, 164, 165, 166, 168, 172, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 187, 188, 189, 191, 196, 197, 199, 201, 202, 208, 210, 212, 214, 216, 218, 219, 220, 221, 222, 224, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 249, 250, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "default_activation_bw": [8, 214, 226], "default_beta_rang": [11, 23, 184, 197], "default_bitwidth": 226, "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "default_config_fil": [23, 184, 197], "default_config_per_channel": [231, 244], "default_data_col": [204, 206, 207, 209], "default_data_typ": [22, 181, 188, 229, 230], "default_forward_fn": [182, 189, 198, 208], "default_new": 178, "default_num_iter": [11, 23, 184, 197, 242, 250, 256, 257], "default_output_bw": [21, 22, 179, 181, 187, 188, 196, 197, 198, 200, 204, 206, 207, 208, 209, 214, 226, 228, 229, 230, 237, 239, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "default_param_bw": [8, 11, 21, 22, 23, 179, 181, 184, 187, 188, 196, 197, 198, 200, 204, 206, 207, 208, 209, 214, 226, 228, 229, 230, 237, 239, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "default_quant_schem": [11, 23, 184, 197, 242, 250, 256], "default_reg_param": [11, 23, 184, 197], "default_warm_start": [11, 23, 184, 197], "defer": 193, "defin": [2, 12, 20, 30, 172, 173, 175, 180, 185, 198, 204, 209, 210, 214, 216, 220, 222, 224, 226, 229, 231, 232, 233, 241, 246, 249, 250, 252, 253, 254, 256, 259, 260, 261, 262, 264, 265], "definit": [2, 12, 29, 30, 169, 171, 172, 174, 178, 185, 188, 213, 216, 220, 222, 226, 229, 232, 233, 237, 241, 244, 245, 249, 250, 255, 256, 259, 260, 261, 262, 265], "degrad": [238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261, 264, 267, 268], "degre": [220, 222], "deleg": 180, "delet": [2, 12, 185, 214, 226, 237, 241, 255], "deliber": [237, 241, 249, 250, 255, 262], "delta": [8, 20, 21, 179, 187, 195, 210, 214, 245, 246, 265], "demand": 191, "demonstr": [249, 262], "denable_cuda": 194, "denable_onnx": 194, "denable_tensorflow": 194, "denable_test": 194, "denable_torch": 194, "denot": [209, 220, 223, 224], "dens": [15, 20, 201, 210, 232, 244, 245, 246], "dep": 194, "depend": [158, 159, 160, 172, 195, 210, 217, 231, 235, 236, 250, 264], "deploi": [190, 265, 266, 267], "deploy": [190, 191, 192, 193, 229, 263, 264, 266, 268], "deprec": [181, 210, 229, 230], "depth": [197, 200, 202, 210, 234], "depthwis": 210, "depthwise_conv2d": 202, "depthwiseconv2d": [15, 201, 202], "dequant": [158, 159, 160, 162, 165, 166, 180, 191, 195, 210, 267, 268], "dequantizedtensor": [159, 160, 162, 180, 181, 196], "dequantizelinear": [9, 177, 210, 264], "deriv": [161, 162, 164, 165, 180, 197, 241, 255, 265], "descend": 166, "describ": [166, 171, 194, 195, 196, 209, 215, 219, 221, 224, 226, 231, 234, 250, 265, 266, 267, 268], "descript": [175, 224, 237, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "design": [173, 191, 197, 215, 236, 237, 241, 246, 249, 250, 255, 262, 264], "desir": [8, 22, 160, 179, 180, 181, 187, 188, 214, 217, 219, 220, 226, 227, 229, 230, 234, 237, 241, 252, 253, 254, 255, 263, 264, 268], "detach": [160, 177], "detail": [25, 172, 186, 192, 199, 215, 216, 218, 221, 224, 228, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 268], "detect": [219, 264], "determin": [8, 21, 22, 166, 170, 175, 179, 187, 188, 191, 198, 199, 203, 204, 206, 207, 208, 209, 214, 215, 224, 229, 230, 237, 245, 255, 257, 264], "determinist": 172, "dev": 194, "develop": [168, 190, 191, 193, 195, 212, 267], "devic": [6, 13, 23, 25, 160, 172, 181, 184, 185, 186, 188, 190, 191, 192, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 211, 213, 226, 228, 229, 230, 235, 237, 238, 239, 240, 244, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268], "diable_missing_quant": 229, "diagnost": 234, "diagram": [206, 207, 245], "dict": [1, 2, 8, 9, 10, 12, 17, 21, 22, 24, 25, 29, 166, 169, 172, 174, 175, 179, 185, 186, 187, 188, 197, 198, 199, 208, 213, 214, 216, 220, 222, 224, 226, 228, 229, 230, 233, 238], "dictat": 267, "dictionari": [8, 12, 17, 21, 29, 166, 179, 181, 187, 188, 209, 214, 216, 218, 219, 220, 222, 226, 229, 230, 231, 243, 244, 246], "dicuss": 246, "did": 250, "didn": 258, "diff": 210, "differ": [17, 29, 171, 172, 178, 191, 203, 205, 208, 215, 216, 218, 219, 220, 222, 224, 226, 227, 228, 229, 234, 237, 241, 244, 245, 250, 252, 253, 254, 255, 260, 261, 263, 264, 265, 268], "dim": [170, 180, 215, 255], "dimens": [7, 170, 175, 215, 220, 222, 224, 234, 265], "dir": [9, 229, 244, 249, 250, 251], "dir_path": [6, 169, 213], "direct": [1, 192, 197, 201, 210, 213, 215, 224, 226, 264, 265, 267, 268], "directli": [8, 14, 193, 196, 200, 206, 210, 214, 228, 229, 244, 258, 265, 267], "directori": [2, 6, 8, 9, 12, 13, 17, 19, 21, 25, 29, 169, 179, 185, 186, 187, 194, 197, 199, 200, 201, 202, 208, 213, 214, 216, 220, 222, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263], "directrori": 226, "disabl": [2, 8, 21, 171, 175, 178, 179, 181, 182, 187, 188, 189, 201, 208, 209, 214, 218, 219, 226, 229, 230, 231, 241, 252, 254, 265], "disable_missing_quant": 229, "discard": 214, "disciplin": 197, "discuss": [197, 241, 252, 253, 254], "disk": [6, 19, 169, 213], "displai": [212, 221, 236, 241, 245], "display_comp_ratio_plot": 221, "display_eval_scor": 221, "dist": 194, "distict": 210, "distil": [198, 203, 204], "distinct": 173, "distort": 265, "distribut": [17, 29, 191, 210, 216, 220, 222, 234, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 265], "divbackward0": 180, "dive": 236, "diverg": 191, "divid": [215, 217], "divis": [180, 191, 215], "dlc": 263, "dlc_path": 263, "dlcompress": 210, "dlequal": 210, "dlf": 191, "do": [9, 12, 20, 23, 172, 173, 178, 181, 184, 195, 197, 201, 204, 206, 209, 214, 219, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 246, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264, 265, 267, 268], "do_constant_fold": [201, 237, 239, 240], "do_not_trace_m": 172, "doc": [172, 178, 194, 210, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "dockerfil": 194, "docstr": 215, "document": [178, 195, 201, 210, 213, 215, 221, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 251, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "doe": [20, 30, 172, 185, 198, 204, 209, 210, 212, 214, 216, 218, 220, 222, 230, 233, 234, 237, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268], "doesn": [30, 229, 237, 239, 240, 242, 243, 251, 252, 253, 254, 255, 256, 259, 260, 261, 267], "doesnt": [174, 228], "don": [30, 172, 194, 206, 207, 214, 241, 244, 245, 249, 250, 262], "done": [161, 162, 180, 210, 219, 229, 231, 232, 241, 246, 250, 258], "down": [205, 224, 250], "down_proj": 209, "download": [192, 195, 229, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "downsampl": [252, 254], "downstream": [210, 223, 224, 264], "dq": 9, "dq_output": 30, "drastic": [218, 268], "draw": 264, "drawback": 265, "drift": 230, "driver": [193, 195, 196, 220], "drop": [2, 12, 13, 25, 175, 185, 186, 199, 210, 214, 217, 219, 225, 226, 230, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 265], "dropout": [20, 65, 245, 246], "dropout1": [20, 246], "dropout1d": 66, "dropout2": [20, 246], "dropout2d": 67, "dropout3d": 68, "dtype": [158, 159, 160, 166, 170, 171, 172, 174, 196, 204, 206, 207, 209, 210, 215, 224, 228], "due": [20, 173, 210, 215, 230, 264], "dummi": [8, 23, 25, 27, 28, 169, 173, 179, 181, 184, 185, 186, 187, 188, 197, 199, 201, 202, 206, 207, 213, 214, 229, 230, 237, 249, 255, 262], "dummy_attention_mask": [206, 207], "dummy_data": 214, "dummy_input": [8, 9, 23, 24, 25, 27, 28, 168, 169, 178, 179, 181, 184, 185, 186, 187, 188, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 212, 213, 214, 215, 226, 228, 229, 230, 233, 237, 238, 239, 240, 255, 256, 257, 258, 259, 260, 261, 262], "dummy_input_dict": 213, "dummy_input_id": [206, 207], "dummy_model": [198, 204], "dummymodel": [24, 198], "dump": [204, 210, 244], "duplic": [160, 172], "dure": [1, 2, 9, 10, 11, 12, 13, 17, 20, 23, 24, 25, 29, 30, 166, 168, 175, 184, 185, 186, 191, 192, 197, 198, 199, 208, 210, 211, 212, 216, 219, 220, 221, 222, 226, 229, 230, 231, 240, 242, 244, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 260, 261, 262, 264, 265, 268], "dynam": [166, 172, 191, 202, 210, 230, 265], "dynamic_ax": [197, 201, 208, 226, 229, 237, 238, 239, 240], "dynamo": 177, "e": [8, 9, 22, 176, 181, 188, 202, 203, 204, 214, 224, 226, 229, 230, 237, 238, 241, 244, 249, 250, 255, 258, 262, 264], "e78dbec": 210, "e7d10c7": 210, "each": [1, 2, 8, 9, 10, 11, 12, 20, 21, 23, 30, 168, 173, 175, 176, 179, 180, 184, 185, 187, 191, 197, 198, 201, 202, 204, 208, 209, 211, 212, 214, 215, 216, 217, 218, 219, 220, 224, 226, 229, 231, 234, 235, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265], "earli": [2, 12, 185, 226], "easi": [210, 228, 267], "easier": 171, "easili": [180, 216, 220, 222], "echo": 194, "ecosystem": 190, "ed": 197, "edg": [191, 192, 267], "edit": [210, 224, 237, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "effect": [11, 14, 22, 23, 26, 175, 184, 191, 197, 200, 203, 209, 214, 228, 229, 230, 240, 245, 247, 248, 260, 261, 267, 268], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 244, 258, 263, 264, 265, 266, 267, 268, 269], "efficientnetb4": 210, "effort": [13, 25, 174, 186, 199, 228, 243, 257, 264, 267, 268], "eigen": 194, "eights_pdf": [8, 21, 179, 187, 214], "either": [17, 29, 170, 174, 180, 209, 215, 216, 220, 222, 223, 226, 228, 233, 237, 242, 250, 252, 253, 254, 255, 264], "element": 224, "elementari": 245, "elementwis": [172, 175, 210], "elev": 264, "elimin": [191, 201, 214, 265], "els": [2, 12, 172, 173, 185, 196, 197, 198, 199, 200, 201, 202, 204, 208, 209, 220, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264], "elu": 69, "embed": [20, 70, 172, 177, 181, 188, 210, 219, 229, 230, 234, 245, 246], "embed_dim": [20, 245, 246], "embed_token": 209, "embedding_lay": [20, 246], "embeddingbag": 71, "embodi": 191, "empir": [243, 257, 264], "emploi": [191, 192], "empti": [7, 231], "emul": 265, "enabl": [2, 7, 8, 12, 13, 17, 21, 25, 29, 166, 171, 174, 178, 179, 182, 185, 186, 187, 189, 191, 193, 199, 200, 201, 203, 208, 209, 210, 213, 215, 216, 220, 221, 222, 226, 227, 228, 231, 241, 248, 250, 252, 254, 255, 261, 263, 265, 267], "enable_convert_op_reduct": [12, 174, 185, 226, 241, 255], "enable_onnx_check": [181, 188, 229, 230], "enable_per_layer_mse_loss": [8, 21, 214, 249, 262], "enbl": 215, "enc": 30, "enc_typ": 224, "encapsul": [2, 12, 174, 179, 185, 187, 214, 226], "encaptur": 178, "encod": [1, 2, 8, 9, 10, 11, 12, 13, 21, 22, 23, 24, 25, 30, 158, 159, 160, 161, 162, 166, 171, 174, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 195, 197, 198, 199, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 215, 226, 228, 230, 238, 239, 240, 242, 243, 244, 245, 247, 248, 250, 251, 256, 257, 259, 260, 261, 263, 264, 267], "encoding_analyz": [161, 162, 166], "encoding_file_path": [229, 230], "encoding_path": [197, 199, 242, 243, 250, 256, 257], "encoding_vers": [215, 224], "encodinganalyz": [161, 162, 166], "encodinganalyzerforpython": 195, "encodingbas": [158, 159, 166], "encodingmismatchinfo": 229, "encodingtyp": 224, "encount": [30, 245], "encourag": [20, 172, 232, 233], "end": [14, 24, 161, 162, 163, 164, 165, 172, 173, 192, 196, 198, 200, 219, 226, 229, 236, 244, 245, 247, 248, 252, 253, 254, 258, 260, 261, 264, 267], "end_beta": [11, 23, 184, 197], "end_idx": 199, "enforc": 166, "engin": [192, 201, 210, 213, 215, 224, 226, 264, 265, 267, 268], "enhanc": [8, 21, 178, 179, 187, 210, 214, 229, 249, 262, 265], "enough": [197, 241, 252, 253, 254, 268], "ensur": [175, 196, 200, 206, 207, 210, 213, 218, 226, 234, 250, 268], "enter": [30, 175, 199], "entir": [8, 21, 179, 180, 187, 214, 216, 219, 220, 222, 237, 238, 239, 240, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "entireti": [20, 250], "entri": [25, 181, 186, 188, 199, 224, 229, 230, 231], "enum": [2, 9, 12, 17, 23, 29, 169, 181, 184, 185, 188, 197, 213, 216, 220, 222, 224, 226, 229, 230, 237, 241, 244, 245, 255, 258], "enumer": [17, 29, 169, 204, 206, 207, 208, 213, 216, 220, 222, 226, 229, 246], "environ": [191, 226, 236, 237, 241, 243, 244, 247, 248, 255, 262], "ep": [201, 202, 210], "epoch": [200, 206, 207, 216, 219, 220, 222, 230, 238, 239, 240, 242, 243, 244, 245, 247, 248, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261], "epsilon": [20, 210, 245, 246], "equal": [4, 13, 16, 25, 28, 159, 160, 170, 180, 186, 197, 199, 214, 215, 217, 218, 234, 236, 242, 246, 265], "equalize_model": [4, 16, 28, 202, 239, 251, 259], "equat": [161, 162, 163, 164, 165, 215, 265], "equival": [25, 30, 164, 165, 166, 170, 174, 181, 185, 186, 188, 198, 199, 204, 209, 215, 226, 229, 230, 233, 237, 250, 255, 256, 259, 260, 261, 262, 263], "erorr": 241, "error": [1, 13, 20, 25, 30, 172, 180, 181, 186, 191, 197, 199, 202, 210, 211, 226, 228, 229, 230, 234, 241, 265], "especi": [191, 206, 230, 234, 237, 250, 255, 267], "essenti": [237, 241, 244, 245, 255, 258], "estim": [210, 236, 265, 267], "esults_dir": [8, 21, 179, 187, 214], "etc": [24, 191, 194, 198, 210, 217, 224, 226, 237, 241, 244, 245, 250, 255], "eval": [2, 8, 12, 13, 17, 21, 24, 25, 29, 172, 174, 179, 182, 185, 186, 187, 189, 196, 197, 198, 199, 200, 201, 202, 208, 214, 216, 218, 219, 220, 221, 222, 226, 228, 229, 230, 237, 238, 239, 240, 241, 244, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "eval_callback": [8, 13, 17, 21, 25, 29, 179, 186, 187, 199, 214, 216, 220, 222, 226, 243, 244, 249, 252, 253, 254, 257, 262], "eval_callback_factori": [237, 255], "eval_callback_fn": 214, "eval_callback_for_phase1": [2, 12, 185, 226, 237, 255], "eval_callback_for_phase2": [2, 12, 185, 226, 237, 255], "eval_callback_for_phase_1": 226, "eval_callback_for_phase_2": 226, "eval_callback_phase1": 241, "eval_callback_phase2": 241, "eval_data_load": [226, 229], "eval_dataset": [197, 199, 200, 214, 229, 230, 243, 244], "eval_dataset_s": [199, 243, 244, 257], "eval_func": [220, 226, 241, 249], "eval_iter": [17, 29, 216, 220, 222, 252, 253, 254], "eval_scor": [8, 17, 21, 29, 179, 187, 214, 216, 220, 222], "evalcallbackfactori": [2, 174, 185, 226, 237, 255], "evalfunct": 220, "evalu": [2, 8, 12, 13, 17, 21, 25, 29, 174, 179, 185, 186, 187, 196, 197, 198, 199, 200, 204, 208, 209, 216, 218, 219, 220, 221, 222, 226, 230, 236, 243, 245, 257, 264, 265, 268], "evaluate_accuraci": 255, "evaluate_model": [216, 220, 222], "even": [30, 180, 226, 228, 230, 237, 241, 244, 245, 250, 255], "evenli": 215, "eventu": 228, "everi": [6, 8, 11, 19, 21, 23, 169, 179, 180, 184, 187, 197, 198, 204, 211, 213, 214, 218, 219, 230, 237, 238, 239, 240, 241, 244, 245, 247, 248, 249, 250, 252, 253, 254, 255, 258, 260, 261, 262, 265], "evid": 264, "evlauat": 241, "ex": 178, "exactli": [13, 24, 30, 100, 166, 175, 198, 199, 249, 262, 265], "examin": 172, "exampl": [7, 9, 17, 24, 29, 30, 158, 159, 160, 161, 162, 164, 165, 166, 168, 170, 173, 174, 175, 177, 178, 180, 181, 182, 189, 191, 194, 196, 197, 198, 199, 201, 202, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 217, 223, 226, 228, 229, 230, 231, 232, 233, 234, 235, 245, 246, 257, 264, 265, 268], "exce": [168, 212], "exceed": [168, 212], "except": [20, 182, 189, 201, 202, 208, 210, 215, 226, 228, 237, 239, 240, 246, 249, 262], "exchang": 191, "exclud": [7, 25, 172, 173, 179, 182, 186, 187, 189, 199, 208, 210, 214, 224], "excluded_lay": 224, "excluded_nod": 7, "exclus": [166, 170, 215], "execut": [2, 9, 12, 13, 25, 172, 181, 185, 186, 188, 199, 201, 210, 221, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264], "exercis": [237, 244, 249, 250, 255, 258, 262], "exhibit": 264, "exist": [30, 100, 166, 175, 181, 188, 215, 226, 229, 230, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 267], "exist_ok": [237, 241, 242, 244, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "exit": [2, 12, 13, 25, 30, 175, 185, 186, 199, 226], "expand": [210, 215], "expand_dim": 199, "expanded_conv_depthwis": [197, 202], "expans": [215, 219], "expect": [2, 8, 13, 17, 21, 22, 23, 25, 29, 168, 172, 173, 174, 176, 179, 181, 182, 184, 185, 186, 187, 188, 189, 197, 199, 204, 208, 212, 214, 216, 219, 220, 222, 226, 228, 229, 230, 232, 237, 244, 252, 253, 254, 255, 258, 264], "experi": [17, 29, 170, 193, 195, 215, 216, 219, 220, 222, 238, 242, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261, 264, 268], "experiment": [0, 167, 193, 198, 204, 209, 210, 215, 219], "experss": [20, 246], "expert": [243, 257], "explain": [210, 215, 216, 219, 237, 241, 244, 245, 249, 255, 258, 262], "explan": [192, 238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "explicit": 210, "explicitli": [13, 25, 186, 199, 223], "explor": [226, 264], "expon": [166, 170, 215], "exponent_bit": [166, 170, 215], "export": [8, 9, 21, 22, 24, 25, 169, 171, 174, 178, 179, 181, 182, 186, 187, 188, 189, 193, 194, 196, 197, 198, 199, 200, 201, 202, 204, 208, 210, 211, 213, 214, 219, 224, 226, 227, 228, 230, 233, 236, 237, 238, 239, 240, 241, 242, 245, 247, 248, 250, 251, 255, 256, 257, 259, 260, 261, 263, 264], "export_int32_bia": 177, "export_model": [9, 178, 181, 188, 229, 230], "export_param": [237, 239, 240], "export_per_layer_encoding_min_max_rang": [8, 21, 179, 187, 214], "export_per_layer_mse_loss": [8, 21, 179, 187, 214], "export_per_layer_stats_histogram": [8, 21, 179, 187, 214], "export_to_torchscript": [181, 188, 229, 230], "exported_model": 215, "expos": [172, 197, 209, 214], "express": [17, 29, 206, 207, 216, 220, 222], "extend": [171, 210], "extens": [175, 236, 263], "extent": 264, "extern": 210, "extra": [166, 175, 194, 210], "extract": [191, 229, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "extrem": [237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 265], "ey": 214, "f": [172, 173, 177, 180, 194, 195, 196, 197, 199, 200, 201, 202, 208, 226, 229, 230, 238, 244, 257], "f0": 217, "f333188": 210, "f547a49": 210, "f961ed4": 210, "f9d0d6c": 210, "facebook": [191, 206, 207], "facilit": 192, "fact": 206, "factor": [202, 210, 215, 217, 219, 230, 244, 245, 247, 248, 252, 253, 254, 258, 260, 261], "factori": [2, 12, 174, 185, 226], "fail": [172, 173, 174, 181, 199, 201, 202, 210, 226, 228, 229, 230, 237, 239, 240], "failur": 210, "fair": 191, "fairli": [241, 244, 245, 249, 250, 262], "fake": [162, 165, 166, 174, 185, 210, 226, 229, 237, 238, 239, 240, 242, 244, 245, 247, 248, 250, 251, 255, 256, 257, 258, 259, 260, 261], "fakequ": [181, 188, 229, 230], "fall": [218, 231, 252, 253, 254, 268], "fallback": 263, "fals": [2, 7, 9, 12, 13, 17, 20, 22, 25, 29, 30, 158, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 177, 178, 180, 181, 185, 186, 188, 195, 197, 199, 201, 202, 204, 206, 207, 209, 213, 215, 216, 220, 222, 224, 226, 228, 229, 230, 231, 237, 239, 240, 241, 242, 244, 245, 247, 248, 250, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "famili": [191, 210, 228], "familiar": 236, "far": [244, 258], "farther": [238, 242, 256], "fashion": 250, "fast": [12, 194, 226], "faster": [210, 226, 237, 243, 244, 250, 255, 257, 264], "fastest": 193, "fc": [219, 222], "fc1": 172, "fc2": 172, "fe66376": 210, "fea395f": 210, "featur": [7, 20, 24, 168, 171, 172, 173, 177, 191, 192, 198, 199, 201, 202, 204, 209, 210, 212, 219, 221, 226, 228, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265], "featurealphadropout": 72, "feed": [20, 245, 246, 265], "feel": [228, 244, 245, 258], "feez": [182, 189, 208], "fefd504": 210, "few": [200, 217, 230, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "fewer": [219, 267], "ff_dim": [20, 245, 246], "ffn": [20, 246], "ffn_output": [20, 246], "field": [2, 166, 185, 224, 226, 237, 255], "figur": [197, 199, 216, 218, 220, 221, 222, 223, 234, 265], "file": [2, 8, 9, 11, 13, 17, 21, 22, 23, 25, 29, 174, 179, 181, 182, 184, 185, 186, 187, 188, 189, 194, 195, 197, 199, 200, 208, 210, 214, 215, 216, 220, 222, 224, 226, 228, 229, 230, 237, 239, 240, 243, 245, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267], "file_path": [188, 196, 197, 201, 202, 208, 226, 229], "filenam": [8, 9, 11, 22, 23, 181, 184, 188, 197, 198, 200, 204, 214, 229, 230, 237, 238, 239, 240], "filename_prefix": [9, 11, 22, 23, 181, 184, 188, 197, 198, 200, 204, 208, 226, 229, 230, 237, 238, 241, 242, 244, 250, 251, 255, 256, 257, 258, 259, 260, 261], "filename_prefix_encod": [178, 181, 188, 229, 230], "fill": [25, 181, 186, 188, 199, 229, 230], "filter": [20, 191, 250], "final": [17, 20, 29, 173, 182, 189, 206, 207, 208, 209, 214, 215, 216, 217, 218, 220, 222, 226, 234, 237, 241, 244, 245, 249, 255, 258, 262, 267], "find": [2, 12, 15, 173, 174, 182, 185, 188, 189, 193, 198, 201, 204, 208, 214, 218, 227, 228, 230, 236, 237, 238, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 265, 268], "fine": [17, 22, 29, 178, 188, 192, 216, 217, 220, 222, 229, 230, 238, 239, 240, 242, 243, 245, 247, 248, 251, 256, 257, 259, 260, 261, 264, 267, 268], "finer": [166, 180, 215], "finess": 268, "finetun": [238, 239, 242, 243, 244, 250, 251, 252, 253, 254, 256, 258, 260, 261], "finetuned_accuraci": [258, 260, 261], "finetuned_accuracy_bn_reestim": 258, "finetuned_model": [252, 253], "finish": [247, 248, 260, 261], "finit": 166, "first": [20, 23, 172, 175, 178, 180, 182, 184, 189, 197, 208, 210, 219, 220, 234, 244, 245, 246, 249, 250, 252, 253, 254, 258, 262, 267], "fit": [17, 29, 200, 216, 218, 220, 222, 230, 244, 245, 247, 248, 258, 267], "five": [239, 259], "fix": [173, 210, 224, 226, 230, 237, 238, 239, 240, 265], "flag": [2, 12, 13, 25, 168, 171, 172, 173, 174, 181, 185, 186, 188, 194, 199, 212, 226, 228, 229, 230, 237, 241, 255], "flatten": [73, 172, 206, 207, 224, 244], "flexibl": [237, 255], "flexround": 198, "flip": 264, "float": [2, 8, 11, 12, 13, 17, 21, 22, 23, 25, 29, 30, 158, 159, 166, 170, 174, 179, 181, 184, 185, 186, 187, 188, 191, 197, 199, 201, 202, 209, 210, 214, 215, 216, 220, 222, 224, 226, 229, 230, 234, 237, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268], "float16": [9, 166, 170, 171, 215, 229, 264], "float32": [9, 196, 214, 229], "float8": 210, "float_fallback": 263, "floatencod": [166, 210], "floatquant": [166, 171], "floatquantizedequant": 171, "flow": [25, 172, 186, 199, 234, 265], "fly": 177, "fo": 250, "focu": 258, "fold": [3, 13, 14, 15, 25, 27, 74, 181, 186, 191, 197, 199, 200, 202, 209, 210, 214, 226, 238, 241, 264], "fold_all_batch_norm": [15, 27, 201, 226, 230, 241, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261], "fold_all_batch_norms_to_scal": [14, 200, 244, 258], "fold_all_batch_norms_to_weight": [3, 201, 237, 238, 239, 240], "fold_param_quant": 181, "folded_model": 201, "folder": [214, 249, 262], "follow": [0, 5, 6, 8, 9, 18, 19, 20, 21, 22, 30, 100, 167, 169, 171, 172, 173, 174, 175, 178, 179, 181, 187, 188, 194, 195, 196, 197, 199, 200, 201, 202, 208, 210, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268], "footprint": [191, 192], "forall_": [161, 162, 163, 164, 165], "forc": [194, 202, 237, 238, 239, 240, 252, 253, 254, 255, 256, 259, 260, 261, 262], "forg": 194, "form": [20, 249, 262], "formal": 215, "format": [8, 11, 21, 22, 23, 160, 176, 179, 181, 184, 187, 188, 191, 192, 193, 197, 199, 204, 210, 214, 215, 226, 229, 230, 232, 238, 263, 264, 265, 267], "former": 233, "forward": [2, 8, 9, 12, 13, 20, 21, 22, 23, 24, 25, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 166, 172, 173, 174, 175, 176, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 197, 198, 199, 200, 201, 204, 206, 207, 208, 209, 210, 214, 226, 229, 230, 233, 234, 237, 240, 241, 244, 245, 246, 247, 248, 249, 255, 256, 258, 259, 260, 261, 262, 268], "forward_fn": [2, 13, 23, 24, 25, 26, 174, 176, 182, 184, 185, 186, 189, 197, 198, 199, 200, 204, 208, 226, 237, 244, 255, 258], "forward_one_batch": [237, 255], "forward_pass": [196, 197, 198, 206, 207, 208, 226], "forward_pass_arg": 229, "forward_pass_call_back": [226, 241], "forward_pass_callback": [2, 8, 9, 12, 21, 22, 179, 181, 185, 187, 188, 214, 226, 229, 230, 237, 239, 240, 242, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "forward_pass_callback_2": [12, 226], "forward_pass_callback_arg": [9, 22, 181, 188, 226, 229, 230, 241, 242, 244, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261], "forward_pass_callback_fn": 214, "found": [171, 246, 264, 265], "four": [199, 226, 265], "fp": [203, 209, 230], "fp16": [174, 210, 228, 264, 267], "fp32": [2, 6, 8, 12, 19, 21, 169, 174, 179, 182, 185, 187, 189, 191, 198, 204, 208, 210, 213, 214, 224, 226, 244, 245, 262, 263, 265, 267, 268], "fp32_acccuraci": 241, "fp32_layer_output": 213, "fp32_layer_output_util": 213, "fp32_output": [2, 226], "fp_qdq": 171, "fp_quantiz": 171, "frac": [161, 162, 163, 164, 165, 166, 180, 202, 265], "fraction": [226, 268], "fractionalmaxpool2d": 75, "fractionalmaxpool3d": 76, "framework": [191, 192, 195, 196, 198, 201, 204, 209, 213, 229, 231, 236, 263, 265, 268], "free": [228, 239, 244, 245, 251, 258, 259], "freez": [171, 198, 204, 208, 210, 242, 250, 256], "freeze_encod": 171, "friendli": [178, 199, 202, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 267], "from": [2, 7, 9, 12, 13, 20, 22, 23, 24, 25, 26, 30, 159, 160, 161, 162, 166, 167, 170, 172, 173, 174, 175, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 192, 193, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 222, 223, 224, 226, 228, 229, 230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268], "from_encod": 166, "from_modul": 30, "from_numpi": 177, "from_pretrain": [204, 206, 207, 209], "from_str": [9, 22, 181, 188, 229, 230], "from_tensor_slic": 214, "front": [2, 12, 13, 25, 185, 186, 199, 237, 241, 255], "frozen": [1, 197, 206, 207, 229, 238, 250], "full": [24, 191, 198, 220, 226, 227, 232, 233, 237, 238, 239, 240, 241, 242, 244, 247, 248, 251, 255, 256, 258, 259, 260, 261], "fulli": [0, 167, 177, 210, 219, 222, 224, 229], "func": [2, 12, 174, 179, 185, 187, 214, 226], "func_callback_arg": [2, 12, 174, 179, 185, 187, 214, 226, 237, 255], "func_wrapp": [220, 226, 241], "function": [0, 2, 5, 8, 9, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 158, 159, 160, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 197, 198, 199, 200, 202, 204, 206, 207, 208, 209, 210, 212, 214, 215, 216, 218, 219, 220, 221, 222, 226, 229, 230, 232, 233, 238, 239, 241, 242, 245, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 263, 265, 268], "function_nam": [252, 253, 254], "functional_callback": 245, "functional_model": [244, 245, 246], "functional_model_weight_ord": 246, "functional_op": 173, "fundament": 171, "furhter": [243, 257], "furo": 210, "further": [158, 161, 162, 163, 164, 165, 172, 180, 216, 219, 230, 235, 254, 264, 265, 268], "furthermor": 20, "fuse": [206, 209, 231, 244, 265], "fuse_bn_into_conv": 201, "fusion": [191, 209, 210], "futur": [168, 209, 212], "fx": [25, 172, 186, 199], "g": [22, 176, 188, 202, 204, 214, 224, 226, 229, 230, 237, 238, 241, 244, 249, 250, 255, 258, 262, 264], "gain": [197, 216, 217, 219, 238, 242, 247, 248, 250, 251, 252, 253, 254, 256, 259, 260, 261, 264, 268], "gamma": 198, "gap": 227, "gate_proj": 209, "gaussiannllloss": 81, "gave": 199, "gelu": 77, "gemm": [7, 210, 231, 244], "gemma3": 210, "gener": [6, 19, 30, 161, 162, 163, 164, 165, 169, 177, 180, 185, 191, 194, 197, 198, 200, 201, 202, 204, 206, 207, 208, 209, 210, 214, 215, 219, 224, 226, 228, 229, 230, 231, 234, 236, 237, 241, 245, 255, 257, 263, 264, 265, 267], "generate_calibration_callback": [206, 207], "generate_layer_output": [6, 19, 169, 213], "get": [2, 12, 17, 29, 166, 172, 174, 178, 185, 192, 195, 197, 207, 208, 210, 213, 219, 220, 222, 226, 235, 236, 237, 238, 239, 240, 241, 244, 245, 246, 249, 264], "get_activation_quant": [2, 226], "get_active_param_quant": [12, 226], "get_active_quant": [2, 12, 174, 185, 226], "get_available_provid": [197, 208, 237, 238, 239, 240], "get_calibration_and_eval_data_load": 229, "get_candid": [2, 12, 174, 185, 226], "get_data_loader_wrapp": [226, 241], "get_default_kernel": 30, "get_devic": 257, "get_encod": 166, "get_eval_func": [220, 226, 241], "get_extra_st": 166, "get_input": [197, 226, 237, 238, 239, 240], "get_input_quantizer_modul": [174, 185, 226], "get_kernel": 30, "get_loss_fn": [182, 189, 208], "get_model": [232, 241], "get_offset": 180, "get_original_models_weights_in_functional_model_ord": 246, "get_param_quant": [2, 226], "get_path_for_per_channel_config": [196, 197, 199, 200, 228, 229], "get_peft_model": 178, "get_pre_processed_input": 213, "get_quant_scheme_candid": [13, 25, 186, 199], "get_scal": [160, 180], "get_subclass_model_with_functional_lay": 20, "get_text_classificaiton_model": 20, "get_top5_acc": 241, "get_val_dataload": [237, 239, 240, 252, 254, 255, 256, 258, 259, 260, 261, 262], "get_val_dataset": [242, 243, 244, 249, 250, 251], "get_weight": [201, 202, 246], "git": 236, "github": [195, 236], "give": [197, 210, 214, 219, 225, 226, 241, 245, 249, 250, 252, 253, 254, 262, 264, 266], "given": [2, 4, 12, 13, 16, 17, 22, 23, 25, 26, 28, 29, 30, 168, 174, 175, 181, 182, 184, 185, 186, 188, 189, 197, 199, 200, 202, 208, 209, 212, 216, 218, 219, 220, 222, 225, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264, 268], "global": [191, 208, 234], "globalaveragepooling1d": [20, 245, 246], "glu": 78, "go": [20, 210, 237, 241, 244, 245, 249, 250, 255, 258, 262], "goal": [13, 25, 186, 199, 214, 230, 237, 255, 267], "good": [20, 178, 219, 244, 245, 247, 248, 252, 253, 254, 258, 260, 261, 264, 268], "googl": 191, "got": [3, 14, 27, 172, 200, 201], "gpu": [191, 193, 194, 195, 196, 210, 213, 216, 220, 222, 229, 252, 253, 254, 256, 258, 259, 260, 261, 262], "grad_fn": [158, 159, 160, 161, 162, 177, 180, 196], "gradient": [158, 159, 160, 206, 207, 210, 229], "grant": 236, "granular": [17, 29, 215, 216, 219, 220, 222, 234, 245, 252, 253, 254], "graph": [9, 20, 23, 25, 160, 172, 177, 181, 184, 186, 188, 197, 199, 201, 202, 208, 210, 220, 221, 224, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 252, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 267], "graph_optimization_level": 177, "graphmodul": 172, "graphoptimizationlevel": 177, "greater": [17, 29, 215, 216, 217, 218, 220, 222, 264], "greedi": [17, 29, 216, 219, 220, 221, 222], "greedili": [237, 241, 255], "greedy_param": [216, 220, 222, 252, 253, 254], "greedy_select_param": [17, 29, 216, 220, 252, 253, 254], "greedymixedprecisionalgo": [12, 174, 185, 226, 241, 255], "greedyselectionparamet": [17, 29, 216, 220, 222, 252, 253, 254], "green": 223, "grid": [180, 182, 189, 202, 208, 215, 265], "group": [2, 7, 12, 170, 174, 185, 202, 210, 215, 231, 237, 255, 265], "groupedblockquantizedequant": 215, "groupnorm": 82, "gru": [79, 210], "grucel": 80, "guarante": [20, 210], "guid": [0, 167, 192, 197, 205, 210, 229, 235, 236, 237, 241, 243, 255, 256, 257, 259, 260, 261, 262, 264, 267], "guidebook": 219, "guidelin": [172, 197, 229, 255, 256, 259, 260, 261, 262], "gz": 229, "h": [222, 223, 236], "h5": [213, 263], "ha": [0, 8, 11, 20, 23, 25, 30, 158, 167, 171, 172, 173, 181, 184, 185, 186, 188, 191, 193, 194, 197, 198, 199, 204, 205, 210, 214, 215, 217, 218, 221, 223, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 267], "had": [232, 233], "half": 217, "hand": [237, 255], "handl": [13, 14, 25, 26, 186, 195, 196, 199, 200, 210, 215, 265], "hard": 172, "hardshrink": 83, "hardsigmoid": 84, "hardswish": 85, "hardtanh": 86, "hardwar": [9, 181, 188, 191, 215, 229, 230, 265, 267], "hat": 265, "have": [8, 20, 21, 30, 168, 169, 170, 171, 172, 173, 176, 179, 180, 187, 191, 193, 196, 202, 204, 206, 208, 209, 210, 212, 213, 214, 215, 219, 224, 229, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 268], "hba": [239, 251, 259], "he": 219, "head": [20, 245, 246], "heavi": [168, 212, 221, 264], "height": [201, 202, 220, 222, 223, 237, 238, 239, 240, 245, 255, 256, 258, 259, 260, 261, 262], "held": [30, 175, 260, 261], "help": [171, 173, 174, 181, 188, 190, 203, 209, 214, 215, 218, 219, 226, 227, 228, 229, 230, 231, 234, 236, 239, 249, 251, 259, 262, 267, 268], "helper": [174, 180, 185, 206, 209, 226, 244, 258], "hen": [13, 25, 186, 199], "here": [23, 171, 178, 180, 184, 193, 197, 202, 206, 207, 220, 229, 231, 237, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 258, 259, 260, 261, 262, 267, 268], "heterogen": 267, "heurist": [243, 257], "hidden": [20, 245, 246], "hide": 228, "high": [2, 4, 12, 16, 28, 171, 185, 191, 192, 202, 203, 205, 210, 217, 218, 226, 230, 238, 239, 240, 242, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261, 264], "higher": [23, 184, 185, 197, 210, 215, 216, 218, 220, 222, 226, 227, 228, 230, 234, 237, 241, 252, 253, 254, 255, 264, 265, 267], "highest": [13, 25, 186, 199, 218], "highli": 264, "highlight": 221, "hingeembeddingloss": 87, "histogram": [8, 21, 179, 187, 210, 211, 249, 262, 265], "histogram_freq": 245, "historgram": [8, 214], "histori": [244, 245, 247, 248], "hold": [158, 159, 160, 175, 200, 231, 234], "honor": [216, 220, 222], "hood": 171, "hook": 265, "hope": [237, 241, 244, 245, 250, 255, 258], "hopefulli": 246, "host": [195, 210, 221], "hotspot": [8, 21, 179, 187, 214, 225], "hover": 210, "how": [2, 12, 173, 175, 178, 180, 181, 185, 191, 192, 194, 196, 197, 205, 209, 210, 214, 215, 219, 220, 222, 224, 225, 226, 228, 229, 230, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 266, 268], "howev": [20, 171, 177, 195, 205, 210, 215, 216, 220, 222, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 264, 265, 267, 268], "html": [8, 21, 168, 172, 179, 187, 194, 195, 210, 211, 212, 214, 236, 249, 262], "htp": 263, "htp_v66": [9, 181, 188, 229, 230], "htp_v68": [9, 181, 188, 229, 230], "htp_v69": [9, 181, 188, 229, 230], "htp_v73": [9, 181, 188, 229, 230], "htp_v75": [9, 181, 188, 229, 230], "htp_v79": [9, 181, 188, 229, 230], "htp_v81": [9, 181, 188, 229, 230], "http": [172, 183, 195, 198, 204, 209, 210, 221, 229, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "hub": [190, 192, 201, 210, 213, 215, 224, 226, 264, 265, 267, 268], "huberloss": 88, "huggingfac": [178, 206, 207, 209, 210], "hx": [79, 80, 96, 97, 123, 124], "hxwx5": 223, "hxwx8": 223, "hyper": [200, 238, 242, 244, 245, 247, 248, 251, 252, 253, 254, 256, 258, 259, 260, 261, 267], "hyperparamet": [230, 264], "i": [0, 1, 2, 4, 7, 8, 9, 12, 13, 17, 20, 21, 22, 23, 24, 25, 28, 29, 30, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 235, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261, 263, 264, 265, 266, 267, 268], "i_": [161, 162, 163, 164, 165], "i_0": [161, 162, 163, 164, 165], "i_d": [161, 162, 163, 164, 165], "iccv": [219, 239, 251, 259], "id": [6, 13, 25, 186, 199, 206, 207, 213, 221], "idea": 202, "ideal": [210, 241, 249, 250, 262], "idempot": 160, "ident": [201, 232, 233, 246], "identifi": [170, 173, 209, 210, 211, 214, 215, 216, 223, 225, 226, 234, 236, 264, 265, 267], "ieee": [166, 219], "ignor": [2, 17, 29, 172, 185, 216, 220, 222, 226, 229, 237, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 262, 268], "ignore_quant_ops_list": [23, 184, 197], "illustr": [197, 216, 218, 220, 221, 222, 223, 250, 252, 253, 254, 265, 267], "ilsvrc": [199, 226, 229], "ilsvrc2012": [237, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "ilsvrc2012_devkit_t12": 229, "ilsvrc2012_img_v": 229, "imag": [8, 21, 179, 187, 191, 194, 197, 199, 200, 208, 210, 214, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "image_bw": 233, "image_dataset": 214, "image_dataset_from_directori": [197, 199, 200, 220, 226, 229, 230, 241, 242, 247, 248, 250], "image_height": [242, 250], "image_net_config": [237, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "image_net_data_load": [237, 239, 240, 252, 254, 256, 258, 259, 260, 261, 262], "image_net_dataset": [242, 243, 244, 249, 250, 251], "image_net_evalu": [237, 239, 240, 242, 243, 244, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "image_net_train": [252, 253, 254, 258, 260, 261], "image_rgb": 233, "image_s": [197, 199, 200, 220, 226, 229, 230, 237, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "image_width": [242, 250], "imagefold": [255, 257], "imagenet": [196, 197, 199, 200, 201, 202, 208, 220, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "imagenet_data": [197, 208, 238], "imagenet_dataset": [197, 199, 200, 229, 230, 257], "imagenet_dir": [247, 248], "imagenetdataload": [237, 239, 240, 249, 252, 254, 256, 258, 259, 260, 261, 262], "imagenetdatapipelin": [237, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "imagenetdataset": [242, 243, 244, 249, 250, 251], "imagenetevalu": [237, 239, 240, 242, 243, 244, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "imagenettrain": [252, 253, 254, 258, 260, 261], "images_dir": 258, "images_mean": 255, "images_std": 255, "imagetnet_dataset": 229, "imagin": 250, "imbal": 202, "imdb": 245, "img": [220, 226, 241], "img_height": [226, 241], "img_width": [226, 241], "immedi": [239, 243, 251, 257, 259], "impact": [191, 218, 230, 234, 235, 237, 250, 255, 264], "implement": [6, 19, 22, 30, 169, 180, 199, 206, 207, 209, 210, 213, 214, 229, 230, 234, 236, 237, 255, 267, 268], "impli": [226, 237, 255], "import": [11, 20, 22, 23, 30, 158, 159, 160, 161, 162, 164, 165, 166, 167, 170, 171, 172, 173, 175, 177, 178, 180, 181, 184, 188, 191, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 215, 216, 220, 222, 224, 226, 228, 229, 230, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "impos": 215, "improv": [192, 197, 198, 201, 203, 204, 209, 210, 215, 217, 219, 226, 227, 229, 234, 238, 239, 240, 242, 243, 244, 245, 247, 248, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 264, 266, 268], "in1": 228, "in2": 228, "in_channel": [170, 215], "in_eval_mod": 257, "in_featur": [30, 171, 175, 181], "in_plac": [22, 181, 188, 204, 206, 207, 209, 229, 230], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "includ": [2, 13, 25, 168, 185, 186, 191, 195, 199, 201, 202, 209, 210, 212, 214, 218, 221, 224, 226, 230, 231, 264, 265, 268], "include_top": [241, 242, 250, 251], "incompat": [210, 244, 245], "incorrect": [174, 210, 228], "incorrectli": [210, 268], "increas": [17, 29, 203, 216, 218, 220, 222, 230, 237, 250, 255, 264, 265], "increment": 268, "incur": [214, 226], "independ": [172, 191, 234], "index": [8, 21, 175, 179, 187, 194, 210, 214, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "index_0": [249, 262], "index_1": [249, 262], "index_n": [249, 262], "indic": [17, 29, 109, 110, 111, 174, 175, 180, 181, 210, 216, 217, 220, 222, 223, 224, 228, 229, 230, 243, 250, 255, 257], "indirect": [226, 237, 255], "individu": [8, 21, 179, 187, 192, 208, 214, 218, 225, 231], "induc": 265, "infer": [9, 11, 13, 22, 23, 25, 181, 184, 186, 191, 196, 197, 198, 199, 201, 204, 208, 210, 214, 215, 217, 220, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 268], "inferencesess": [2, 8, 9, 177, 213, 214, 226, 229, 237, 238, 239, 240], "influenc": 265, "info": [15, 173, 201, 210, 241], "inform": [2, 12, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 171, 173, 185, 190, 192, 210, 224, 226, 237, 241, 255, 258], "inherit": [20, 30, 100, 175, 210], "init": [201, 202, 209], "initi": [6, 9, 161, 162, 166, 175, 180, 181, 188, 197, 201, 202, 213, 229, 237, 239, 240, 242, 248, 249, 250, 255, 256, 260, 261, 262], "initial_accuraci": [199, 257], "initializd": 175, "inner": 234, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "inp_data": 220, "inp_symmetri": [182, 189, 208], "inplac": [166, 173, 201, 202], "input": [1, 2, 6, 7, 8, 9, 10, 12, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 191, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 212, 214, 215, 216, 218, 219, 220, 222, 223, 224, 226, 229, 230, 231, 232, 233, 237, 238, 239, 240, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 258, 262, 263, 265], "input1": [46, 62, 105], "input2": [46, 62, 105], "input_": [161, 162, 163, 164, 165], "input_1": [197, 201, 202], "input_batch": 213, "input_channel": [7, 170, 215], "input_data": [199, 229, 237, 239, 240, 255, 256, 258, 259, 260, 261, 262], "input_dim": [20, 245, 246], "input_dlc": 263, "input_id": [204, 206, 207, 209], "input_inst": [6, 19, 169, 213], "input_lay": [20, 246], "input_length": 48, "input_list": 263, "input_nam": [177, 181, 188, 196, 197, 201, 208, 226, 229, 230, 237, 238, 239, 240], "input_network": 263, "input_op_nam": [17, 220], "input_q": 180, "input_qdq": 180, "input_qtzr": 30, "input_quant": [12, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 174, 175, 181, 185, 226, 229, 230], "input_shap": [12, 27, 28, 29, 172, 197, 201, 202, 208, 214, 216, 220, 222, 226, 228, 229, 232, 237, 238, 239, 240, 241, 242, 246, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261], "input_tensor": [20, 172, 241, 242, 250, 251], "inputlay": [20, 197, 201, 202], "inputs_batch": [229, 237, 239, 240, 255, 256, 258, 259, 260, 261, 262], "insert": [172, 203, 209, 226, 229, 237, 238, 239, 240, 242, 244, 245, 247, 248, 250, 251, 252, 254, 255, 256, 258, 259, 260, 261, 265, 267], "insid": [20, 24, 30, 172, 175, 194, 198, 210, 220, 228, 245, 246], "insight": [221, 234, 264], "inspect": [219, 245], "instabl": [244, 258], "instal": [191, 192, 210, 221, 236, 247, 248, 251, 257, 259, 260, 261, 267], "instanc": [1, 6, 10, 19, 30, 169, 172, 173, 188, 197, 208, 213, 221, 230, 264, 268], "instancenorm1d": 89, "instancenorm2d": 90, "instancenorm3d": 91, "instanti": [178, 180, 206, 207, 209, 210, 215, 221, 224, 226, 231, 233, 237, 249, 250, 255, 258, 262], "instead": [166, 172, 173, 197, 210, 215, 220, 223, 224, 229, 232, 233, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "instruct": [192, 195, 204, 209, 210, 215, 236, 263, 268], "int": [1, 2, 6, 7, 8, 10, 11, 12, 13, 14, 17, 21, 22, 23, 24, 25, 26, 29, 59, 60, 61, 109, 110, 111, 161, 162, 163, 164, 165, 166, 170, 171, 174, 176, 179, 181, 182, 184, 185, 186, 187, 188, 189, 197, 198, 199, 200, 204, 206, 207, 208, 210, 213, 214, 215, 216, 220, 222, 224, 226, 229, 230, 237, 241, 243, 244, 249, 252, 253, 254, 255, 257, 265], "int16": [9, 174, 196, 199, 226, 228, 229, 230, 237, 241, 255, 264, 265, 267, 268], "int32": [177, 206, 207, 224], "int4": [9, 174, 197, 199, 208, 210, 228, 229, 230, 267], "int8": [9, 159, 160, 174, 191, 196, 197, 199, 208, 226, 228, 229, 230, 237, 238, 239, 240, 241, 250, 255, 256, 259, 260, 261, 264, 265, 267, 268], "int_multipli": 30, "integ": [22, 164, 165, 170, 180, 188, 191, 197, 206, 210, 214, 215, 224, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264, 267, 268], "integr": [13, 25, 178, 186, 198, 199], "intel": [193, 195, 196], "intellig": 219, "intend": [191, 214, 216, 220, 222, 224, 228], "intens": 230, "inter": 211, "interact": [168, 171, 191], "intercept": 265, "interdepend": 226, "interest": [8, 179, 187, 214], "interfac": [195, 210, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "intermedi": [2, 6, 12, 19, 25, 169, 176, 181, 185, 186, 188, 191, 199, 204, 210, 211, 213, 226, 229, 230, 237, 241, 255], "intern": [13, 20, 25, 171, 186, 188, 199, 209, 219, 231, 242, 245, 246, 250, 256], "interpol": [12, 218, 226, 237, 255], "interpret": [215, 229], "intersect": 264, "introduc": [175, 198, 202, 203, 204, 210, 231, 265], "invalid": [172, 215], "invoc": [252, 253, 254], "invok": [8, 9, 21, 175, 179, 181, 187, 188, 214, 219, 221, 229, 230, 250], "involv": [2, 12, 171, 185, 200, 205, 206, 207, 226, 234, 237, 241, 255, 268], "io": [174, 228], "ip": 236, "ipynb": 236, "irrespect": [24, 198], "is_avail": [196, 197, 198, 199, 200, 201, 202, 204, 208, 209, 226, 228, 229, 230, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "is_bfloat16": 166, "is_float16": 166, "is_initi": [30, 161, 162, 166, 175, 180], "is_input_quant": [231, 244], "is_leaf_modul": 172, "is_output_quant": [231, 244], "is_quant": [231, 244], "is_sym": 224, "is_symmetr": [195, 224, 231, 244], "is_train": [226, 237, 239, 240, 241, 252, 254, 256, 258, 259, 260, 261, 262], "is_unsigned_symmetr": 171, "isinst": [170, 206, 207, 215], "islic": [197, 208, 230, 238], "isol": [194, 210, 265], "issu": [20, 173, 200, 210, 211, 213, 221, 228, 234, 268], "item": [166, 204, 206, 207, 209, 226, 229, 230, 246, 249, 262, 263], "iter": [8, 9, 11, 17, 23, 24, 25, 29, 176, 184, 186, 193, 197, 198, 199, 204, 209, 210, 214, 216, 220, 222, 226, 229, 237, 238, 239, 240, 241, 242, 243, 244, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 264], "itertool": [197, 206, 207, 208, 230, 238], "its": [8, 12, 20, 30, 100, 158, 166, 174, 175, 191, 192, 198, 204, 206, 207, 214, 215, 216, 220, 223, 226, 228, 230, 231, 236, 237, 239, 240, 242, 243, 244, 246, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 265, 268], "itself": [213, 219, 220, 249, 258, 262], "j_": [161, 162, 163, 164, 165], "j_0": [161, 162, 163, 164, 165], "j_d": [161, 162, 163, 164, 165], "jaderberg": 219, "jan": 219, "jenkin": 194, "jian": 219, "jianhua": 219, "jit": [206, 207, 233], "job": [244, 245, 247, 248, 252, 253, 254, 258, 260, 261, 263], "join": [196, 197, 201, 202, 208, 216, 220, 222, 226, 229, 242, 247, 248, 250, 255, 256, 257], "jointli": [248, 260, 261], "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 244, 249, 262, 263, 264, 265, 266, 267, 268, 269], "jupyt": [192, 210, 236], "just": [210, 223, 228, 237, 241, 244, 245, 249, 250, 255, 260, 261, 262, 265], "k": [180, 204, 206, 207, 209, 222, 257], "k_proj": 209, "kaim": 219, "kd": 210, "keep": [172, 202, 231, 240, 247, 264, 268], "kei": [166, 174, 196, 204, 206, 207, 209, 224, 228, 238, 239, 240, 242, 246, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261, 264], "kept": [12, 226, 229, 234], "kera": [11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 192, 194, 197, 199, 200, 201, 202, 210, 213, 214, 220, 226, 229, 230, 232, 236, 241, 242, 243, 247, 248, 249, 250, 251], "keraslayeroutput": [19, 213], "kernel": [30, 210, 215, 216, 220, 222, 250], "kernel_s": [20, 172, 173, 181, 201, 202, 229, 230, 250], "key_dim": [20, 245, 246], "keyword": [30, 173], "kl": 191, "kldivloss": 92, "know": [30, 237, 244, 245, 250, 255, 258], "knowledg": [198, 203, 204, 224], "known": [173, 209, 210, 215, 265], "kullback": 191, "kuzmin": 219, "kwarg": [9, 20, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 173, 174, 175, 177, 181, 210, 226, 229, 230, 246], "l1": [182, 189, 208, 228], "l1loss": 93, "l2": 228, "lab": [191, 267], "label": [194, 197, 199, 200, 204, 206, 207, 208, 209, 214, 220, 226, 229, 230, 237, 238, 241, 242, 243, 244, 247, 248, 250, 255, 257], "label_dataset": 214, "label_mod": [197, 199, 200, 220, 226, 229, 230, 241, 242, 247, 248, 250], "labeled_data": 199, "labeled_data_load": 199, "lambda": [25, 170, 174, 176, 185, 186, 197, 199, 200, 204, 210, 214, 215, 226, 229, 230, 241, 242, 243, 244, 245, 246, 250], "laptop": [191, 192], "larg": [178, 202, 203, 205, 210, 217, 219, 220, 222, 230, 241, 268], "larger": [215, 220, 222, 264], "last": [12, 226, 228], "latenc": [193, 210, 217, 226, 227, 264, 268], "later": [188, 193, 195, 196, 209, 210, 229, 237, 244, 245, 249, 250, 258, 262], "latest": [174, 193, 195, 196, 210, 228], "launch": 236, "layer": [1, 3, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 100, 168, 169, 170, 173, 175, 178, 179, 181, 182, 184, 186, 187, 188, 189, 191, 197, 198, 199, 200, 201, 204, 206, 207, 208, 209, 210, 212, 215, 216, 217, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 235, 236, 238, 241, 245, 252, 253, 254, 264, 265, 267, 268], "layer1": [249, 262], "layer2": [249, 262], "layer_nam": [8, 21, 176, 179, 187, 204, 214], "layer_output_util": [0, 5, 18, 167, 213], "layern": [249, 262], "layernorm": [20, 98, 210, 245, 246], "layernorm1": [20, 246], "layernorm2": [20, 246], "layeroutpututil": [6, 19, 169, 213], "layers_to_exclud": 173, "layout": [160, 172], "lazili": 210, "lceil": [161, 162, 164, 165, 166, 265], "lead": [8, 10, 197, 208, 214, 234, 251], "leaf": [172, 174, 198, 204, 210], "leakyrelu": [99, 210], "learn": [171, 188, 190, 191, 192, 193, 197, 201, 202, 205, 206, 207, 210, 219, 230, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 264, 268], "learnabl": [161, 162, 198, 203, 204, 210, 230], "learnedgrid": 210, "learnedgridquant": 171, "learning_r": [200, 230, 252, 253, 254, 258, 260, 261], "learning_rate_schedul": [252, 253, 254, 258, 260, 261], "learnt": 204, "least": [185, 238, 242, 256], "leav": [197, 207], "left": [161, 162, 163, 164, 165, 166, 180, 205, 215, 218, 223, 226, 237, 238, 239, 240, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 265], "leftarrow": 202, "legaci": [195, 210], "leibler": 191, "len": [9, 23, 172, 184, 197, 199, 204, 206, 207, 209, 220, 226, 230, 241, 245, 255, 257], "length": [170, 175, 215, 224], "leq": [161, 162, 163, 164, 165], "less": [191, 215, 216, 218, 227, 231, 241, 255, 265, 267], "lesser": [237, 255], "let": [172, 202, 204, 229, 237, 241, 244, 245, 250, 255, 267], "level": [1, 2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27, 28, 29, 170, 171, 172, 174, 176, 179, 180, 182, 183, 184, 185, 186, 187, 188, 189, 192, 197, 198, 199, 200, 201, 202, 204, 205, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 220, 221, 222, 226, 227, 228, 229, 230, 234, 238, 240, 241, 242, 245, 247, 248, 252, 253, 254, 256, 260, 261, 265], "leverag": 264, "lfloor": [161, 162, 163, 164, 165, 166], "liblapack": 195, "libpymo": [171, 195, 210], "libqnnhtp": 263, "libqnnmodeldlc": 263, "librari": [9, 178, 191, 229, 263], "lie": 215, "light": [237, 238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262], "lightweight": 205, "like": [20, 171, 192, 213, 214, 219, 224, 225, 226, 229, 230, 231, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267], "limit": [204, 209, 210, 229, 230, 237, 238, 239, 240, 242, 243, 244, 245, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "limitedbatchdataload": [204, 209], "line": [229, 263], "linear": [3, 11, 14, 15, 23, 27, 30, 100, 170, 171, 172, 173, 175, 178, 184, 197, 200, 201, 202, 203, 209, 210, 215, 216], "linear1": [170, 215], "linear_1": 215, "link": [210, 236], "list": [1, 2, 3, 6, 7, 9, 12, 13, 14, 15, 17, 19, 20, 23, 25, 27, 28, 29, 59, 60, 61, 109, 110, 111, 164, 165, 166, 169, 170, 172, 173, 174, 175, 179, 182, 184, 185, 186, 187, 189, 197, 199, 200, 201, 202, 204, 206, 207, 208, 209, 213, 214, 215, 216, 218, 220, 222, 224, 228, 229, 231, 237, 238, 241, 244, 246, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 268], "list_of_module_comp_ratio_pair": [17, 29, 216, 220, 222], "listen": 221, "lite": [192, 201, 210, 213, 215, 224, 226, 263, 265, 267, 268], "litemp": 264, "liter": [174, 228], "littl": [171, 219, 238, 242, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261, 268], "ll": [195, 236], "llama": [204, 209, 210], "llama3": 204, "llamadecoderlay": [198, 204], "llamaforcausallm": [204, 209], "llamamodel": [198, 204], "llm": [210, 267], "llm_configur": 210, "lm_head": 209, "load": [22, 24, 172, 173, 188, 191, 192, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 216, 219, 220, 222, 226, 229, 230, 237, 238, 239, 240, 245, 263, 267], "load_adapt": [206, 207], "load_checkpoint": 188, "load_data": 245, "load_dataset": [197, 199, 200, 204, 206, 207, 209, 226], "load_encod": [197, 229], "load_encodings_to_sim": [210, 213, 229, 230], "load_model": [196, 197, 201, 202, 208, 213, 226, 229, 237, 238, 239, 240], "load_state_dict": 166, "loader": [2, 8, 23, 25, 26, 174, 182, 184, 185, 186, 189, 197, 199, 200, 208, 214, 226, 229, 230, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 254, 256, 258, 259, 260, 261, 262], "local": [208, 221, 229, 236], "localresponsenorm": 101, "locat": [209, 229, 237, 241, 244, 249, 250, 255, 258, 262, 268], "log": [173, 174, 214, 226, 228, 241, 244, 245, 247, 248], "log_2": 166, "log_dir": [244, 245, 247, 248], "log_fil": [174, 228], "log_input": 122, "log_prob": 48, "logdir": 245, "logger": 173, "logic": [2, 12, 30, 100, 175, 185, 210, 226, 237, 238, 239, 240, 252, 253, 254, 255, 256, 259, 260, 261, 262], "logit": [206, 207, 226, 230, 255, 257], "logsigmoid": 102, "logsoftmax": 103, "long": [204, 206, 209], "longer": [171, 181, 195, 215, 224, 229, 230, 252, 253, 254, 267], "look": [20, 192, 224, 237, 241, 244, 245, 246, 249, 250, 255, 262, 263], "lookup": 246, "lookup_quant": [12, 226], "loop": [172, 230, 234, 237, 238, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "lora": [176, 178, 191, 204, 210], "lora_a": [206, 207], "lora_a_lay": 207, "lora_add_lay": 207, "lora_alpha": 178, "lora_b": [206, 207], "lora_b_lay": 207, "lora_config": 178, "lora_dropout": 178, "lora_mul_lay": 207, "loraconfig": 178, "loralay": [206, 207], "lose": 223, "loss": [8, 10, 11, 21, 23, 158, 179, 182, 184, 187, 189, 192, 197, 199, 200, 206, 207, 208, 210, 219, 227, 229, 230, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 256, 259, 260, 261, 265], "loss_fn": [182, 189, 200, 206, 207, 208, 230, 244], "lost": [265, 268], "lot": 264, "low": [171, 180, 191, 203, 205, 210, 219, 224, 229, 250, 252, 254], "lower": [10, 23, 177, 184, 191, 197, 202, 208, 215, 218, 226, 227, 230, 234, 237, 241, 250, 255, 264, 267], "lowest": [216, 264], "lpbq": [170, 210, 215, 224], "lppool1d": 94, "lppool2d": 95, "lr": [200, 210, 230], "lstm": [96, 210], "lstmcell": 97, "lsvrc": [237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "lt": 195, "lwc": 204, "m": [194, 195, 222, 236, 245], "mac": [17, 29, 216, 217, 219, 220, 222, 225, 226, 237, 252, 253, 254, 255], "machin": [190, 191, 205, 210, 219, 268], "made": [172, 174, 191, 210, 228, 231, 255, 256, 259, 260, 261], "magnitud": 216, "mai": [2, 10, 13, 20, 25, 30, 158, 159, 160, 168, 171, 172, 175, 185, 186, 199, 202, 208, 209, 210, 212, 214, 215, 219, 224, 226, 229, 230, 237, 238, 239, 241, 244, 245, 249, 250, 251, 255, 258, 259, 262, 264, 268], "main": [200, 230, 231], "maintain": [191, 199, 210, 218, 219, 242, 256, 264], "major": [219, 224], "make": [174, 175, 178, 191, 202, 210, 226, 228, 231, 232, 233, 244, 245, 246, 250, 258, 267, 268], "make_dummy_input": 196, "makedir": [237, 241, 242, 244, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "manag": [9, 195, 267], "mandatori": 213, "manditori": 20, "mani": [172, 197, 210, 217, 237, 241, 244, 245, 249, 250, 252, 253, 254, 255, 262], "manner": [13, 25, 186, 199, 202], "mantissa": [166, 170, 215], "mantissa_bit": [166, 170, 215], "manual": [17, 29, 161, 162, 171, 174, 195, 199, 210, 216, 219, 220, 222, 224, 252, 253, 254, 264], "manual_param": [216, 220, 222], "manual_se": 255, "manualmodeparam": [17, 29, 216, 220, 222], "manylinux_2_34_x86_64": 195, "map": [12, 13, 22, 30, 159, 164, 165, 173, 175, 191, 197, 199, 200, 204, 206, 207, 209, 210, 214, 218, 224, 226, 229, 230, 231, 241, 242, 243, 244, 246, 250, 265], "map_loc": 213, "marginrankingloss": 105, "mark": 209, "marku": 219, "mask": 30, "maskedadd": 30, "match": [166, 181, 188, 210, 214, 215, 216, 219, 220, 222, 223, 229, 230, 231, 234, 246], "math": [199, 226, 268], "mathemat": [180, 191, 202, 209, 233, 237, 250, 255], "matmul": [7, 210], "matmul_8": 173, "matric": 206, "matrix": [250, 264], "matter": [30, 262], "max": [8, 21, 161, 162, 166, 168, 171, 179, 187, 195, 208, 210, 211, 212, 219, 224, 229, 240, 247, 248, 250, 263, 267], "max_epoch": [252, 253, 254, 258, 260, 261], "max_iter": [204, 206, 207], "maximum": [2, 12, 13, 21, 25, 164, 165, 166, 185, 186, 199, 214, 226, 237, 241, 242, 243, 244, 249, 250, 251, 255, 262, 265], "maxlen": [20, 245, 246], "maxpool1d": 106, "maxpool2d": 107, "maxpool3d": 108, "maxpooling2d": 244, "maxunpool1d": 109, "maxunpool2d": 110, "maxunpool3d": 111, "mdoel": 200, "mean": [26, 175, 197, 199, 200, 208, 211, 226, 229, 231, 237, 238, 241, 244, 245, 246, 249, 250, 252, 253, 254, 255, 257, 258, 259, 262, 264, 265], "meaning": 241, "measur": [2, 8, 12, 21, 29, 179, 185, 187, 191, 214, 216, 220, 222, 226, 229, 237, 252, 253, 254, 255, 264, 268], "mechan": [22, 172, 180, 197, 229, 230], "meet": [13, 25, 185, 186, 199, 217, 218, 226, 237, 241, 255, 264, 268], "member": 231, "memori": [17, 29, 160, 178, 191, 192, 210, 216, 217, 219, 220, 222, 223, 225, 227, 230, 252, 253, 254, 264, 268], "memory_format": 160, "merg": [191, 209], "messag": 241, "met": [2, 12, 185, 226, 237, 241, 255], "meta": [204, 209], "metadata": [176, 204], "metapackag": 236, "method": [6, 13, 19, 22, 24, 25, 30, 100, 160, 169, 171, 172, 174, 175, 181, 185, 186, 188, 191, 197, 198, 199, 202, 203, 206, 213, 218, 219, 226, 227, 228, 229, 230, 234, 239, 240, 242, 243, 244, 245, 246, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 264, 265], "methodologi": 268, "metric": [17, 29, 197, 199, 200, 214, 216, 220, 222, 226, 229, 230, 237, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 262, 264, 265, 268], "mha": [210, 245], "middl": 268, "might": [2, 12, 185, 197, 202, 210, 214, 224, 226, 234, 237, 238, 241, 242, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 262, 264, 268], "migrat": [0, 167, 210, 229], "mimic": 267, "mimick": 171, "min": [8, 21, 161, 162, 168, 171, 179, 187, 195, 209, 210, 211, 212, 224, 240, 247, 248, 250, 263, 267], "min_max": [9, 197, 210, 229, 237, 238, 240], "min_max_rang": [214, 249, 262], "miniconda": 194, "miniforg": 194, "minim": [182, 189, 191, 192, 202, 203, 208, 225, 226, 227, 237, 240, 241, 247, 248, 255, 260, 261, 264, 265], "minima": 208, "minimum": [164, 165, 172, 198, 204, 208, 210, 249, 262, 265], "minmaxencodinganalyz": 166, "minor": [210, 224, 230], "miou": [237, 255, 264], "mish": 112, "mismatch": [180, 194, 211, 213, 229, 250], "miss": [20, 166, 173, 210, 263], "missing_kei": 166, "mistral": [209, 210], "mistralforcausallm": 209, "mix": [2, 12, 13, 25, 174, 185, 186, 193, 199, 210, 232, 236, 268], "mixed_precis": [0, 5, 18, 167, 226, 228, 237, 241, 255], "mixed_precision_algo": [2, 174, 185, 226, 237, 241, 255], "mixed_preision_quant_model": 226, "mixedprecisionconfigur": [174, 228], "mixin": [30, 175], "mkdir": 236, "ml": [192, 219, 240, 245, 247, 248, 260, 261], "mmp": 227, "mmp_log": [174, 228], "mnist": [216, 220, 222], "mnist_after_bn_re_estimation_qat_range_learn": 244, "mnist_torch_model": 216, "mnist_trained_on_gpu": [216, 220, 222], "mnt": 194, "mobil": [191, 192], "mobilenet": [191, 197, 200], "mobilenet_v2": [196, 197, 199, 200, 201, 202, 208, 226, 228, 229, 230], "mobilenet_v2_weight": [197, 201, 202, 208, 226, 229], "mobilenetv2": [197, 199, 200, 201, 202, 229, 230], "mobilenetv2_1": [197, 201, 202], "mode": [13, 17, 22, 25, 29, 172, 181, 186, 188, 199, 210, 216, 220, 222, 229, 230, 231, 252, 253, 254], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 235, 236, 263, 265, 266, 267, 269], "model_after_qat": [245, 247, 248], "model_config": [204, 209], "model_id": [204, 206, 207, 209], "model_input": [173, 244], "model_or_pipelin": 197, "model_output": 244, "model_prepar": [0, 18, 167, 201, 202, 210, 214, 246, 255, 256, 258, 260, 261, 262], "model_prepare_requir": [25, 186, 199], "model_preparer_elementwise_add_exampl": 172, "model_preparer_functional_exampl": 172, "model_preparer_reused_exampl": 172, "model_preparer_subclassed_model_with_functional_lay": 20, "model_preparer_two_subclassed_lay": 20, "model_torch": 213, "model_transform": 172, "model_valid": [0, 167], "model_weights_in_correct_ord": 246, "modelcompressor": [17, 29, 216, 220, 222, 252, 253, 254], "modeling_llama": [204, 209], "modeling_opt": [206, 207], "modelprepar": [20, 172, 246, 255, 256, 260, 261, 262], "modelproto": [3, 4, 6, 8, 9, 201, 202, 213, 214, 229], "modelvalid": 173, "modelwithconsecutivelinearblock": 198, "modelwithelementwiseaddop": 172, "modelwithfunctionallinear": 173, "modelwithfunctionalrelu": 172, "modelwithlinear": 198, "modelwithnontorchfunct": 172, "modelwithoutfunctionallinear": 173, "modelwithoutreusednod": 173, "modelwithreusednod": 173, "modelwithreusedrelu": 172, "modif": [255, 256, 259, 260, 261], "modifi": [22, 24, 172, 181, 183, 188, 197, 198, 201, 202, 206, 207, 208, 209, 210, 213, 223, 226, 229, 230, 237, 238, 239, 240, 242, 243, 244, 245, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 265, 267, 268], "modul": [2, 7, 12, 17, 23, 24, 25, 26, 27, 28, 29, 30, 100, 166, 168, 169, 170, 172, 173, 174, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 220, 222, 223, 226, 228, 229, 230, 232, 233, 237, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 268], "module_cl": 30, "module_classes_to_exclud": 172, "module_nam": [206, 207], "module_to_exclud": 172, "modulecompratiopair": [17, 29, 216, 220, 222], "moduledict": [30, 171, 175, 181, 229, 230], "modulelist": [30, 171, 175, 181, 229, 230], "modules_to_exclud": [25, 172, 182, 186, 189, 199, 208], "modules_to_ignor": [17, 29, 179, 187, 214, 216, 220, 222, 252, 253, 254], "momentum": [200, 201, 202], "monitor": 214, "monoton": [17, 29, 216, 218, 220, 222], "more": [17, 22, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 173, 175, 178, 184, 185, 188, 191, 197, 202, 210, 214, 216, 218, 219, 220, 221, 222, 226, 227, 228, 229, 230, 231, 234, 237, 241, 244, 245, 246, 249, 250, 255, 258, 262, 263, 264, 265, 267, 268], "most": [180, 191, 195, 199, 231, 245, 264, 265], "move": [226, 236], "movement": 210, "movi": 245, "mp": [174, 228], "mp_configur": 228, "mse": [8, 10, 21, 171, 179, 182, 187, 189, 210, 214, 230, 265], "mseloss": 104, "much": [223, 252, 253, 254, 268], "mul_scal": 207, "multi": [210, 245], "multigpu": 210, "multiheadattent": [20, 210, 245, 246], "multilabelmarginloss": 113, "multilabelsoftmarginloss": 114, "multimarginloss": 115, "multipl": [17, 25, 27, 28, 29, 160, 169, 172, 173, 174, 175, 178, 181, 186, 188, 199, 201, 202, 210, 213, 215, 216, 219, 220, 222, 228, 229, 230, 249, 262, 264, 265], "multipli": [175, 217, 219, 220, 222, 225, 237, 255], "must": [20, 160, 166, 170, 173, 175, 197, 198, 199, 200, 204, 208, 209, 214, 215, 217, 223, 228, 229, 231, 238, 239, 240, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 259, 260, 261, 262, 267], "mutual": [166, 170, 215], "my_functional_model": 20, "myfunctionalmodel": 20, "mymodel": 209, "n": [8, 13, 21, 179, 180, 187, 199, 214, 215, 222, 230], "nagel": 219, "name": [1, 2, 7, 8, 12, 17, 20, 21, 30, 168, 169, 174, 175, 178, 179, 181, 185, 187, 188, 194, 197, 201, 202, 204, 206, 207, 208, 209, 210, 212, 213, 214, 220, 221, 224, 226, 228, 229, 230, 237, 238, 239, 240, 244, 246, 249, 262, 263, 265], "name_": [8, 21, 179, 187, 214, 249, 262], "name_to_quantizer_dict": [2, 12, 174, 185, 226], "named_modul": [206, 207], "named_paramet": 206, "namedtupl": 166, "namespac": [167, 171, 184, 185, 186, 187, 188, 189], "naming_schem": [169, 213], "namingschem": [169, 213], "nativ": [175, 210], "navig": 236, "na\u00efv": 203, "nconv": 201, "ndarrai": [1, 2, 6, 8, 9, 10, 12, 197, 199, 208, 213, 214, 226, 229, 238, 246], "nearest": [13, 21, 22, 25, 186, 188, 197, 199, 214, 229, 230, 238, 241, 242, 245, 247, 248, 250, 251, 256], "necessari": [158, 159, 160, 181, 194, 201, 209, 214, 216, 220, 221, 222, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 258, 259, 260, 261, 262, 268], "necessarili": [237, 241, 255], "necessit": 195, "need": [8, 9, 17, 19, 20, 21, 22, 23, 24, 29, 169, 174, 179, 184, 187, 191, 195, 197, 198, 200, 201, 202, 204, 208, 209, 210, 213, 214, 215, 216, 220, 222, 224, 226, 228, 229, 230, 231, 232, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "neg": [148, 149, 164, 165, 210, 230, 237, 244, 245, 250, 255], "negat": [242, 256], "nest": 210, "net": [229, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263], "network": [20, 175, 191, 210, 217, 218, 219, 221, 230, 234, 243, 245, 246, 250, 257, 265], "neural": [191, 217, 219, 230, 234, 237, 243, 250, 255, 257, 264, 265], "neuron": 191, "new": [15, 20, 160, 161, 162, 171, 172, 188, 192, 195, 201, 205, 206, 210, 215, 226, 228, 232, 233, 242, 246, 247, 248, 250, 251, 255, 262], "new_empti": 160, "next": [197, 202, 209, 210, 229, 234, 237, 239, 241, 244, 245, 250, 255, 258, 265], "next_conv_weight": 202, "nfolded_model": 201, "night": [238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "nllloss": 116, "nllloss2d": 117, "nmodel": [201, 202], "nn": [0, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 167, 168, 170, 171, 172, 173, 174, 177, 181, 197, 198, 199, 200, 206, 207, 208, 209, 210, 212, 214, 215, 216, 220, 222, 226, 228, 229, 230, 233, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 268], "nncf": 191, "nnext": 202, "no_grad": [173, 181, 196, 197, 198, 206, 207, 208, 226, 229, 230, 255, 256, 257, 259, 260, 261, 262], "node": [1, 7, 9, 22, 25, 172, 181, 185, 186, 188, 191, 197, 199, 201, 202, 210, 229, 230, 233, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 259, 260, 261, 264, 267, 268], "node_names_to_optim": [1, 197], "noffset": 180, "nois": [202, 214, 225, 230, 231, 267], "noisi": [244, 258], "non": [9, 172, 174, 178, 182, 189, 198, 204, 208, 210, 221, 229, 243, 257, 263, 265, 267], "none": [1, 2, 6, 7, 8, 9, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 59, 60, 61, 71, 79, 80, 96, 97, 100, 109, 110, 111, 123, 124, 160, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 175, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 195, 196, 197, 198, 199, 200, 201, 202, 208, 212, 213, 214, 215, 216, 220, 221, 222, 226, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "norm": [15, 197, 202, 210, 214, 226, 241, 251], "normal": [15, 191, 197, 199, 201, 208, 210, 214, 226, 229, 238, 257], "notabl": 224, "note": [2, 8, 12, 13, 20, 21, 22, 24, 25, 171, 179, 180, 181, 182, 185, 186, 187, 188, 189, 195, 198, 199, 202, 208, 213, 214, 215, 216, 220, 221, 222, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264], "note1": [237, 255, 258, 262], "note2": [237, 255, 258, 262], "notebook": [210, 235, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261], "noth": [17, 29, 216, 220, 222, 268], "notic": [168, 210, 212], "notimplementederror": 30, "now": [20, 172, 173, 180, 181, 196, 197, 201, 202, 210, 224, 226, 229, 230, 237, 239, 240, 241, 244, 245, 246, 247, 248, 250, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "np": [1, 2, 9, 10, 20, 195, 197, 199, 208, 214, 220, 226, 229, 238, 241, 246], "nprepared_model": [201, 202], "nprev": 202, "nscale": 180, "nullptr": 210, "num": [164, 165], "num_batch": [8, 11, 21, 23, 26, 182, 184, 189, 197, 198, 199, 200, 208, 209, 214, 229, 238, 239, 242, 243, 244, 249, 250, 256, 257, 258, 259, 262], "num_calibration_sampl": [197, 199, 200, 208, 226, 229, 230, 238], "num_candid": [10, 182, 189, 208], "num_channel": 224, "num_class": 214, "num_comp_ratio_candid": [17, 29, 216, 220, 222, 252, 253, 254], "num_epoch": [200, 210, 230], "num_eval_sampl": [197, 226, 238], "num_head": [20, 245, 246], "num_iter": [1, 24, 176, 197, 198, 204, 210, 220, 226, 241], "num_of_sampl": 199, "num_reconstruction_sampl": [29, 216, 252, 254], "num_sampl": [2, 174, 185, 198, 214, 226, 243, 244, 255, 257], "num_samples_for_phase_1": [13, 25, 186, 199], "num_samples_for_phase_2": [13, 25, 186, 199], "num_step": [164, 165], "num_word": 245, "num_work": [197, 199, 200, 208, 237, 238, 239, 240, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "number": [1, 2, 8, 10, 11, 13, 14, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 164, 165, 166, 170, 172, 174, 175, 176, 182, 184, 185, 186, 188, 189, 191, 197, 198, 199, 200, 204, 205, 208, 209, 210, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 265, 267, 268], "numer": [169, 213, 264], "numpi": [177, 195, 196, 197, 199, 208, 214, 220, 226, 229, 237, 238, 239, 240, 246], "numpy_help": [201, 202], "nupi": [12, 226], "nvidia": [191, 193, 195, 196, 229], "o": [195, 196, 197, 201, 202, 208, 210, 216, 220, 222, 226, 229, 237, 238, 241, 242, 243, 244, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "o_proj": 209, "object": [1, 2, 7, 8, 9, 12, 13, 22, 24, 25, 30, 158, 159, 160, 166, 168, 170, 173, 174, 175, 176, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 197, 198, 199, 204, 208, 209, 210, 211, 212, 213, 214, 215, 216, 220, 222, 226, 227, 228, 229, 230, 237, 243, 244, 249, 255, 256, 257, 259, 260, 261, 262, 264, 265, 267, 268], "observ": [9, 30, 166, 175, 181, 210, 214, 219, 229, 230, 250, 258, 264, 265], "obtain": [6, 169, 214, 217, 224, 263], "obvious": [216, 220, 222], "occur": [168, 209, 212, 224, 226], "occurr": [8, 21, 179, 187, 214, 216], "oct": 219, "off": [11, 22, 23, 184, 194, 197, 200, 226, 229, 230, 237, 241, 255, 268], "offer": [8, 179, 187, 199, 208, 211, 214, 215, 227, 243, 257, 264], "offset": [8, 21, 30, 71, 161, 162, 163, 164, 165, 179, 180, 187, 195, 210, 214, 224, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 263, 265, 267], "offset_": [161, 162, 163, 164, 165], "often": [199, 201, 202, 219, 230, 264], "older": [195, 228], "omit": [218, 231], "omniqu": [0, 167, 198, 210], "onc": [30, 160, 173, 180, 210, 216, 219, 228, 232, 233, 240, 245, 247, 248, 252, 253, 254, 259, 260, 261, 263, 267, 268], "one": [9, 23, 25, 27, 172, 173, 174, 180, 181, 184, 185, 186, 188, 195, 197, 198, 199, 201, 204, 206, 207, 210, 215, 216, 219, 220, 221, 222, 224, 226, 228, 229, 230, 236, 237, 238, 239, 240, 241, 242, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 267], "ones": [204, 209, 264, 265], "ones_lik": [161, 162], "onli": [2, 8, 9, 12, 22, 158, 159, 160, 164, 165, 168, 170, 171, 172, 174, 175, 177, 179, 180, 181, 183, 185, 187, 188, 193, 194, 195, 197, 201, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 219, 220, 221, 223, 224, 226, 228, 229, 230, 231, 232, 233, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264, 265, 268], "onnx": [0, 1, 2, 3, 5, 6, 7, 8, 9, 25, 167, 169, 174, 181, 186, 188, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 208, 210, 212, 213, 214, 215, 224, 226, 228, 229, 230, 231, 233, 236, 263, 264, 265, 267, 268], "onnx_data": [197, 238], "onnx_data_gener": 229, "onnx_encoding_path": 229, "onnx_export_arg": [25, 169, 181, 186, 188, 199, 213, 229, 230], "onnx_file_nam": 233, "onnx_model": [196, 214], "onnx_output": 177, "onnx_qdq": 9, "onnx_util": 213, "onnxexportapiarg": [25, 169, 181, 186, 188, 199, 213, 229, 230], "onnxmodel": [8, 214], "onnxruntim": [9, 177, 194, 195, 197, 208, 210, 213, 214, 229, 237, 238, 239, 240], "onnxsim": [197, 201, 202, 213, 214, 226, 229, 237, 238, 239, 240], "onto": 267, "op": [2, 9, 11, 12, 15, 17, 22, 23, 25, 173, 181, 184, 185, 186, 188, 197, 199, 201, 209, 210, 220, 226, 229, 230, 231, 237, 238, 239, 240, 242, 244, 245, 247, 248, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 263, 265], "op_list": [231, 244], "op_typ": [2, 7, 9, 185, 226, 231, 237, 244, 255], "op_type_map": 173, "open": [191, 244], "opencv": 210, "oper": [7, 17, 20, 30, 171, 172, 173, 175, 191, 196, 198, 202, 203, 204, 208, 209, 210, 213, 220, 226, 229, 231, 233, 234, 237, 240, 241, 246, 247, 248, 255, 260, 261, 265, 267, 268], "oppos": [239, 251, 259], "opset": [177, 210], "opset_vers": [177, 181, 188, 229, 230], "opt": [206, 207, 241], "optim": [1, 2, 10, 11, 12, 13, 23, 24, 25, 29, 166, 176, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 214, 216, 219, 221, 225, 226, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268], "optimized_accuraci": [199, 257], "optimum": 230, "option": [2, 7, 8, 9, 11, 12, 13, 17, 21, 22, 23, 24, 25, 26, 29, 30, 161, 162, 163, 164, 165, 166, 170, 172, 174, 177, 178, 179, 181, 182, 184, 185, 186, 187, 188, 189, 195, 197, 198, 199, 200, 208, 210, 214, 215, 216, 220, 222, 226, 229, 230, 231, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265], "optlearnedpositionalembed": [206, 207], "optmiz": [209, 237, 255], "orang": 223, "order": [30, 100, 173, 175, 211, 216, 228, 231, 232, 233, 238, 244, 245, 246, 247, 248, 249, 252, 253, 254, 256, 258, 260, 261, 262, 263, 267], "org": [172, 183, 195, 198, 204, 209, 210, 229, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "org_top1": 226, "organ": [191, 266], "origin": [17, 20, 29, 30, 171, 172, 175, 182, 189, 191, 203, 208, 209, 210, 213, 214, 216, 217, 218, 220, 221, 222, 226, 237, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264, 265, 268], "original_model": [20, 246], "original_model_weight": 246, "ort": [9, 177, 197, 208, 229, 237, 238, 239, 240], "ort_disable_al": 177, "oscil": 200, "other": [160, 170, 171, 172, 174, 192, 195, 196, 197, 198, 204, 205, 206, 207, 209, 210, 215, 218, 228, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 266], "otherwis": [13, 161, 162, 164, 165, 170, 173, 181, 188, 199, 202, 215, 224, 229, 230, 234, 238, 239, 240, 242, 243, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261, 268], "our": [178, 244, 245, 258, 264], "out": [8, 20, 21, 22, 161, 162, 163, 164, 165, 166, 172, 179, 180, 181, 187, 188, 195, 196, 210, 214, 229, 230, 235, 246], "out1": [20, 228, 246], "out2": 228, "out3": 228, "out_": [161, 162, 163, 164, 165], "out_channel": [170, 215], "out_featur": [30, 171, 175, 181], "outlier": [203, 209, 214, 265], "outlin": [195, 205, 217, 264], "output": [2, 6, 7, 8, 10, 12, 13, 17, 19, 20, 21, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 164, 166, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 185, 186, 187, 188, 189, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 208, 210, 214, 215, 216, 219, 220, 222, 223, 224, 226, 229, 230, 231, 232, 233, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265], "output_bw": [13, 25, 186, 199], "output_dim": [20, 245, 246], "output_dir": 263, "output_dir_path": 263, "output_dlc": 263, "output_encod": 30, "output_nam": [177, 181, 188, 197, 201, 208, 226, 229, 230, 237, 238, 239, 240], "output_op_nam": [12, 17, 220, 226], "output_path": [176, 204, 263], "output_qtzr": 30, "output_quant": [12, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 174, 175, 181, 185, 226, 229, 230], "output_s": [59, 60, 61, 109, 110, 111], "output_shap": 246, "outsid": [209, 231], "over": [8, 20, 25, 164, 165, 175, 180, 186, 199, 203, 210, 214, 216, 218, 219, 220, 222, 226, 264, 265], "overal": [202, 217, 218, 226, 234], "overfit": 208, "overflow": 210, "overhead": [241, 252, 254, 255, 265], "overlin": [162, 165], "overload": [9, 164, 165, 181, 229, 230], "overrid": [25, 160, 172, 181, 186, 188, 199, 228, 229, 230, 255, 256, 259, 260, 261], "overridden": [30, 175, 231], "overtax": 268, "overview": [171, 225, 264, 266], "overwri": 229, "overwriiten": 229, "overwritten": [171, 207, 229], "own": [199, 209, 214, 267], "p": [229, 237, 255], "p1": 228, "p2": 228, "pack": 235, "packag": [0, 167, 193, 210, 226, 236], "pad": [20, 172, 173, 181, 201, 202, 229, 230, 244], "pad_sequ": 245, "page": [192, 194, 195, 196, 215, 217, 265, 268], "pair": [3, 14, 17, 27, 29, 200, 201, 209, 216, 220, 222], "pairwis": 204, "pairwisedist": 119, "pandoc": 194, "paper": 209, "param": [2, 11, 12, 17, 21, 23, 29, 30, 100, 173, 174, 175, 179, 182, 184, 185, 187, 189, 197, 201, 202, 206, 208, 210, 214, 215, 216, 220, 222, 224, 226, 228, 237, 239, 240, 242, 243, 244, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 267], "param_bitwidth": [197, 224, 229, 230], "param_bw": [13, 25, 186, 199], "param_bw_override_list": [23, 184, 197], "param_encod": [171, 224], "param_nam": [8, 21, 179, 187, 214], "param_name_": [8, 21, 179, 187, 214, 249, 262], "param_quant": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 175, 181, 215, 229, 230], "param_typ": [9, 196, 197, 208, 229, 237, 238, 239, 240], "paramet": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 219, 220, 222, 224, 225, 226, 227, 228, 229, 231, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268], "parameter": [9, 229], "parameter_quant": [2, 12, 174, 185, 226], "parent": [30, 100, 175, 210, 226], "pareto": [2, 12, 13, 25, 185, 186, 199, 237, 241, 255], "pareto_front": 199, "pareto_front_list": [226, 237, 255], "pars": [23, 181, 184, 188, 197, 229, 230], "part": [8, 20, 21, 179, 187, 214, 215, 219, 229, 267], "partial": [25, 172, 186, 199, 210, 229], "particular": [170, 173, 215, 226, 231, 237, 241, 255], "pass": [2, 8, 9, 12, 13, 20, 21, 22, 23, 24, 25, 26, 30, 100, 166, 169, 171, 172, 173, 174, 175, 179, 181, 182, 184, 185, 186, 187, 188, 189, 191, 197, 198, 199, 200, 206, 207, 208, 210, 213, 214, 215, 220, 221, 226, 228, 229, 230, 233, 234, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 268], "pass_calibration_data": [197, 200, 229, 230, 237, 239, 240, 242, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "past": [210, 236], "patch": 224, "path": [2, 8, 9, 11, 12, 13, 17, 21, 22, 23, 25, 29, 168, 172, 174, 176, 179, 181, 184, 185, 186, 187, 188, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 212, 213, 214, 216, 220, 222, 226, 228, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263], "path_to_imagenet": [226, 229, 230], "pathlik": 229, "pattern": [210, 219], "pb": 263, "pcq": [8, 21, 179, 187, 214], "pcq_quantsim_config": 250, "pdf": [8, 21, 179, 183, 187, 209, 210, 214], "peft": [0, 167, 176, 204, 206, 207, 210], "peft_model": 178, "peft_model_id": [206, 207], "peftmixedmodel": 178, "penalti": 226, "pend": [24, 198], "pendyam": 219, "per": [7, 8, 17, 21, 29, 169, 170, 175, 179, 187, 191, 200, 201, 203, 210, 211, 213, 215, 216, 220, 221, 222, 224, 225, 226, 231, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265], "per_block": 224, "per_block_int_scal": 224, "per_channel": 224, "per_channel_quant": [175, 224, 231, 244, 250], "per_layer_mse_loss": [214, 249, 262], "per_layer_quant_dis": [214, 249, 262], "per_layer_quant_en": [214, 249, 262], "per_sample_weight": 71, "per_tensor": 224, "percentag": [191, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262, 264], "perform": [2, 3, 4, 8, 12, 13, 16, 21, 23, 25, 26, 28, 30, 100, 161, 162, 166, 175, 178, 179, 182, 184, 185, 186, 187, 189, 191, 192, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 214, 216, 217, 218, 219, 220, 222, 224, 225, 229, 230, 237, 238, 239, 240, 241, 242, 243, 245, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 262, 265, 266, 267, 268], "perform_per_layer_analysis_by_disabling_quant": [8, 214], "perform_per_layer_analysis_by_disabling_quant_wrapp": [21, 179, 187, 214], "perform_per_layer_analysis_by_enabling_quant": [8, 214], "perform_per_layer_analysis_by_enabling_quant_wrapp": [21, 179, 187, 214], "perhap": [237, 244, 249, 250, 255, 258, 262], "period": [11, 23, 184, 197], "perman": 210, "persist": 166, "person": 191, "perspect": [198, 204, 237, 250, 255], "phase": [2, 12, 13, 25, 174, 185, 186, 199, 219, 237, 241, 255], "phase1": [2, 12, 185, 226, 237, 255], "phase1_optim": [2, 12, 185, 226, 237, 255], "phase2": 185, "phase2_revers": 185, "phi": 210, "phone": [191, 192], "php": [236, 237, 241, 244, 249, 250, 255, 258, 262], "pick": [2, 12, 20, 185, 217, 218, 226, 237, 241, 255], "pickl": [17, 29, 216, 220, 222], "pictur": [238, 239, 240, 241, 242, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "piec": [20, 172], "pin": [194, 210], "pin_memori": [160, 172, 255], "pink": 223, "pinpoint": 214, "pip": [195, 196, 210, 221, 236], "pipelin": [25, 186, 199, 210, 213, 234, 245, 247, 248, 265, 267], "piptool": 194, "pitr": 219, "pixelshuffl": 120, "pixelunshuffl": 121, "place": [2, 4, 12, 22, 23, 24, 28, 181, 183, 184, 185, 188, 197, 198, 202, 209, 210, 226, 229, 230, 231, 237, 238, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "place_model": [206, 207], "placehold": [206, 207], "placement": [209, 210, 237, 238, 239, 240, 252, 253, 254, 255, 256, 259, 260, 261, 262], "plai": 250, "plan": [195, 210, 267], "platform": [190, 191, 195, 214, 238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "pleas": [171, 173, 178, 181, 188, 195, 210, 214, 216, 220, 222, 229, 230, 237, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 258, 262, 263], "plot": [2, 12, 168, 185, 210, 212, 214, 226, 237, 241, 249, 255, 262], "pmatrix": [161, 162, 163, 164, 165], "point": [2, 8, 12, 20, 21, 30, 158, 159, 170, 171, 179, 185, 187, 188, 191, 201, 202, 203, 209, 210, 214, 215, 224, 226, 229, 230, 234, 237, 239, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 264, 265, 267, 268], "pointer": [241, 244, 245, 249, 250, 262], "poissonnllloss": 122, "polish": 210, "pool": [241, 242, 244, 250, 251], "poor": 264, "pop": 246, "popul": 224, "popular": 205, "port": [210, 221], "portabl": 191, "portion": [20, 202], "pos_emb": [20, 246], "posit": [20, 148, 149, 164, 165, 210, 244, 245, 246], "possibl": [2, 3, 12, 13, 20, 22, 25, 173, 177, 181, 185, 186, 188, 199, 201, 214, 218, 226, 229, 230, 231, 232, 233, 234, 237, 241, 243, 246, 255, 257, 268], "post": [13, 25, 186, 191, 193, 196, 199, 202, 210, 214, 219, 224, 230, 238, 239, 240, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 265, 268], "post_training_tf": [8, 11, 21, 22, 23, 168, 179, 184, 187, 197, 199, 210, 212, 214, 224, 229, 230, 237, 241, 242, 244, 245, 247, 250, 251, 255, 258, 259], "post_training_tf_enhanc": [8, 11, 13, 21, 22, 23, 25, 179, 181, 184, 186, 187, 188, 197, 199, 214, 224, 229, 230, 237, 239, 241, 244, 245, 249, 255, 256, 258, 259, 260, 262], "potenti": [221, 244, 258], "power": [191, 210, 224, 227, 234, 264], "pp": 219, "practic": [215, 219, 226, 237, 241, 243, 244, 245, 249, 250, 252, 253, 254, 255, 257, 262], "pre": [195, 198, 201, 204, 208, 209, 210, 211, 213, 214, 216, 219, 225, 229, 236, 238], "preced": [200, 203, 223, 228], "precis": [2, 12, 13, 25, 161, 162, 163, 164, 165, 166, 174, 185, 186, 191, 193, 199, 210, 224, 230, 234, 236, 238, 239, 240, 242, 244, 245, 247, 248, 249, 250, 251, 256, 258, 259, 260, 261, 262], "precomput": [206, 229], "precursor": 201, "pred": [220, 226, 241, 255], "pred_label": [197, 199, 208, 226, 229, 238], "pred_prob": [197, 199, 226, 229, 238], "predefin": [243, 257], "predict": [12, 191, 213, 214, 220, 226, 229, 241, 249], "prefer": [171, 195, 236, 267], "prefix": [11, 22, 23, 175, 181, 184, 188, 197, 229, 230], "prelu": [118, 210], "prepar": [20, 24, 25, 172, 178, 186, 198, 199, 201, 202, 209, 210, 226, 232, 233, 243, 266, 268], "prepare_model": [20, 172, 201, 202, 214, 246, 255, 256, 258, 260, 261, 262], "prepared_model": [172, 201, 202, 214], "prepend": [175, 236], "preprocess": [197, 199, 200, 208, 220, 226, 229, 230, 238, 241, 242, 245, 247, 248, 250], "preprocess_input": [197, 199, 200, 220, 226, 229, 230, 241, 242, 247, 248, 250, 251], "prequantize_const": 177, "prerequisit": 221, "presenc": 230, "present": [169, 171, 173, 178, 213, 215, 224, 246, 258], "preserv": [166, 172, 197, 210, 226], "preserve_format": 160, "preset": 226, "pretrain": [197, 199, 200, 201, 202, 208, 214, 226, 228, 230, 237, 238, 239, 240, 241, 242, 247, 248, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 265], "pretti": [216, 220, 222], "prev": 202, "prev_conv_weight": 202, "prevent": [171, 172, 207, 210, 216], "previou": [2, 12, 17, 29, 180, 185, 191, 216, 217, 220, 222, 226, 234, 237, 241, 255, 263, 264], "print": [30, 164, 165, 171, 172, 173, 175, 180, 181, 195, 196, 197, 199, 200, 201, 202, 208, 214, 216, 220, 222, 226, 229, 230, 237, 238, 239, 240, 244, 245, 246, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "prior": [2, 12, 185, 210, 224, 226, 237, 241, 255, 256, 259, 260, 261], "privileg": 236, "probabl": [191, 210], "problem": [172, 210, 234, 268], "problemat": [172, 234], "proce": [13, 25, 186, 199, 201, 229, 264, 268], "procedur": [210, 218, 221, 239, 251, 252, 254, 259], "proceed": [201, 202, 226, 237, 239, 240], "process": [191, 199, 201, 210, 213, 216, 218, 219, 220, 226, 228, 229, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 259, 260, 261, 264, 265, 267], "processor": [191, 193, 195, 196], "produc": [13, 25, 158, 159, 168, 172, 186, 191, 199, 202, 211, 212, 214, 218, 224, 226, 237, 249, 250, 255, 262, 265], "product": [191, 192, 217], "profil": [174, 193, 194, 226, 263, 264], "progbar": [242, 247, 248, 250, 251], "progbar_stat_upd": [242, 247, 248, 250, 251], "progress": [210, 221], "project": 191, "promot": [210, 226], "prone": [199, 228], "pronounc": 227, "propag": [210, 223, 228], "propagate_encod": [25, 181, 186, 188, 199, 229, 230], "proper": [209, 210], "properli": [161, 162, 210, 213], "properti": [166, 171], "protobuf": 194, "protocol_buffers_python_implement": 194, "provid": [0, 2, 5, 8, 9, 12, 18, 20, 21, 23, 25, 30, 166, 167, 171, 173, 178, 179, 181, 184, 185, 186, 187, 188, 191, 193, 194, 195, 197, 199, 201, 206, 208, 210, 214, 215, 217, 218, 219, 220, 221, 224, 226, 228, 229, 230, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268], "proxi": 172, "prune": [29, 191, 210, 217, 218, 220, 222, 223, 225, 236, 253], "pt_model": [197, 201, 202, 208, 226, 229, 237, 238, 239, 240], "pth": [22, 181, 188, 213, 216, 220, 222, 229, 230], "ptq": [13, 25, 186, 191, 192, 193, 198, 199, 202, 203, 204, 209, 210, 214, 219, 229, 230, 243, 257, 265, 267, 268], "public": [0, 167, 210, 229], "publish": 210, "pure": [20, 233], "purpos": [191, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "put": [185, 238, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "py38": 195, "pylint": 209, "pypi": [196, 210], "pyproject": 194, "pytest": 194, "python": [20, 191, 193, 195, 196, 210], "python3": [194, 195, 210, 236], "pythonpath": [213, 236], "pytorch": [0, 25, 167, 169, 172, 173, 175, 180, 181, 182, 186, 188, 189, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 231, 236, 242, 243, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 268], "q": [30, 158, 159, 160, 161, 162, 164, 165, 166, 171, 175, 180, 206, 207, 265], "q_": 265, "q_modul": 171, "q_output": 30, "q_proj": 209, "qadaround": 197, "qadd": 175, "qairt": 263, "qat": [11, 23, 184, 191, 193, 197, 200, 206, 207, 210, 234, 236, 237, 240, 245, 250, 255, 265, 267, 268], "qat2": 210, "qc": 210, "qc_op": 9, "qc_quantize_op": 210, "qcquantizeop": [2, 9, 210, 226], "qcquantizewrapp": [14, 171, 200, 210], "qdo": 191, "qdq": [162, 166, 206, 210, 264, 267, 268], "qlinear": [30, 171, 175], "qmax": [161, 162, 164, 165, 180, 181, 229, 230], "qmin": [161, 162, 164, 165, 180, 181, 229, 230], "qmodul": 171, "qmul": 175, "qmult": 30, "qnn": [193, 210, 250, 251, 263, 267], "qol": 210, "qsim": [24, 198, 244], "qtype": [9, 229], "qtzr": [180, 206], "quad": [161, 162, 163, 164, 165], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 264, 265, 266, 267, 268, 269], "qualiti": [237, 255], "quant": [2, 8, 9, 13, 21, 22, 23, 25, 179, 181, 182, 184, 186, 187, 188, 189, 197, 199, 208, 210, 214, 226, 229, 230, 236, 237, 238, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 257, 258, 259, 260, 261], "quant_analyz": [0, 5, 18, 167, 171, 214, 249, 262], "quant_analyzer_result": 214, "quant_dequ": 158, "quant_schem": [8, 9, 13, 21, 22, 25, 168, 179, 181, 186, 187, 188, 195, 197, 198, 199, 200, 204, 209, 212, 214, 224, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "quant_sim": [176, 204, 228], "quant_sim_model": [188, 229], "quant_stats_visu": [168, 212], "quant_wrapp": 171, "quantanalyz": [8, 21, 171, 179, 187, 210, 211, 264], "quantiz": [0, 1, 2, 5, 7, 8, 9, 11, 12, 13, 14, 18, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 165, 166, 167, 168, 170, 172, 174, 177, 178, 179, 181, 182, 184, 185, 186, 187, 188, 189, 191, 193, 195, 197, 198, 200, 201, 202, 204, 207, 208, 209, 210, 212, 213, 217, 219, 221, 227, 228, 231, 233, 235, 236, 243, 246, 252, 253, 254, 257], "quantizablemultiheadattent": 210, "quantizaiton": 7, "quantization_overrid": 263, "quantization_tf": 195, "quantizationdatatyp": [2, 12, 13, 22, 25, 171, 174, 181, 185, 186, 188, 199, 226, 229, 230, 237, 241, 255], "quantizationmixin": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 175, 206, 207], "quantizationmod": 195, "quantizationsim": [237, 239, 240, 242, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 264], "quantizationsimmodel": [1, 2, 7, 8, 9, 10, 12, 13, 14, 21, 22, 24, 25, 30, 168, 170, 171, 174, 176, 177, 179, 180, 181, 182, 186, 187, 188, 189, 196, 197, 198, 199, 200, 204, 206, 208, 209, 210, 212, 213, 214, 215, 224, 226, 228, 229, 230, 231, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265], "quantizationsimmodelv1": 171, "quantizationsimmodelv2": 171, "quantized_": [197, 198, 204], "quantized_callback": [244, 245, 247, 248], "quantized_dlc": 263, "quantized_linear": 30, "quantized_mobilenet_v2": [197, 200, 208, 229], "quantized_model": [181, 229, 230], "quantized_repr": [30, 158, 159, 160], "quantizedadd": 175, "quantizedconv2d": [171, 175, 181, 215, 229, 230], "quantizedequant": [158, 171, 175, 180, 181, 195, 206, 215, 229, 230], "quantizedlinear": [30, 171, 175, 181, 215], "quantizedmaskedadd": 30, "quantizedmultipli": [30, 175], "quantizedoptlearnedpositionalembed": [206, 207], "quantizedrelu": 171, "quantizedsoftmax": 175, "quantizedtensor": [30, 158, 160, 161, 180], "quantizelinear": [9, 177, 210, 264], "quantizer_arg": 224, "quantizer_config": [12, 226], "quantizer_group": [2, 12, 174, 185, 226], "quantizer_info": [2, 226], "quantizer_nam": [12, 226], "quantizerbas": [30, 175, 180, 207], "quantizergroup": [2, 12, 13, 25, 174, 185, 186, 199, 226], "quantschem": [8, 9, 11, 13, 21, 22, 23, 25, 168, 179, 181, 184, 186, 187, 188, 196, 197, 198, 199, 200, 204, 209, 210, 212, 214, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "quantsim": [0, 5, 6, 8, 11, 18, 19, 21, 23, 167, 168, 169, 171, 174, 178, 179, 184, 187, 191, 196, 197, 198, 200, 201, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 224, 226, 227, 228, 230, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 255, 256, 258, 259, 260, 261, 264, 267, 268], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "quantsim_layer_output": 213, "quantsim_layer_output_util": 213, "quantsimmodel": [198, 204, 209], "quatiz": 267, "quic": [191, 195, 236], "quick": [180, 192, 193, 195, 210, 229, 246, 250, 252, 254], "quickli": [192, 237, 241, 249, 250, 255, 262], "qwa": 210, "qwen": [204, 209], "qwen2": [204, 210], "qwen2forcausallm": 209, "r": [2, 8, 12, 21, 178, 179, 185, 187, 194, 214, 226, 250], "r1": [183, 209, 210], "r1_fusion_pair": 209, "r1_placement": 209, "r2": 209, "r3": 209, "r4": 209, "radic": 268, "rais": [20, 30, 166, 199, 209, 226, 246], "rand": [173, 198, 213, 214, 255, 256, 258, 259, 260, 261, 262], "randint": [204, 209], "randn": [30, 159, 160, 161, 162, 172, 173, 175, 181, 195, 196, 197, 199, 200, 201, 202, 208, 214, 226, 228, 229, 230, 237, 238, 239, 240, 257], "random": [20, 195, 196, 214, 216, 226, 246, 257], "random_input": [20, 246], "random_split": 229, "randperm": 255, "rang": [8, 20, 21, 164, 165, 168, 172, 179, 180, 187, 188, 191, 197, 200, 202, 203, 204, 206, 207, 209, 210, 211, 212, 218, 226, 229, 230, 234, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 250, 251, 255, 256, 257, 258, 259, 260, 264, 265], "rank": [17, 29, 203, 205, 220, 222], "rank_select": 222, "rank_select_schem": [29, 222], "rankselectschem": [29, 216, 220, 222], "rapidli": 193, "rare": [210, 264], "rate": [20, 210, 219, 230, 244, 245, 246, 247, 248, 252, 253, 254, 258, 260, 261], "rather": [172, 208, 231, 244, 258], "ratio": [17, 29, 216, 217, 220, 222, 252, 253, 254], "raw": [204, 206, 207, 209], "rceil": [164, 165], "re": [2, 12, 180, 182, 185, 189, 208, 210, 226, 236, 237, 246, 255, 268], "re_estimation_dataset": 244, "re_estimation_dataset_s": 244, "reach": [199, 243, 257], "read": 214, "reader": [237, 244, 249, 250, 255, 258, 262], "readi": [22, 180, 181, 188, 209, 229, 230, 234, 237, 239, 240, 241, 244, 245, 247, 248, 250, 252, 253, 254, 255, 256, 259, 260, 261, 262, 264, 267], "readili": [237, 241, 244, 249, 250, 255, 258, 262], "real": [158, 159, 196, 221, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "realiz": [174, 228], "realli": [237, 244, 249, 250, 258, 262], "reason": [173, 230, 238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261, 268], "reassess": 264, "recalcul": [244, 258], "receiv": 172, "recent": 267, "recip": 210, "recogn": [173, 265], "recommend": [1, 2, 8, 9, 20, 23, 177, 180, 184, 193, 195, 196, 197, 198, 200, 201, 202, 204, 214, 217, 226, 229, 234, 237, 238, 239, 240, 241, 242, 243, 246, 250, 256, 257, 264, 267, 268], "recomput": [210, 238], "reconfigur": 210, "reconstruct": [11, 23, 184, 197, 252, 254], "record": [8, 21, 179, 187, 214], "recov": [210, 234, 239, 251, 252, 253, 254, 259, 267, 268], "recoveri": 268, "recurr": 210, "recurs": 210, "redefin": 173, "redesign": 210, "reduc": [1, 10, 175, 178, 191, 192, 197, 200, 202, 203, 208, 209, 210, 215, 219, 223, 225, 227, 230, 234, 237, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265], "reduct": [217, 252, 253, 254, 268], "redund": [201, 219], "reestim": [14, 26, 200, 244], "reestimate_bn_stat": [14, 26, 200, 244, 258], "refer": [169, 171, 178, 182, 188, 189, 208, 213, 215, 224, 226, 229, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263], "reflect": [247, 248, 260, 261, 265], "reflectionpad1d": 128, "reflectionpad2d": 129, "reflectionpad3d": 130, "regard": [2, 12, 185, 226, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "regardless": [198, 204], "regist": [30, 166, 198, 206, 207, 209, 210, 237], "regress": 216, "regular": [11, 12, 22, 23, 30, 180, 181, 184, 188, 197, 226, 229, 230, 237, 238, 245], "rel": [12, 17, 29, 216, 217, 220, 222, 226, 234, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 264], "relat": [171, 216, 220, 222, 265], "relationship": 215, "releas": [194, 195, 204, 236], "release_tag": 236, "relev": [252, 253, 254, 267], "reli": [171, 208, 230, 237, 239, 240, 241, 244, 245, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "reliabl": 195, "reload": 259, "relu": [20, 126, 171, 172, 173, 197, 201, 202, 210, 223, 231, 232, 233, 244, 245, 246], "relu1": [20, 173, 233], "relu2": [20, 173, 232, 233], "relu6": [127, 201, 202, 210], "remain": [24, 171, 182, 189, 197, 198, 202, 204, 208, 209, 210, 230], "remov": [14, 25, 26, 172, 186, 191, 199, 200, 201, 207, 210, 216, 223, 224, 229, 246, 265], "remove_all_quant": 207, "remove_column": [204, 206, 207, 209], "renam": 210, "reorder": 246, "reorgan": 210, "repeat": [178, 216, 250, 264, 268], "repeatedli": 267, "replac": [9, 171, 172, 175, 178, 206, 210, 215, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "replace_lora_layers_with_quantizable_lay": [178, 206, 207], "replicationpad1d": 131, "replicationpad2d": 132, "replicationpad3d": 133, "report": [210, 228, 264, 268], "repositori": 236, "repres": [2, 8, 9, 13, 17, 20, 21, 22, 25, 158, 159, 160, 166, 174, 175, 179, 181, 185, 186, 187, 188, 191, 196, 199, 208, 210, 214, 216, 218, 220, 222, 224, 226, 228, 229, 230, 233, 249, 262, 263, 265], "represent": [158, 159, 160, 166, 180, 191, 210, 246, 263, 265], "requant": 201, "request": 228, "requir": [2, 17, 20, 29, 30, 169, 171, 172, 178, 180, 181, 188, 191, 194, 195, 197, 198, 201, 202, 204, 206, 208, 209, 210, 213, 214, 215, 216, 217, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 233, 237, 241, 246, 249, 252, 253, 254, 255, 256, 259, 260, 261, 262, 264, 265, 267, 268], "requires_grad": [159, 160, 172, 202, 210, 229], "requires_grad_": [171, 206, 207], "rerun": 173, "resblock": 20, "rescal": 202, "research": 191, "resembl": 20, "resid": 210, "residu": 216, "resiz": [197, 199, 208, 210, 226, 229, 238, 255], "resnet": [181, 217, 220, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251], "resnet18": [181, 214, 229, 230, 237, 238, 239, 240, 242, 243, 247, 248, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "resnet18_after_adaround": 238, "resnet18_after_cle_bc": [256, 257, 259], "resnet18_after_qat": [258, 260, 261], "resnet18_mixed_precis": [237, 255], "resnet50": [214, 220, 226, 241, 242, 243, 247, 248, 249, 250, 251], "resnet50_after_adaround": 242, "resnet50_after_amp": 241, "resnet50_after_cl": 251, "resnet50_pcq_adaround": 250, "resolv": [173, 210], "resort": 234, "resourc": [191, 237, 244, 245, 246, 250, 255, 258, 268], "respecit": [237, 255], "respect": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 175, 180, 182, 189, 203, 208, 210, 214, 232, 247, 248], "respond": [249, 262], "respons": [2, 12, 185, 191, 219, 226, 237, 241, 255, 268], "ressembl": 246, "rest": [210, 234, 242, 247, 248, 250, 251], "restor": [188, 210, 234, 252, 253, 254, 265, 267], "restrict": 215, "resu": 20, "result": [2, 8, 12, 13, 20, 21, 25, 30, 158, 159, 160, 179, 180, 181, 185, 186, 187, 188, 197, 198, 199, 200, 202, 204, 206, 207, 208, 209, 211, 214, 216, 217, 219, 220, 222, 226, 229, 230, 231, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265], "results_dir": [2, 8, 12, 13, 21, 25, 179, 185, 186, 187, 199, 214, 226, 237, 241, 249, 255, 262], "retain": [191, 226], "retest": 268, "retrain": [191, 233, 267], "retriev": 30, "retrieve_context": 263, "retuern": 226, "return": [2, 3, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 158, 159, 160, 161, 162, 166, 168, 169, 170, 172, 173, 174, 176, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 212, 213, 214, 215, 216, 218, 220, 222, 226, 229, 230, 232, 233, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "return_dict": [204, 206, 207, 209], "reus": [172, 173, 232, 233, 237, 255], "reveal": 234, "revert": 234, "review": 245, "revis": 224, "revisit": 217, "rework": 224, "rewrit": 173, "rfloor": [161, 162, 163, 164, 165, 166, 265], "rgb": [199, 226, 233], "rgb_output": 233, "right": [161, 162, 163, 164, 165, 166, 175, 180, 215, 223, 226, 237, 241, 243, 245, 255, 257, 265], "rmsnorm": [209, 210], "rmsnorm_fusion_pair": 209, "rmsnorm_linear_pair": 209, "rmsnormal": 210, "rnn": [123, 210], "rnncell": 124, "robust": [177, 208, 210, 267], "root": [194, 197, 208, 210, 255, 257], "rotat": [183, 203, 209, 210], "rough": [237, 255], "roughli": [25, 174, 185, 186, 199, 226], "round": [1, 11, 13, 17, 21, 22, 23, 25, 29, 184, 186, 188, 191, 199, 210, 214, 220, 222, 229, 230, 236, 250, 264, 265], "round_nearest": 195, "rounding_mod": [13, 21, 22, 25, 181, 186, 188, 199, 210, 214, 229, 230, 241, 242, 245, 247, 248, 250, 251], "roundingmod": 195, "routin": [229, 237, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "rrelu": 125, "rtype": [9, 12, 20, 158, 159, 160, 174, 185, 226], "rule": [22, 210, 215, 229, 230, 231, 267], "run": [2, 8, 9, 12, 13, 17, 20, 21, 22, 24, 25, 26, 29, 161, 162, 171, 172, 173, 174, 175, 177, 179, 180, 181, 185, 186, 187, 188, 191, 193, 196, 197, 198, 199, 200, 208, 210, 213, 215, 216, 219, 220, 221, 222, 224, 226, 227, 228, 229, 235, 238, 239, 240, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "run_forward_pass": [9, 181, 229, 230], "run_infer": [13, 25, 186, 199, 257], "runnabl": 267, "runtim": [10, 22, 30, 180, 181, 188, 191, 192, 193, 201, 208, 210, 213, 214, 215, 216, 217, 219, 220, 222, 224, 226, 229, 230, 237, 238, 239, 240, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261, 263, 264, 266, 267, 268], "runtimeerror": [166, 181, 229, 230], "s2": 198, "s3": 198, "s_1": 215, "s_2": 215, "s_n": 215, "safe": 160, "safetensor": [176, 204], "sai": [172, 217, 228, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "sake": [213, 239, 259], "same": [8, 20, 21, 25, 30, 100, 158, 159, 169, 171, 172, 173, 174, 175, 177, 178, 179, 181, 184, 185, 186, 187, 188, 189, 199, 202, 203, 205, 209, 213, 214, 215, 228, 229, 230, 231, 237, 239, 240, 241, 242, 244, 245, 246, 247, 248, 250, 252, 253, 254, 255, 256, 258, 259, 260, 261], "sampl": [1, 2, 8, 9, 10, 13, 21, 22, 24, 25, 168, 174, 175, 179, 182, 185, 186, 187, 188, 189, 190, 196, 197, 198, 199, 201, 202, 204, 206, 207, 208, 209, 210, 212, 214, 216, 220, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265], "sampled_dataset": [243, 244, 249], "sandeep": 219, "saniti": [180, 210], "satisfactori": [197, 198, 200, 204, 208, 234, 268], "satisfi": [172, 174, 199, 215, 228, 243, 257], "saurabh": 219, "save": [2, 6, 8, 9, 11, 12, 13, 17, 19, 21, 22, 23, 25, 29, 168, 169, 176, 179, 181, 184, 185, 186, 187, 188, 197, 199, 204, 210, 211, 212, 213, 214, 216, 220, 222, 226, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 267], "save_checkpoint": 188, "save_dir": [19, 213], "save_path": [168, 212], "saved_eval_scores_dict": [17, 29, 216, 220, 222], "saw": 250, "scalar": [8, 13, 21, 179, 187, 199, 210, 214], "scale": [14, 30, 159, 160, 161, 162, 163, 164, 165, 166, 176, 180, 191, 195, 200, 202, 203, 204, 210, 214, 215, 224, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 263, 265, 267], "scale_": [161, 162, 163, 164, 165, 180], "scenario": [223, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 256, 259, 260, 261, 262], "scene": 267, "schedul": [210, 252, 253, 254, 258, 260, 261], "schema": 210, "scheme": [8, 9, 11, 13, 17, 21, 22, 23, 25, 29, 169, 179, 181, 184, 186, 187, 188, 191, 197, 199, 210, 213, 214, 215, 216, 219, 220, 222, 229, 230, 237, 241, 244, 245, 249, 252, 253, 254, 255, 258, 262], "scope": 172, "score": [2, 8, 12, 13, 17, 21, 25, 29, 179, 185, 186, 187, 199, 214, 216, 218, 219, 220, 221, 222, 226, 237, 238, 239, 240, 241, 243, 244, 257, 258], "script": 258, "sdk": [192, 237, 250, 255, 264, 267], "search": [2, 12, 13, 25, 182, 185, 186, 189, 199, 203, 208, 210, 226, 230, 231, 237, 244, 245, 247, 248, 252, 253, 254, 255, 258, 260, 261, 264, 267], "searcher": 219, "sec": [237, 241, 250, 255], "second": [9, 13, 20, 23, 175, 178, 181, 182, 184, 189, 197, 199, 201, 208, 215, 226, 229, 230, 249, 262], "secondari": 210, "section": [2, 173, 185, 206, 207, 210, 215, 225, 226, 231, 235, 237, 244, 255, 257, 264, 265, 266, 268], "see": [0, 17, 20, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 167, 188, 192, 195, 196, 197, 202, 210, 215, 216, 219, 220, 221, 222, 229, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 268], "seed": 216, "seem": 219, "seen": [214, 249, 262], "select": [13, 17, 25, 29, 186, 191, 195, 199, 206, 207, 214, 217, 220, 221, 222, 223, 227, 230, 231, 237, 241, 255, 263, 264, 265], "select_param": [29, 222], "self": [13, 20, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 172, 173, 199, 204, 209, 226, 232, 233, 246, 257], "selu": 134, "sens": [175, 226], "sensit": [2, 8, 12, 21, 179, 185, 187, 200, 210, 211, 218, 225, 227, 230, 237, 239, 241, 251, 255, 259, 264, 265, 267, 268], "sentiment": 245, "separ": [11, 20, 22, 23, 171, 172, 173, 181, 184, 188, 197, 200, 210, 214, 229, 230, 234, 241], "separableconv2d": 210, "seq_length": [204, 206, 207, 209], "seq_ms": [0, 167, 171, 208], "seqms": [203, 208, 210], "seqmseparam": [182, 189, 208], "sequanti": 246, "sequenc": [181, 197, 199, 203, 229, 230, 231, 243, 245, 257], "sequenti": [10, 20, 171, 182, 189, 201, 202, 210, 230, 231, 232, 234, 244, 245, 246], "seri": [25, 174, 181, 186, 188, 199, 228, 229, 230, 243, 257, 263], "serial": [229, 263], "serializetostr": [213, 237, 238, 239, 240], "serv": [175, 197, 199, 209, 214, 221], "servic": 228, "sess": [9, 177, 220, 229, 237, 238, 239, 240], "sess_opt": 177, "session": [9, 17, 196, 197, 208, 210, 213, 214, 220, 226, 229, 237, 238, 239, 240], "sessionopt": 177, "set": [1, 2, 7, 8, 9, 10, 11, 12, 13, 17, 20, 21, 25, 29, 30, 166, 168, 170, 171, 172, 173, 174, 175, 177, 179, 185, 186, 187, 188, 191, 196, 197, 199, 200, 205, 206, 207, 208, 210, 212, 214, 215, 216, 217, 218, 219, 220, 222, 224, 226, 227, 229, 230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268], "set_activation_quantizers_to_float": [170, 215], "set_adapt": 178, "set_adaround_param": [13, 25, 186, 199, 243, 257], "set_and_freeze_param_encod": [197, 242, 250, 256], "set_blockwise_quantization_for_weight": [170, 215], "set_default_kernel": 30, "set_export_param": [25, 186, 199], "set_extra_st": 166, "set_grouped_blockwise_quantization_for_weight": [0, 5, 170, 215], "set_kernel": 30, "set_mixed_precision_param": [13, 25, 186, 199], "set_model_input_precis": [174, 228], "set_model_output_precis": [174, 228], "set_model_preparer_param": [25, 186, 199], "set_precis": [174, 228], "set_quant": 210, "set_quant_scheme_candid": [13, 25, 186, 199], "set_quantizers_to_candid": [2, 12, 174, 185, 226], "set_rang": [158, 159], "set_transform": [199, 226], "set_verbos": [226, 241], "settabl": 265, "setup": [171, 194, 195, 196, 201, 226], "sever": [173, 175, 208, 210, 214, 217, 232, 233, 264], "sgd": [200, 229, 230], "sh": 194, "shall": 224, "shape": [9, 12, 20, 27, 28, 29, 158, 159, 160, 161, 162, 166, 170, 171, 172, 173, 175, 180, 181, 197, 201, 202, 206, 208, 214, 215, 216, 220, 222, 226, 229, 230, 232, 237, 238, 239, 240, 244, 245, 246, 250, 255, 256, 258, 259, 260, 261, 262], "share": [173, 175, 209, 215], "sharp": 219, "sharpli": [252, 253, 254, 268], "shift": 251, "shift_label": [206, 207], "shift_logit": [206, 207], "should": [8, 12, 17, 20, 21, 22, 24, 29, 30, 100, 160, 169, 171, 172, 175, 179, 181, 182, 187, 188, 189, 195, 196, 197, 198, 199, 208, 209, 213, 214, 215, 216, 217, 220, 222, 224, 226, 229, 230, 232, 233, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 268], "shouldn": 180, "show": [173, 178, 209, 211, 215, 218, 226, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264], "showcas": [236, 249, 262], "shown": [171, 178, 194, 195, 196, 199, 214, 223, 226, 233, 234, 249, 262], "shuffl": [197, 198, 199, 200, 204, 208, 209, 216, 220, 226, 229, 230, 238, 241, 242, 247, 248, 250], "side": 223, "sigmoid": [20, 136, 172], "sign": [164, 165, 171, 180, 265], "signatur": [9, 17, 29, 30, 164, 165, 181, 199, 210, 214, 216, 220, 222, 229, 230, 252, 253, 254], "signific": [197, 226, 234], "significantli": [210, 230, 264], "silu": 135, "sim": [1, 2, 7, 8, 9, 10, 12, 14, 21, 22, 24, 168, 170, 171, 174, 177, 178, 179, 181, 182, 185, 187, 188, 189, 196, 197, 198, 199, 200, 204, 207, 208, 209, 210, 212, 214, 215, 224, 226, 228, 229, 230, 238, 241, 244, 245, 250, 257], "sim1": 171, "sim2": 171, "sim_model": [242, 247, 248, 249, 250, 251, 256, 258, 259, 260, 261, 262], "similar": [178, 180, 191, 195, 209, 215, 264, 265], "similarli": [234, 237, 241, 249, 250, 255, 258, 262, 264, 268], "simpl": [172, 214, 216, 220, 222, 241, 244, 245, 249, 250, 262], "simpler": 171, "simpli": [8, 21, 22, 172, 179, 187, 188, 214, 229, 230, 237, 241, 244, 245, 250, 255, 258], "simplic": [178, 213], "simplif": [202, 264], "simplifi": [1, 2, 8, 9, 171, 197, 201, 202, 213, 214, 226, 229, 263], "simuat": [238, 239, 242, 247, 248, 250, 251, 256, 259, 260, 261], "simul": [9, 22, 166, 170, 175, 178, 181, 188, 191, 193, 197, 198, 200, 201, 204, 208, 209, 210, 211, 212, 213, 215, 224, 226, 227, 229, 230, 231, 233, 235, 236, 244, 245, 252, 253, 254, 262, 264, 268], "sinc": [181, 209, 215, 217, 226, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 265], "singl": [2, 6, 13, 19, 25, 169, 172, 174, 185, 186, 199, 210, 213, 214, 215, 216, 218, 219, 220, 222, 224, 226, 249, 250, 262, 265], "singular": [219, 220, 222, 253, 254], "situat": 214, "six": 231, "size": [7, 8, 20, 160, 161, 162, 163, 164, 165, 170, 173, 175, 180, 191, 197, 198, 204, 214, 215, 220, 222, 224, 225, 226, 229, 242, 243, 244, 245, 246, 250, 256, 264, 267, 268], "skbuild_build_target": 194, "skew": [238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "skip": [23, 25, 182, 184, 186, 189, 194, 197, 199, 200, 206, 208, 216, 229, 230, 239, 251, 259, 264], "skipped_optim": 201, "slight": 250, "slightli": [205, 237, 255], "slim": 210, "slow": 219, "slower": 210, "small": [9, 181, 200, 205, 208, 215, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 264], "smaller": [2, 12, 23, 184, 185, 197, 202, 219, 220, 222, 226, 227, 234, 237, 250, 255, 267], "smoothl1loss": 137, "snapdragon": [237, 250, 255], "snippet": [172, 215, 237, 255], "snpe": [210, 250, 251], "so": [8, 9, 22, 23, 24, 171, 172, 173, 175, 181, 184, 188, 191, 195, 197, 198, 201, 202, 206, 207, 209, 214, 216, 218, 220, 222, 229, 230, 237, 241, 242, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256, 258, 260, 261, 262, 263, 268], "softmarginloss": 138, "softmax": [20, 139, 175, 245, 246, 255], "softmax2d": 140, "softmin": 141, "softplu": [142, 172], "softshrink": 143, "softsign": 144, "softwar": [191, 192, 210], "sole": 214, "solid": 230, "solut": [218, 226, 230, 234, 241, 255], "some": [17, 20, 29, 171, 172, 175, 180, 197, 202, 206, 207, 216, 218, 220, 222, 223, 226, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 264, 265, 267, 268], "someth": [22, 188, 214, 219, 229, 230], "sometim": [214, 216, 219, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 255, 256, 259, 260, 261, 268], "somewher": 268, "soon": 199, "sort": [24, 198, 246], "sourc": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 172, 173, 174, 175, 177, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 197, 199, 200, 201, 202, 208, 209, 210, 212, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 234], "space": [29, 216, 220, 222, 226, 265], "spars": 210, "sparse_categorical_crossentropi": 245, "spatial": [17, 29, 210, 216, 217, 218, 222, 225, 236], "spatial_svd": [29, 216, 220, 222, 253, 254], "spatial_svd_auto_mod": 220, "spatial_svd_manual_mod": 220, "spatialsvdparamet": [17, 29, 216, 220, 222, 253, 254], "spconv": 210, "special": [25, 172, 186, 191, 199], "specif": [8, 17, 20, 22, 25, 29, 30, 170, 173, 175, 179, 181, 186, 187, 188, 194, 199, 203, 205, 209, 210, 214, 215, 216, 220, 222, 229, 230, 231, 237, 238, 241, 242, 244, 246, 250, 255, 256, 258, 263, 264, 265, 268], "specifi": [2, 8, 13, 17, 21, 22, 23, 29, 161, 162, 163, 164, 165, 166, 174, 179, 181, 184, 185, 187, 188, 197, 199, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 228, 229, 230, 231, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "speed": [17, 29, 201, 210, 216, 219, 220, 222, 225, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "speedup": [182, 189, 208, 237, 250, 255], "spinquant": [0, 167, 210], "spinquant_optim": 209, "split": [182, 189, 197, 199, 200, 204, 206, 207, 208, 209, 226, 229, 230, 238, 265], "sqnr": [2, 13, 25, 174, 182, 185, 186, 189, 199, 208, 226, 237, 241, 255, 265], "sqrt": 172, "squar": [211, 265], "squeez": [244, 255], "ssvd": 217, "ssvd_comp_stat": 254, "ssvd_compressed_model": 254, "ssvd_cp_compressed_model": 254, "ssvd_cp_finetuned_model": 254, "ssvd_finetuned_model": 254, "stabl": [172, 177, 197, 230, 244, 258], "stack": [190, 226], "stand": 210, "standalon": [197, 210], "standard": [12, 166, 172, 175, 191, 226, 240, 245, 247, 248, 260, 261], "start": [2, 11, 12, 20, 23, 164, 165, 172, 173, 178, 184, 185, 192, 195, 197, 210, 217, 226, 230, 231, 235, 236, 237, 241, 244, 245, 246, 247, 248, 249, 250, 252, 253, 254, 255, 258, 260, 261, 262, 264, 265, 268], "start_beta": [11, 23, 184, 197], "start_i": [226, 241], "start_x": [226, 241], "starting_op_nam": [12, 226], "stat": [8, 26, 168, 200, 212, 214, 216, 220, 222], "statatist": 258, "state": [166, 182, 189, 190, 193, 208, 226, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262], "state_dict": 166, "stateless": 233, "statement": 172, "static": [12, 17, 20, 29, 172, 173, 210, 216, 220, 222, 226], "static_patch_count": 20, "staticgridperchannelquant": 171, "staticgridquant": 171, "staticgridquantwrapp": 171, "staticmethod": [172, 237, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "statist": [8, 9, 17, 21, 26, 29, 161, 162, 166, 168, 175, 179, 181, 187, 200, 203, 211, 212, 216, 220, 222, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261], "std": [197, 199, 208, 226, 229, 238, 255, 257], "step": [1, 8, 12, 21, 164, 165, 172, 174, 178, 179, 182, 187, 189, 194, 195, 196, 202, 205, 206, 207, 216, 217, 218, 219, 227, 234, 236, 237, 239, 240, 241, 243, 244, 245, 249, 250, 255, 258, 262, 263, 265, 266, 267], "still": [171, 195, 197, 215, 232, 234, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 268], "stochast": [21, 22, 188, 214, 229, 230], "stop": [11, 23, 184, 197, 199, 264], "stopiter": [199, 209, 226], "storag": [215, 265], "store": [11, 22, 23, 159, 174, 181, 184, 188, 197, 215, 228, 229, 230, 237, 241, 255], "str": [1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 17, 19, 21, 22, 23, 25, 166, 168, 169, 172, 174, 176, 179, 181, 182, 184, 185, 186, 187, 188, 189, 197, 199, 204, 208, 212, 213, 214, 220, 226, 228, 229, 230, 233, 238, 241, 246], "str_idx": 199, "straightforward": [195, 268], "strateg": 226, "strategi": 268, "stream": [197, 199, 200], "streamlin": 264, "strict": [7, 166, 174, 228, 229, 231, 265], "strict_symmetr": [195, 231, 244], "strict_valid": [13, 25, 186, 199], "strictli": [166, 209, 229], "stride": [160, 172, 173, 181, 201, 202, 229, 230], "strike": 219, "string": [9, 22, 174, 181, 188, 224, 228, 229, 230, 231], "strongli": [20, 172, 180, 238, 242, 250, 256], "structur": [25, 172, 175, 186, 191, 199, 219, 228, 249, 254, 262], "stude": 235, "style": 250, "sub": [249, 262], "subbackward0": 180, "subclass": [20, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 180, 210, 229, 232, 244, 245], "subdirectori": [237, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "subfold": [237, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "sublay": 173, "submit": 263, "submodul": [20, 233], "subpackag": [30, 210], "subsequ": [158, 191, 197, 201, 202, 209, 210, 229, 231, 237, 239, 250, 251, 255], "subset": [8, 9, 21, 179, 181, 187, 200, 214, 215, 223, 228, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264], "subsetrandomsampl": 257, "subsidiari": 192, "substanti": 224, "success": [243, 257], "successfulli": [181, 213, 229, 230], "sudo": 236, "suffic": [238, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "suffici": [214, 229, 265, 267], "suggest": [22, 188, 218, 219, 229, 230, 252, 254, 264, 268], "suit": [191, 199, 206, 243, 257], "suitabl": [191, 264], "sum": [197, 199, 208, 220, 226, 229, 230, 238, 241, 255, 257], "summari": [192, 197, 201, 202, 210, 219, 245], "sun": 219, "super": [20, 30, 172, 173, 199, 209, 210, 226, 246], "supergroup": [210, 244, 265], "suppli": 215, "support": [1, 2, 8, 11, 12, 20, 21, 22, 23, 29, 30, 172, 173, 174, 177, 179, 181, 182, 183, 184, 185, 187, 188, 189, 192, 195, 196, 197, 198, 199, 200, 201, 204, 208, 209, 210, 212, 214, 215, 216, 217, 219, 220, 222, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 237, 241, 244, 245, 246, 255, 258, 260, 261, 264, 265, 268], "supported_kernel": [2, 185, 226, 237, 255], "supported_kernel_op": [174, 185, 226], "supported_module_dict": 209, "suppos": [2, 12, 185, 202, 226, 237, 241, 255], "suscept": 230, "svd": [17, 29, 210, 216, 217, 218, 225, 236], "swap": 210, "sweep": [10, 208], "switch": [210, 215], "symbol": [20, 172], "symbolic_trac": [25, 172, 186, 199], "symfp": [182, 189, 208], "symmetr": [30, 158, 159, 160, 161, 162, 170, 171, 175, 180, 181, 206, 210, 215, 224, 229, 230, 231, 246, 265], "symmetri": [171, 182, 189, 208, 210], "symqt": [182, 189, 208], "syntax": 224, "system": 194, "systemat": 191, "t": [2, 9, 12, 25, 30, 172, 180, 181, 182, 185, 186, 189, 194, 199, 204, 206, 207, 208, 209, 210, 214, 226, 229, 230, 231, 237, 241, 244, 245, 249, 250, 255, 258, 262, 267], "tabl": [168, 209, 212, 221, 224, 236], "tag": 236, "take": [1, 2, 9, 12, 13, 24, 25, 29, 30, 170, 173, 174, 177, 178, 180, 181, 185, 186, 197, 198, 199, 200, 204, 209, 210, 215, 216, 218, 219, 220, 222, 223, 226, 228, 229, 230, 234, 235, 237, 241, 243, 244, 246, 249, 250, 251, 252, 253, 254, 255, 258, 262, 264, 267], "taken": [20, 24, 198, 223], "tanh": [145, 210], "tanhshrink": 146, "tap": [21, 179, 187, 214], "tar": [29, 222, 229], "target": [9, 17, 20, 22, 29, 41, 42, 48, 62, 64, 81, 87, 88, 92, 93, 104, 105, 113, 114, 115, 116, 117, 122, 137, 138, 178, 181, 185, 188, 191, 198, 199, 201, 204, 208, 210, 211, 213, 216, 217, 218, 219, 220, 222, 224, 226, 229, 230, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 268], "target_comp_ratio": [17, 29, 216, 220, 222, 252, 253, 254], "target_data": [255, 256, 258, 259, 260, 261, 262], "target_length": 48, "target_modul": 178, "task": [191, 221, 224, 237, 238, 239, 240, 241, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 264], "taxonomi": 219, "tbd": 212, "techniqu": [13, 25, 186, 191, 193, 198, 199, 201, 202, 204, 205, 206, 208, 209, 210, 214, 216, 217, 220, 222, 230, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 265, 267, 268], "technologi": [191, 192], "tell": 250, "tempfil": 210, "temporari": [9, 197, 201, 202, 226, 229], "temporarili": [171, 207], "tend": [197, 230], "tensor": [1, 2, 9, 12, 13, 19, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 172, 173, 174, 175, 177, 179, 180, 181, 184, 185, 186, 187, 188, 197, 198, 199, 201, 202, 204, 206, 207, 210, 213, 214, 215, 216, 220, 222, 224, 225, 226, 228, 229, 230, 231, 233, 234, 237, 239, 246, 249, 250, 251, 255, 259, 260, 261, 262, 265], "tensor_nam": 224, "tensor_quant": 171, "tensorboard": [244, 245, 247, 248], "tensorflow": [0, 18, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 204, 208, 210, 212, 213, 214, 215, 220, 224, 226, 228, 229, 230, 231, 236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 262, 263, 264, 265, 267, 268], "tensorquant": [12, 226], "tensorquantizationsimforpython": 195, "term": [181, 192, 215, 220, 222, 229, 230], "termin": 236, "test": [8, 21, 179, 187, 204, 206, 207, 214, 244, 245, 246, 267, 268], "test_dataload": [204, 206, 207], "test_dataset": [204, 206, 207], "text": [20, 161, 162, 163, 164, 165, 204, 206, 207, 209, 246], "textclassif": 246, "tf": [2, 8, 11, 14, 16, 17, 20, 21, 179, 187, 194, 197, 199, 200, 202, 210, 213, 214, 220, 226, 229, 232, 237, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 258, 262], "tf_cpp_min_log_level": [226, 241, 243, 244, 247, 248], "tf_dataset": [242, 250, 251], "tf_enhanc": [9, 22, 229, 230, 237, 241, 244, 245, 255, 258], "tfencod": 171, "tflite": [237, 250, 255], "tfoplambda": [20, 210, 246], "than": [17, 23, 29, 171, 172, 173, 184, 185, 188, 197, 208, 209, 210, 215, 216, 217, 220, 222, 226, 228, 230, 231, 237, 238, 242, 244, 245, 255, 256, 258, 264, 267], "thei": [172, 173, 205, 206, 207, 215, 221, 230, 232, 234, 237, 238, 239, 240, 242, 245, 246, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 264, 265, 267], "them": [13, 20, 22, 25, 166, 171, 172, 173, 175, 186, 191, 197, 199, 202, 214, 219, 225, 228, 229, 230, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "theme": 210, "therebi": [226, 237, 255], "therefor": [2, 12, 185, 201, 217, 226, 237, 241, 244, 245, 246, 255], "theta_": [161, 162], "thi": [2, 6, 8, 9, 10, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 29, 30, 100, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 184, 185, 186, 187, 188, 189, 192, 194, 195, 196, 197, 198, 199, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261, 263, 264, 265, 266, 267, 268], "thing": [238, 239, 240, 242, 243, 250, 251, 252, 253, 254, 256, 259, 260, 261, 263], "those": [8, 179, 187, 206, 214, 231, 246, 264, 267], "though": [30, 180, 215, 231, 237, 241, 244, 245, 250, 255, 268], "three": [172, 217, 237, 252, 253, 254, 255, 267], "threshold": [147, 168, 199, 212, 264], "through": [2, 9, 20, 21, 160, 171, 172, 174, 175, 179, 185, 187, 195, 197, 202, 206, 207, 209, 214, 226, 228, 229, 230, 235, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 264, 265, 268], "throughout": [24, 191, 198, 230, 231], "throw": [30, 180, 181, 229, 230], "thrown": [210, 229], "thu": [158, 159, 160, 226], "tie_word_embed": 209, "tijmen": 219, "till": [2, 12, 185, 226, 237, 241, 255, 264], "time": [17, 29, 160, 172, 173, 178, 193, 197, 199, 210, 214, 215, 216, 219, 220, 221, 222, 228, 230, 241, 243, 249, 257, 262, 264, 267], "tmp": [8, 13, 21, 25, 179, 186, 187, 194, 196, 197, 199, 200, 201, 202, 214, 226, 229, 244, 249, 262], "tmpdir": 178, "to_arrai": [201, 202], "to_list": [2, 12, 174, 185, 226], "to_onnx_qdq": [9, 210], "todo": [174, 228], "togeth": 250, "toggl": 194, "toi": 214, "token": [20, 204, 206, 207, 209, 234, 245, 246], "token_and_position_embed": 246, "token_emb": [20, 246], "tokenandpositionembed": [20, 246], "tokenized_dummy_text": [206, 207], "toler": [199, 217, 243, 257], "tolist": 255, "toml": 194, "too": [215, 252, 254], "tool": [21, 179, 187, 191, 193, 194, 210, 214, 223, 246, 263, 266, 268], "toolchain": 264, "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 263, 264, 265, 266, 267, 268, 269], "tooltip": 210, "top": [1, 2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27, 28, 29, 170, 174, 176, 179, 182, 183, 184, 185, 186, 187, 188, 189, 197, 198, 199, 200, 201, 202, 204, 208, 209, 214, 215, 216, 220, 221, 222, 226, 228, 229, 230, 237, 239, 240, 241, 242, 243, 244, 245, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262], "top1": [220, 226, 241, 257], "top1_accuraci": 255, "top5": 241, "topk": [255, 257], "torch": [13, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 177, 180, 181, 185, 186, 188, 193, 194, 196, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 233, 236, 237, 238, 239, 240, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "torch_stabl": 195, "torchscript": [25, 169, 181, 186, 188, 191, 199, 213, 229, 230], "torchvis": [181, 196, 197, 199, 200, 201, 202, 208, 214, 226, 228, 229, 230, 237, 238, 239, 240, 242, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "total": [206, 207, 218, 220, 226, 229, 241, 246, 265], "total_length": [204, 206, 207, 209], "total_sampl": [197, 208, 226, 229, 238], "totensor": [197, 199, 208, 226, 229, 238, 255, 257], "touch": 20, "toward": 234, "tpu": 191, "tqdm": [197, 206, 207, 208, 226, 229, 230, 238, 255, 257], "trace": [20, 25, 168, 172, 186, 199, 212, 233], "traceabl": [172, 206, 207, 233], "traceback": 172, "traceerror": 172, "tracer": 172, "track": [214, 249, 262], "track_running_stat": [201, 202], "trade": [11, 23, 184, 197, 226, 237, 241, 255], "tradeoff": [191, 226, 237, 241, 255, 268], "train": [8, 9, 13, 14, 17, 20, 21, 25, 26, 29, 176, 179, 181, 186, 187, 191, 193, 196, 197, 198, 199, 200, 202, 204, 205, 208, 209, 210, 211, 213, 214, 216, 219, 220, 222, 226, 234, 236, 237, 241, 246, 255, 257, 265, 268], "train_dataload": [181, 204, 206, 207, 209, 229, 230], "train_dataset": [204, 206, 207, 209, 244], "train_dataset_s": 244, "train_flag": [29, 216, 220, 222], "train_load": [216, 258], "train_model": [29, 216, 220, 222], "train_one_epoch": [206, 207], "trainabl": [198, 204, 205, 206, 207, 248, 261], "trainer": [17, 29, 216, 220, 222, 236, 252, 253, 254, 258, 260, 261], "training_range_learning_with_tf_init": [181, 198, 200, 204, 209, 229, 230, 244, 248, 258, 261], "trainingextens": 194, "trainingmod": [237, 240], "transact": 219, "transform": [20, 172, 197, 199, 204, 206, 207, 208, 209, 210, 215, 226, 229, 238, 246, 255, 256, 257, 260, 261, 262], "transformer_block": [20, 246], "transformerblock": [20, 246], "translat": 250, "transpos": 180, "trap": 208, "travers": 228, "treat": 230, "tri": [193, 219, 252, 253, 254], "tripletmarginloss": 148, "tripletmarginwithdistanceloss": 149, "true": [2, 9, 12, 13, 17, 22, 25, 29, 30, 159, 160, 161, 162, 166, 170, 171, 172, 173, 174, 175, 177, 180, 181, 185, 186, 188, 195, 197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 210, 214, 215, 216, 220, 222, 224, 226, 228, 229, 230, 231, 237, 238, 239, 240, 241, 242, 244, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "true_quant": [206, 207], "truli": 172, "trust_remote_cod": [204, 209], "try": [17, 29, 196, 197, 199, 201, 202, 216, 217, 219, 220, 222, 226, 234, 237, 239, 240, 241, 250, 252, 253, 254, 255, 264, 267], "tune": [17, 22, 29, 178, 188, 192, 216, 217, 220, 222, 229, 230, 238, 239, 240, 242, 243, 245, 247, 248, 251, 256, 257, 259, 260, 261, 264, 267, 268], "tuner": 206, "tupl": [2, 3, 6, 7, 8, 11, 12, 13, 14, 15, 17, 19, 21, 22, 23, 24, 25, 27, 28, 29, 97, 161, 162, 163, 164, 165, 169, 170, 173, 174, 179, 180, 181, 184, 185, 186, 187, 188, 196, 197, 198, 199, 200, 201, 202, 204, 209, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 233, 237, 241, 249, 255, 262], "turn": [193, 210, 231], "tutori": 190, "tweak": [178, 264], "twice": [20, 178], "two": [12, 20, 24, 29, 171, 172, 173, 174, 185, 191, 193, 198, 202, 209, 214, 215, 218, 219, 220, 222, 226, 227, 228, 230, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 259, 260, 261, 262, 263, 265, 267], "txt": [174, 194, 228, 263], "ty": 210, "type": [1, 2, 3, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 25, 26, 27, 29, 30, 158, 159, 160, 161, 162, 166, 168, 170, 171, 172, 173, 174, 175, 179, 181, 182, 184, 185, 186, 187, 188, 189, 191, 197, 198, 199, 200, 201, 202, 208, 209, 210, 212, 214, 215, 216, 219, 220, 221, 222, 226, 229, 230, 231, 237, 241, 243, 244, 246, 249, 252, 253, 254, 255, 257, 260, 261, 263, 267], "typeerror": 172, "typic": [20, 30, 175, 191, 197, 209, 214, 217, 226, 229, 237, 238, 242, 243, 244, 245, 247, 248, 250, 252, 254, 255, 256, 257, 258, 260, 261, 262, 264, 265, 267, 268], "u": [245, 250], "ubuntu": [193, 195, 196], "ubuntu22": 210, "uint": 210, "uint16": 224, "uint32": 224, "uint8": [158, 224], "unaccept": 268, "unattain": 226, "uncalibr": 210, "unchang": [182, 189, 202, 208, 229], "uncompress": 217, "under": [1, 2, 9, 12, 168, 171, 185, 197, 200, 212, 214, 226, 231, 234, 237, 255], "undergo": 191, "underli": [30, 180, 234], "understand": [171, 180, 236, 237, 241, 244, 245, 246, 249, 250, 255, 258, 262], "undo": [14, 26, 200], "uneven": 234, "unexpect": 166, "unexpected_kei": 166, "unflatten": 150, "unfold": 151, "unid": [13, 25, 186, 199], "uniniti": [24, 197, 198, 204, 208, 209], "unintuit": [13, 25, 186, 199], "union": [6, 7, 8, 15, 17, 19, 20, 22, 23, 25, 27, 28, 29, 169, 170, 173, 174, 179, 181, 184, 185, 186, 187, 188, 197, 199, 201, 202, 213, 214, 215, 216, 220, 222, 228, 229, 230, 264], "uniqu": 250, "unit": 191, "unknown": 217, "unlabel": [8, 13, 25, 186, 197, 199, 208, 214, 226, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 259, 260, 261, 262], "unlabeled_data": [199, 208], "unlabeled_data_load": [199, 214, 226], "unlabeled_dataset": [21, 197, 199, 200, 214, 243, 244, 249], "unlabeled_dataset_iter": [8, 214], "unlabeled_imagenet_data_load": 257, "unlabeled_imagenet_dataset": 257, "unlabeleddatasetwrapp": [243, 257], "unlabelled_data_load": 199, "unless": [13, 25, 30, 181, 186, 188, 199, 223, 229, 230, 268], "unlik": [166, 177, 201, 258], "unmodifi": [205, 218], "unnecessari": [201, 210, 223, 237, 250, 255], "unpin": 210, "unrol": [172, 210], "unsign": [231, 265], "unsigned_symmetr": [195, 231, 244], "unsigned_zero": 166, "unsimplifi": [201, 202, 226, 237, 239, 240], "unsqueez": [206, 207], "until": [13, 25, 161, 162, 181, 186, 199, 200, 229, 230, 243, 257], "untouch": 229, "unwrap": 246, "up": [11, 17, 20, 22, 23, 29, 174, 177, 184, 188, 191, 194, 197, 200, 210, 214, 216, 219, 220, 222, 223, 226, 228, 229, 230, 231, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 267], "up_proj": 209, "updat": [1, 24, 166, 176, 183, 197, 198, 200, 204, 207, 209, 210, 221, 224, 229, 230, 236, 238, 240, 242, 246, 247, 248, 250, 251, 256, 257, 259, 260, 261, 265], "update_lora_weight": [176, 204], "updatestat": 195, "upgrad": [0, 167, 210], "upon": [14, 26, 30, 175, 200, 210, 263], "upsampl": 152, "upsamplingbilinear2d": 153, "upsamplingnearest2d": 154, "upstream": [216, 223], "upto": [226, 237, 244, 249, 250, 255, 258, 262], "url": [17, 29, 216, 220, 221, 222, 236], "us": [1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 19, 20, 21, 22, 23, 25, 26, 29, 30, 158, 159, 161, 162, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 184, 185, 186, 187, 188, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 217, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268], "usabl": 224, "usag": [24, 173, 178, 180, 191, 198, 204, 210, 215, 224, 226, 230, 263, 264], "use_all_amp_candid": [2, 185, 226, 237, 255], "use_cach": [204, 206, 207, 209], "use_cuda": [17, 29, 195, 210, 213, 216, 220, 222, 241, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "use_embedded_encod": [181, 188, 229, 230], "use_fast": [204, 209], "use_monotonic_fit": [17, 29, 216, 220, 222], "use_strict_symmetr": 171, "use_symmetric_encod": [171, 210], "user": [2, 9, 12, 13, 14, 17, 20, 22, 23, 24, 25, 29, 30, 171, 172, 174, 180, 181, 184, 185, 186, 188, 192, 194, 195, 197, 198, 199, 200, 201, 209, 210, 214, 216, 219, 220, 222, 224, 225, 226, 228, 229, 230, 232, 233, 236, 237, 239, 240, 241, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264], "user_onnx_lib": [9, 229], "userflow": [182, 189, 208], "usual": [200, 219, 230, 265, 268], "util": [6, 8, 20, 171, 173, 179, 187, 196, 197, 198, 199, 200, 206, 207, 208, 210, 213, 214, 215, 226, 228, 229, 230, 237, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "v": [2, 11, 12, 23, 184, 185, 194, 197, 215, 226, 237, 241, 249, 255, 262, 265, 268], "v1": [0, 171, 195, 204, 206, 207, 209, 210, 213, 226, 241, 255, 256, 257, 258, 259, 260, 261, 262], "v2": [0, 30, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 170, 174, 181, 198, 206, 207, 210, 212, 215, 228, 229, 230], "v_proj": 209, "val": [197, 208, 229, 237, 238, 239, 240, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "val_dataset": 249, "val_transform": 257, "valid": [2, 8, 12, 21, 173, 179, 185, 187, 190, 196, 197, 199, 210, 214, 215, 216, 220, 222, 224, 226, 229, 237, 241, 244, 245, 247, 248, 249, 250, 255, 257, 258, 262], "validate_example_model": 173, "validate_for_missing_modul": 173, "validate_for_reused_modul": 173, "validate_model": 173, "validation_check": 173, "validation_d": [220, 226, 241], "validation_data": [244, 245, 247, 248], "valu": [1, 2, 8, 9, 12, 13, 17, 21, 22, 23, 29, 30, 159, 160, 161, 162, 164, 165, 166, 168, 169, 170, 172, 174, 179, 181, 184, 185, 187, 188, 191, 195, 196, 197, 199, 202, 204, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 224, 226, 228, 229, 230, 231, 237, 238, 239, 241, 242, 243, 244, 245, 246, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 262, 265], "value_qtzr": 30, "vanilla": 245, "var": [26, 81, 200], "vari": [2, 12, 185, 217, 226], "variabl": [17, 20, 29, 161, 162, 172, 216, 218, 220, 222, 236], "varianc": [244, 258], "variant": [191, 193, 194, 195, 196, 210], "varieti": 202, "variou": [2, 17, 29, 174, 185, 193, 210, 216, 219, 220, 222, 225, 226, 234, 243, 249, 257, 262, 264, 265], "vector": [210, 238, 242, 250, 256], "vedaldi": 219, "venic": 219, "venv": 194, "ver": 210, "ver_cuda": 194, "ver_onnxruntim": 194, "ver_python": 194, "ver_tensorflow": 194, "ver_torch": 194, "verbos": 241, "veri": [24, 198, 214, 217, 219, 226, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 256, 257, 259, 260, 261, 262, 264], "verifi": [20, 172, 256, 259, 260, 261], "versa": [218, 234, 237, 241, 255, 265], "version": [30, 100, 168, 171, 172, 175, 181, 188, 191, 193, 194, 196, 206, 207, 210, 212, 213, 229, 230, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 268], "via": [20, 25, 186, 191, 193, 194, 199, 210, 217, 263, 265], "vice": [218, 234, 237, 241, 255, 265], "view": [168, 173, 180, 192, 195, 206, 207, 211, 212, 221], "view_a": 257, "viewabl": 236, "virtual": 194, "vision": 219, "visit": 193, "visual": [17, 29, 193, 210, 214, 216, 219, 220, 222, 226, 245, 249, 262], "visualization_tool": [0, 167, 212], "visualization_url": [17, 29, 216, 220, 222], "visualize_stat": [168, 212], "visualizecompress": 221, "vocab_s": [20, 204, 206, 207, 209, 245, 246], "vol": 219, "volum": 223, "w": [2, 8, 12, 21, 179, 185, 187, 199, 210, 214, 222, 223, 226, 229, 244], "w1616": 264, "w16a16": [210, 226, 264], "w4a16": 210, "w4a8": [199, 210, 267, 268], "w4fp16": 210, "w8a16": [199, 226, 229, 264, 267, 268], "w8a8": [199, 226, 230, 264, 267, 268], "w_1": 202, "w_2": 202, "wa": [17, 29, 158, 180, 210, 213, 216, 219, 220, 222, 224, 226, 228, 237, 241, 242, 244, 245, 246, 250, 252, 254, 255, 256, 258, 264], "wai": [171, 180, 182, 188, 189, 195, 208, 213, 229, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256, 259, 260, 261, 262, 264, 267], "walk": 235, "want": [2, 6, 12, 22, 30, 169, 172, 185, 188, 194, 206, 207, 213, 226, 229, 230, 237, 241, 244, 245, 249, 250, 255, 262, 268], "warm": [11, 23, 184, 197], "warn": [24, 173, 198, 204], "wasn": 210, "we": [2, 6, 12, 20, 169, 171, 172, 173, 175, 178, 180, 185, 193, 197, 201, 202, 204, 206, 207, 209, 213, 215, 217, 219, 220, 226, 229, 234, 237, 238, 241, 242, 243, 244, 245, 246, 249, 250, 252, 254, 255, 256, 257, 258, 262, 264, 267, 268], "websit": [192, 217], "websocket": 221, "weight": [1, 2, 3, 7, 8, 10, 11, 12, 20, 21, 23, 24, 27, 29, 30, 168, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 183, 184, 185, 187, 191, 193, 196, 197, 198, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 215, 217, 220, 225, 226, 228, 229, 230, 231, 237, 238, 239, 240, 241, 242, 243, 246, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 264, 265, 267, 268], "weight_decai": 200, "weight_info": 246, "weight_nam": 246, "weight_q": 180, "weight_qdq": 180, "weight_svd": [29, 216, 220, 222], "weight_svd_auto_mod": 222, "weight_svd_manual_mod": 222, "weights_in_correct_ord": 246, "weights_pdf": [214, 249, 262], "weightsvdparamet": [29, 216, 220, 222], "well": [158, 173, 197, 209, 214, 219, 226, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 256, 257, 259, 260, 261, 262, 264], "were": [210, 213, 217, 223, 224, 229, 230, 231, 238, 243, 250, 257], "weren": 172, "wget": 229, "what": [192, 209, 221, 246, 264, 267], "when": [2, 8, 9, 12, 17, 20, 21, 22, 25, 29, 30, 166, 172, 175, 179, 181, 182, 185, 186, 187, 188, 189, 198, 199, 200, 202, 204, 208, 209, 210, 214, 215, 216, 219, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265], "whenev": [177, 210, 230, 233], "where": [2, 11, 12, 13, 17, 22, 23, 29, 161, 162, 163, 164, 165, 166, 172, 180, 181, 184, 185, 188, 191, 197, 199, 200, 209, 210, 214, 216, 218, 220, 221, 222, 223, 224, 226, 229, 230, 233, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265], "wherea": [180, 185, 237, 255], "wherein": [6, 169, 213], "whether": [24, 25, 166, 172, 173, 174, 186, 198, 199, 213, 218, 228, 229, 237, 238, 239, 240, 242, 250, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "which": [2, 6, 7, 11, 12, 13, 17, 19, 20, 23, 25, 29, 30, 158, 159, 160, 161, 162, 164, 165, 166, 169, 170, 172, 173, 174, 175, 178, 180, 184, 185, 186, 188, 191, 193, 195, 197, 198, 199, 201, 202, 203, 204, 205, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 228, 229, 231, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 267], "while": [7, 17, 29, 166, 175, 205, 209, 210, 216, 218, 220, 222, 224, 229, 230, 234, 237, 238, 239, 240, 241, 242, 245, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 264, 265, 267, 268], "whl": [194, 195], "who": 224, "whole": [13, 199, 243, 244, 265], "whose": [168, 169, 170, 172, 212, 213, 215, 226, 231, 263], "why": [234, 237, 250, 255], "wide": [191, 201, 202, 264], "width": [8, 21, 179, 187, 201, 202, 214, 215, 220, 222, 223, 224, 225, 226, 227, 234, 237, 238, 239, 240, 250, 255, 256, 258, 259, 260, 261, 262, 265, 267, 268], "wikitext": [204, 206, 207, 209], "wildcard": 180, "wise": [8, 21, 179, 182, 187, 189, 191, 200, 208, 210, 214, 234, 264], "wiseconv2d": [197, 202], "wish": [197, 201, 202, 208, 226, 228], "within": [30, 158, 159, 160, 175, 191, 194, 210, 214, 217, 228, 229, 245, 265, 267], "without": [13, 22, 25, 158, 160, 166, 168, 178, 181, 183, 186, 188, 199, 208, 209, 210, 212, 223, 226, 229, 230, 238, 242, 243, 245, 247, 248, 250, 251, 256, 257, 259, 260, 261, 267, 268], "won": [25, 172, 182, 186, 189, 199, 208, 231], "word": [160, 245], "work": [12, 22, 171, 173, 188, 193, 194, 195, 202, 205, 210, 215, 219, 226, 229, 230, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 264, 268], "workaround": 172, "workflow": [192, 203, 205, 210, 217, 235, 263, 267], "workspac": [194, 236], "world": 196, "wors": 219, "worth": 180, "would": [20, 171, 174, 206, 210, 220, 228, 229, 231, 237, 241, 244, 249, 250, 255, 258, 262, 264], "wq": 210, "wrap": [30, 171, 172, 245, 249, 262], "wrap_linear": 171, "wrapped_module_nam": [21, 179, 187, 214], "wrapper": [21, 179, 187, 210, 214, 216, 220, 222, 226, 241, 245, 255, 256, 259, 260, 261], "write": [174, 215, 228, 229, 230, 239, 240, 242, 247, 248, 251, 256, 259, 260, 261], "written": [9, 181, 229, 230, 237, 238, 239, 240, 241, 242, 243, 244, 245, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "wrong": 210, "wrt": [12, 226], "wsl2": 210, "www": 236, "x": [0, 20, 158, 159, 160, 166, 167, 172, 173, 175, 177, 195, 197, 199, 200, 201, 202, 210, 214, 217, 220, 224, 226, 229, 230, 232, 233, 237, 238, 239, 240, 241, 242, 245, 246, 249, 250, 255, 256, 258, 259, 260, 261, 262, 265], "x1": [63, 119, 172], "x2": [63, 119, 172, 232, 233], "x86": [193, 195, 196], "x86_64": 195, "x_": 265, "x_c": 166, "x_dq": 159, "x_q": [159, 160], "x_qdq": 158, "x_train": 245, "x_val": 245, "xiangyu": 219, "xx": 224, "y": [172, 194, 197, 199, 200, 214, 226, 229, 230, 241, 242, 249, 250, 262], "y_train": 245, "y_val": 245, "y_zero_point": 210, "ybelkada": [206, 207], "ye": 219, "yet": [177, 198, 204, 215, 237, 241, 244, 245, 250, 255], "yield": [2, 8, 13, 23, 25, 26, 174, 182, 184, 185, 186, 189, 197, 199, 200, 208, 214, 217, 226, 229, 234, 244, 258, 264, 265], "yihui": 219, "you": [8, 17, 25, 29, 30, 171, 172, 179, 180, 186, 187, 188, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 206, 208, 209, 210, 214, 215, 216, 217, 219, 220, 221, 222, 226, 228, 229, 231, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268], "your": [8, 25, 30, 171, 172, 173, 179, 186, 187, 194, 195, 196, 197, 199, 200, 206, 208, 211, 213, 214, 217, 219, 221, 225, 229, 235, 236, 237, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 265, 267, 268], "your_imagenet_validation_data_path": [197, 199, 200, 229, 230], "yy": 224, "zero": [11, 23, 184, 197, 210, 265], "zero_grad": [200, 206, 207, 230], "zeropad1d": 155, "zeropad2d": 156, "zeropad3d": 157, "zhang": 219, "zip": [214, 220, 226, 241], "zisserman": 219, "zlib": 210, "zou": 219, "zz": 224, "\u00aa": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00b2": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00b3": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00b5": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00b9": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00ba": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00bc": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00bd": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u00be": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u03c9": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u210e": 220, "\u215b": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u215c": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u215d": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\u215e": [192, 201, 210, 213, 215, 224, 226, 263, 264, 265, 267, 268], "\ud835\udc58": [220, 222], "\ud835\udc5a": 220, "\ud835\udc5b": 220, "\ud835\udc64": 220}, "titles": ["AIMET API", "aimet_onnx.apply_adaround", "aimet_onnx.mixed_precision", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.apply_seq_mse", "aimet_tensorflow.adaround", "aimet_tensorflow.mixed_precision", "aimet_tensorflow.auto_quant_v2", "aimet_tensorflow.keras.bn_reestimation", "aimet_tensorflow.batch_norm_fold", "aimet_tensorflow.cross_layer_equalization", "aimet_tensorflow.compress", "aimet_tensorflow API", "aimet_tensorflow.layer_output_utils", "aimet_tensorflow.model_preparer", "aimet_tensorflow.quant_analyzer", "aimet_tensorflow.quantsim", "aimet_torch.adaround", "aimet_torch.experimental.adascale", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.experimental.omniquant", "aimet_torch.onnx.export (beta)", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.experimental.spinquant", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "External resources", "Glossary", "AIMET Documentation", "What is AIMET?", "Building from source", "Installation", "Quick Start", "Adaptive rounding", "AdaScale", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Post Training Quantization Techniques", "OmniQuant", "Quantized LoRa", "QW-LoRa", "QWA-LoRa", "Sequential MSE", "SpinQuant", "Release notes", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Blockwise quantization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Encoding Format Specification", "Techniques", "Automatic mixed precision", "Mixed precision", "Manual mixed precision", "Post Training Quantization", "Quantization-aware training", "Runtime configuration", "TensorFlow model guidelines", "PyTorch model guidelines", "Quantization debugging guidelines", "Tutorials", "Example Notebooks", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quantization simulation", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Quantization-Aware Training with a Keras Transformer Model", "Keras Model Preparer", "Quantization-aware training", "Quantization-Aware training with range learning", "Quant Analyzer", "Quantsim and Adaround - Per Channel Quantization (PCQ)", "Cross-Layer Equalization with QuantSim", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "On-target inference", "Quantization workflow", "Quantization simulation guide", "Quantization user guide", "AIMET features", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [210, 224, 226, 241], "1": [171, 197, 198, 199, 200, 201, 204, 208, 209, 210, 213, 214, 224, 226, 228, 229, 230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "10": [194, 210], "11": 210, "13": 210, "16": 210, "17": 210, "18": 210, "19": 210, "2": [171, 197, 198, 199, 200, 201, 204, 208, 209, 210, 213, 214, 224, 226, 228, 229, 230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "20": 210, "21": 210, "22": 210, "23": 210, "24": 210, "25": 210, "26": 210, "27": 210, "28": 210, "29": 210, "3": [194, 197, 198, 199, 200, 201, 204, 208, 209, 210, 213, 214, 224, 226, 229, 230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "30": 210, "31": 210, "32": 210, "33": 210, "34": 210, "35": 210, "4": [197, 198, 199, 200, 204, 208, 209, 210, 213, 214, 229, 231, 234, 237, 238, 239, 241, 242, 243, 244, 246, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261, 264, 268], "5": [198, 199, 204, 208, 209, 210, 214, 229, 231, 234, 243, 244, 258, 264], "6": [199, 210, 214, 224, 234], "7": [199, 210, 234], "8": [210, 234], "9": 210, "For": [238, 239, 240, 242, 243, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261], "On": [263, 266], "accuraci": [237, 238, 239, 240, 241, 242, 243, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 264, 267, 268], "activ": [214, 234], "adapt": [197, 203, 238, 242, 256], "adaround": [11, 23, 184, 238, 242, 250, 256], "adascal": [24, 198, 203], "advanc": 264, "affin": [171, 180], "ai": 263, "aimet": [0, 192, 193, 194, 196, 221, 265, 266, 267, 269], "aimet_onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "aimet_tensorflow": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "aimet_torch": [23, 24, 25, 26, 27, 28, 29, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189], "algorithm": [226, 237, 241, 255], "also": 265, "altern": 195, "amp": [237, 241, 255, 264], "an": [237, 238, 239, 240], "analysi": [211, 214, 225, 226, 234, 249, 262], "analyz": [211, 214, 249, 262], "api": [0, 5, 18, 20, 167, 172, 173, 175, 178, 180, 192, 197, 198, 199, 200, 201, 202, 204, 208, 209, 212, 213, 214, 215, 216, 220, 222, 226, 228, 229, 230, 237, 241, 255, 267], "appli": [228, 238, 239, 242, 243, 249, 250, 251, 256, 259, 262], "apply_adaround": 1, "apply_seq_ms": 10, "arg": 224, "auto_qu": [25, 186], "auto_quant_v2": 13, "automat": [199, 203, 226, 227, 237, 241, 255, 264], "autoqu": [243, 257], "awar": [225, 230, 244, 245, 247, 248, 258, 260, 261, 264, 267], "base": [206, 228], "baselin": [237, 238, 239, 240, 241, 242, 243, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261, 264], "batch": [200, 201, 203, 237, 239, 240, 242, 247, 248, 250, 251, 255, 256, 259, 260, 261], "batch_norm_fold": [3, 15, 27], "batchnorm": [244, 258], "beta": 177, "between": 246, "block": 180, "blockwis": [215, 225], "bn_reestim": [14, 26], "bokeh": 221, "brows": 236, "build": [194, 195], "calibr": [207, 214, 229], "call": [237, 241, 255, 265], "callback": [207, 214, 229, 237, 244, 255], "case": [219, 226], "channel": [180, 216, 219, 250, 252, 254], "check": 234, "choos": 195, "cle": [239, 251, 259], "code": [20, 171, 172, 216, 220, 222, 236], "compil": [194, 263], "complementari": 197, "compress": [17, 29, 216, 217, 218, 219, 220, 221, 222, 225, 252, 253, 254], "comput": [175, 229, 230, 237, 241, 255], "conda": 194, "confid": 234, "config_util": 170, "configur": [175, 231, 265], "constant": [243, 244, 257], "contain": 194, "context": [197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 212, 213, 214, 216, 220, 222, 226, 228], "convers": 263, "convert": [226, 237, 238, 239, 240, 246], "cp": 219, "creat": [194, 207, 214, 226, 229, 237, 238, 239, 240, 241, 242, 244, 246, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261], "cross": [202, 203, 239, 251, 259], "cross_layer_equ": [4, 16, 28], "cuda": 194, "data": 224, "dataset": [237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "debug": [234, 266], "default": 231, "defin": [237, 243, 244, 255, 257], "definit": 22, "depend": 194, "deploi": [264, 268], "deploy": 267, "dequant": 163, "dequantizedtensor": 158, "descript": 214, "design": 221, "desir": 194, "detail": 264, "determin": [238, 239, 240, 242, 243, 247, 248, 250, 251, 256, 259, 260, 261, 265], "dictionari": 224, "differ": 246, "direct": 263, "disabl": [249, 262], "discuss": 246, "docker": 194, "document": [192, 194, 269], "download": 236, "enabl": [214, 249, 262], "encod": [175, 214, 224, 229, 237, 241, 249, 255, 262, 265], "engin": 263, "enum": 22, "environ": 194, "equal": [202, 203, 239, 251, 259], "error": 214, "estim": [200, 203, 244, 258], "evalu": [214, 229, 237, 238, 239, 240, 241, 242, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "exampl": [20, 171, 172, 192, 216, 220, 222, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "execut": [200, 202, 215, 263], "experiment": [24, 176, 183], "explor": 218, "export": [177, 215, 229, 244, 258, 265, 267], "extern": 190, "fake": 241, "faq": 219, "fast": 241, "featur": [217, 266, 267], "file": 231, "find": [226, 241, 264], "fine": [219, 252, 253, 254], "fix": 234, "float": 171, "floatquantizedequant": 166, "flow": [178, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "fold": [201, 203, 237, 239, 240, 242, 244, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261], "format": 224, "fp16": 268, "fp32": [234, 237, 238, 239, 240, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261], "from": [171, 194, 195], "front": 226, "function": [237, 243, 244, 246, 255, 257], "gener": [211, 213, 268], "get": [193, 242, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261], "glossari": [191, 192], "granular": 265, "greedi": 218, "group": 226, "guid": [171, 265, 266], "guidebook": 217, "guidelin": [232, 233, 234, 266, 268], "helper": [243, 257], "histogram": 214, "how": [171, 218, 223, 231, 265], "hub": 263, "hyper": 197, "i": [193, 237, 241, 249, 250, 255, 258, 262], "import": [213, 214], "improv": 267, "individu": 234, "infer": [263, 266, 267], "inform": [238, 239, 240, 242, 243, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261], "initi": 230, "input": [213, 228], "insert": 241, "instal": [194, 195, 196], "instanti": [238, 239, 240, 241, 242, 243, 244, 247, 248, 251, 252, 253, 254, 256, 259, 260, 261], "interact": [211, 212], "kera": [14, 244, 245, 246], "layer": [202, 203, 211, 213, 214, 218, 219, 228, 234, 237, 239, 240, 242, 244, 246, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262], "layer_output_util": [6, 19, 169], "leaf": 228, "learn": [248, 261], "learnedgrid": 171, "level": 224, "librari": 214, "limit": [20, 172, 246], "list": 226, "lite": 264, "load": [213, 241, 242, 243, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "local": 194, "lora": [203, 205, 206, 207], "loss": [214, 249, 262], "low": 215, "manual": [227, 228], "max": [214, 249, 262, 265], "mean": 214, "method": 241, "migrat": 171, "min": [214, 249, 262, 265], "mix": [225, 226, 227, 228, 237, 241, 255, 264, 267], "mixed_precis": [2, 12, 174, 185], "mmp": 228, "model": [196, 206, 213, 214, 215, 219, 228, 229, 232, 233, 234, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 268], "model_input": 231, "model_output": 231, "model_prepar": [20, 172], "model_valid": 173, "modifi": 231, "modul": [171, 175], "more": [238, 239, 240, 242, 243, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261], "move": 171, "mse": [203, 208, 249, 262], "new": 194, "next": [238, 242, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261, 264, 268], "nn": 175, "nois": 265, "non": 228, "norm": [200, 201, 203, 239, 259], "normal": [237, 240, 242, 247, 248, 250, 251, 255, 256, 260, 261], "note": [192, 210, 219], "notebook": [192, 236, 237, 241, 249, 250, 255, 258, 262], "nvidia": 194, "obtain": 213, "old": 195, "omniqu": [176, 203, 204], "onnx": [177, 237, 238, 239, 240], "op": 241, "option": [219, 228], "origin": 246, "output": [211, 213, 228], "overal": [237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262], "overhead": 226, "overview": [192, 218, 219, 221, 223, 231, 265], "packag": [194, 195], "param": 231, "paramet": [197, 230, 237, 241, 255, 265], "pareto": 226, "path": 267, "pcq": 250, "pdf": [249, 262], "peft": 178, "per": [180, 214, 218, 219, 234, 249, 250, 262], "perform": [226, 234, 244, 247, 248, 258, 260, 261, 264], "phase": 226, "pip": 194, "pipelin": [237, 238, 239, 240, 242, 243, 244, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262], "platform": [193, 196], "post": [192, 203, 225, 229, 264, 267], "power": 215, "precis": [225, 226, 227, 228, 237, 241, 255, 264, 267, 268], "prepar": [214, 244, 246], "prerequisit": [195, 197, 198, 199, 200, 204, 208, 209, 214, 228, 229], "pretrain": [243, 249, 257], "procedur": [198, 199, 201, 204, 208, 209, 216, 226, 268], "process": 171, "profil": 228, "prune": [216, 219, 252, 254], "ptq": [206, 264], "pypi": 195, "python": 194, "pytorch": [233, 237, 238, 239, 240], "qat": [230, 244, 247, 248, 258, 260, 261, 264], "qualcomm": 263, "quant": [249, 262], "quant_analyz": [8, 21, 179, 187], "quantanalyz": [214, 249, 262], "quantiz": [161, 164, 171, 175, 180, 192, 196, 199, 203, 205, 206, 211, 214, 215, 224, 225, 226, 229, 230, 234, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268], "quantizationmixin": 30, "quantizationsim": 244, "quantizationsimmodel": 207, "quantize_dequant": 165, "quantizedadaptiveavgpool1d": 31, "quantizedadaptiveavgpool2d": 32, "quantizedadaptiveavgpool3d": 33, "quantizedadaptivemaxpool1d": 34, "quantizedadaptivemaxpool2d": 35, "quantizedadaptivemaxpool3d": 36, "quantizedalphadropout": 37, "quantizedavgpool1d": 38, "quantizedavgpool2d": 39, "quantizedavgpool3d": 40, "quantizedbatchnorm1d": 43, "quantizedbatchnorm2d": 44, "quantizedbatchnorm3d": 45, "quantizedbceloss": 41, "quantizedbcewithlogitsloss": 42, "quantizedbilinear": 46, "quantizedcelu": 47, "quantizedchannelshuffl": 49, "quantizedcircularpad1d": 50, "quantizedcircularpad2d": 51, "quantizedcircularpad3d": 52, "quantizedconstantpad1d": 53, "quantizedconstantpad2d": 54, "quantizedconstantpad3d": 55, "quantizedconv1d": 56, "quantizedconv2d": 57, "quantizedconv3d": 58, "quantizedconvtranspose1d": 59, "quantizedconvtranspose2d": 60, "quantizedconvtranspose3d": 61, "quantizedcosineembeddingloss": 62, "quantizedcosinesimilar": 63, "quantizedcrossentropyloss": 64, "quantizedctcloss": 48, "quantizeddropout": 65, "quantizeddropout1d": 66, "quantizeddropout2d": 67, "quantizeddropout3d": 68, "quantizedelu": 69, "quantizedembed": 70, "quantizedembeddingbag": 71, "quantizedequant": 162, "quantizedfeaturealphadropout": 72, "quantizedflatten": 73, "quantizedfold": 74, "quantizedfractionalmaxpool2d": 75, "quantizedfractionalmaxpool3d": 76, "quantizedgaussiannllloss": 81, "quantizedgelu": 77, "quantizedglu": 78, "quantizedgroupnorm": 82, "quantizedgru": 79, "quantizedgrucel": 80, "quantizedhardshrink": 83, "quantizedhardsigmoid": 84, "quantizedhardswish": 85, "quantizedhardtanh": 86, "quantizedhingeembeddingloss": 87, "quantizedhuberloss": 88, "quantizedinstancenorm1d": 89, "quantizedinstancenorm2d": 90, "quantizedinstancenorm3d": 91, "quantizedkldivloss": 92, "quantizedl1loss": 93, "quantizedlayernorm": 98, "quantizedleakyrelu": 99, "quantizedlinear": 100, "quantizedlocalresponsenorm": 101, "quantizedlogsigmoid": 102, "quantizedlogsoftmax": 103, "quantizedlppool1d": 94, "quantizedlppool2d": 95, "quantizedlstm": 96, "quantizedlstmcel": 97, "quantizedmarginrankingloss": 105, "quantizedmaxpool1d": 106, "quantizedmaxpool2d": 107, "quantizedmaxpool3d": 108, "quantizedmaxunpool1d": 109, "quantizedmaxunpool2d": 110, "quantizedmaxunpool3d": 111, "quantizedmish": 112, "quantizedmseloss": 104, "quantizedmultilabelmarginloss": 113, "quantizedmultilabelsoftmarginloss": 114, "quantizedmultimarginloss": 115, "quantizednllloss": 116, "quantizednllloss2d": 117, "quantizedpairwisedist": 119, "quantizedpixelshuffl": 120, "quantizedpixelunshuffl": 121, "quantizedpoissonnllloss": 122, "quantizedprelu": 118, "quantizedreflectionpad1d": 128, "quantizedreflectionpad2d": 129, "quantizedreflectionpad3d": 130, "quantizedrelu": 126, "quantizedrelu6": 127, "quantizedreplicationpad1d": 131, "quantizedreplicationpad2d": 132, "quantizedreplicationpad3d": 133, "quantizedrnn": 123, "quantizedrnncel": 124, "quantizedrrelu": 125, "quantizedselu": 134, "quantizedsigmoid": 136, "quantizedsilu": 135, "quantizedsmoothl1loss": 137, "quantizedsoftmarginloss": 138, "quantizedsoftmax": 139, "quantizedsoftmax2d": 140, "quantizedsoftmin": 141, "quantizedsoftplu": 142, "quantizedsoftshrink": 143, "quantizedsoftsign": 144, "quantizedtanh": 145, "quantizedtanhshrink": 146, "quantizedtensor": 159, "quantizedtensorbas": 160, "quantizedthreshold": 147, "quantizedtripletmarginloss": 148, "quantizedtripletmarginwithdistanceloss": 149, "quantizedunflatten": 150, "quantizedunfold": 151, "quantizedupsampl": 152, "quantizedupsamplingbilinear2d": 153, "quantizedupsamplingnearest2d": 154, "quantizedzeropad1d": 155, "quantizedzeropad2d": 156, "quantizedzeropad3d": 157, "quantsim": [7, 9, 22, 170, 181, 188, 229, 250, 251, 265], "quantwrapp": 171, "quick": 196, "quickli": 196, "qw": [205, 206], "qwa": [205, 207], "rang": [214, 248, 249, 261, 262], "rank": 219, "ratio": [218, 219, 221], "re": [200, 203, 244, 258], "recommend": 230, "reconstruct": 216, "reduc": [226, 268], "reestim": 258, "refer": [175, 180, 192, 219], "regular": 241, "relat": 236, "releas": [192, 210], "resourc": 190, "restor": 268, "round": [197, 203, 219, 238, 242, 256], "run": [194, 207, 214, 230, 236, 237, 241, 255, 257], "runtim": [231, 265], "scheme": 265, "score": [242, 247, 248, 250, 251, 252, 253, 254, 255, 256, 259, 260, 261], "sdk": 263, "select": [216, 218, 219], "sensit": [214, 226, 234], "seq_ms": [182, 189], "sequenti": [203, 208], "server": [221, 236], "session": 221, "set": [194, 228], "set_grouped_blockwise_quantization_for_weight": 7, "setup": [197, 198, 200, 202, 204, 206, 207, 208, 209, 216, 220, 222, 228, 230], "show": 246, "signal": 265, "sim": [237, 239, 240, 242, 247, 248, 251, 255, 256, 258, 259, 260, 261], "similar": 246, "simplifi": [237, 238, 239, 240], "simul": [237, 238, 239, 240, 241, 242, 247, 248, 250, 251, 255, 256, 258, 259, 260, 261, 265, 267], "small": 196, "sourc": [194, 195], "spatial": [219, 220, 253, 254], "specif": 224, "spinquant": [183, 203, 209], "squar": 214, "start": [193, 196, 221], "staticgrid": 171, "statist": [214, 249, 258, 262], "step": [197, 198, 199, 200, 201, 204, 208, 209, 213, 214, 226, 228, 229, 230, 238, 242, 247, 248, 251, 252, 253, 254, 256, 257, 259, 260, 261, 264, 268], "structur": [224, 231], "subclass": 246, "summari": [241, 244, 246, 250, 258], "supergroup": 231, "support": [193, 194, 267], "svd": [219, 220, 222, 253, 254], "target": [263, 266, 267], "techniqu": [192, 197, 203, 219, 225, 259, 264], "tensorflow": 232, "terminologi": 178, "test": [194, 196], "tf": 265, "thi": [237, 241, 249, 250, 255, 258, 262], "tool": [168, 211, 225, 267], "top": 224, "tradeoff": 264, "train": [192, 203, 206, 207, 225, 229, 230, 238, 239, 240, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 258, 259, 260, 261, 262, 264, 267], "transform": 245, "try": 268, "tune": [219, 252, 253, 254], "tutori": [192, 235], "type": [224, 228], "typic": 230, "unit": 194, "updat": 206, "us": [171, 216, 219, 220, 222, 226, 252, 253, 254, 264], "user": [178, 266], "v": [171, 264], "v1": [167, 184, 185, 186, 187, 188, 189], "valid": [238, 239, 240, 242, 243, 251, 252, 253, 254, 256, 259, 260, 261], "variabl": 194, "variant": 230, "verifi": [195, 196, 268], "version": [195, 224, 269], "visual": [168, 211, 212, 221, 234], "visualization_tool": 168, "w16a16": 268, "weight": [206, 214, 216, 219, 222, 234], "what": [193, 237, 241, 249, 250, 255, 258, 262], "wheel": 194, "winnow": [216, 223], "work": [218, 223, 265], "workflow": [197, 198, 199, 200, 201, 202, 204, 206, 207, 208, 209, 212, 213, 214, 216, 220, 222, 226, 228, 229, 230, 234, 264, 265, 266, 268], "wrapper": [249, 262], "x": 171}})