<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AIMET QuantAnalyzer &mdash; AI Model Efficiency Toolkit Documentation: ver 1.35.1</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="../torch_docs/index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        1.35.1
      </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/tutorials/quickstart_guide.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantized_modules.html">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.quantization_mixin.html">QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/affine/index.html">quantization.affine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/float/index.html">quantization.float</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/encoding_analyzer.html">Encoding Analyzers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../torch_docs/index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../torch_docs/index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AIMET QuantAnalyzer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/quant_analyzer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="aimet-quantanalyzer">
<span id="ug-quant-analyzer"></span><h1>AIMET QuantAnalyzer<a class="headerlink" href="#aimet-quantanalyzer" title="Permalink to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>The QuantAnalyzer performs several analyses to identify sensitive areas and hotspots in the model. These analyses are performed automatically. To use QuantAnalyzer, you pass in callbacks to perform forward passes and evaluations, and optionally a dataloader for MSE loss analysis.</p>
<p>For each analysis, QuantAnalyzer outputs JSON and/or HTML files containing data and plots for visualization.</p>
</section>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading"></a></h2>
<dl class="simple">
<dt>To call the QuantAnalyzer API, you must provide the following:</dt><dd><ul class="simple">
<li><p>An FP32 pre-trained model for analysis</p></li>
<li><p>A dummy input for the model that can contain random values but which must match the shape of the model’s expected input</p></li>
<li><p>A user-defined function for passing 500-1000 representative data samples through the model for quantization calibration</p></li>
<li><p>A user-defined function for passing labeled data through the model for evaluation, returning an accuracy metric</p></li>
<li><p>(Optional, for running MSE loss analysis) A dataloader providing unlabeled data to be passed through the model</p></li>
</ul>
</dd>
</dl>
<p>Other quantization-related settings are also provided in the call to analyze a model.
See <span class="xref std std-doc">PyTorch QuantAnalyzer API Docs</span> for more about how to call the QuantAnalyzer feature.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Typically on quantized runtimes, batch normalization (BN) layers are folded where possible. So that you don’t have to call a separate API to do so, QuantAnalyzer automatically performs Batch Norm Folding before running its analyses.</p>
</div>
</section>
<section id="detailed-analysis-descriptions">
<h2>Detailed analysis descriptions<a class="headerlink" href="#detailed-analysis-descriptions" title="Permalink to this heading"></a></h2>
<p>QuantAnalyzer performs the following analyses:</p>
<dl>
<dt>Sensitivity analysis to weight and activation quantization</dt><dd><p>QuantAnalyzer compares the accuracies of the original FP32 model, an activation-only quantized model, and a weight-only quantized model. This helps determine which AIMET quantization technique(s) will be more beneficial for the model.</p>
<p>For example, in situations where the model is more sensitive to activation quantization, PTQ techniques like Adaptive Rounding or Cross Layer Equalization might not be very helpful.</p>
<p>Accuracy values for each model are printed as part of AIMET logging.</p>
</dd>
<dt>Per-layer quantizer enablement analysis</dt><dd><p>Sometimes the accuracy drop incurred from quantization can be attributed to only a subset of quantizers within the model. QuantAnalyzer finds such layers by enabling and disabling individual quantizers to observe how the model accuracy changes.</p>
<p>The following two types of quantizer enablement analyses are performed:</p>
<ol class="arabic simple">
<li><p>Disable all quantizers across the model and, for each layer, enable only that layer’s output quantizer and perform evaluation with the provided callback. This results in accuracy values obtained for each layer in the model when only that layer’s quantizer is enabled, exposing the effects of individual layer quantization and pinpointing culprit layer(s) and hotspots.</p></li>
<li><p>Enable all quantizers across the model and, for each layer, disable only that layer’s output quantizer and perform evaluation with the provided callback. Once again, accuracy values are produced for each layer in the model when only that layer’s quantizer is disabled.</p></li>
</ol>
<p>As a result of these analyses, AIMET outputs <cite>per_layer_quant_enabled.html</cite> and <cite>per_layer_quant_disabled.html</cite> respectively, containing plots mapping layers on the x-axis to model accuracy on the y-axis.</p>
<p>JSON files <cite>per_layer_quant_enabled.json</cite> and <cite>per_layer_quant_disabled.json</cite> are also produced, containing the data shown in the .html plots.</p>
</dd>
<dt>Per-layer encodings min-max range analysis</dt><dd><p>As part of quantization, encoding parameters for each quantizer must be obtained.
These parameters include scale, offset, min, and max, and are used to map floating point values to quantized integer values.</p>
<p>QuantAnalyzer tracks the min and max encoding parameters computed by each quantizer in the model as a result of forward passes through the model with representative data (from which the scale and offset values can be directly obtained).</p>
<p>As a result of this analysis, AIMET outputs html plots and json files for each activation quantizer and each parameter quantizer (contained in the min_max_ranges folder) containing the encoding min/max values for each.</p>
<p>If Per Channel Quantization (PCQ) is enabled, encoding min and max values for all the channels of each weight are shown.</p>
</dd>
<dt>Per-layer statistics histogram</dt><dd><p>Under the TF Enhanced quantization scheme, encoding min/max values for each quantizer are obtained by collecting a histogram of tensor values seen at that quantizer and deleting outliers.</p>
<p>When this quantization scheme is selected, QuantAnalyzer outputs plots for each quantizer in the model, displaying the histogram of tensor values seen at that quantizer.
These plots are available as part of the <cite>activations_pdf</cite> and <cite>weights_pdf</cite> folders, containing a separate .html plot for each quantizer.</p>
</dd>
<dt>Per layer mean-square-error (MSE) loss (optional)</dt><dd><p>QuantAnalyzer can monitor each layer’s output in the original FP32 model as well as the corresponding layer output in the quantized model and calculate the MSE loss between the two.
This helps identify which layers may contribute more to quantization noise.</p>
<p>To enable this optional analysis, you pass in a dataloader that QuantAnalyzer reads from.
Approximately 256 samples/images are sufficient for the analysis.</p>
<p>A <cite>per_layer_mse_loss.html</cite> file is generated containing a plot that maps layer quantizers on the x-axis to MSE loss on the y-axis. A corresponding <cite>per_layer_mse_loss.json</cite> file is generated containing data corresponding to the .html file.</p>
</dd>
</dl>
</section>
<section id="quantanalyzer-api">
<h2>QuantAnalyzer API<a class="headerlink" href="#quantanalyzer-api" title="Permalink to this heading"></a></h2>
<p>See the links below to view the QuantAnalyzer API for each AIMET variant:</p>
<ul class="simple">
<li><p><span class="xref std std-ref">QuantAnalyzer for PyTorch</span></p></li>
<li><p><span class="xref std std-ref">QuantAnalyzer for Keras</span></p></li>
<li><p><span class="xref std std-ref">QuantAnalyzer for ONNX</span></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>