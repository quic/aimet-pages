Search.setIndex({"alltitles": {"1. Define Constants and Helper functions": [[237, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[230, "1.-Example-evaluation-and-training-pipeline"], [238, "1.-Example-evaluation-and-training-pipeline"], [242, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[227, "1.-Example-evaluation-pipeline"], [235, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence checks": [[224, "fp32-confidence-checks"]], "1. Instantiate the example training and validation pipeline": [[228, "1.-Instantiate-the-example-training-and-validation-pipeline"], [229, "1.-Instantiate-the-example-training-and-validation-pipeline"], [231, "1.-Instantiate-the-example-training-and-validation-pipeline"], [232, "1.-Instantiate-the-example-training-and-validation-pipeline"], [233, "1.-Instantiate-the-example-training-and-validation-pipeline"], [234, "1.-Instantiate-the-example-training-and-validation-pipeline"], [236, "1.-Instantiate-the-example-training-and-validation-pipeline"], [239, "1.-Instantiate-the-example-training-and-validation-pipeline"], [240, "1.-Instantiate-the-example-training-and-validation-pipeline"], [241, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Run the notebook server": [[226, "run-the-notebook-server"]], "1. Sensitivity to weight and activation quantization": [[203, "sensitivity-to-weight-and-activation-quantization"]], "1. Versioning": [[213, "versioning"]], "1. defaults": [[222, "defaults"]], "1.13.0": [[199, "id49"]], "1.16.0": [[199, "id48"]], "1.16.1": [[199, "id47"]], "1.16.2": [[199, "id46"]], "1.17.0": [[199, "id45"]], "1.18.0": [[199, "id44"]], "1.19.1": [[199, "id43"]], "1.20.0": [[199, "id42"]], "1.21.0": [[199, "id41"]], "1.22.0": [[199, "id40"]], "1.22.1": [[199, "id39"]], "1.22.2": [[199, "id38"]], "1.23.0": [[199, "id37"]], "1.24.0": [[199, "id36"]], "1.25.0": [[199, "id35"]], "1.26.0": [[199, "id34"]], "1.27.0": [[199, "id33"]], "1.28.0": [[199, "id32"]], "1.29.0": [[199, "id31"]], "1.30.0": [[199, "id30"]], "1.31.0": [[199, "id29"]], "1.32.0": [[199, "id28"]], "1.33.0": [[199, "id27"]], "1.33.5": [[199, "id26"]], "1.34.0": [[199, "id25"]], "1.35.0": [[199, "id24"]], "1.35.1": [[199, "id23"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[227, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [228, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [229, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [231, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Download the example notebooks and related code": [[226, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[238, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[237, "2.-Load-a-pretrained-FP32-model"]], "2. Load the model": [[230, "2.-Load-the-model"], [242, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[232, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [233, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [234, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [235, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [236, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [239, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [240, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [241, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement": [[203, "per-layer-quantizer-enablement"]], "2. Version 0.6.1": [[213, "version-0-6-1"]], "2. Weights or activations quantization": [[224, "weights-or-activations-quantization"]], "2. params": [[222, "params"]], "2.0.0": [[199, "id22"]], "2.1. Encoding specification": [[213, "encoding-specification"]], "2.1.0": [[199, "id21"]], "2.10.0": [[199, "id8"]], "2.11.0": [[199, "id6"]], "2.12.0": [[199, "id5"]], "2.13.0": [[199, "id4"]], "2.14.0": [[199, "id3"]], "2.15.0": [[199, "id1"]], "2.2.0": [[199, "id20"]], "2.3.0": [[199, "id19"]], "2.4.0": [[199, "id18"]], "2.5.0": [[199, "id17"]], "2.6.0": [[199, "id16"]], "2.7.0": [[199, "id15"]], "2.8.0": [[199, "id11"]], "2.9.0": [[199, "id9"]], "3. Apply QuantAnalyzer to the model": [[230, "3.-Apply-QuantAnalyzer-to-the-model"], [242, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[232, "3.-Compress-the-model-and-fine-tune"], [233, "3.-Compress-the-model-and-fine-tune"], [234, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[227, "3.-Create-a-quantization-simulation-model"], [235, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[238, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[228, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [229, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [231, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [236, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [239, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [240, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [241, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Fixing weight quantization": [[224, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range": [[203, "per-layer-encodings-min-max-range"]], "3. Run AutoQuant": [[237, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[226, "run-the-notebooks"]], "3. Version 1.0.0": [[213, "version-1-0-0"]], "3. supergroups": [[222, "supergroups"]], "3.1. Encoding specification": [[213, "id1"]], "4. Apply AdaRound": [[228, "4.-Apply-AdaRound"]], "4. Apply Adaround": [[236, "4.-Apply-Adaround"]], "4. Apply CLE": [[229, "4.-Apply-CLE"], [239, "4.-Apply-CLE"]], "4. Fixing activation quantization": [[224, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[203, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[238, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[240, "4.-Perform-QAT"], [241, "4.-Perform-QAT"]], "4. Run AMP algorithm on the quantized model": [[227, "4.-Run-AMP-algorithm-on-the-quantized-model"], [235, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. model_input": [[222, "model-input"]], "5. Export Model": [[238, "5.-Export-Model"]], "5. Per-layer mean-square-error loss": [[203, "per-layer-mean-square-error-loss"]], "5. Performing per-layer analysis": [[224, "performing-per-layer-analysis"]], "5. model_output": [[222, "model-output"]], "6. Visualizing sensitive layers": [[224, "visualizing-sensitive-layers"]], "7. Fixing individual quantizers": [[224, "fixing-individual-quantizers"]], "8. Quantize the model": [[224, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[181, null]], "AIMET documentation versions": [[249, null]], "AIMET features": [[246, "aimet-features"], [247, null]], "AIMET visualization": [[210, null]], "API": [[161, "api"], [162, "api"], [167, "api"], [186, "api"], [187, "api"], [188, "api"], [189, "api"], [190, "api"], [191, "api"], [193, "api"], [197, "api"], [198, "api"], [201, "api"], [202, "api"], [203, "api"], [204, "api"], [205, "api"], [209, "api"], [211, "api"], [215, "api"], [216, "api"], [218, "api"], [219, "api"], [220, "api"], [221, "api"]], "API Reference": [[181, "api-reference"]], "API reference": [[164, "api-reference"], [169, "api-reference"]], "Accuracy improvement tools": [[247, "accuracy-improvement-tools"]], "Accuracy-vs-Performance Tradeoff": [[244, "accuracy-vs-performance-tradeoff"]], "AdaScale": [[187, null], [192, "adascale"]], "Adaptive Rounding (AdaRound)": [[228, null], [236, null]], "Adaptive rounding": [[186, null], [192, "adaptive-rounding"]], "Affine quantizers": [[169, "affine-quantizers"]], "Alternative packages": [[184, "alternative-packages"]], "Analysis descriptions": [[203, "analysis-descriptions"]], "Analysis tools": [[200, null], [214, "analysis-tools"]], "Apply LPBQ": [[215, "apply-lpbq"]], "AutoQuant": [[237, null]], "Automatic Mixed-Precision (AMP)": [[227, null], [235, null]], "Automatic mixed precision": [[216, null], [217, "automatic-mixed-precision"]], "Automatic quantization": [[188, null], [192, "automatic-quantization"]], "Batch norm folding": [[190, null], [192, "batch-norm-folding"]], "Batch norm re-estimation": [[189, null], [192, "batch-norm-re-estimation"]], "Blockwise Quantization": [[204, null], [214, "blockwise-quantization"]], "Browse the notebooks": [[226, "browse-the-notebooks"]], "Build AIMET documentation": [[183, "build-aimet-documentation"]], "Build AIMET wheel and run unit tests": [[183, "build-aimet-wheel-and-run-unit-tests"], [183, "id2"]], "Build and run docker container locally": [[183, "build-and-run-docker-container-locally"]], "Building from source": [[183, null], [184, "building-from-source"]], "CLE": [[239, "CLE"]], "Calibration Callback": [[196, "calibration-callback"]], "Call AMP API": [[227, "Call-AMP-API"], [235, "Call-AMP-API"]], "Channel pruning": [[205, null]], "Channel pruning (CP)": [[208, "channel-pruning-cp"]], "Channel selection": [[205, "channel-selection"]], "Choose and install a package": [[184, "choose-and-install-a-package"]], "Code Examples": [[160, "code-examples"], [161, "code-examples"]], "Code example": [[205, "code-example"], [209, "code-example"], [211, "code-example"]], "Compilation": [[243, "compilation"]], "Compile and install pip package dependencies": [[183, "compile-and-install-pip-package-dependencies"]], "Compressing using Spatial SVD": [[209, "compressing-using-spatial-svd"]], "Compression": [[208, null], [210, "compression"], [214, "compression"]], "Compression features Guidebook": [[206, null]], "Compression ratio selection": [[207, "compression-ratio-selection"], [208, "compression-ratio-selection"]], "Compression using Channel Pruning": [[205, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[211, "compression-using-weight-svd"]], "Compute Encodings": [[227, "Compute-Encodings"], [235, "Compute-Encodings"]], "Computing encodings": [[164, "computing-encodings"]], "Conda environment": [[183, "conda-environment"]], "Configuration": [[164, "configuration"]], "Configuration file structure": [[222, "configuration-file-structure"]], "Context": [[186, "context"], [187, "context"], [188, "context"], [189, "context"], [190, "context"], [191, "context"], [193, "context"], [195, "context"], [196, "context"], [197, "context"], [198, "context"], [201, "context"], [202, "context"], [203, "context"], [205, "context"], [209, "context"], [211, "context"], [216, "context"], [219, "context"]], "Conversion": [[243, "conversion"]], "Create Quantization Sim Model": [[227, "Create-Quantization-Sim-Model"], [235, "Create-Quantization-Sim-Model"], [238, "Create-Quantization-Sim-Model"]], "Create QuantizationSimModel": [[196, "create-quantizationsimmodel"]], "Create a new conda environment with Python 3.10": [[183, "create-a-new-conda-environment-with-python-3-10"]], "Create the Quantization Sim Model": [[229, "Create-the-Quantization-Sim-Model"], [231, "Create-the-Quantization-Sim-Model"], [236, "Create-the-Quantization-Sim-Model"], [239, "Create-the-Quantization-Sim-Model"], [240, "Create-the-Quantization-Sim-Model"], [241, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[229, null], [239, null]], "Cross-layer equalization": [[191, null], [192, "cross-layer-equalization"]], "Data type": [[213, "id5"]], "Dataset": [[227, "Dataset"], [228, "Dataset"], [229, "Dataset"], [230, "Dataset"], [231, "Dataset"], [232, "Dataset"], [233, "Dataset"], [234, "Dataset"], [235, "Dataset"], [236, "Dataset"], [237, "Dataset"], [238, "Dataset"], [239, "Dataset"], [240, "Dataset"], [241, "Dataset"], [242, "Dataset"]], "Debugging guidelines": [[246, "debugging-guidelines"]], "Debugging workflow": [[224, "debugging-workflow"]], "Define callback functions for AMP": [[227, "Define-callback-functions-for-AMP"], [235, "Define-callback-functions-for-AMP"]], "Deployment paths": [[247, "deployment-paths"]], "DequantizedTensor": [[147, null]], "Design": [[210, "design"]], "Detailed Workflow": [[244, "detailed-workflow"]], "Determine quantization parameters (encodings)": [[245, "determine-quantization-parameters-encodings"]], "Docker environment": [[183, "docker-environment"]], "Encoding Format Specification": [[213, null]], "Encoding dictionary structure": [[213, "id3"]], "Encoding min/max ranges": [[230, "Encoding-min/max-ranges"], [242, "Encoding-min/max-ranges"]], "Encoding type": [[213, "id4"]], "Example Notebooks": [[181, "example-notebooks"], [226, null]], "Executing blockwise quantization": [[204, "executing-blockwise-quantization"]], "Execution": [[189, "execution"], [191, "execution"], [243, "execution"]], "Export API": [[247, "export-api"]], "Export tools": [[247, "export-tools"]], "Exported Encodings": [[245, "exported-encodings"]], "External resources": [[179, null]], "FAQs": [[208, "faqs"]], "FloatQuantizeDequantize": [[155, null]], "Fold Batch Norm layers": [[229, "Fold-Batch-Norm-layers"], [239, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[227, "Fold-Batch-Normalization-layers"], [231, "Fold-Batch-Normalization-layers"], [235, "Fold-Batch-Normalization-layers"], [236, "Fold-Batch-Normalization-layers"], [240, "Fold-Batch-Normalization-layers"], [241, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[238, "Fold-BatchNorm-Layers"]], "For more information": [[228, "For-more-information"], [229, "For-more-information"], [231, "For-more-information"], [232, "For-more-information"], [233, "For-more-information"], [234, "For-more-information"], [236, "For-more-information"], [237, "For-more-information"], [239, "For-more-information"], [240, "For-more-information"], [241, "For-more-information"]], "General guidelines": [[248, "general-guidelines"]], "Get Started": [[182, "get-started"]], "Glossary": [[180, null], [181, "glossary"]], "Greedy compression ratio selection": [[207, null]], "How it works": [[207, "how-it-works"]], "How quantization simulation works": [[245, "how-quantization-simulation-works"]], "How to modify configuration file": [[222, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[160, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[212, "how-winnowing-works"]], "Installation": [[184, null]], "Installing AIMET": [[185, "installing-aimet"]], "Interactive visualization": [[200, "interactive-visualization"], [201, null]], "Layer output generation": [[200, "layer-output-generation"], [202, null]], "Limitations": [[161, "limitations"]], "Lite mixed precision": [[217, "lite-mixed-precision"], [218, null]], "LoRa Training": [[195, "lora-training"]], "Low-Power Blockwise Quantization": [[214, "low-power-blockwise-quantization"]], "Low-Power Blockwise Quantization (LPBQ)": [[215, null]], "Manual mixed precision": [[217, "manual-mixed-precision"], [219, null]], "Migration Process": [[160, "migration-process"]], "Migration guide": [[160, null]], "Min-Max (also called \u201cTF\u201d in AIMET)": [[245, "min-max-also-called-tf-in-aimet"]], "Mixed Precision": [[214, "mixed-precision"]], "Mixed Precision Algorithm": [[216, "mixed-precision-algorithm"]], "Mixed precision": [[217, null], [247, "mixed-precision"]], "Model compression": [[208, "model-compression"]], "Model compression using channel pruning": [[232, null]], "Model compression using spatial SVD": [[233, null]], "Model compression using spatial SVD and channel pruning": [[234, null]], "Model guidelines": [[223, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[160, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[160, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "NOTE": [[208, null]], "NVIDIA CUDA support": [[183, "nvidia-cuda-support"]], "Next steps": [[228, "Next-steps"], [232, "Next-steps"], [233, "Next-steps"], [234, "Next-steps"], [236, "Next-steps"], [237, "Next-steps"], [239, "Next-steps"], [240, "Next-steps"], [241, "Next-steps"]], "Next: Deploying the model": [[244, "next-deploying-the-model"]], "Next: deploying the model": [[248, "next-deploying-the-model"]], "Old versions": [[184, "old-versions"]], "OmniQuant": [[192, "omniquant"], [193, null]], "On-target inference": [[243, null], [246, "on-target-inference"]], "Optional techniques": [[208, "optional-techniques"]], "Overall flow": [[227, "Overall-flow"], [228, "Overall-flow"], [229, "Overall-flow"], [230, "Overall-flow"], [231, "Overall-flow"], [232, "Overall-flow"], [233, "Overall-flow"], [234, "Overall-flow"], [235, "Overall-flow"], [236, "Overall-flow"], [237, "Overall-flow"], [238, "Overall-flow"], [239, "Overall-flow"], [240, "Overall-flow"], [241, "Overall-flow"], [242, "Overall-flow"]], "Overview": [[181, "overview"], [207, "overview"], [208, "overview"], [210, "overview"], [212, "overview"], [222, "overview"], [245, "overview"]], "PDF of statistics": [[230, "PDF-of-statistics"], [242, "PDF-of-statistics"]], "PTQ": [[195, "ptq"]], "Parameters for AMP algorithm": [[227, "Parameters-for-AMP-algorithm"], [235, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[169, "per-block-quantization"]], "Per-channel quantization": [[169, "per-channel-quantization"]], "Per-layer MSE loss": [[230, "Per-layer-MSE-loss"], [242, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[230, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [242, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[207, "per-layer-exploration"]], "Per-layer fine-tuning": [[208, "per-layer-fine-tuning"]], "Perform QAT": [[238, "Perform-QAT"]], "Phase 0: Find quantizer groups": [[216, "phase-0-find-quantizer-groups"]], "Phase 1: Perform sensitivity analysis": [[216, "phase-1-perform-sensitivity-analysis"]], "Phase 2: Create a Pareto-front list": [[216, "phase-2-create-a-pareto-front-list"]], "Phase 3: Reduce Convert overhead": [[216, "phase-3-reduce-convert-overhead"]], "Post Training Quantization": [[214, "post-training-quantization"], [220, null]], "Post Training Quantization Techniques": [[181, "post-training-quantization-techniques"], [192, null]], "Post-training quantization": [[247, "post-training-quantization"]], "Prerequisites": [[184, "prerequisites"], [186, "prerequisites"], [187, "prerequisites"], [188, "prerequisites"], [189, "prerequisites"], [193, "prerequisites"], [197, "prerequisites"], [198, "prerequisites"], [203, "prerequisites"], [218, "prerequisites"], [219, "prerequisites"], [220, "prerequisites"]], "Procedure": [[187, "procedure"], [188, "procedure"], [190, "procedure"], [193, "procedure"], [197, "procedure"], [198, "procedure"], [205, "procedure"], [216, "procedure"], [248, "procedure"]], "PyPI": [[184, "pypi"]], "PyTorch model guidelines": [[223, null]], "QW-LoRa": [[194, "qw-lora"], [195, null]], "QWA-LoRa": [[194, "qwa-lora"], [196, null]], "Qualcomm\u00ae AI Engine Direct SDK": [[243, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[243, "qualcommreg-ai-hub"]], "Quant Analyzer": [[230, null], [242, null]], "QuantSim workflow": [[245, "quantsim-workflow"]], "Quantization": [[243, "quantization"]], "Quantization Aware Training": [[214, "quantization-aware-training"]], "Quantization analyzer": [[200, "quantization-analyzer"], [203, null]], "Quantization debugging guidelines": [[224, null]], "Quantization granularity": [[245, "quantization-granularity"]], "Quantization schemes": [[245, "quantization-schemes"]], "Quantization simulation": [[231, null], [247, "quantization-simulation"]], "Quantization simulation guide": [[245, null]], "Quantization user guide": [[246, null]], "Quantization workflow": [[244, null], [246, "quantization-workflow"], [248, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[238, null]], "Quantization-aware training": [[221, null], [240, null], [247, "quantization-aware-training"]], "Quantization-aware training with range learning": [[241, null]], "QuantizationMixin": [[19, null]], "Quantize": [[150, null]], "Quantize a small model quickly with AIMET": [[185, "quantize-a-small-model-quickly-with-aimet"]], "Quantize and Update Base Model Weights": [[195, "quantize-and-update-base-model-weights"]], "QuantizeDequantize": [[151, null]], "Quantized LoRa": [[192, "quantized-lora"], [194, null]], "Quantized modules": [[164, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[20, null]], "QuantizedAdaptiveAvgPool2d": [[21, null]], "QuantizedAdaptiveAvgPool3d": [[22, null]], "QuantizedAdaptiveMaxPool1d": [[23, null]], "QuantizedAdaptiveMaxPool2d": [[24, null]], "QuantizedAdaptiveMaxPool3d": [[25, null]], "QuantizedAlphaDropout": [[26, null]], "QuantizedAvgPool1d": [[27, null]], "QuantizedAvgPool2d": [[28, null]], "QuantizedAvgPool3d": [[29, null]], "QuantizedBCELoss": [[30, null]], "QuantizedBCEWithLogitsLoss": [[31, null]], "QuantizedBatchNorm1d": [[32, null]], "QuantizedBatchNorm2d": [[33, null]], "QuantizedBatchNorm3d": [[34, null]], "QuantizedBilinear": [[35, null]], "QuantizedCELU": [[36, null]], "QuantizedCTCLoss": [[37, null]], "QuantizedChannelShuffle": [[38, null]], "QuantizedCircularPad1d": [[39, null]], "QuantizedCircularPad2d": [[40, null]], "QuantizedCircularPad3d": [[41, null]], "QuantizedConstantPad1d": [[42, null]], "QuantizedConstantPad2d": [[43, null]], "QuantizedConstantPad3d": [[44, null]], "QuantizedConv1d": [[45, null]], "QuantizedConv2d": [[46, null]], "QuantizedConv3d": [[47, null]], "QuantizedConvTranspose1d": [[48, null]], "QuantizedConvTranspose2d": [[49, null]], "QuantizedConvTranspose3d": [[50, null]], "QuantizedCosineEmbeddingLoss": [[51, null]], "QuantizedCosineSimilarity": [[52, null]], "QuantizedCrossEntropyLoss": [[53, null]], "QuantizedDropout": [[54, null]], "QuantizedDropout1d": [[55, null]], "QuantizedDropout2d": [[56, null]], "QuantizedDropout3d": [[57, null]], "QuantizedELU": [[58, null]], "QuantizedEmbedding": [[59, null]], "QuantizedEmbeddingBag": [[60, null]], "QuantizedFeatureAlphaDropout": [[61, null]], "QuantizedFlatten": [[62, null]], "QuantizedFold": [[63, null]], "QuantizedFractionalMaxPool2d": [[64, null]], "QuantizedFractionalMaxPool3d": [[65, null]], "QuantizedGELU": [[66, null]], "QuantizedGLU": [[67, null]], "QuantizedGRU": [[68, null]], "QuantizedGRUCell": [[69, null]], "QuantizedGaussianNLLLoss": [[70, null]], "QuantizedGroupNorm": [[71, null]], "QuantizedHardshrink": [[72, null]], "QuantizedHardsigmoid": [[73, null]], "QuantizedHardswish": [[74, null]], "QuantizedHardtanh": [[75, null]], "QuantizedHingeEmbeddingLoss": [[76, null]], "QuantizedHuberLoss": [[77, null]], "QuantizedInstanceNorm1d": [[78, null]], "QuantizedInstanceNorm2d": [[79, null]], "QuantizedInstanceNorm3d": [[80, null]], "QuantizedKLDivLoss": [[81, null]], "QuantizedL1Loss": [[82, null]], "QuantizedLPPool1d": [[83, null]], "QuantizedLPPool2d": [[84, null]], "QuantizedLSTM": [[85, null]], "QuantizedLSTMCell": [[86, null]], "QuantizedLayerNorm": [[87, null]], "QuantizedLeakyReLU": [[88, null]], "QuantizedLinear": [[89, null]], "QuantizedLocalResponseNorm": [[90, null]], "QuantizedLogSigmoid": [[91, null]], "QuantizedLogSoftmax": [[92, null]], "QuantizedMSELoss": [[93, null]], "QuantizedMarginRankingLoss": [[94, null]], "QuantizedMaxPool1d": [[95, null]], "QuantizedMaxPool2d": [[96, null]], "QuantizedMaxPool3d": [[97, null]], "QuantizedMaxUnpool1d": [[98, null]], "QuantizedMaxUnpool2d": [[99, null]], "QuantizedMaxUnpool3d": [[100, null]], "QuantizedMish": [[101, null]], "QuantizedMultiLabelMarginLoss": [[102, null]], "QuantizedMultiLabelSoftMarginLoss": [[103, null]], "QuantizedMultiMarginLoss": [[104, null]], "QuantizedNLLLoss": [[105, null]], "QuantizedNLLLoss2d": [[106, null]], "QuantizedPReLU": [[107, null]], "QuantizedPairwiseDistance": [[108, null]], "QuantizedPixelShuffle": [[109, null]], "QuantizedPixelUnshuffle": [[110, null]], "QuantizedPoissonNLLLoss": [[111, null]], "QuantizedRNN": [[112, null]], "QuantizedRNNCell": [[113, null]], "QuantizedRReLU": [[114, null]], "QuantizedReLU": [[115, null]], "QuantizedReLU6": [[116, null]], "QuantizedReflectionPad1d": [[117, null]], "QuantizedReflectionPad2d": [[118, null]], "QuantizedReflectionPad3d": [[119, null]], "QuantizedReplicationPad1d": [[120, null]], "QuantizedReplicationPad2d": [[121, null]], "QuantizedReplicationPad3d": [[122, null]], "QuantizedSELU": [[123, null]], "QuantizedSiLU": [[124, null]], "QuantizedSigmoid": [[125, null]], "QuantizedSmoothL1Loss": [[126, null]], "QuantizedSoftMarginLoss": [[127, null]], "QuantizedSoftmax": [[128, null]], "QuantizedSoftmax2d": [[129, null]], "QuantizedSoftmin": [[130, null]], "QuantizedSoftplus": [[131, null]], "QuantizedSoftshrink": [[132, null]], "QuantizedSoftsign": [[133, null]], "QuantizedTanh": [[134, null]], "QuantizedTanhshrink": [[135, null]], "QuantizedTensor": [[148, null]], "QuantizedTensorBase": [[149, null]], "QuantizedThreshold": [[136, null]], "QuantizedTripletMarginLoss": [[137, null]], "QuantizedTripletMarginWithDistanceLoss": [[138, null]], "QuantizedUnflatten": [[139, null]], "QuantizedUnfold": [[140, null]], "QuantizedUpsample": [[141, null]], "QuantizedUpsamplingBilinear2d": [[142, null]], "QuantizedUpsamplingNearest2d": [[143, null]], "QuantizedZeroPad1d": [[144, null]], "QuantizedZeroPad2d": [[145, null]], "QuantizedZeroPad3d": [[146, null]], "Quantizer Args structure": [[213, "id6"]], "Quantizers": [[169, "quantizers"]], "Quick Start": [[185, null]], "Rank Rounding": [[208, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[238, "Re-estimate-BatchNorm-Statistics"]], "References": [[208, "references"]], "Release Notes": [[181, "release-notes"]], "Release notes": [[199, null]], "Run QWA-LoRa": [[196, "run-qwa-lora"]], "Running the notebooks": [[226, "running-the-notebooks"]], "Runtime configuration": [[222, null], [245, "runtime-configuration"]], "Sequential MSE": [[192, "sequential-mse"], [197, null]], "Set environment variables to build desired AIMET wheel": [[183, "set-environment-variables-to-build-desired-aimet-wheel"], [183, "id1"]], "Set model input precision": [[219, "set-model-input-precision"]], "Set model output precision": [[219, "set-model-output-precision"]], "Set precision based on layer type": [[219, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[219, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[219, "set-precision-of-a-non-leaf-layer"]], "Setup": [[186, "setup"], [187, "setup"], [189, "setup"], [191, "setup"], [193, "setup"], [195, "setup"], [196, "setup"], [197, "setup"], [198, "setup"], [205, "setup"], [209, "setup"], [211, "setup"], [219, "setup"]], "Signal-to-Quantization-Noise": [[245, "signal-to-quantization-noise"]], "Simulate quantization noise": [[245, "simulate-quantization-noise"]], "Spatial SVD": [[208, "spatial-svd"], [209, null]], "SpinQuant": [[192, "spinquant"], [198, null]], "Starting a Bokeh server session": [[210, "starting-a-bokeh-server-session"]], "Step 1": [[186, "step-1"], [187, "step-1"], [188, "step-1"], [189, "step-1"], [190, "step-1"], [193, "step-1"], [197, "step-1"], [198, "step-1"], [216, "step-1"]], "Step 1 Importing libraries": [[203, "step-1-importing-libraries"]], "Step 1: Applying MMP API options": [[219, "step-1-applying-mmp-api-options"]], "Step 1: Creating a QuantSim model": [[218, "step-1-creating-a-quantsim-model"], [220, "step-1-creating-a-quantsim-model"]], "Step 1: Find baseline precision": [[244, "step-1-find-baseline-precision"]], "Step 1: Importing the API": [[202, "step-1-importing-the-api"]], "Step 1: Setup": [[221, "step-1-setup"]], "Step 1: Trying FP16 precision (no quantization)": [[248, "step-1-trying-fp16-precision-no-quantization"]], "Step 2": [[186, "step-2"], [187, "step-2"], [188, "step-2"], [189, "step-2"], [190, "step-2"], [193, "step-2"], [197, "step-2"], [198, "step-2"], [216, "step-2"]], "Step 2 Preparing calibration callback": [[203, "step-2-preparing-calibration-callback"]], "Step 2: Applying the profile": [[219, "step-2-applying-the-profile"]], "Step 2: Compute initial quantization parameters": [[221, "step-2-compute-initial-quantization-parameters"]], "Step 2: Computing encodings": [[218, "step-2-computing-encodings"]], "Step 2: Creating a calibration callback": [[220, "step-2-creating-a-calibration-callback"]], "Step 2: Loading a model": [[202, "step-2-loading-a-model"]], "Step 2: Use lite mixed precision": [[244, "step-2-use-lite-mixed-precision"]], "Step 2: Verifying W16A16 quantization": [[248, "step-2-verifying-w16a16-quantization"]], "Step 3": [[186, "step-3"], [187, "step-3"], [188, "step-3"], [189, "step-3"], [190, "step-3"], [193, "step-3"], [197, "step-3"], [198, "step-3"]], "Step 3 Preparing evaluation callback": [[203, "step-3-preparing-evaluation-callback"]], "Step 3. Reducing precision": [[248, "step-3-reducing-precision"]], "Step 3: Computing encodings": [[220, "step-3-computing-encodings"]], "Step 3: Evaluation of w8a8 base precision": [[218, "step-3-evaluation-of-w8a8-base-precision"]], "Step 3: Obtaining inputs": [[202, "step-3-obtaining-inputs"]], "Step 3: Run quantization-aware training": [[221, "step-3-run-quantization-aware-training"]], "Step 3: Use Automatic Mixed Precision (AMP)": [[244, "step-3-use-automatic-mixed-precision-amp"]], "Step 4": [[186, "step-4"], [187, "step-4"], [188, "step-4"], [189, "step-4"], [193, "step-4"], [197, "step-4"], [198, "step-4"]], "Step 4 Preparing model": [[203, "step-4-preparing-model"]], "Step 4. Restoring accuracy": [[248, "step-4-restoring-accuracy"]], "Step 4: Evaluation": [[220, "step-4-evaluation"]], "Step 4: Generating layer outputs": [[202, "step-4-generating-layer-outputs"]], "Step 4: Perform sensitivity analysis": [[218, "step-4-perform-sensitivity-analysis"]], "Step 4: Use advanced Post-Training Quantization (PTQ) techniques": [[244, "step-4-use-advanced-post-training-quantization-ptq-techniques"]], "Step 5": [[187, "step-5"], [188, "step-5"], [193, "step-5"], [197, "step-5"], [198, "step-5"]], "Step 5 Creating QuantAnalyzer": [[203, "step-5-creating-quantanalyzer"]], "Step 5: Apply precision adjustment": [[218, "step-5-apply-precision-adjustment"]], "Step 5: Exporting the model": [[220, "step-5-exporting-the-model"]], "Step 5: Use Quantization-Aware Training (QAT)": [[244, "step-5-use-quantization-aware-training-qat"]], "Step 6": [[188, "step-6"]], "Step 6 Running the analysis": [[203, "step-6-running-the-analysis"]], "Step 6: Recompute encodings": [[218, "step-6-recompute-encodings"]], "Step 7": [[188, "step-7"]], "Step 7: Evaluation of w8a8_mixed precision": [[218, "step-7-evaluation-of-w8a8-mixed-precision"]], "Summary": [[238, "Summary"]], "Supported platform": [[182, "supported-platform"]], "Supported precisions for on-target inference": [[247, "supported-precisions-for-on-target-inference"]], "Techniques": [[181, "techniques"], [214, null], [239, "Techniques"]], "Terminology": [[167, "terminology"]], "Tested platform": [[185, "tested-platform"]], "Top level structure": [[213, "id2"]], "Training Callback": [[196, "training-callback"]], "Tutorials": [[181, "tutorials"], [225, null]], "Typical recommendations": [[221, "typical-recommendations"]], "Use Case": [[208, "use-case"]], "Use Cases": [[216, "use-cases"]], "User flow": [[167, "user-flow"]], "Variants of QAT": [[221, "variants-of-qat"]], "Verifying the installation": [[184, "verifying-the-installation"], [185, "verifying-the-installation"]], "Visualization Tools": [[157, "visualization-tools"]], "Visualizing compression ratios": [[210, "visualizing-compression-ratios"]], "Weight SVD": [[208, "weight-svd"], [211, null]], "Weight reconstruction": [[205, "weight-reconstruction"]], "What is AIMET?": [[182, null]], "What this notebook is not": [[227, "What-this-notebook-is-not"], [230, "What-this-notebook-is-not"], [235, "What-this-notebook-is-not"], [238, "What-this-notebook-is-not"], [242, "What-this-notebook-is-not"]], "Winnowing": [[205, "winnowing"], [212, null], [212, "id1"]], "Workflow": [[186, "workflow"], [186, "id2"], [187, "workflow"], [188, "workflow"], [189, "workflow"], [190, "workflow"], [191, "workflow"], [193, "workflow"], [195, "workflow"], [196, "workflow"], [197, "workflow"], [198, "workflow"], [201, "workflow"], [202, "workflow"], [203, "workflow"], [205, "workflow"], [209, "workflow"], [211, "workflow"], [216, "workflow"], [218, "workflow"], [219, "workflow"], [220, "workflow"], [221, "workflow"]], "aimet_onnx API": [[5, null]], "aimet_onnx.apply_adaround": [[1, null]], "aimet_onnx.apply_seq_mse": [[11, null]], "aimet_onnx.batch_norm_fold": [[3, null]], "aimet_onnx.cross_layer_equalization": [[4, null]], "aimet_onnx.layer_output_utils": [[6, null]], "aimet_onnx.lite_mp": [[7, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[9, null]], "aimet_onnx.quantsim": [[10, null]], "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights": [[8, null]], "aimet_torch": [[156, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[160, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[156, null]], "aimet_torch.adaround": [[12, null]], "aimet_torch.auto_quant": [[14, null]], "aimet_torch.batch_norm_fold": [[16, null]], "aimet_torch.bn_reestimation": [[15, null]], "aimet_torch.compress": [[18, null]], "aimet_torch.cross_layer_equalization": [[17, null]], "aimet_torch.experimental.adascale": [[13, null]], "aimet_torch.experimental.omniquant": [[165, null]], "aimet_torch.experimental.spinquant": [[172, null]], "aimet_torch.layer_output_utils": [[158, null]], "aimet_torch.mixed_precision": [[163, null]], "aimet_torch.model_preparer": [[161, null]], "aimet_torch.model_validator": [[162, null]], "aimet_torch.nn": [[164, null]], "aimet_torch.onnx.export (beta)": [[166, null]], "aimet_torch.peft": [[167, null]], "aimet_torch.quant_analyzer": [[168, null]], "aimet_torch.quantization": [[169, null]], "aimet_torch.quantsim": [[170, null]], "aimet_torch.quantsim.config_utils": [[159, null]], "aimet_torch.seq_mse": [[171, null]], "aimet_torch.v1": [[156, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[173, null]], "aimet_torch.v1.auto_quant": [[175, null]], "aimet_torch.v1.mixed_precision": [[174, null]], "aimet_torch.v1.quant_analyzer": [[176, null]], "aimet_torch.v1.quantsim": [[177, null]], "aimet_torch.v1.seq_mse": [[178, null]], "aimet_torch.visualization_tools": [[157, null]], "dequantize": [[152, null]], "quantize": [[153, null]], "quantize_dequantize": [[154, null]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/litemp", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/torch/adaround", "apiref/torch/adascale", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/omniquant", "apiref/torch/onnx", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/spinquant", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "external/index", "glossary", "index", "overview/index", "overview/install/build_from_source", "overview/install/index", "overview/install/quick-start", "ptq_techniques/adaround", "ptq_techniques/adascale", "ptq_techniques/autoquant", "ptq_techniques/bn", "ptq_techniques/bnf", "ptq_techniques/cle", "ptq_techniques/index", "ptq_techniques/omniquant", "ptq_techniques/quantized_LoRa/index", "ptq_techniques/quantized_LoRa/qw_lora", "ptq_techniques/quantized_LoRa/qwa_lora", "ptq_techniques/seq_mse", "ptq_techniques/spinquant", "release_notes", "techniques/analysis_tools/index", "techniques/analysis_tools/interactive_visualization", "techniques/analysis_tools/layer_output_generation", "techniques/analysis_tools/quant_analyzer", "techniques/blockwise", "techniques/compression/channel_pruning", "techniques/compression/feature_guidebook", "techniques/compression/greedy_compression_ratio_selection", "techniques/compression/index", "techniques/compression/spatial_svd", "techniques/compression/visualization_compression", "techniques/compression/weight_svd", "techniques/compression/winnowing", "techniques/encoding_spec", "techniques/index", "techniques/lpbq", "techniques/mixed_precision/amp", "techniques/mixed_precision/index", "techniques/mixed_precision/litemp", "techniques/mixed_precision/mmp", "techniques/ptq", "techniques/qat", "techniques/runtime_config", "techniques/torch/model_guidelines", "tutorials/debugging_guidelines", "tutorials/index", "tutorials/notebooks", "tutorials/notebooks/onnx/quantization/AMP", "tutorials/notebooks/onnx/quantization/adaround", "tutorials/notebooks/onnx/quantization/cle", "tutorials/notebooks/onnx/quantization/quant_analyzer", "tutorials/notebooks/onnx/quantization/quantsim", "tutorials/notebooks/torch/compression/channel_pruning", "tutorials/notebooks/torch/compression/spatial_svd", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning", "tutorials/notebooks/torch/quantization/AMP", "tutorials/notebooks/torch/quantization/adaround", "tutorials/notebooks/torch/quantization/autoquant", "tutorials/notebooks/torch/quantization/bn_reestimation", "tutorials/notebooks/torch/quantization/cle", "tutorials/notebooks/torch/quantization/qat", "tutorials/notebooks/torch/quantization/qat_range_learning", "tutorials/notebooks/torch/quantization/quant_analyzer", "tutorials/on_target_inference", "tutorials/quantization_workflow", "tutorials/quantsim", "userguide/index", "userguide/quantization_tools", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/litemp.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/torch/adaround.rst", "apiref/torch/adascale.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/omniquant.rst", "apiref/torch/onnx.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/spinquant.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "external/index.rst", "glossary.rst", "index.rst", "overview/index.rst", "overview/install/build_from_source.rst", "overview/install/index.rst", "overview/install/quick-start.rst", "ptq_techniques/adaround.rst", "ptq_techniques/adascale.rst", "ptq_techniques/autoquant.rst", "ptq_techniques/bn.rst", "ptq_techniques/bnf.rst", "ptq_techniques/cle.rst", "ptq_techniques/index.rst", "ptq_techniques/omniquant.rst", "ptq_techniques/quantized_LoRa/index.rst", "ptq_techniques/quantized_LoRa/qw_lora.rst", "ptq_techniques/quantized_LoRa/qwa_lora.rst", "ptq_techniques/seq_mse.rst", "ptq_techniques/spinquant.rst", "release_notes.rst", "techniques/analysis_tools/index.rst", "techniques/analysis_tools/interactive_visualization.rst", "techniques/analysis_tools/layer_output_generation.rst", "techniques/analysis_tools/quant_analyzer.rst", "techniques/blockwise.rst", "techniques/compression/channel_pruning.rst", "techniques/compression/feature_guidebook.rst", "techniques/compression/greedy_compression_ratio_selection.rst", "techniques/compression/index.rst", "techniques/compression/spatial_svd.rst", "techniques/compression/visualization_compression.rst", "techniques/compression/weight_svd.rst", "techniques/compression/winnowing.rst", "techniques/encoding_spec.rst", "techniques/index.rst", "techniques/lpbq.rst", "techniques/mixed_precision/amp.rst", "techniques/mixed_precision/index.rst", "techniques/mixed_precision/litemp.rst", "techniques/mixed_precision/mmp.rst", "techniques/ptq.rst", "techniques/qat.rst", "techniques/runtime_config.rst", "techniques/torch/model_guidelines.rst", "tutorials/debugging_guidelines.rst", "tutorials/index.rst", "tutorials/notebooks.rst", "tutorials/notebooks/onnx/quantization/AMP.ipynb", "tutorials/notebooks/onnx/quantization/adaround.ipynb", "tutorials/notebooks/onnx/quantization/cle.ipynb", "tutorials/notebooks/onnx/quantization/quant_analyzer.ipynb", "tutorials/notebooks/onnx/quantization/quantsim.ipynb", "tutorials/notebooks/torch/compression/channel_pruning.ipynb", "tutorials/notebooks/torch/compression/spatial_svd.ipynb", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning.ipynb", "tutorials/notebooks/torch/quantization/AMP.ipynb", "tutorials/notebooks/torch/quantization/adaround.ipynb", "tutorials/notebooks/torch/quantization/autoquant.ipynb", "tutorials/notebooks/torch/quantization/bn_reestimation.ipynb", "tutorials/notebooks/torch/quantization/cle.ipynb", "tutorials/notebooks/torch/quantization/qat.ipynb", "tutorials/notebooks/torch/quantization/qat_range_learning.ipynb", "tutorials/notebooks/torch/quantization/quant_analyzer.ipynb", "tutorials/on_target_inference.rst", "tutorials/quantization_workflow.rst", "tutorials/quantsim.rst", "userguide/index.rst", "userguide/quantization_tools.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[180, "term-Accelerator", true]], "accuracy": [[180, "term-Accuracy", true]], "activation": [[180, "term-Activation", true]], "activation quantization": [[180, "term-Activation-Quantization", true]], "adaround": [[180, "term-AdaRound", true]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[173, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[180, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[180, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [203, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [219, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[173, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adascale() (in module aimet_torch.experimental.adascale)": [[13, "aimet_torch.experimental.adascale.apply_adascale", false], [187, "aimet_torch.experimental.adascale.apply_adascale", false]], "apply_omniquant() (in module aimet_torch.experimental.omniquant)": [[165, "aimet_torch.experimental.omniquant.apply_omniquant", false], [193, "aimet_torch.experimental.omniquant.apply_omniquant", false]], "apply_seq_mse() (in module aimet_onnx)": [[11, "aimet_onnx.apply_seq_mse", false], [197, "aimet_onnx.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[171, "aimet_torch.seq_mse.apply_seq_mse", false], [197, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[178, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "apply_spinquant() (in module aimet_torch.experimental.spinquant)": [[172, "aimet_torch.experimental.spinquant.apply_spinquant", false], [198, "aimet_torch.experimental.spinquant.apply_spinquant", false]], "autoquant": [[180, "term-AutoQuant", true]], "batch normalization": [[180, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[180, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[180, "term-BN", true]], "callbackfunc (class in aimet_common.defs)": [[2, "aimet_common.defs.CallbackFunc", false], [163, "aimet_common.defs.CallbackFunc", false], [174, "aimet_common.defs.CallbackFunc", false], [216, "aimet_common.defs.CallbackFunc", false], [216, "id0", false]], "callbackfunc (class in aimet_common.utils)": [[168, "aimet_common.utils.CallbackFunc", false], [176, "aimet_common.utils.CallbackFunc", false], [203, "aimet_common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [216, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[163, "aimet_torch.mixed_precision.choose_mixed_precision", false], [216, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[174, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[180, "term-CNN", true]], "compression": [[180, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[177, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (in module aimet_onnx)": [[10, "aimet_onnx.compute_encodings", false]], "convolutional layer": [[180, "term-Convolutional-Layer", true]], "convolutional neural network": [[180, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[152, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[147, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[180, "term-Device", true]], "dlf": [[180, "term-DLF", true]], "dynamic layer fusion": [[180, "term-Dynamic-Layer-Fusion", true]], "edge device": [[180, "term-Edge-device", true]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [203, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[180, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[4, "aimet_onnx.cross_layer_equalization.equalize_model", false], [191, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[17, "aimet_torch.cross_layer_equalization.equalize_model", false], [191, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [216, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[163, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [216, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "exponent_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.exponent_bits", false]], "export() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.export", false]], "export() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[177, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export() (in module aimet_torch.onnx)": [[166, "aimet_torch.onnx.export", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[16, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [190, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[3, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [190, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "fold_param_quantizers() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.fold_param_quantizers", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[89, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[150, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[171, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [197, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[178, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[180, "term-FP32", true]], "from_encodings() (aimet_torch.quantization.float.floatquantizedequantize class method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.from_encodings", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.from_module", false]], "from_str() (aimet_common.defs.quantscheme class method)": [[10, "aimet_common.defs.QuantScheme.from_str", false], [170, "aimet_common.defs.QuantScheme.from_str", false], [177, "aimet_common.defs.QuantScheme.from_str", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [202, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[158, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [202, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_loss_fn() (aimet_torch.seq_mse.seqmseparams method)": [[171, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false], [197, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false]], "get_loss_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[178, "aimet_torch.v1.seq_mse.SeqMseParams.get_loss_fn", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[180, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[180, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[180, "term-KL-Divergence", true]], "layer": [[180, "term-Layer", true]], "layer-wise quantization": [[180, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [202, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[158, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [202, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[177, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lora mobilenet": [[180, "term-LoRA-MobileNet", true]], "mantissa_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.mantissa_bits", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [219, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[180, "term-Model", true]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[158, "aimet_torch.layer_output_utils.NamingScheme", false], [202, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[180, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[180, "term-NNCF", true]], "node": [[180, "term-Node", true]], "normalization": [[180, "term-Normalization", true]], "onnx": [[180, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [202, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "open neural network exchange": [[180, "term-Open-Neural-Network-Exchange", true]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "per-channel quantization": [[180, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[180, "term-Post-Training-Quantization", true]], "prepare_model() (in module aimet_torch.model_preparer)": [[161, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[180, "term-Pruning", true]], "ptq": [[180, "term-PTQ", true]], "pytorch": [[180, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [202, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[180, "term-QAT", true]], "qdo": [[180, "term-QDO", true]], "qualcomm innovation center": [[180, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [203, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [203, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[176, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[180, "term-Quantization", true]], "quantization simulation": [[180, "term-Quantization-Simulation", true]], "quantization-aware training": [[180, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[19, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx)": [[10, "aimet_onnx.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch)": [[170, "aimet_torch.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[177, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantize (class in aimet_torch.quantization.affine)": [[150, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[153, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[154, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[20, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[21, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[22, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[23, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[24, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[25, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[26, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[27, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[28, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[29, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[148, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[149, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantscheme (class in aimet_common.defs)": [[10, "aimet_common.defs.QuantScheme", false], [170, "aimet_common.defs.QuantScheme", false], [177, "aimet_common.defs.QuantScheme", false]], "quantsim": [[180, "term-QuantSim", true]], "quic": [[180, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[15, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [189, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[177, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[171, "aimet_torch.seq_mse.SeqMseParams", false], [197, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[178, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_onnx.quantsim)": [[8, "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [219, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [219, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [219, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [216, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[163, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [216, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[180, "term-Target-Hardware-Accelerator", true]], "target runtime": [[180, "term-Target-Runtime", true]], "tensorflow": [[180, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [216, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [216, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_onnx_qdq() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.to_onnx_qdq", false]], "torchscript": [[180, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [202, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[180, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[157, "aimet_torch.v2.visualization_tools.visualize_stats", false], [201, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[180, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_common.defs": [[216, 0, 1, "id0", "CallbackFunc"], [177, 0, 1, "", "QuantScheme"]], "aimet_common.defs.QuantScheme": [[177, 1, 1, "", "from_str"]], "aimet_common.utils": [[203, 0, 1, "", "CallbackFunc"]], "aimet_onnx": [[10, 0, 1, "", "QuantizationSimModel"], [197, 2, 1, "", "apply_seq_mse"], [10, 2, 1, "", "compute_encodings"]], "aimet_onnx.QuantizationSimModel": [[10, 1, 1, "", "compute_encodings"], [10, 1, 1, "", "export"], [10, 1, 1, "", "to_onnx_qdq"]], "aimet_onnx.amp.mixed_precision_algo": [[216, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[216, 1, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[216, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[216, 1, 1, "", "get_activation_quantizers"], [216, 1, 1, "", "get_active_quantizers"], [216, 1, 1, "", "get_candidate"], [216, 1, 1, "", "get_param_quantizers"], [216, 1, 1, "", "set_quantizers_to_candidate"], [216, 1, 1, "", "to_list"]], "aimet_onnx.batch_norm_fold": [[190, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.cross_layer_equalization": [[191, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[202, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[202, 1, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[216, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[203, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[203, 1, 1, "", "analyze"], [203, 1, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[8, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch": [[170, 0, 1, "", "QuantizationSimModel"]], "aimet_torch.QuantizationSimModel": [[170, 1, 1, "", "compute_encodings"], [170, 1, 1, "", "export"], [170, 1, 1, "", "fold_param_quantizers"]], "aimet_torch.amp.mixed_precision_algo": [[216, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[216, 1, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[216, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[216, 1, 1, "", "get_active_quantizers"], [216, 1, 1, "", "get_candidate"], [216, 1, 1, "", "get_input_quantizer_modules"], [216, 1, 1, "", "set_quantizers_to_candidate"], [216, 1, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[190, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[189, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.cross_layer_equalization": [[191, 2, 1, "", "equalize_model"]], "aimet_torch.experimental.adascale": [[187, 2, 1, "", "apply_adascale"]], "aimet_torch.experimental.omniquant": [[193, 2, 1, "", "apply_omniquant"]], "aimet_torch.experimental.spinquant": [[198, 2, 1, "", "apply_spinquant"]], "aimet_torch.layer_output_utils": [[202, 0, 1, "", "LayerOutputUtil"], [202, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[202, 1, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[202, 3, 1, "", "ONNX"], [202, 3, 1, "", "PYTORCH"], [202, 3, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[216, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[161, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[162, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[162, 1, 1, "", "add_check"], [162, 1, 1, "", "validate_model"]], "aimet_torch.nn": [[19, 0, 1, "", "QuantizationMixin"], [20, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [21, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [22, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [23, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [24, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [25, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [26, 0, 1, "", "QuantizedAlphaDropout"], [27, 0, 1, "", "QuantizedAvgPool1d"], [28, 0, 1, "", "QuantizedAvgPool2d"], [29, 0, 1, "", "QuantizedAvgPool3d"], [30, 0, 1, "", "QuantizedBCELoss"], [31, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [32, 0, 1, "", "QuantizedBatchNorm1d"], [33, 0, 1, "", "QuantizedBatchNorm2d"], [34, 0, 1, "", "QuantizedBatchNorm3d"], [35, 0, 1, "", "QuantizedBilinear"], [36, 0, 1, "", "QuantizedCELU"], [37, 0, 1, "", "QuantizedCTCLoss"], [38, 0, 1, "", "QuantizedChannelShuffle"], [39, 0, 1, "", "QuantizedCircularPad1d"], [40, 0, 1, "", "QuantizedCircularPad2d"], [41, 0, 1, "", "QuantizedCircularPad3d"], [42, 0, 1, "", "QuantizedConstantPad1d"], [43, 0, 1, "", "QuantizedConstantPad2d"], [44, 0, 1, "", "QuantizedConstantPad3d"], [45, 0, 1, "", "QuantizedConv1d"], [46, 0, 1, "", "QuantizedConv2d"], [47, 0, 1, "", "QuantizedConv3d"], [48, 0, 1, "", "QuantizedConvTranspose1d"], [49, 0, 1, "", "QuantizedConvTranspose2d"], [50, 0, 1, "", "QuantizedConvTranspose3d"], [51, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [52, 0, 1, "", "QuantizedCosineSimilarity"], [53, 0, 1, "", "QuantizedCrossEntropyLoss"], [54, 0, 1, "", "QuantizedDropout"], [55, 0, 1, "", "QuantizedDropout1d"], [56, 0, 1, "", "QuantizedDropout2d"], [57, 0, 1, "", "QuantizedDropout3d"], [58, 0, 1, "", "QuantizedELU"], [59, 0, 1, "", "QuantizedEmbedding"], [60, 0, 1, "", "QuantizedEmbeddingBag"], [61, 0, 1, "", "QuantizedFeatureAlphaDropout"], [62, 0, 1, "", "QuantizedFlatten"], [63, 0, 1, "", "QuantizedFold"], [64, 0, 1, "", "QuantizedFractionalMaxPool2d"], [65, 0, 1, "", "QuantizedFractionalMaxPool3d"], [66, 0, 1, "", "QuantizedGELU"], [67, 0, 1, "", "QuantizedGLU"], [68, 0, 1, "", "QuantizedGRU"], [69, 0, 1, "", "QuantizedGRUCell"], [70, 0, 1, "", "QuantizedGaussianNLLLoss"], [71, 0, 1, "", "QuantizedGroupNorm"], [72, 0, 1, "", "QuantizedHardshrink"], [73, 0, 1, "", "QuantizedHardsigmoid"], [74, 0, 1, "", "QuantizedHardswish"], [75, 0, 1, "", "QuantizedHardtanh"], [76, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [77, 0, 1, "", "QuantizedHuberLoss"], [78, 0, 1, "", "QuantizedInstanceNorm1d"], [79, 0, 1, "", "QuantizedInstanceNorm2d"], [80, 0, 1, "", "QuantizedInstanceNorm3d"], [81, 0, 1, "", "QuantizedKLDivLoss"], [82, 0, 1, "", "QuantizedL1Loss"], [83, 0, 1, "", "QuantizedLPPool1d"], [84, 0, 1, "", "QuantizedLPPool2d"], [85, 0, 1, "", "QuantizedLSTM"], [86, 0, 1, "", "QuantizedLSTMCell"], [87, 0, 1, "", "QuantizedLayerNorm"], [88, 0, 1, "", "QuantizedLeakyReLU"], [89, 0, 1, "", "QuantizedLinear"], [90, 0, 1, "", "QuantizedLocalResponseNorm"], [91, 0, 1, "", "QuantizedLogSigmoid"], [92, 0, 1, "", "QuantizedLogSoftmax"], [93, 0, 1, "", "QuantizedMSELoss"], [94, 0, 1, "", "QuantizedMarginRankingLoss"], [95, 0, 1, "", "QuantizedMaxPool1d"], [96, 0, 1, "", "QuantizedMaxPool2d"], [97, 0, 1, "", "QuantizedMaxPool3d"], [98, 0, 1, "", "QuantizedMaxUnpool1d"], [99, 0, 1, "", "QuantizedMaxUnpool2d"], [100, 0, 1, "", "QuantizedMaxUnpool3d"], [101, 0, 1, "", "QuantizedMish"], [102, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [103, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [104, 0, 1, "", "QuantizedMultiMarginLoss"], [105, 0, 1, "", "QuantizedNLLLoss"], [106, 0, 1, "", "QuantizedNLLLoss2d"], [107, 0, 1, "", "QuantizedPReLU"], [108, 0, 1, "", "QuantizedPairwiseDistance"], [109, 0, 1, "", "QuantizedPixelShuffle"], [110, 0, 1, "", "QuantizedPixelUnshuffle"], [111, 0, 1, "", "QuantizedPoissonNLLLoss"], [112, 0, 1, "", "QuantizedRNN"], [113, 0, 1, "", "QuantizedRNNCell"], [114, 0, 1, "", "QuantizedRReLU"], [115, 0, 1, "", "QuantizedReLU"], [116, 0, 1, "", "QuantizedReLU6"], [117, 0, 1, "", "QuantizedReflectionPad1d"], [118, 0, 1, "", "QuantizedReflectionPad2d"], [119, 0, 1, "", "QuantizedReflectionPad3d"], [120, 0, 1, "", "QuantizedReplicationPad1d"], [121, 0, 1, "", "QuantizedReplicationPad2d"], [122, 0, 1, "", "QuantizedReplicationPad3d"], [123, 0, 1, "", "QuantizedSELU"], [124, 0, 1, "", "QuantizedSiLU"], [125, 0, 1, "", "QuantizedSigmoid"], [126, 0, 1, "", "QuantizedSmoothL1Loss"], [127, 0, 1, "", "QuantizedSoftMarginLoss"], [128, 0, 1, "", "QuantizedSoftmax"], [129, 0, 1, "", "QuantizedSoftmax2d"], [130, 0, 1, "", "QuantizedSoftmin"], [131, 0, 1, "", "QuantizedSoftplus"], [132, 0, 1, "", "QuantizedSoftshrink"], [133, 0, 1, "", "QuantizedSoftsign"], [134, 0, 1, "", "QuantizedTanh"], [135, 0, 1, "", "QuantizedTanhshrink"], [136, 0, 1, "", "QuantizedThreshold"], [137, 0, 1, "", "QuantizedTripletMarginLoss"], [138, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [139, 0, 1, "", "QuantizedUnflatten"], [140, 0, 1, "", "QuantizedUnfold"], [141, 0, 1, "", "QuantizedUpsample"], [142, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [143, 0, 1, "", "QuantizedUpsamplingNearest2d"], [144, 0, 1, "", "QuantizedZeroPad1d"], [145, 0, 1, "", "QuantizedZeroPad2d"], [146, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[19, 1, 1, "", "compute_encodings"], [19, 1, 1, "", "forward"], [19, 1, 1, "", "from_module"], [19, 1, 1, "", "get_default_kernel"], [19, 1, 1, "", "get_kernel"], [19, 1, 1, "", "implements"], [19, 3, 1, "", "input_quantizers"], [19, 3, 1, "", "output_quantizers"], [19, 3, 1, "", "param_quantizers"], [19, 1, 1, "", "set_default_kernel"], [19, 1, 1, "", "set_kernel"], [19, 1, 1, "", "wrap"]], "aimet_torch.nn.QuantizedLinear": [[89, 1, 1, "", "forward"]], "aimet_torch.onnx": [[166, 2, 1, "", "export"]], "aimet_torch.quant_analyzer": [[203, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[203, 1, 1, "", "analyze"], [203, 1, 1, "", "check_model_sensitivity_to_quantization"], [203, 1, 1, "", "export_per_layer_encoding_min_max_range"], [203, 1, 1, "", "export_per_layer_mse_loss"], [203, 1, 1, "", "export_per_layer_stats_histogram"], [203, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [203, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[147, 0, 1, "", "DequantizedTensor"], [148, 0, 1, "", "QuantizedTensor"], [149, 0, 1, "", "QuantizedTensorBase"]], "aimet_torch.quantization.DequantizedTensor": [[147, 1, 1, "", "dequantize"], [147, 1, 1, "", "quantize"], [147, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[148, 1, 1, "", "dequantize"], [148, 1, 1, "", "quantize"], [148, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[149, 1, 1, "", "clone"], [149, 1, 1, "", "dequantize"], [149, 1, 1, "", "detach"], [149, 1, 1, "", "new_empty"], [149, 1, 1, "", "quantize"], [149, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[150, 0, 1, "", "Quantize"], [151, 0, 1, "", "QuantizeDequantize"], [152, 2, 1, "", "dequantize"], [153, 2, 1, "", "quantize"], [154, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[150, 1, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[151, 1, 1, "", "forward"]], "aimet_torch.quantization.float": [[155, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[155, 4, 1, "", "bitwidth"], [155, 1, 1, "", "compute_encodings"], [155, 4, 1, "", "exponent_bits"], [155, 1, 1, "", "forward"], [155, 1, 1, "", "from_encodings"], [155, 1, 1, "", "get_encodings"], [155, 1, 1, "", "get_extra_state"], [155, 1, 1, "", "is_bfloat16"], [155, 1, 1, "", "is_float16"], [155, 1, 1, "", "load_state_dict"], [155, 4, 1, "", "mantissa_bits"], [155, 1, 1, "", "set_extra_state"]], "aimet_torch.seq_mse": [[197, 0, 1, "", "SeqMseParams"], [197, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[197, 1, 1, "", "forward_fn"], [197, 1, 1, "", "get_loss_fn"]], "aimet_torch.v1.adaround.adaround_weight": [[173, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[173, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[174, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[176, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[176, 1, 1, "", "analyze"], [176, 1, 1, "", "check_model_sensitivity_to_quantization"], [176, 1, 1, "", "export_per_layer_encoding_min_max_range"], [176, 1, 1, "", "export_per_layer_mse_loss"], [176, 1, 1, "", "export_per_layer_stats_histogram"], [176, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [176, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[177, 0, 1, "", "QuantizationSimModel"], [177, 1, 1, "", "load_checkpoint"], [177, 1, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[177, 1, 1, "", "compute_encodings"], [177, 1, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[178, 0, 1, "", "SeqMseParams"], [178, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[178, 1, 1, "", "forward_fn"], [178, 1, 1, "", "get_loss_fn"]], "aimet_torch.v2.mixed_precision": [[219, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[219, 1, 1, "", "apply"], [219, 1, 1, "", "set_model_input_precision"], [219, 1, 1, "", "set_model_output_precision"], [219, 1, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[159, 2, 1, "", "set_activation_quantizers_to_float"], [159, 2, 1, "", "set_blockwise_quantization_for_weights"], [159, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[201, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:attribute", "4": "py:property"}, "terms": {"": [2, 8, 9, 11, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 155, 159, 161, 163, 164, 168, 171, 174, 175, 176, 177, 178, 180, 183, 185, 186, 187, 188, 191, 193, 197, 198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 218, 219, 220, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244, 245, 247, 248], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], "00": [153, 154, 191, 232, 233, 234], "000": 245, "0000": [147, 154, 166, 169], "0000e": [153, 154], "001": [189, 227, 235], "0014807": 184, "00183112e": 190, "00215936e": 190, "0022e": 191, "0030": 169, "00317": 187, "0032": 169, "0034": 169, "0035": 169, "0036": [151, 169], "0037": 169, "0038": 169, "0039": [151, 169], "0059": 169, "0063": 169, "0064": 169, "0068": 169, "0069": 169, "0073": 169, "0074": 169, "0078": 169, "0086": 185, "01": [12, 153, 154, 173, 186, 188, 190, 191, 237], "0115": 151, "0117": 169, "01392324": 185, "0142": 149, "0156": 169, "0158": 151, "01675645": 185, "0176": 151, "0195": 169, "02": [153, 154, 190, 191], "0234": [169, 185], "0235": 188, "0244e": 191, "0252": 185, "026354755942277083": 184, "02635476": 184, "0271e": 191, "0273": 169, "0278355": 184, "02887694e": 191, "0293162": 184, "0295": 151, "03": [190, 191], "0303": 185, "0312": 169, "03513189": 185, "0352": 169, "03658897": 185, "0375e": 191, "03798249e": 190, "0386": 151, "0391": 169, "04": [182, 184, 185], "04025269e": 190, "0406616e": 191, "0422": 185, "0424": 151, "0428": 189, "0430": 169, "0469": 169, "0471": 151, "05": [153, 154, 190, 191], "0500e": [153, 154], "0508": 169, "05270951": 184, "0541903": 184, "0549": 151, "0564": 151, "0597e": 191, "05f6810": 199, "0600": 185, "0639": 151, "0667": 154, "0680": 151, "0680e": 191, "0784": 151, "07906426": 184, "08": [153, 154], "080545": 184, "0819": 151, "0820258": 184, "0859": 155, "0870": 185, "087e9b1": 199, "0882": 149, "0889": 155, "0891": 155, "0897725": 185, "08c17b8": 199, "0901e": 191, "09111059e": 190, "0932e": 191, "0947": 155, "0973e": 191, "0fe6701": 199, "0x7f127685a598": 162, "0x7f9dd9bd90d0": 162, "0x7ff5703eff28": 162, "0x7ff577373598": 162, "1": [0, 2, 12, 13, 14, 18, 19, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 183, 184, 185, 191, 194, 195, 196, 204, 205, 207, 208, 209, 211, 212, 215, 245, 249], "10": [7, 10, 18, 19, 147, 150, 151, 153, 161, 162, 164, 169, 170, 177, 182, 184, 185, 204, 205, 207, 208, 209, 211, 216, 218, 220, 221, 232, 233, 234, 238, 240, 241, 249], "100": [2, 15, 160, 174, 180, 184, 189, 191, 216, 218, 238], "1000": [9, 149, 168, 176, 203, 205, 209, 211, 218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "10000": [1, 186, 227, 228, 229, 230, 231, 235, 236], "1000e": [153, 154], "102": 169, "1024": [216, 220], "103": [150, 169], "10541902": 184, "106": 150, "1060": 151, "1068997": 184, "107": 169, "107b339": 199, "108": 169, "1080": 185, "109": 150, "10984787": 185, "10k": [1, 12, 173, 186], "11": [148, 149, 153, 169, 230, 234, 249], "110": [150, 169], "1108": 185, "111": [150, 169], "1128": 151, "11446196": 185, "1176": 151, "118": 169, "11899511": 185, "119": 169, "12": [153, 169, 183, 184, 204, 249], "12039044e": 191, "122": 150, "1228e": 191, "1232": 151, "127": [148, 149, 169, 170, 220, 221], "1279": 185, "128": [2, 14, 148, 149, 159, 161, 163, 169, 170, 174, 175, 184, 188, 197, 204, 215, 216, 220, 221], "129": 150, "13": [153, 169, 230, 249], "1307": 151, "131": 150, "13137": [187, 193, 199], "1316e": 191, "13177378": 184, "1333": 154, "1377e": 191, "1398": 208, "14": [153, 169, 230, 249], "1406": 208, "141": 150, "143": 150, "144": 150, "145": 150, "1450607": 185, "146": 150, "1489e": 191, "1493fe1": 199, "14c8e81": 199, "15": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 230, 238, 240, 241, 243, 244, 245, 246, 247, 248, 249], "150": 150, "1500": [13, 187], "15000": 186, "1500e": [153, 154], "152": 150, "153": 148, "155": 150, "15717569e": 190, "15812853": 184, "15e": [232, 234], "15k": [1, 12, 173, 186], "16": [2, 19, 150, 155, 161, 163, 164, 166, 167, 169, 170, 174, 177, 185, 187, 188, 191, 193, 195, 196, 198, 204, 216, 220, 221, 227, 230, 235, 244, 248, 249], "1609": 185, "162": 150, "16406": [172, 198, 199], "1647": 151, "1676": 185, "16839484e": 191, "16966406e": 191, "17": [150, 169, 191, 249], "1706e": 191, "1709": 151, "172": 150, "1727": 155, "1729": 155, "1741": 151, "178": 150, "179": 150, "18": [169, 227, 228, 229, 230, 231, 249], "181": 150, "18136823e": 190, "18448329": 184, "186": 150, "18673885e": 190, "187": 150, "188": 150, "1889": 151, "18dfedc": 199, "19": [150, 169, 249], "192": 150, "1921e": [153, 154], "194": 150, "1943": 208, "1945": 149, "1955": 208, "1977": 151, "19778645e": 190, "1b": [193, 198], "1e": [189, 190, 191, 221], "1k": [186, 188, 189, 216], "1m": [228, 229, 230, 231, 236, 239, 240, 241, 242], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 191, 192, 194, 195, 196, 200, 201, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 217, 223, 225, 243, 245, 246, 247, 249], "20": [11, 12, 149, 166, 171, 173, 178, 186, 189, 195, 196, 197, 238, 240, 241, 249], "200": [160, 237], "2000": [12, 148, 154, 173, 186, 236, 237], "2000e": [153, 154], "2012": [218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "2014": 208, "2016": 208, "2017": 208, "2018e": 191, "2019": [229, 239], "2029": 185, "203": 150, "2048": [193, 198], "205": 148, "2050e": 191, "207": 150, "20722957": 185, "21": [149, 166, 249], "21066449e": 190, "21083805": 184, "2118": 151, "2123188": 184, "21250950e": 190, "2137995": 184, "216": 150, "218": 150, "2196": 151, "22": [169, 182, 183, 184, 185, 249], "2205": 149, "2212": 151, "224": [170, 185, 186, 188, 189, 190, 191, 197, 202, 203, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "225": [186, 188, 197, 216, 218, 220, 228, 237], "22583652e": 190, "2259": 185, "226": 150, "2260": 149, "2265": 151, "22884297": 185, "229": [186, 188, 197, 216, 218, 220, 228, 237], "2298e": 191, "22b5c94": 199, "22cac5c": 199, "23": [148, 149, 249], "2306": 187, "2308": [187, 193, 199], "2353": 151, "2355": 151, "2363": 151, "237": 150, "23719281": 184, "23799022": 185, "238": 150, "24": 249, "240": 169, "2401543": 184, "2405": [172, 198, 199], "241": 169, "242": 169, "24257803e": 190, "243": 169, "244": 169, "2443e": 191, "245": 169, "2458": 151, "246": 169, "24665177": 185, "247": 169, "248": 169, "249": 169, "2494": 185, "2494d90": 199, "25": [169, 249], "250": 169, "2500e": [153, 154], "251": 169, "252": 169, "253": 169, "254": 169, "2546": 151, "255": [147, 150, 169, 170, 220, 221], "256": [9, 169, 186, 188, 197, 203, 216, 218, 220, 228, 235], "2568": 151, "2592": 162, "26": [148, 169, 249], "26179108e": 190, "26354757": 184, "2650282": 184, "2667": 154, "27": 249, "27045077": 185, "2771": 151, "28": [148, 149, 205, 209, 211, 249], "28065038": 185, "28238320e": 191, "28990233": 184, "29": [169, 249], "291383": 184, "2921": 149, "2930528e": 191, "2998e": 191, "2b": 206, "2c64eac": 199, "2c8ae88": 199, "2d": [204, 205, 208, 212], "2d4e0eb": 199, "2ed8305": 199, "2f05175": 199, "2x": 199, "3": [2, 14, 18, 147, 149, 153, 154, 158, 160, 161, 162, 163, 167, 169, 170, 171, 174, 175, 178, 182, 184, 185, 191, 195, 204, 205, 206, 209, 211, 212, 219, 249], "30": [150, 169, 249], "300": 235, "3000": 148, "3038": 151, "31": [9, 12, 168, 170, 173, 176, 177, 186, 203, 220, 221, 249], "31080866e": 190, "312": 148, "3137": 151, "31625706": 184, "3178": 185, "31ca7fd": 199, "32": [150, 161, 162, 169, 170, 177, 180, 186, 187, 188, 189, 190, 191, 197, 213, 216, 218, 220, 221, 224, 227, 228, 229, 232, 233, 234, 235, 236, 237, 239, 240, 241, 249], "3209": 185, "32141271e": 190, "3216": 151, "3258": 185, "33": [150, 232, 233, 234, 249], "3333": 154, "339a225": 199, "34": 249, "34215236e": 191, "34261182": 184, "3434e": 191, "3435e": 191, "3451": 151, "3467390e": 191, "34694423e": 191, "347054": 184, "3470540046691895": 184, "3479e": 191, "35": [170, 220, 221, 249], "350m": [195, 196], "35107604": 185, "35139937e": 191, "35536635": 185, "3576329": 185, "3587": 185, "35ad990": 199, "36": 150, "3603": 185, "3657e": 191, "36678016": 185, "3687": 185, "36896658": 184, "37": [149, 150, 169], "3706": 185, "3724": 185, "3734087606114667": 184, "374e8db": 199, "3750526": 199, "37757687e": 191, "3792e": 191, "38": [148, 149, 208], "3851556": 185, "39": 147, "3938e": 191, "3992": 151, "3c92bb7": 199, "3d": 204, "3d5e0dd": 199, "4": [2, 8, 9, 12, 14, 148, 149, 153, 154, 159, 160, 161, 166, 167, 168, 169, 170, 173, 175, 176, 177, 185, 190, 191, 195, 196, 204, 205, 207, 209, 211, 212, 213, 215, 216, 221, 231, 232, 233, 234, 237, 242, 249], "40": [148, 149, 193, 198], "4000": [147, 154], "406": [186, 188, 197, 216, 218, 220, 228, 237], "4082": 185, "4094e": 191, "41": 147, "41059163e": 191, "4130": 185, "4132449": 185, "414cdde": 199, "4157": 151, "4186": 185, "42": 169, "42083430e": 190, "4216761": 184, "4231569": 184, "4236": 185, "4246376": 184, "42477691e": 190, "43": 169, "43178225": 185, "43477735": 185, "4392": 151, "4404": 185, "44632760e": 191, "4475": 151, "44803086": 184, "4495116": 184, "44993666e": 191, "4503196": 185, "45040053": 185, "4549": 151, "455": [182, 184, 185], "456": [186, 188, 197, 216, 218, 220, 228, 237], "4578e": 191, "4585028e": 191, "4599525": 185, "45c2a65": 199, "46642041e": 191, "4667": 154, "46723792": 185, "4677236": 185, "4686e": 191, "4694": 151, "4706": 151, "47438562": 184, "4758663": 184, "4784": 151, "48": 169, "485": [186, 188, 197, 216, 218, 220, 228, 237], "4863": 151, "4881": 185, "49": [150, 169], "4933": 185, "4943e": 191, "499df9f": 199, "4ad0703": 199, "4b94ca9": 199, "4d": 204, "4f": [188, 218, 220], "4febdd4": 199, "5": [149, 150, 151, 153, 154, 155, 160, 161, 164, 167, 169, 182, 184, 185, 189, 190, 191, 204, 205, 206, 209, 211, 216, 221, 229, 231, 232, 233, 234, 236, 239, 240, 241, 249], "50": [206, 216, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "500": [203, 205, 220, 228, 229, 230, 231, 236, 237, 239, 240, 241, 242, 245], "5000": 237, "50000": [186, 216, 228], "5000e": [153, 154], "5006": 210, "50074035": 184, "5022211": 184, "50366503": 185, "5091695": 185, "50f35dd": 199, "51": 147, "512": [195, 196], "5176e": 191, "5181860": 199, "51876003e": 190, "51f8990": 199, "52": 147, "5203": 185, "521": 147, "5220": 151, "5255": 151, "52709514": 184, "52974629e": 190, "5315115": 185, "5317543": 185, "5333": [151, 154], "54": 169, "5430": 185, "55": [150, 169], "55344987": 184, "5540": 151, "5549306": 184, "56632766e": 190, "56810045": 185, "5695": 151, "57": [148, 149, 169], "57021021e": 191, "57980466": 184, "5803": 185, "5825": 185, "5856506e": 191, "5876": 189, "5897": 151, "59": [150, 169], "59643137e": 190, "5985": 185, "5996e": 191, "5aac9c5": 199, "5e": [233, 234, 238, 240, 241], "6": [149, 153, 154, 161, 169, 190, 191, 204, 216, 234, 236, 239, 240, 241, 249], "60": 169, "6000": [147, 148, 149, 154], "6014": 185, "6039": 151, "6054": 151, "6061594": 184, "6079": 151, "60808927": 185, "61": 169, "61087702e": 190, "6169e": 191, "6177": 151, "6196": 151, "62": 169, "6208e": 191, "6213797e": 191, "6216ca0": 199, "62274104": 185, "6247": 151, "62f5879": 199, "63": 150, "63172388e": 190, "6325141": 184, "64": [8, 9, 150, 159, 169, 170, 182, 184, 185, 187, 188, 203, 204, 215, 216, 220, 221], "6431": 151, "6463e": 191, "65": 206, "654f4b1": 199, "6588689": 184, "66": [150, 206, 232, 233, 234], "6603496": 184, "6618e": 191, "6636515": 199, "6667": 154, "6695": 151, "68": 147, "68016": 221, "6836": 218, "68522364": 184, "6865": 218, "6867044": 184, "6885": 218, "6998855": 185, "6c92a97": 199, "6c9f584": 199, "6d1664c": 199, "6d3aa97": 199, "6f670a4": 199, "7": [149, 153, 154, 155, 169, 170, 190, 191, 212, 220, 221, 233, 234, 236, 238, 239, 240, 241, 249], "70": 169, "70029c5": 199, "70130579e": 191, "70838": 221, "71": 149, "7115784": 184, "7119e": 191, "7159e": 191, "7164": 188, "7173": 220, "72468403e": 191, "73242160e": 190, "7333": 154, "7336ead": 199, "7364b37": 199, "73793316": 184, "74": 150, "74478185e": 190, "75": 206, "754d030": 199, "76": 169, "76428795": 184, "77": 169, "77213307e": 190, "7741": 151, "7765": 151, "7894": 151, "7932": 151, "7978e": 191, "8": [1, 2, 8, 9, 12, 14, 19, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 168, 169, 170, 173, 174, 175, 176, 177, 180, 184, 185, 186, 188, 190, 191, 197, 203, 205, 209, 211, 212, 215, 216, 219, 220, 221, 227, 228, 229, 230, 231, 233, 234, 235, 236, 238, 239, 240, 241, 242, 249], "80": 150, "800": [165, 193], "8000": [151, 154], "80053532e": 190, "8009871": 199, "8078": 151, "80cd141": 199, "81699747": 184, "81760912e": 190, "81884710e": 190, "8229": 151, "83": [150, 169], "8347e": 191, "83640555e": 191, "8365e": 191, "836ab1": 199, "84": 169, "8433522": 184, "84edcf5": 199, "86": 150, "8667": 154, "86945379e": 190, "8706": 151, "8711877": 184, "87656835e": 190, "8789e": 191, "8796": 151, "8836": 151, "88706ef": 199, "89": 150, "89074164e": 191, "89348674e": 190, "8945e": 191, "8984": 155, "8994": 155, "8998": 155, "8bit": 199, "9": [147, 153, 154, 161, 169, 189, 190, 191, 216, 220, 230, 232, 233, 234, 235, 236, 240, 241, 242, 249], "90": 216, "9086e": 191, "91": 169, "91109af": 199, "911af75": 199, "9157": 151, "9176": 151, "92": 169, "9216": 161, "92f63f5": 199, "93": 169, "9333": 154, "93787616e": 190, "94": [150, 169], "9487": 151, "94877124": 184, "9490": 151, "95": 169, "95260113e": 190, "9570": 169, "9585e": 191, "96": [169, 187], "9609": 169, "96155685e": 190, "9648": 169, "9688": 169, "97": 147, "9700": 151, "9727": 169, "9766": 169, "9805": 169, "9844": 169, "9883": 169, "9922": 169, "9961": [151, 169], "9999": 166, "9a2a407": 199, "9b8c655": 199, "A": [2, 3, 9, 10, 12, 14, 16, 17, 18, 159, 163, 167, 168, 170, 173, 174, 175, 176, 177, 179, 180, 186, 188, 190, 191, 195, 197, 198, 203, 204, 205, 207, 208, 209, 211, 213, 215, 216, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247], "And": [227, 235, 247], "As": [19, 188, 198, 203, 204, 206, 207, 223, 227, 229, 232, 233, 234, 235, 238, 239, 240, 241], "At": [7, 198, 206, 210, 218, 244], "But": [161, 208, 227, 230, 235, 238, 242], "By": [10, 18, 164, 167, 170, 186, 205, 208, 209, 211, 220, 221, 235, 236, 238, 239, 240, 241, 245], "For": [9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 161, 162, 164, 166, 167, 168, 169, 176, 181, 182, 184, 185, 188, 191, 198, 199, 202, 203, 204, 205, 206, 207, 208, 213, 216, 219, 220, 221, 222, 223, 226, 227, 230, 235, 238, 242, 244, 245, 247, 248], "If": [1, 2, 9, 10, 12, 14, 18, 19, 89, 150, 151, 152, 153, 154, 155, 160, 161, 162, 163, 164, 166, 168, 169, 170, 173, 174, 175, 176, 177, 186, 187, 188, 189, 193, 195, 197, 199, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "In": [19, 149, 160, 161, 162, 164, 170, 174, 177, 185, 188, 189, 190, 195, 196, 198, 203, 204, 206, 207, 208, 210, 212, 215, 216, 217, 218, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247], "It": [1, 2, 6, 8, 9, 10, 160, 161, 170, 177, 186, 187, 188, 193, 197, 199, 200, 202, 203, 205, 207, 212, 213, 215, 216, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242], "Its": 212, "NOT": [14, 175, 188], "No": [162, 190, 247], "Not": [9, 168, 176, 187, 189, 193, 203, 204, 207, 219, 221, 227, 230, 238, 242], "ON": 183, "OR": 160, "Of": [232, 233, 234, 240, 241], "On": [227, 235], "One": [203, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 245], "Or": [161, 177, 202, 203, 208], "Such": 161, "That": [212, 228, 229, 231, 236, 239, 240, 241], "The": [1, 2, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 153, 154, 155, 157, 158, 160, 161, 162, 163, 164, 166, 167, 169, 170, 171, 172, 173, 174, 175, 177, 178, 180, 181, 184, 186, 187, 188, 189, 191, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "Then": [9, 168, 176, 191, 203, 208, 240, 241, 244], "There": [158, 160, 162, 177, 188, 202, 203, 221, 227, 228, 230, 235, 236, 238, 239, 240, 241, 242, 247, 248], "These": [14, 160, 162, 164, 175, 177, 186, 188, 193, 195, 196, 203, 205, 206, 222, 224, 227, 229, 235, 237, 239, 244, 245], "To": [2, 19, 156, 160, 163, 164, 167, 174, 181, 184, 185, 186, 187, 189, 190, 191, 193, 197, 198, 203, 204, 207, 210, 213, 215, 216, 219, 220, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248], "With": [147, 182, 184, 221, 230, 242], "_": [9, 10, 19, 150, 151, 154, 157, 164, 168, 169, 170, 176, 181, 184, 186, 187, 190, 191, 195, 197, 199, 201, 202, 203, 204, 213, 215, 216, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 247, 248], "__getitem__": 237, "__init__": [14, 161, 162, 164, 175, 188, 193, 198, 199, 216, 223, 237], "__iter__": [188, 193, 198, 216], "__len__": [9, 193, 198, 203, 237], "__next__": [188, 198, 216], "__quant_init__": [19, 164], "__torch_function__": 161, "__version__": 185, "_batch_index": 188, "_create_sampled_data_load": 237, "_current_iter": 216, "_data": [188, 216], "_dataset": 237, "_default_forward_fn": [14, 175, 188], "_default_r1_fusion_func": 198, "_default_rmsnorm_linear_pairs_func": 198, "_encodingmismatchinfo": 220, "_get_unlabled_data_load": 203, "_infer_activation_dtyp": 199, "_int": 199, "_is_encoding_frozen": 160, "_max": 155, "_module_to_wrap": 160, "_not_specifi": [170, 220, 221], "_q": 199, "_quantizationsimmodelinterfac": [168, 174, 176, 177, 203], "_quantschemepair": [14, 175, 188], "_remove_input_quant": 160, "_remove_output_quant": 160, "_remove_param_quant": 160, "_step": [153, 154], "_tie_quant": 199, "_unlabel": 216, "a2adae2": 199, "a967b8f": 199, "a97354f": 199, "ab": [187, 193], "abe8782": 199, "abil": [199, 248], "abl": [9, 14, 147, 148, 149, 161, 162, 175, 188, 203, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 244], "about": [147, 179, 181, 191, 213, 215, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 248], "abov": [161, 163, 203, 204, 205, 208, 209, 210, 211, 212, 219, 227, 230, 232, 233, 234, 235, 242, 244, 245], "absolut": [150, 151, 205, 209, 211, 216], "absolute_path_to_workspac": 226, "absorpt": [229, 239], "abstract": [19, 149, 164], "ac05d10": 199, "acc_top1": 188, "acceler": [170, 180, 184, 185, 199, 205, 208, 209, 211, 218, 220, 231, 240, 241], "accept": [199, 207, 216, 220, 224, 244, 247, 248], "access": [160, 198, 199, 235, 236, 239, 240, 241], "accord": [10, 158, 170, 202, 220, 221], "accordingli": 199, "account": [13, 187, 199, 207, 224], "accumul": [206, 208, 209, 211, 214], "accur": 190, "accuraci": [2, 11, 12, 14, 18, 167, 173, 174, 175, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 214, 216, 217, 218, 220, 221, 224, 225, 230, 237, 238, 242, 245], "achiev": [169, 180, 182, 206, 216, 217, 219, 228, 236, 244, 247, 248], "acronym": 181, "across": [161, 180, 184, 191, 192, 203, 244, 245], "act": [2, 9, 163, 168, 174, 176, 203, 216, 227], "action": [203, 212], "activ": [2, 9, 10, 13, 157, 159, 161, 163, 164, 167, 168, 171, 174, 176, 178, 180, 182, 183, 186, 187, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244, 245, 247, 248], "activation_bitwidth": 213, "activation_encod": 213, "activation_quant": [2, 216], "activation_typ": [10, 185, 186, 197, 218, 220, 227, 228, 229, 231], "activations_pdf": [203, 230, 242], "actual": [200, 203, 227, 235, 238], "ad": [162, 167, 194, 198, 199, 213, 222, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241], "ada": 228, "ada_model": 236, "adamw": [195, 196], "adapt": [12, 15, 167, 171, 173, 178, 180, 188, 189, 193, 194, 195, 196, 197, 199, 203, 238, 244], "adapter1": 167, "adaptiveavgpool1d": 20, "adaptiveavgpool2d": 21, "adaptiveavgpool3d": 22, "adaptivemaxpool1d": 23, "adaptivemaxpool2d": 24, "adaptivemaxpool3d": 25, "adaptiveround": 199, "adaptor": [165, 193], "adaround": [0, 1, 14, 156, 160, 175, 180, 186, 188, 199, 203, 224, 237, 244], "adaround_data_load": [188, 237], "adaround_dataset_s": [188, 237], "adaround_param": [14, 175, 188, 237], "adaround_weight": [12, 160, 173, 186, 188, 236, 237], "adarounded_model": 186, "adaroundparamet": [12, 14, 173, 175, 186, 188, 236, 237], "adascal": [0, 156, 199], "adascale_model_config_dict": 187, "adascale_optim": 187, "adascalemodelconfig": 187, "add": [161, 162, 164, 167, 177, 199, 212, 213, 222, 223, 230, 231, 232, 233, 234, 240, 241, 245, 247, 248], "add_adapt": 167, "add_check": 162, "add_lora_to_r": 196, "addit": [19, 169, 170, 177, 189, 199, 213, 215, 220, 221, 222, 227, 230, 235, 238, 242, 243, 244], "address": [191, 199, 210, 224], "adequ": 247, "adher": 248, "adjac": [180, 190, 191, 192, 198, 222, 227, 228, 229, 231, 235, 236, 239, 240, 241], "adjust": [157, 192, 195, 196, 199, 201, 205, 206, 215, 221, 224, 244], "admin": 226, "advanc": [169, 180, 182, 199, 225], "advantag": [221, 244], "ae02aa8": 199, "ae981f7": 199, "affect": [12, 173, 180, 186, 215, 222, 228, 248], "affin": [8, 19, 147, 148, 149, 150, 151, 152, 153, 154, 159, 164, 190, 191, 195, 204, 215], "affine_q": 160, "affine_qdq": 160, "affine_quant": 160, "affinequant": [155, 160], "after": [1, 9, 155, 161, 162, 164, 168, 169, 176, 177, 180, 186, 188, 189, 190, 191, 198, 199, 203, 206, 208, 210, 218, 220, 221, 224, 227, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 244], "afterward": [13, 187], "again": [229, 232, 233, 234, 239, 248], "against": [9, 168, 176, 203, 205, 238, 240, 241], "aggress": 180, "agre": 248, "ahead": 225, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 244, 245, 246, 247, 248, 249], "aim": [216, 232, 233, 234, 238, 244, 248], "aimet": [5, 18, 19, 156, 160, 161, 162, 164, 167, 169, 180, 184, 186, 187, 188, 189, 190, 191, 192, 193, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 214, 215, 216, 217, 218, 219, 220, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 248], "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], "aimet_export_artifact": 202, "aimet_exported_model": 243, "aimet_exported_model_path": 243, "aimet_omniquant_artifact": [165, 193], "aimet_onnx": [0, 184, 185, 186, 190, 191, 197, 199, 202, 203, 213, 215, 216, 218, 220, 227, 228, 229, 230, 231], "aimet_repo_path": 230, "aimet_tf": 184, "aimet_torch": [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 209, 211, 213, 215, 216, 219, 220, 221, 227, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "algo": [174, 216], "algorithm": [2, 18, 174, 205, 206, 207, 208, 209, 211, 212, 213, 224, 244], "alia": [10, 170, 177, 220, 221], "aliasbackward0": [147, 148, 149, 150, 151, 166, 169, 185], "align": [199, 220, 222], "all": [0, 1, 2, 3, 8, 9, 10, 16, 19, 89, 156, 157, 159, 161, 162, 163, 164, 168, 169, 170, 171, 174, 176, 177, 178, 183, 184, 185, 186, 187, 190, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 207, 208, 209, 211, 213, 215, 216, 218, 219, 220, 221, 222, 224, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244, 245, 247], "all_q_modul": 160, "all_quant_wrapp": 160, "allclos": 161, "allow": [2, 14, 18, 147, 148, 149, 159, 161, 166, 174, 175, 188, 199, 200, 204, 205, 208, 209, 210, 211, 213, 215, 216, 223, 224, 227, 235, 237, 244], "allow_custom_downsample_op": [18, 205, 232, 234], "allow_overwrit": [160, 186, 196, 220], "allowed_accuracy_drop": [2, 14, 174, 175, 188, 216, 227, 235, 237], "alon": [167, 245], "along": [148, 167, 169, 216, 244, 245], "alpha": [191, 216], "alphadropout": 26, "alreadi": [17, 19, 169, 170, 177, 191, 195, 198, 199, 216, 218, 220, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "also": [12, 149, 161, 169, 173, 174, 175, 176, 177, 178, 183, 184, 186, 187, 190, 193, 199, 200, 203, 204, 205, 208, 212, 213, 216, 218, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 247], "alter": [186, 236], "altern": [9, 168, 176, 203, 204, 215, 227, 232, 233, 234, 235], "alwai": [195, 196, 199, 207, 216], "am": [9, 168, 176, 203], "among": [12, 14, 173, 175, 186, 188, 247], "amount": [197, 230, 242], "amp": [0, 2, 14, 156, 163, 174, 175, 188, 199, 216, 217], "amp_search_algo": [2, 174, 216, 227, 235], "ampsearchalgo": [2, 174, 216, 227, 235], "an": [9, 14, 18, 19, 147, 148, 149, 155, 157, 159, 160, 161, 162, 164, 169, 170, 175, 177, 180, 183, 186, 188, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 224, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248], "analys": [203, 230, 242], "analysi": [2, 9, 18, 168, 174, 176, 199, 205, 208, 209, 211, 227, 232, 233, 234, 235], "analyt": 210, "analyz": [9, 18, 150, 151, 164, 168, 176, 184, 188, 192, 199, 205, 209, 210, 211, 219, 237], "analyze_per_layer_sensit": [7, 199, 218], "anchor": [137, 138], "andrea": 208, "andrei": 208, "andrew": 208, "ani": [2, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 161, 162, 163, 168, 170, 173, 174, 175, 176, 177, 184, 186, 187, 188, 189, 195, 199, 203, 204, 205, 209, 211, 215, 216, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 247], "anneal": [12, 173, 186], "anoth": [167, 170, 177, 180, 211, 212, 213, 216, 220, 221, 222], "anyth": [2, 174, 216, 227, 230, 235, 242], "api": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 157, 158, 159, 160, 163, 165, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 195, 196, 199, 200, 210, 213, 223, 226, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243], "app": 179, "appear": [18, 161, 162, 205, 209, 211], "appli": [1, 2, 8, 9, 14, 19, 89, 150, 151, 152, 153, 154, 159, 162, 163, 164, 171, 172, 175, 178, 180, 181, 184, 186, 187, 188, 190, 191, 192, 193, 195, 197, 198, 199, 203, 204, 208, 210, 214, 216, 221, 222, 224, 226, 227, 232, 233, 234, 235, 237, 238, 244, 245, 247, 248], "applic": [2, 174, 216, 227, 228, 235, 236, 239, 243, 248], "apply_adaround": [0, 5, 12, 173, 186, 199, 228, 236], "apply_adascal": [13, 187], "apply_omniqu": [165, 193], "apply_seq_ms": [0, 5, 160, 171, 178, 197, 199], "apply_spinqu": [172, 198], "approach": [163, 195, 197, 219, 221, 244, 245], "appropri": [12, 164, 170, 173, 174, 177, 184, 186, 195, 196, 207, 220, 221, 224, 227, 230, 235, 238, 242, 244, 245], "approx": 245, "approxim": [9, 197, 203, 206, 245, 248], "ar": [2, 7, 8, 9, 10, 12, 14, 18, 19, 89, 150, 151, 155, 158, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "arang": [153, 154, 161, 169], "arbitrari": 204, "architectur": [180, 198, 199], "area": [203, 214], "aren": 183, "arg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 159, 163, 164, 166, 170, 183, 199, 204, 215, 216, 219, 220, 221], "argmax": [186, 188, 197, 216, 218, 220, 221, 228], "argument": [2, 10, 12, 13, 14, 19, 155, 159, 161, 162, 163, 165, 166, 168, 170, 171, 173, 174, 175, 176, 177, 178, 183, 186, 187, 188, 193, 197, 202, 203, 204, 213, 215, 216, 220, 221, 230, 242, 243], "around": [182, 199, 203], "arrai": [159, 185, 204, 213, 215], "arrang": 210, "art": [179, 182, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242], "artifact": [10, 163, 167, 199, 217, 219, 220, 243, 247], "arxiv": [172, 187, 193, 198, 199], "asic": 180, "ask": [19, 227, 230, 235, 238, 242], "assert": [160, 161, 169, 220, 235], "assess": 207, "assign": [19, 150, 151, 155, 164, 215, 217, 244], "associ": [2, 14, 19, 149, 162, 163, 164, 169, 170, 174, 175, 188, 216, 230, 242, 243], "assum": [8, 14, 159, 175, 188, 204, 207, 215], "astyp": [203, 230], "asym": [171, 178, 197], "asymmetr": [150, 151, 169, 222, 245], "attention_mask": [193, 195, 196, 198], "attribut": [19, 160, 161, 163, 164, 169, 174, 198, 199, 203, 216], "attributeerror": 199, "augment": 210, "auto": [18, 205, 209, 211, 213, 227, 232, 233, 234, 235], "auto_param": [205, 209, 211, 232, 233, 234], "auto_qu": [188, 237], "autoconfig": [193, 195, 196, 198], "autograd": [147, 148, 149], "autom": [161, 180, 223, 235, 236, 240, 241, 242], "automat": [14, 18, 163, 175, 180, 199, 203, 204, 205, 206, 208, 209, 211, 214, 215, 218, 237], "automodelforcausallm": [195, 196], "automodeparam": [18, 205, 209, 211, 232, 233, 234], "autoqu": [0, 5, 14, 156, 175, 180, 188, 199], "autoquantwithautomixedprecis": [14, 175, 188], "autotoken": [193, 195, 196, 198], "avail": [161, 166, 171, 173, 174, 175, 176, 177, 178, 183, 184, 197, 199, 203, 218, 222, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 246], "avgpool1d": 27, "avgpool2d": [28, 219], "avgpool3d": 29, "avoid": [9, 168, 176, 177, 180, 199, 203, 206, 223, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242], "awai": 221, "awar": [19, 180, 182, 189, 192, 208, 224, 245, 248], "axi": [169, 186, 188, 197, 199, 203, 213, 216, 218, 220, 228, 230, 242, 245], "b": [19, 150, 151, 152, 153, 154, 169, 204], "b1415bd": 199, "b2350b2": 199, "b47a97": 199, "b5521f3": 199, "b73bb71": 199, "b_": [150, 151, 152, 153, 154], "b_0": [150, 151, 152, 153, 154], "b_1": [150, 151, 152, 153, 154, 204], "b_2": 204, "b_d": [150, 151, 152, 153, 154], "b_n": 204, "back": [2, 147, 148, 163, 168, 174, 176, 203, 216, 222, 245], "backend": [10, 170, 177, 220, 221, 243, 247], "backpropag": [147, 148, 149], "backslash": 199, "backward": [0, 156, 189, 190, 195, 196, 199, 220, 221], "balanc": [180, 191, 208, 214, 218, 244, 248], "bandwidth": 206, "bar": 199, "base": [2, 7, 9, 19, 149, 150, 151, 153, 154, 155, 160, 164, 166, 167, 168, 169, 174, 176, 184, 187, 193, 194, 196, 197, 199, 203, 204, 205, 206, 207, 210, 216, 217, 220, 227, 235, 236, 237, 238, 243, 244, 247], "baselin": [2, 174, 207, 208, 216, 221, 237], "basi": 199, "basic": [199, 216, 221, 227, 235, 238, 243, 244], "batch": [2, 9, 12, 14, 15, 155, 163, 171, 173, 174, 175, 178, 180, 186, 187, 188, 191, 193, 195, 196, 197, 198, 199, 203, 216, 220, 221, 230, 232, 233, 234, 238], "batch_cntr": [188, 227, 229, 230, 231, 236, 238, 239, 240, 241, 242], "batch_data": 216, "batch_id": [193, 195, 196], "batch_idx": 197, "batch_norm": [3, 16, 190], "batch_norm_fold": [0, 5, 156, 189, 190, 216, 221, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241], "batch_siz": [12, 173, 186, 187, 188, 189, 190, 193, 195, 196, 197, 198, 203, 216, 218, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "batchnorm": [14, 15, 175, 188, 189, 190, 191, 199, 212, 227, 228, 229, 231, 235, 236, 239, 240, 241], "batchnorm1d": [16, 32, 190], "batchnorm2d": [16, 33, 162, 190, 191], "batchnorm3d": 34, "batchnrom": 238, "bb93c76": 199, "bc": [229, 239], "bceloss": 30, "bcewithlogitsloss": 31, "becaus": [161, 204, 216, 238, 247], "becom": [199, 217], "becuas": 202, "been": [0, 19, 147, 156, 157, 162, 180, 185, 195, 199, 201, 212, 213, 237, 243, 244, 245], "befor": [1, 2, 9, 10, 13, 19, 157, 160, 164, 169, 186, 187, 188, 189, 190, 191, 194, 195, 196, 198, 199, 201, 202, 203, 208, 216, 218, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 245, 247], "begin": [150, 151, 152, 153, 154, 161, 162, 181], "behav": [19, 89, 164, 199, 224], "behavior": [160, 161, 164, 190, 199, 222, 227, 235, 236, 239, 240, 241, 245, 247], "behind": 247, "being": [18, 160, 161, 162, 168, 176, 189, 199, 203, 204, 205, 209, 211, 213, 223], "below": [19, 150, 151, 153, 154, 160, 163, 164, 167, 169, 183, 184, 185, 195, 196, 198, 202, 204, 205, 207, 208, 210, 213, 218, 219, 220, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "benefici": [227, 230, 235, 242], "benefit": [197, 213, 215, 221, 244, 245], "bert": 224, "bespok": 195, "best": [14, 163, 175, 182, 184, 188, 192, 206, 208, 214, 219, 225, 237, 245], "beta": [0, 12, 156, 173, 186, 187, 199], "better": [188, 208, 216, 220, 221, 228, 232, 233, 234, 236, 238, 239, 240, 241, 247], "between": [2, 9, 12, 18, 158, 160, 163, 164, 168, 173, 174, 176, 180, 186, 191, 192, 197, 200, 202, 203, 204, 205, 209, 211, 216, 218, 220, 222, 227, 235, 244, 245, 248], "bfba557": 199, "bfloat16": [155, 199], "bia": [19, 155, 160, 161, 162, 164, 166, 167, 170, 190, 191, 198, 199, 205, 220, 221, 222, 229, 239], "bias": [190, 227, 229, 235, 239], "bilinear": 35, "billion": [247, 248], "bin": [183, 243], "binari": [2, 174, 216, 227, 235, 243], "binary_fil": 243, "binary_file_nam": 243, "bit": [1, 2, 9, 12, 155, 159, 166, 168, 169, 173, 174, 176, 180, 182, 184, 185, 186, 195, 199, 203, 204, 213, 214, 215, 216, 217, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 245, 247, 248], "bitop": [2, 174, 216], "bitwidth": [1, 2, 8, 9, 12, 14, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 163, 164, 168, 169, 170, 173, 174, 175, 176, 177, 180, 184, 186, 188, 191, 195, 199, 203, 204, 213, 215, 216, 220, 221, 227, 235, 236, 245, 247], "biwidth": 236, "bkd": [13, 187, 192, 193], "blankevoort": 208, "block": [8, 14, 150, 151, 152, 153, 154, 159, 160, 165, 175, 187, 188, 193, 195, 196, 199, 204, 213, 214, 215, 245], "block_group": [159, 215], "block_siz": [8, 150, 151, 152, 153, 154, 159, 169, 204, 213, 215], "blockwis": [8, 159, 169, 187, 192, 193, 199, 213, 245], "bn": [3, 15, 16, 180, 189, 190, 192, 199, 203, 227, 228, 229, 231, 235, 236, 239, 240, 241], "bn1": 162, "bn_conv": 199, "bn_reestim": [0, 156, 189, 238], "bnf": [189, 190, 191], "bokeh": 216, "bool": [2, 10, 14, 18, 150, 151, 153, 154, 155, 159, 161, 162, 163, 166, 170, 174, 175, 177, 188, 204, 205, 209, 211, 213, 215, 216, 219, 220, 221, 230, 232, 233, 234, 236, 238, 239, 240, 241, 242], "boolean": [19, 163, 198, 219], "both": [153, 154, 160, 161, 164, 168, 176, 182, 184, 185, 190, 194, 203, 208, 209, 210, 212, 214, 216, 220, 221, 222, 224, 227, 234, 235, 240, 241, 243, 244, 245, 248], "bottleneck": 224, "box": [199, 225], "bq": [159, 199, 204, 215], "branch": [161, 222, 226], "break": [188, 193, 195, 196, 197, 199, 216, 220, 227, 229, 230, 231, 236, 238, 239, 240, 241, 242], "bridg": 217, "brief": 214, "british": 208, "broad": 244, "broken": [194, 213], "browser": 226, "bruteforc": [227, 235], "buffer": 155, "bug": [199, 213], "bugfix": 199, "build": [10, 160, 199, 220], "build_sess": 199, "buildx": 183, "built": [2, 19, 163, 164, 174, 183, 184, 216], "bw": [2, 8, 14, 159, 163, 170, 174, 175, 177, 188, 199, 213, 215, 216, 220, 221, 223], "bw_output": 223, "c": [169, 183, 206], "c0bdb46": 199, "c96894f": 199, "c_": 169, "cach": [2, 14, 165, 174, 175, 188, 193, 199, 216, 227, 235], "cache_id": [14, 175, 188], "calcul": [2, 14, 164, 174, 175, 177, 188, 194, 195, 199, 203, 207, 216, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 245], "calculate_and_fuse_encodings_into_weight": 195, "calibr": [9, 10, 150, 151, 157, 164, 167, 168, 170, 176, 177, 180, 185, 195, 199, 200, 201, 214, 215, 216, 217, 218, 221, 224, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 245, 247], "calibration_batch": 216, "calibration_callback": [195, 196], "calibration_data": 185, "calibration_data_load": [164, 218, 220], "calibration_dataset": [218, 220], "calibration_dataset_s": [188, 237], "calibration_wrapp": [193, 198], "call": [2, 9, 13, 14, 19, 89, 147, 148, 149, 155, 161, 163, 164, 168, 169, 174, 175, 176, 187, 188, 199, 203, 204, 205, 209, 211, 215, 216, 218, 219, 220, 228, 229, 230, 231, 232, 233, 234, 236, 239, 240, 241, 242, 243, 247, 248], "call_funct": 161, "callabl": [2, 9, 10, 12, 13, 14, 15, 18, 19, 159, 161, 162, 163, 165, 168, 170, 171, 173, 174, 175, 176, 178, 186, 187, 188, 189, 193, 197, 203, 204, 205, 209, 211, 215, 216, 220, 221], "callal": [2, 174, 216], "callback": [2, 9, 18, 163, 168, 174, 176, 177, 188, 195, 205, 209, 211, 215, 216, 221, 230, 232, 233, 234, 242, 245], "callbackfunc": [2, 163, 168, 174, 176, 203, 216, 227, 230, 235, 242], "callbak": [227, 235], "can": [2, 6, 8, 9, 10, 11, 12, 16, 17, 147, 149, 150, 151, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 173, 174, 176, 177, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 193, 194, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "candid": [2, 11, 14, 18, 163, 171, 174, 175, 178, 187, 188, 193, 197, 199, 205, 207, 208, 209, 211, 216, 227, 235, 244], "cannot": [150, 151, 161, 162, 227, 228, 229, 231, 235, 236, 239, 240, 241], "capabl": [164, 182, 184, 185, 210], "captur": [6, 158, 161, 170, 200, 202, 220, 221, 228, 229, 230, 231, 236, 239, 240, 241, 242], "card": [182, 184, 185], "care": 208, "carefulli": 180, "carri": [147, 148, 149], "case": [153, 154, 159, 160, 161, 162, 164, 185, 194, 198, 199, 203, 204, 207, 215, 219, 222, 223, 225, 228, 230, 242, 248], "cast": 155, "cat": 169, "catch": 199, "caus": [199, 208, 216, 218, 222, 224, 232, 233, 234], "cbe67a": 199, "cd": [183, 226], "cdot": [150, 151, 152, 153, 154], "ce68e75": 199, "ceil": [12, 173, 186, 188, 216, 218], "cell": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "celu": 36, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "centercrop": [186, 188, 197, 216, 218, 220, 228, 235, 237], "certain": [161, 168, 176, 199, 203, 208, 213, 216, 219, 220, 222, 223, 235, 236, 239, 240, 241, 242], "chain": [193, 195, 196, 198], "challeng": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "chang": [2, 13, 149, 157, 160, 161, 167, 169, 170, 174, 186, 187, 189, 190, 191, 193, 197, 198, 199, 201, 203, 208, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 247], "channel": [8, 9, 18, 159, 164, 168, 176, 180, 183, 189, 190, 191, 192, 199, 203, 204, 206, 207, 209, 211, 212, 213, 214, 215, 222, 224, 226, 227, 228, 229, 230, 231, 233, 235, 236, 238, 239, 240, 241, 242, 245], "channel_index": [9, 168, 176, 203], "channel_index_0": [230, 242], "channel_index_1": [230, 242], "channel_index_n": [230, 242], "channel_prun": [18, 205, 209, 211, 232, 234], "channel_pruning_auto_mod": 205, "channel_pruning_manual_mod": 205, "channelpruningparamet": [18, 205, 209, 211, 232, 234], "channelshuffl": 38, "characterist": [227, 230, 235, 238, 242], "chart": 224, "check": [14, 161, 162, 169, 175, 181, 185, 188, 199, 200, 202, 223, 227, 230, 235, 238, 242], "check_model_sensitivity_to_quant": [9, 168, 176, 203], "checker": [162, 199], "checkpoint": [171, 177, 178, 197], "checkpoints_config": [171, 178, 197], "chipset": 247, "choic": [184, 208, 213, 230, 242, 245], "choos": [177, 180, 186, 187, 193, 197, 199, 202, 205, 206, 208, 216, 222, 227, 232, 233, 234, 235], "choose_mixed_precis": [2, 163, 174, 216, 227, 235], "chose": 167, "chosen": [226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242], "chunk": 245, "ci": 183, "cin": 169, "circularpad1d": 39, "circularpad2d": 40, "circularpad3d": 41, "cl": [19, 199, 229, 239], "clamp": [150, 151, 153, 154, 155, 245], "class": [2, 6, 9, 10, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 155, 158, 160, 161, 162, 163, 164, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 186, 188, 193, 195, 196, 197, 198, 202, 203, 205, 209, 211, 216, 219, 220, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "classif": [180, 186, 208, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "classmethod": [10, 19, 155, 170, 177, 220], "cle": [4, 17, 188, 191, 199, 203, 224], "clean": [199, 216], "clean_start": [2, 174, 216, 227, 235], "clearli": 199, "clip": [187, 190, 193, 222, 245], "clone": [149, 226], "close": [206, 245], "closer": [228, 236], "cloud": [180, 247], "cmake": 183, "cmake_arg": 183, "cnn": 180, "cnt": 199, "coars": 191, "code": [184, 186, 188, 191, 198, 199, 202, 204, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242], "codebas": 184, "collate_fn": [193, 195, 196, 198, 230], "collect": [1, 9, 11, 14, 157, 168, 175, 176, 179, 180, 186, 188, 197, 200, 201, 203, 207, 220, 227, 228, 229, 231, 235, 236, 239, 240, 241], "color": [199, 212], "column": 207, "com": [184, 226], "combin": [2, 161, 174, 180, 188, 199, 204, 206, 208, 216, 227, 235], "come": [208, 217, 221, 244], "command": [184, 210, 226, 243], "common": [160, 169, 199, 215, 224, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "commonli": [12, 169, 173, 180, 186, 220], "comp": [18, 205, 209, 211], "comp_stat": [232, 233], "compar": [2, 161, 174, 180, 187, 193, 203, 204, 216, 217, 221, 227, 230, 235, 238, 242, 244, 247], "comparison": [200, 202, 229, 231, 236, 239, 240, 241], "compat": [0, 156, 159, 182, 184, 185, 199, 204, 215, 220, 223, 228], "compil": [10, 182, 199, 220, 248], "complet": [2, 174, 216, 224, 227, 232, 233, 234, 235, 243, 244], "complex": [177, 203, 219], "compli": [235, 236, 240, 241, 242], "complic": 199, "compon": [160, 169, 193, 199, 218, 220], "compos": [163, 186, 188, 197, 216, 218, 219, 220, 228, 235, 237], "comprehens": 244, "compress": [0, 156, 180, 181, 199, 212, 213, 226], "compress_model": [18, 205, 209, 210, 211, 232, 233, 234], "compress_schem": [18, 205, 209, 211, 232, 233, 234], "compressed_bw": 213, "compressed_model": [205, 209, 211, 232, 233], "compressionschem": [18, 205, 209, 211, 232, 233, 234], "compressionstat": [18, 205, 209, 211], "compressor": [18, 205, 209, 211], "compris": [193, 207, 227, 235], "compromis": [232, 233, 234], "compuat": [14, 175, 188], "comput": [2, 9, 10, 13, 14, 18, 19, 89, 155, 167, 168, 170, 171, 174, 175, 176, 177, 178, 180, 181, 182, 184, 185, 186, 187, 188, 190, 191, 193, 195, 196, 197, 198, 199, 202, 203, 205, 208, 209, 210, 211, 212, 213, 215, 216, 228, 229, 230, 231, 232, 233, 234, 236, 239, 240, 241, 242, 245, 247, 248], "computation": 210, "compute_encod": [10, 13, 19, 148, 149, 150, 151, 155, 157, 160, 163, 164, 169, 170, 171, 177, 178, 185, 186, 187, 193, 195, 196, 197, 198, 199, 201, 216, 218, 219, 220, 221, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241], "computeencod": 184, "concat": 199, "concatenated_exampl": [193, 195, 196, 198], "concept": [160, 204], "concis": 213, "conclus": 244, "concret": 161, "concrete_arg": [14, 161, 175, 188], "concurr": 167, "conda_env_nam": 183, "conda_install_dir": 183, "condit": [161, 162, 189], "confer": 208, "config": [2, 9, 163, 167, 168, 170, 171, 174, 176, 177, 178, 189, 193, 195, 196, 197, 198, 199, 203, 216, 219, 220, 221, 222, 227, 235], "config_fil": [9, 10, 14, 168, 170, 175, 176, 177, 188, 189, 203, 218, 219, 220, 221, 230, 242], "config_util": [0, 156, 204, 215], "configur": [9, 10, 12, 14, 18, 155, 163, 168, 169, 170, 173, 175, 176, 177, 186, 188, 195, 196, 197, 199, 203, 204, 205, 209, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241, 244], "conflict": [163, 219], "conform": [220, 248], "conjunct": 245, "connect": [180, 205, 208, 211], "connectedgraph": [162, 199], "consecut": [191, 216, 229, 239], "consid": [13, 187, 209, 211, 216, 224, 232, 234, 248], "consist": [158, 159, 160, 198, 199, 202, 204, 207, 215, 227, 235, 245], "consol": 200, "constant": [10, 161, 199, 207, 231, 240, 241], "constantpad2d": [42, 43], "constantpad3d": 44, "constrain": [180, 204], "constraint": [183, 213], "constrast": 221, "construct": [199, 202], "constructor": [6, 158, 161, 202, 223], "consum": [188, 208, 213, 217, 237], "consumpt": 244, "contain": [1, 2, 9, 10, 18, 19, 147, 155, 157, 161, 162, 163, 164, 168, 174, 176, 180, 185, 186, 191, 201, 203, 205, 209, 211, 213, 216, 220, 222, 228, 229, 230, 231, 232, 233, 234, 236, 237, 239, 240, 241, 242, 243, 244, 245, 247], "content": 180, "context": [10, 19, 164, 199, 243, 248], "contigu": [187, 193, 195, 196, 199], "continu": [2, 162, 174, 177, 195, 199, 215, 216, 224, 227, 235, 244], "contrast": 160, "contribut": [184, 203, 224], "control": [14, 161, 164, 169, 175, 188, 245], "conv": [3, 8, 12, 16, 159, 161, 173, 186, 190, 191, 199, 204, 209, 211, 212, 215, 219, 222, 229, 239], "conv1": [8, 161, 162, 170, 191, 205, 209, 211, 215, 220, 221, 223, 232, 233, 234], "conv1d": [16, 45, 190, 199], "conv2": [159, 161, 191, 204, 205, 209, 211, 215, 223], "conv2d": [16, 19, 46, 159, 160, 161, 162, 164, 170, 190, 191, 199, 204, 205, 208, 212, 215, 220, 221], "conv2dnormactiv": [190, 191], "conv3d": [47, 199], "conv3dtranspos": 199, "conv_1": 204, "conv_weight": 190, "conv_weight_arrai": 190, "conv_weight_nam": 190, "conveni": 247, "convent": [199, 222], "converg": [180, 199, 221, 244], "convers": [199, 220, 223, 244], "convert": [2, 147, 160, 161, 163, 170, 174, 180, 186, 188, 190, 191, 218, 220, 221, 235, 243, 244, 248], "convolut": [159, 180, 189, 190, 191, 192, 204, 205, 206, 208, 209, 211, 212, 215, 216, 224, 227, 228, 229, 231, 232, 234, 235, 236, 239, 240, 241], "convtranspos": 199, "convtranspose1d": [48, 199], "convtranspose2d": [16, 49, 190], "convtranspose3d": 50, "copi": [10, 149, 155, 177, 193, 195, 196, 198, 218, 226, 228, 235, 236, 239, 240, 241, 245], "copy_": 160, "correct": [14, 175, 180, 188, 216, 219, 220, 221, 229, 230, 237, 239, 242, 248], "correct_predict": [186, 197, 216, 218, 220, 228], "correctli": 199, "correl": [216, 227, 235], "correspond": [2, 3, 6, 9, 12, 16, 155, 158, 162, 163, 164, 168, 170, 171, 173, 174, 176, 177, 178, 184, 186, 190, 192, 197, 202, 203, 204, 205, 212, 216, 220, 221, 237, 244, 245], "cosine_similar": 166, "cosineembeddingloss": 51, "cosinesimilar": 52, "cost": [18, 205, 207, 208, 209, 211, 214, 217, 221, 232, 234, 245], "cost_metr": [18, 205, 209, 211, 232, 233, 234], "costmetr": [18, 205, 209, 211, 232, 233, 234], "could": [160, 177, 203, 212, 213, 220, 223, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244], "counter": 238, "counterpart": [164, 244], "cours": [232, 233, 234, 240, 241], "cout": 169, "cover": [227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 245], "cp": [205, 206, 207, 212], "cp310": 184, "cp_comp_stat": 234, "cpu": [17, 161, 170, 177, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 197, 198, 199, 202, 216, 219, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "cpuexecutionprovid": [10, 186, 197, 199, 218, 220, 227, 228, 229, 231], "creat": [2, 9, 10, 13, 14, 18, 19, 155, 157, 161, 163, 164, 167, 169, 174, 175, 177, 180, 182, 185, 186, 187, 188, 189, 193, 195, 197, 198, 199, 201, 202, 205, 208, 209, 211, 215, 217, 219, 226, 230, 232, 233, 234, 237, 242, 243, 245, 247, 248], "create_quantsim_and_encod": [9, 203], "creation": 199, "cross": [4, 14, 17, 175, 188, 203, 224], "cross_layer_equ": [0, 5, 156, 191, 229, 239], "crossentropyloss": [53, 189, 195, 196, 221], "crosslayerequ": 199, "ctcloss": 37, "ctivations_pdf": [9, 168, 176, 203], "cu121": 184, "cubla": 199, "cuda": [6, 182, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 202, 203, 205, 216, 218, 219, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "cudaexecutionprovid": [186, 197, 218, 220, 227, 228, 229, 231], "cudnn": [182, 183, 184, 185], "cudnn_conv_algo_search": [227, 228, 229, 231], "cumul": 237, "curat": 181, "current": [13, 18, 19, 149, 155, 162, 163, 172, 174, 187, 193, 198, 199, 205, 209, 211, 216, 235, 242, 249], "current_batch": 198, "curv": [2, 174, 207, 216, 227, 235], "custom": [10, 19, 161, 164, 180, 193, 198, 199, 213, 220, 221, 224, 227, 237, 245], "custom_function_not_to_be_trac": 161, "customdataload": [188, 216], "customdataset": 187, "custommodel": 161, "custommodul": 161, "customsilu": 199, "cycl": 247, "d": [150, 151, 152, 153, 154, 183, 193, 198, 213], "d33e98c": 199, "dangl": 220, "dark": [227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242], "data": [2, 9, 10, 12, 13, 14, 15, 18, 147, 148, 149, 157, 160, 161, 163, 165, 168, 170, 171, 173, 174, 175, 176, 177, 178, 180, 186, 187, 188, 189, 192, 193, 195, 196, 197, 199, 201, 203, 205, 209, 211, 214, 216, 218, 220, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "data_load": [2, 12, 13, 14, 18, 157, 163, 171, 173, 174, 175, 178, 186, 187, 188, 189, 193, 197, 201, 205, 216, 218, 220, 221, 227, 229, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242], "data_set": [13, 187], "data_typ": [14, 160, 170, 175, 177, 188, 199, 220, 221], "dataload": [2, 12, 13, 14, 15, 163, 165, 171, 173, 174, 175, 178, 186, 187, 188, 189, 193, 195, 196, 197, 198, 203, 216, 218, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "dataloader_wrapper_len": 193, "dataloadermnist": 205, "dataset": [2, 9, 10, 13, 14, 15, 168, 170, 174, 175, 176, 185, 186, 187, 188, 189, 195, 196, 197, 203, 216, 218, 220, 221, 226, 245, 247], "dataset_dir": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "dataset_path": 220, "dataset_root": [186, 197], "datatyp": [14, 175, 188, 213], "db99447": 199, "dc34fa4": 199, "de": [169, 245], "debug": [160, 199, 200, 202, 213, 225], "decai": 208, "decid": [171, 178, 187, 193, 197, 210, 216, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242], "decim": [205, 209, 211, 232, 233, 234], "decis": 248, "declar": 19, "decompos": [208, 209, 211], "decomposit": [208, 209, 211, 233, 234], "decompress": [8, 159, 215], "decompressed_bw": [8, 159, 215], "decor": 19, "decreas": [11, 180, 197], "dedic": 180, "deem": [216, 247], "deep": [180, 181, 182, 190, 208], "deepcopi": [218, 228], "deeper": 226, "deepseek": 193, "deepspe": 199, "def": [2, 10, 18, 19, 160, 161, 162, 163, 170, 174, 177, 185, 186, 187, 188, 193, 195, 196, 197, 198, 203, 205, 209, 211, 216, 218, 220, 221, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "default": [2, 9, 10, 12, 14, 18, 19, 149, 150, 151, 153, 154, 155, 157, 161, 163, 164, 166, 167, 168, 170, 171, 173, 174, 175, 176, 177, 178, 180, 185, 186, 188, 190, 191, 197, 199, 201, 203, 205, 207, 208, 209, 210, 211, 213, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "default_activation_bw": [9, 203, 216, 230], "default_beta_rang": [12, 173, 186], "default_bitwidth": 216, "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "default_config_fil": [12, 173, 186], "default_config_per_channel": 222, "default_data_col": [193, 195, 196, 198], "default_data_typ": [170, 177, 220, 221], "default_forward_fn": [171, 178, 187, 197], "default_new": 167, "default_num_iter": [12, 173, 186, 236, 237], "default_output_bw": [168, 170, 176, 177, 185, 187, 193, 195, 196, 197, 198, 203, 216, 219, 220, 221, 227, 229, 235, 236, 238, 239, 240, 241, 242], "default_param_bw": [9, 12, 168, 170, 173, 176, 177, 185, 186, 187, 193, 195, 196, 197, 198, 203, 216, 219, 220, 221, 227, 229, 230, 235, 236, 238, 239, 240, 241, 242], "default_quant_schem": [12, 173, 186, 236], "default_reg_param": [12, 173, 186], "default_warm_start": [12, 173, 186], "defer": 182, "defin": [2, 19, 161, 162, 164, 169, 174, 187, 193, 198, 199, 203, 205, 209, 211, 213, 216, 220, 222, 223, 230, 232, 233, 234, 236, 239, 240, 241, 242, 244, 245], "definit": [2, 18, 19, 158, 160, 161, 163, 167, 174, 177, 202, 205, 209, 211, 216, 220, 223, 227, 230, 235, 236, 239, 240, 241, 242, 245], "degrad": [218, 228, 229, 231, 236, 239, 240, 241, 244, 247, 248], "degre": [209, 211], "deleg": 169, "delet": [2, 174, 199, 203, 216, 227, 235], "deliber": [227, 230, 235, 242], "delta": [9, 168, 176, 184, 199, 203, 245], "demand": 180, "demonstr": [230, 242], "denable_cuda": 183, "denable_onnx": 183, "denable_test": 183, "denable_torch": 183, "denot": [198, 212, 213], "dens": 199, "depend": [147, 148, 149, 161, 199, 206, 222, 225, 226, 244], "deploi": [179, 245, 246, 247], "deploy": [179, 180, 181, 182, 220, 243, 244, 246, 248], "deprec": [170, 199, 220, 221], "depth": [189, 199, 224], "depthwis": 199, "depthwiseconv": 199, "dequant": [147, 148, 149, 151, 154, 155, 169, 180, 184, 199, 247, 248], "dequantizedtensor": [148, 149, 151, 169, 170, 185], "dequantizelinear": [10, 166, 199, 244], "deriv": [150, 151, 153, 154, 169, 186, 235, 245], "descend": 155, "describ": [155, 160, 183, 185, 198, 204, 208, 210, 213, 216, 222, 224, 245, 246, 247, 248], "descript": [164, 213, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "design": [162, 180, 186, 226, 227, 230, 235, 242, 244], "desir": [9, 149, 168, 169, 170, 176, 177, 203, 206, 208, 216, 217, 220, 221, 224, 227, 232, 233, 234, 235, 243, 244, 248], "detach": [149, 166], "detail": [14, 161, 175, 181, 188, 205, 207, 210, 213, 219, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 248], "detect": [208, 244], "determin": [9, 155, 159, 164, 168, 176, 177, 180, 186, 187, 188, 192, 193, 195, 196, 197, 198, 203, 204, 213, 215, 217, 218, 220, 227, 235, 237, 244], "determinist": 161, "dev": 183, "develop": [157, 179, 180, 182, 184, 201, 247], "devic": [6, 12, 14, 149, 161, 170, 173, 174, 175, 177, 179, 180, 181, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 200, 202, 216, 219, 220, 221, 225, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248], "diable_missing_quant": 220, "diagnost": 224, "diagram": [195, 196], "dict": [1, 2, 7, 9, 10, 11, 13, 14, 18, 155, 158, 161, 163, 164, 168, 174, 175, 176, 177, 186, 187, 188, 197, 202, 203, 205, 209, 211, 213, 216, 218, 219, 220, 223, 228], "dictat": 247, "dictionari": [7, 9, 18, 155, 168, 170, 176, 177, 198, 203, 205, 207, 208, 209, 211, 218, 220, 221, 222], "didn": 238, "diff": 199, "differ": [18, 160, 161, 167, 180, 192, 194, 197, 204, 205, 207, 208, 209, 211, 213, 215, 216, 217, 218, 219, 220, 224, 227, 230, 232, 233, 234, 235, 240, 241, 243, 244, 245, 248], "dim": [159, 169, 204, 215, 235], "dimens": [8, 159, 164, 204, 209, 211, 213, 215, 224, 245], "dir": [10, 220], "dir_path": [6, 158, 202], "direct": [1, 181, 186, 190, 199, 202, 204, 213, 215, 216, 218, 244, 245, 247, 248], "directli": [9, 182, 185, 195, 199, 203, 219, 220, 238, 245, 247], "directori": [2, 6, 9, 10, 14, 18, 158, 168, 174, 175, 176, 183, 186, 188, 190, 191, 197, 202, 203, 205, 209, 211, 216, 220, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "disabl": [2, 9, 160, 164, 167, 168, 170, 171, 176, 177, 178, 190, 197, 198, 203, 207, 208, 216, 220, 221, 222, 232, 234, 245], "disable_missing_quant": 220, "discard": 203, "discuss": [232, 233, 234], "disk": [6, 158, 202], "dispatch": 199, "displai": [201, 210, 226], "display_comp_ratio_plot": 210, "display_eval_scor": 210, "dist": 183, "distict": 199, "distil": [187, 192, 193], "distinct": 162, "distort": 245, "distribut": [18, 180, 199, 205, 209, 211, 224, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 245], "divbackward0": 169, "dive": 226, "diverg": 180, "divid": [204, 206], "divis": [169, 180, 204, 215], "dlc": 243, "dlc_path": 243, "dlcompress": 199, "dlequal": 199, "dlf": 180, "do": [10, 12, 161, 162, 167, 170, 173, 184, 186, 190, 193, 195, 198, 203, 208, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244, 245, 247, 248], "do_constant_fold": [190, 227, 229, 231], "do_not_trace_m": 161, "doc": [161, 167, 183, 199, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "dockerfil": 183, "docstr": 204, "document": [167, 184, 190, 199, 202, 204, 210, 213, 215, 216, 218, 226, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "doe": [19, 161, 174, 187, 193, 198, 199, 201, 203, 205, 207, 209, 211, 221, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248], "doesn": [19, 220, 227, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 247], "doesnt": [163, 219], "don": [19, 161, 183, 195, 196, 203, 230, 242], "done": [150, 151, 169, 199, 208, 220, 222, 238], "down": [194, 213], "down_proj": 198, "download": [181, 184, 218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "downsampl": [232, 234], "downstream": [199, 212, 213, 220, 244], "dq": 10, "dq_output": 19, "drastic": [207, 248], "draw": 244, "drawback": 245, "drift": 221, "driver": [182, 184, 185], "drop": [2, 14, 164, 174, 175, 188, 199, 203, 206, 208, 214, 216, 218, 221, 224, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 245], "dropout": 54, "dropout1d": 55, "dropout2d": 56, "dropout3d": 57, "dtype": [147, 148, 149, 155, 159, 160, 161, 163, 185, 193, 195, 196, 198, 199, 204, 213, 219], "due": [162, 199, 204, 221, 244], "dummi": [9, 12, 14, 16, 17, 158, 162, 168, 170, 173, 174, 175, 176, 177, 186, 188, 190, 191, 195, 196, 202, 203, 220, 221, 227, 230, 235, 242], "dummy_attention_mask": [195, 196], "dummy_data": 203, "dummy_input": [9, 10, 12, 13, 14, 16, 17, 157, 158, 167, 168, 170, 173, 174, 175, 176, 177, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 201, 202, 203, 216, 218, 219, 220, 221, 223, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242], "dummy_input_dict": 202, "dummy_input_id": [195, 196], "dummy_model": [187, 193], "dummymodel": [13, 187], "dump": [193, 199], "duplic": [149, 161], "dure": [1, 2, 10, 11, 12, 13, 14, 18, 19, 155, 157, 164, 173, 174, 175, 180, 181, 186, 187, 188, 197, 199, 200, 201, 205, 208, 209, 210, 211, 216, 218, 220, 221, 222, 230, 231, 232, 233, 234, 236, 238, 240, 241, 242, 244, 245, 248], "dynam": [155, 161, 180, 191, 199, 221, 245], "dynamic_ax": [186, 190, 197, 216, 218, 220, 227, 228, 229, 230, 231], "dynamo": 166, "e": [9, 10, 165, 170, 177, 191, 192, 193, 203, 213, 216, 218, 220, 221, 227, 228, 230, 235, 238, 242, 244], "e78dbec": 199, "e7d10c7": 199, "each": [1, 2, 9, 10, 11, 12, 19, 157, 162, 164, 165, 168, 169, 173, 174, 176, 180, 186, 187, 190, 191, 193, 197, 198, 200, 201, 203, 204, 205, 206, 207, 208, 213, 216, 220, 222, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245], "earli": [2, 174, 216], "easi": [199, 219, 247], "easier": 160, "easili": [169, 205, 209, 211], "echo": 183, "ecosystem": 179, "ed": 186, "edg": [180, 181, 247], "edit": [199, 213, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "effect": [12, 15, 164, 173, 180, 186, 189, 192, 198, 203, 219, 231, 240, 241, 247, 248], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 238, 243, 244, 245, 246, 247, 248, 249], "efficientnetb4": 199, "effort": [14, 163, 175, 188, 219, 237, 244, 247, 248], "eigen": 183, "eights_pdf": [9, 168, 176, 203], "either": [7, 18, 159, 163, 169, 198, 204, 205, 209, 211, 212, 215, 216, 218, 219, 223, 227, 232, 233, 234, 235, 244], "element": 213, "elementwis": [161, 164, 199], "elev": 244, "elimin": [180, 190, 203, 245], "els": [2, 161, 162, 174, 185, 186, 187, 188, 189, 190, 191, 193, 197, 198, 216, 219, 220, 221, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244], "elu": 58, "embed": [59, 161, 166, 170, 177, 199, 208, 220, 221, 224], "embed_token": 198, "embeddingbag": 60, "embodi": 180, "empir": [237, 244], "emploi": [180, 181], "empti": [8, 215, 222], "emul": 245, "enabl": [2, 8, 9, 14, 18, 155, 160, 163, 167, 168, 171, 174, 175, 176, 178, 180, 182, 188, 189, 190, 192, 197, 198, 199, 202, 204, 205, 209, 210, 211, 215, 216, 217, 218, 219, 222, 232, 234, 235, 241, 243, 245, 247], "enable_convert_op_reduct": [163, 174, 216, 235], "enable_onnx_check": [170, 177, 220, 221], "enable_per_layer_mse_loss": [9, 203, 230, 242], "enbl": 215, "enc": 19, "enc_typ": 213, "encapsul": [2, 163, 168, 174, 176, 203, 216], "encaptur": 167, "encod": [1, 2, 9, 10, 11, 12, 13, 14, 19, 147, 148, 149, 150, 151, 155, 160, 163, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 180, 184, 186, 187, 188, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 215, 216, 219, 221, 228, 229, 231, 236, 237, 239, 240, 241, 243, 244, 247], "encoding_analyz": [150, 151, 155], "encoding_path": [188, 236, 237], "encoding_vers": 213, "encodinganalyz": [150, 151, 155], "encodinganalyzerforpython": 184, "encodingbas": [147, 148, 155], "encodingmismatchinfo": 220, "encodingtyp": 213, "encount": 19, "encourag": [161, 223], "end": [13, 150, 151, 152, 153, 154, 161, 162, 181, 185, 187, 208, 216, 220, 226, 232, 233, 234, 238, 240, 241, 244, 247], "end_beta": [12, 173, 186], "end_idx": 188, "enforc": 155, "engin": [181, 190, 199, 202, 204, 213, 215, 216, 218, 244, 245, 247, 248], "enhanc": [9, 167, 168, 176, 199, 203, 220, 230, 242, 245], "enough": [230, 232, 233, 234, 248], "ensur": [164, 185, 189, 195, 196, 199, 202, 207, 216, 224, 248], "enter": [19, 164, 188], "entir": [9, 168, 169, 176, 203, 205, 208, 209, 211, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "entri": [14, 170, 175, 177, 188, 213, 220, 221, 222], "enum": [2, 10, 12, 18, 158, 170, 173, 174, 177, 186, 202, 205, 209, 211, 213, 216, 220, 227, 235, 238], "enumer": [18, 158, 193, 195, 196, 197, 202, 205, 209, 211, 216, 218, 220], "environ": [180, 184, 226, 227, 235, 242], "ep": [190, 191, 199], "epoch": [189, 195, 196, 205, 208, 209, 211, 221, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241], "epsilon": 199, "equal": [4, 14, 17, 148, 149, 159, 169, 175, 188, 199, 203, 204, 206, 207, 215, 224, 245], "equalize_model": [4, 17, 191, 229, 239], "equat": [150, 151, 152, 153, 154, 204, 245], "equival": [14, 19, 153, 154, 155, 159, 163, 170, 174, 175, 177, 187, 188, 193, 198, 204, 215, 216, 220, 221, 223, 227, 235, 236, 239, 240, 241, 242, 243], "error": [1, 14, 19, 161, 169, 170, 175, 180, 186, 188, 191, 199, 200, 219, 220, 221, 224, 245], "especi": [180, 195, 221, 224, 227, 235, 247], "essenti": [227, 235, 238], "estim": [199, 245, 247], "esults_dir": [9, 168, 176, 203], "etc": [13, 180, 183, 187, 199, 206, 213, 216, 227, 235], "eval": [2, 9, 13, 14, 18, 161, 163, 168, 171, 174, 175, 176, 178, 185, 186, 187, 188, 189, 190, 191, 197, 203, 205, 207, 208, 209, 210, 211, 216, 219, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "eval_callback": [9, 14, 18, 168, 175, 176, 188, 203, 205, 209, 211, 230, 232, 233, 234, 237, 242], "eval_callback_factori": [227, 235], "eval_callback_fn": 203, "eval_callback_for_phase1": [2, 174, 216, 227, 235], "eval_callback_for_phase2": [2, 174, 216, 227, 235], "eval_callback_for_phase_1": 216, "eval_callback_for_phase_2": 216, "eval_data_load": [216, 218, 220], "eval_dataset": [218, 220], "eval_dataset_s": [188, 237], "eval_fn": 218, "eval_iter": [18, 205, 209, 211, 232, 233, 234], "eval_scor": [9, 18, 168, 176, 203, 205, 209, 211], "evalcallbackfactori": [2, 163, 174, 216, 227, 235], "evalu": [2, 9, 14, 18, 163, 168, 174, 175, 176, 185, 186, 187, 188, 193, 197, 198, 205, 207, 208, 209, 210, 211, 215, 216, 221, 226, 237, 244, 245, 248], "evaluate_accuraci": 235, "evaluate_model": [205, 209, 211], "even": [19, 169, 216, 219, 221, 227, 235], "evenli": 204, "eventu": 219, "everi": [6, 9, 12, 158, 168, 169, 173, 176, 186, 187, 193, 200, 202, 203, 207, 208, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 238, 240, 241, 242, 245], "evid": 244, "ex": 167, "exactli": [13, 19, 89, 155, 164, 187, 230, 242, 245], "examin": 161, "exampl": [8, 10, 13, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 157, 159, 162, 163, 164, 166, 167, 169, 170, 171, 178, 180, 183, 185, 186, 187, 188, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 206, 212, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 237, 244, 245, 248], "exce": [157, 201], "exceed": [157, 201], "except": [171, 178, 190, 191, 197, 199, 216, 219, 227, 229, 230, 231, 242], "exchang": 180, "exclud": [8, 14, 161, 162, 168, 171, 175, 176, 178, 188, 197, 199, 203, 213, 215], "excluded_lay": 213, "excluded_nod": [8, 215], "exclus": [155, 159, 204], "execut": [2, 10, 14, 161, 170, 174, 175, 177, 188, 190, 199, 210, 216, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "exercis": [227, 230, 235, 238, 242], "exhibit": 244, "exist": [19, 89, 155, 164, 170, 177, 204, 215, 216, 220, 221, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 247], "exist_ok": [227, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "exit": [2, 14, 19, 164, 174, 175, 188, 216], "expand": 199, "expand_dim": 188, "expans": 208, "expect": [2, 9, 12, 14, 18, 157, 161, 162, 163, 165, 168, 170, 171, 173, 174, 175, 176, 177, 178, 186, 188, 193, 197, 201, 203, 205, 208, 209, 211, 216, 219, 220, 221, 227, 232, 233, 234, 235, 238, 244], "experi": [18, 159, 182, 184, 205, 208, 209, 211, 215, 228, 232, 233, 234, 236, 239, 240, 241, 244, 248], "experiment": [0, 156, 182, 187, 193, 198, 199, 204, 208], "expert": 237, "explain": [199, 204, 205, 208, 227, 230, 235, 238, 242], "explan": [181, 228, 229, 231, 236, 239, 240, 241], "explicit": 199, "explicitli": [14, 175, 188, 212], "explor": [216, 244], "expon": [155, 159, 204], "exponent_bit": [155, 159, 204], "export": [9, 10, 13, 14, 158, 160, 163, 167, 168, 170, 171, 175, 176, 177, 178, 182, 183, 185, 186, 187, 188, 189, 190, 191, 193, 197, 199, 200, 202, 203, 208, 213, 215, 216, 217, 218, 219, 221, 223, 226, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 243, 244], "export_int32_bia": 166, "export_model": [10, 167, 170, 177, 220, 221], "export_param": [227, 229, 231], "export_per_layer_encoding_min_max_rang": [9, 168, 176, 203], "export_per_layer_mse_loss": [9, 168, 176, 203], "export_per_layer_stats_histogram": [9, 168, 176, 203], "export_to_torchscript": [170, 177, 220, 221], "expos": [161, 198, 203], "express": [18, 195, 196, 205, 209, 211], "extend": [160, 199], "extens": [164, 226, 243], "extent": 244, "extern": 199, "extra": [155, 164, 183, 199], "extract": [180, 220, 229, 230, 231, 236, 239, 240, 241, 242], "extrem": [227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 245], "f": [161, 162, 166, 169, 183, 184, 185, 186, 188, 190, 191, 197, 216, 218, 220, 221, 228, 237], "f0": 206, "f333188": 199, "f39c0bf": 199, "f547a49": 199, "f7e700f": 199, "f961ed4": 199, "f9d0d6c": 199, "facebook": [180, 195, 196], "facilit": 181, "fact": 195, "factor": [191, 199, 206, 208, 221, 232, 233, 234, 238, 240, 241], "factori": [2, 163, 174, 216], "fail": [161, 162, 163, 170, 188, 190, 191, 199, 216, 219, 220, 221, 227, 229, 231], "failur": 199, "fair": 180, "fairli": [230, 242], "fake": [151, 154, 155, 163, 174, 199, 216, 218, 220, 227, 228, 229, 231, 235, 236, 237, 238, 239, 240, 241], "fakequ": [170, 177, 220, 221], "fall": [207, 222, 232, 233, 234, 248], "fallback": 243, "fals": [2, 8, 10, 14, 18, 19, 147, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 166, 167, 169, 170, 174, 175, 177, 184, 186, 188, 190, 191, 193, 195, 196, 198, 202, 204, 205, 209, 211, 213, 215, 216, 218, 219, 220, 221, 222, 227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "famili": [180, 199, 219], "familiar": 226, "far": 238, "farther": [228, 236], "fast": 183, "faster": [199, 216, 227, 235, 237, 244], "fastest": 182, "fc": [208, 211], "fc1": 161, "fc2": 161, "fe66376": 199, "fea395f": 199, "featur": [8, 13, 157, 160, 161, 162, 166, 180, 181, 187, 188, 190, 191, 193, 198, 199, 201, 208, 210, 215, 216, 219, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245], "featurealphadropout": 61, "feed": 245, "feel": [219, 238], "feez": [171, 178, 197], "fefd504": 199, "few": [189, 206, 218, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "fewer": [208, 247], "field": [2, 155, 174, 213, 216, 227, 235], "figur": [186, 188, 205, 207, 209, 210, 211, 212, 224, 245], "file": [2, 9, 10, 12, 14, 18, 163, 168, 170, 171, 173, 174, 175, 176, 177, 178, 183, 184, 186, 188, 189, 197, 199, 203, 205, 209, 211, 213, 216, 219, 220, 221, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247], "file_path": [177, 185, 186, 190, 191, 197, 216, 218, 220], "filenam": [9, 10, 12, 170, 173, 177, 186, 187, 189, 193, 203, 220, 221, 227, 228, 229, 231], "filename_prefix": [10, 12, 170, 173, 177, 186, 187, 189, 193, 197, 220, 221, 227, 228, 235, 236, 237, 238, 239, 240, 241], "filename_prefix_encod": [167, 170, 177, 220, 221], "fill": [14, 170, 175, 177, 188, 220, 221], "filter": 180, "final": [18, 162, 171, 178, 195, 196, 197, 198, 203, 204, 205, 206, 207, 209, 211, 216, 224, 227, 230, 235, 238, 242, 247], "find": [2, 162, 163, 171, 174, 177, 178, 182, 187, 193, 197, 203, 207, 217, 219, 221, 226, 227, 228, 232, 233, 234, 235, 236, 237, 239, 240, 241, 245, 248], "fine": [18, 167, 177, 181, 205, 206, 209, 211, 221, 228, 229, 231, 236, 237, 239, 240, 241, 244, 247, 248], "finer": [155, 169, 204], "finess": 248, "finetun": [228, 229, 232, 233, 234, 236, 238, 240, 241], "finetuned_accuraci": [238, 240, 241], "finetuned_accuracy_bn_reestim": 238, "finetuned_model": [232, 233], "finish": [240, 241], "finit": 155, "first": [12, 161, 164, 167, 169, 171, 173, 178, 186, 197, 199, 208, 224, 230, 232, 233, 234, 238, 242, 247], "fit": [18, 205, 207, 209, 211, 238, 247], "five": [229, 239], "fix": [162, 199, 213, 216, 221, 227, 228, 229, 231, 245], "flag": [2, 14, 157, 160, 161, 162, 163, 170, 174, 175, 177, 183, 188, 201, 216, 219, 220, 221, 227, 235], "flatten": [62, 161, 195, 196, 213], "flexibl": [227, 235], "flexround": 187, "flip": [7, 218, 244], "flip_layers_to_higher_precis": [7, 218], "float": [2, 7, 9, 10, 12, 14, 18, 19, 147, 148, 155, 159, 163, 168, 170, 173, 174, 175, 176, 177, 180, 186, 188, 190, 191, 198, 199, 203, 204, 205, 209, 211, 213, 216, 218, 220, 221, 224, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248], "float16": [7, 10, 155, 159, 160, 204, 218, 220, 244], "float32": [10, 185, 203, 220, 230], "float8": 199, "float_fallback": 243, "floatencod": [155, 199], "floatquant": [155, 160], "floatquantizedequant": 160, "flow": [14, 161, 175, 188, 224, 245], "fly": 166, "focu": 238, "fold": [3, 14, 16, 63, 170, 175, 180, 188, 189, 191, 198, 199, 203, 228, 244], "fold_all_batch_norm": [16, 190, 216, 221, 235, 236, 239, 240, 241], "fold_all_batch_norms_to_scal": [189, 238], "fold_all_batch_norms_to_weight": [3, 190, 199, 227, 228, 229, 231], "fold_param_quant": 170, "folder": [203, 230, 242], "follow": [0, 5, 6, 9, 10, 19, 89, 156, 158, 160, 161, 162, 163, 164, 167, 168, 170, 176, 177, 183, 184, 185, 186, 188, 189, 190, 191, 197, 199, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248], "footprint": [180, 181], "forall_": [150, 151, 152, 153, 154], "forc": [191, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242], "forg": 183, "form": [230, 242], "formal": 204, "format": [9, 12, 149, 165, 168, 170, 173, 176, 177, 180, 181, 182, 186, 188, 193, 199, 203, 220, 221, 228, 243, 244, 245, 247], "former": 223, "forward": [2, 9, 10, 12, 13, 14, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 155, 161, 162, 163, 164, 165, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 193, 195, 196, 197, 198, 199, 203, 216, 220, 221, 223, 224, 227, 230, 231, 235, 236, 238, 239, 240, 241, 242, 248], "forward_callback": 230, "forward_fn": [2, 12, 13, 14, 15, 163, 165, 171, 173, 174, 175, 178, 186, 187, 188, 189, 193, 197, 216, 227, 235, 238], "forward_one_batch": [227, 235], "forward_pass": [185, 186, 187, 195, 196, 197, 216], "forward_pass_arg": 220, "forward_pass_call_back": 216, "forward_pass_callback": [2, 9, 10, 168, 170, 174, 176, 177, 203, 216, 220, 221, 227, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "forward_pass_callback_arg": [10, 170, 177, 216, 220, 221, 235, 236, 238, 239, 240, 241], "forward_pass_callback_fn": 203, "found": [160, 244, 245], "four": [188, 216, 245], "fp": [192, 198, 221], "fp16": [163, 199, 218, 219, 244, 247], "fp32": [2, 6, 9, 158, 163, 168, 171, 174, 176, 178, 180, 187, 193, 197, 199, 202, 203, 213, 216, 218, 230, 242, 243, 245, 247, 248], "fp32_layer_output": 202, "fp32_layer_output_util": 202, "fp32_output": [2, 216], "fp_accuraci": 218, "fp_input": 218, "fp_qdq": 160, "fp_quantiz": 160, "fp_session": 218, "frac": [150, 151, 152, 153, 154, 155, 169, 191, 245], "fraction": [216, 248], "fractionalmaxpool2d": 64, "fractionalmaxpool3d": 65, "framework": [180, 181, 184, 185, 187, 190, 193, 198, 202, 220, 222, 226, 243, 245, 248], "free": [219, 229, 238, 239], "freez": [160, 187, 193, 197, 199, 236], "freeze_encod": 160, "friendli": [167, 188, 191, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 247], "from": [2, 7, 8, 10, 12, 13, 14, 15, 19, 148, 149, 150, 151, 155, 156, 159, 161, 162, 163, 164, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248], "from_encod": 155, "from_modul": 19, "from_numpi": 166, "from_pretrain": [193, 195, 196, 198], "from_str": [10, 170, 177, 220], "front": [2, 14, 174, 175, 188, 227, 235], "frozen": [1, 186, 195, 196, 220, 228], "full": [13, 180, 187, 217, 223, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241], "fulli": [0, 156, 166, 199, 208, 211, 213, 220], "func": [2, 163, 168, 174, 176, 203, 216], "func_callback_arg": [2, 163, 168, 174, 176, 203, 216, 227, 235], "function": [0, 2, 5, 9, 10, 12, 13, 14, 15, 18, 19, 147, 148, 149, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 180, 186, 187, 188, 189, 191, 193, 195, 196, 197, 198, 199, 201, 203, 204, 205, 207, 208, 209, 210, 211, 215, 216, 220, 221, 223, 228, 229, 230, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 245, 248], "function_nam": [232, 233, 234], "functional_op": 162, "fundament": 160, "furhter": 237, "furo": 199, "further": [147, 150, 151, 152, 153, 154, 161, 169, 205, 208, 221, 225, 234, 244, 245, 248], "fuse": [195, 198, 222, 245], "fuse_bn_into_conv": 190, "fusion": [180, 198, 199], "futur": [157, 198, 201], "fx": [14, 161, 175, 188], "g": [165, 177, 191, 193, 203, 213, 218, 227, 228, 230, 235, 238, 242, 244], "gain": [205, 206, 208, 228, 232, 233, 234, 236, 239, 240, 241, 244, 248], "gamma": 187, "gap": 217, "gate_proj": 198, "gaussiannllloss": 70, "gave": 188, "gelu": 66, "gemm": [8, 199, 215, 222], "gemma3": 199, "gener": [6, 19, 150, 151, 152, 153, 154, 158, 166, 169, 174, 180, 183, 186, 187, 189, 190, 191, 193, 195, 196, 197, 198, 199, 203, 204, 208, 213, 215, 216, 218, 219, 220, 221, 222, 224, 226, 227, 235, 237, 243, 244, 245, 247], "generate_calibration_callback": [195, 196], "generate_layer_output": [6, 158, 202], "get": [2, 18, 155, 161, 163, 167, 174, 181, 186, 196, 197, 199, 202, 208, 209, 211, 214, 216, 225, 226, 227, 228, 229, 231, 244], "get_activation_quant": [2, 216], "get_active_quant": [2, 163, 174, 216], "get_available_provid": [186, 197, 227, 228, 229, 231], "get_calibration_and_eval_data_load": [218, 220], "get_candid": [2, 163, 174, 216], "get_default_kernel": 19, "get_devic": 237, "get_encod": 155, "get_extra_st": 155, "get_input": [186, 216, 218, 227, 228, 229, 230, 231], "get_input_quantizer_modul": [163, 174, 216], "get_kernel": 19, "get_loss_fn": [171, 178, 197], "get_offset": 169, "get_param_quant": [2, 216], "get_path_for_per_channel_config": [185, 189, 219, 220], "get_peft_model": 167, "get_pre_processed_input": 202, "get_quant_scheme_candid": [14, 175, 188], "get_scal": [149, 169], "get_unlabeled_dataload": 230, "get_val_dataload": [227, 229, 230, 231, 232, 234, 235, 236, 238, 239, 240, 241, 242], "git": 226, "github": [184, 226], "give": [199, 203, 208, 214, 230, 232, 233, 234, 242, 244, 246], "given": [2, 4, 7, 12, 14, 15, 17, 18, 19, 157, 163, 164, 170, 171, 173, 174, 175, 177, 178, 186, 188, 189, 191, 197, 198, 201, 205, 207, 208, 209, 211, 214, 216, 218, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244, 248], "global": [180, 197, 224], "glu": 67, "go": [199, 227, 230, 235, 238, 242], "goal": [14, 175, 188, 203, 221, 227, 235, 247], "good": [167, 208, 232, 233, 234, 238, 240, 241, 244, 248], "googl": 180, "got": [3, 16, 161, 190], "gpu": [180, 182, 183, 184, 185, 199, 202, 205, 209, 211, 220, 230, 232, 233, 234, 236, 238, 239, 240, 241, 242], "grad_fn": [147, 148, 149, 150, 151, 166, 169, 185], "gradient": [147, 148, 149, 195, 196, 199, 220], "grant": 226, "granular": [18, 199, 204, 205, 208, 209, 211, 224, 232, 233, 234], "graph": [10, 12, 14, 149, 161, 166, 170, 173, 175, 177, 186, 188, 190, 191, 197, 199, 210, 213, 216, 218, 219, 220, 221, 227, 228, 229, 231, 232, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247], "graph_optimization_level": 166, "graphmodul": 161, "graphoptimizationlevel": 166, "greater": [18, 205, 206, 207, 209, 211, 244], "greatest": 218, "greedi": [18, 205, 208, 209, 210, 211], "greedili": [227, 235], "greedy_param": [205, 209, 211, 232, 233, 234], "greedy_select_param": [18, 205, 209, 232, 233, 234], "greedymixedprecisionalgo": [163, 174, 216, 235], "greedyselectionparamet": [18, 205, 209, 211, 232, 233, 234], "green": 212, "grid": [169, 171, 178, 191, 197, 215, 245], "group": [2, 8, 159, 163, 174, 191, 199, 215, 222, 227, 235, 245], "groupnorm": 71, "gru": [68, 199], "grucel": 69, "guarante": 199, "guid": [0, 156, 181, 194, 199, 220, 225, 226, 227, 235, 236, 237, 239, 240, 241, 242, 244, 247], "guidebook": 208, "guidelin": [161, 220, 235, 236, 239, 240, 241, 242], "gz": [218, 220, 230], "h": [211, 212, 226], "ha": [0, 9, 12, 14, 19, 147, 156, 160, 161, 162, 170, 173, 174, 175, 177, 180, 182, 183, 186, 187, 188, 193, 194, 199, 203, 204, 206, 207, 210, 212, 215, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245, 247], "had": 223, "half": 206, "hand": [227, 235], "handl": [14, 15, 175, 184, 185, 188, 189, 199, 204, 245], "hard": 161, "hardshrink": 72, "hardsigmoid": 73, "hardswish": 74, "hardtanh": 75, "hardwar": [10, 170, 177, 180, 204, 220, 221, 245, 247], "hat": 245, "have": [9, 19, 157, 158, 159, 160, 161, 162, 165, 168, 169, 176, 180, 182, 185, 191, 193, 195, 197, 198, 199, 201, 202, 203, 204, 208, 213, 215, 218, 220, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 248], "hba": [229, 239], "he": 208, "heavi": [157, 201, 210, 244], "height": [190, 191, 209, 211, 212, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "held": [19, 164, 240, 241], "help": [160, 162, 163, 170, 177, 179, 192, 198, 203, 204, 207, 208, 216, 217, 219, 220, 221, 222, 224, 226, 229, 230, 239, 242, 247, 248], "helper": [163, 169, 174, 195, 198, 216, 238], "hen": [14, 175, 188], "here": [12, 160, 167, 169, 173, 182, 186, 191, 195, 196, 218, 220, 222, 227, 229, 230, 231, 232, 234, 235, 236, 238, 239, 240, 241, 242, 247, 248], "heterogen": 247, "heurist": 237, "hide": 219, "high": [2, 4, 17, 160, 174, 180, 181, 191, 192, 194, 199, 206, 207, 216, 221, 228, 229, 231, 232, 233, 234, 236, 239, 240, 241, 244], "higher": [7, 12, 173, 174, 186, 199, 205, 207, 209, 211, 215, 216, 217, 218, 219, 221, 224, 227, 232, 233, 234, 235, 244, 245, 247], "highest": [14, 175, 188, 207, 218], "highli": 244, "highlight": 210, "hingeembeddingloss": 76, "histogram": [9, 168, 176, 199, 200, 230, 242, 245], "historgram": [9, 203, 230], "hold": [147, 148, 149, 164, 189, 222, 224], "honor": [205, 209, 211], "hood": 160, "hook": 245, "hope": [227, 235, 238], "host": [184, 199, 210], "hotspot": [9, 168, 176, 203, 214], "hover": 199, "how": [2, 162, 164, 167, 169, 170, 174, 180, 181, 183, 185, 186, 194, 198, 199, 203, 204, 208, 209, 211, 213, 214, 215, 216, 219, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 246, 248], "howev": [160, 166, 184, 186, 194, 199, 204, 205, 209, 211, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 244, 245, 247, 248], "html": [9, 157, 161, 168, 176, 183, 184, 199, 200, 201, 203, 226, 230, 242], "htp": [199, 243], "htp_v66": [10, 170, 177, 220, 221], "htp_v68": [10, 170, 177, 220, 221], "htp_v69": [10, 170, 177, 220, 221], "htp_v73": [10, 170, 177, 220, 221], "htp_v75": [10, 170, 177, 220, 221, 230], "htp_v79": [10, 170, 177, 220, 221], "htp_v81": [10, 170, 177, 220, 221], "http": [161, 172, 184, 187, 193, 198, 199, 210, 218, 220, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "hub": [179, 181, 190, 199, 202, 204, 213, 215, 216, 218, 244, 245, 247, 248], "huberloss": 77, "huggingfac": [167, 195, 196, 198, 199], "hx": [68, 69, 85, 86, 112, 113], "hxwx5": 212, "hxwx8": 212, "hyper": [189, 228, 232, 233, 234, 236, 238, 239, 240, 241, 247], "hyperparamet": [221, 244], "i": [0, 1, 2, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18, 19, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241, 243, 244, 245, 246, 247, 248], "i_": [150, 151, 152, 153, 154], "i_0": [150, 151, 152, 153, 154], "i_d": [150, 151, 152, 153, 154], "iccv": [208, 229, 239], "id": [6, 14, 175, 188, 195, 196, 202, 210], "idea": 191, "ideal": [199, 230, 242], "idempot": 149, "ident": [190, 223], "identifi": [159, 162, 198, 199, 200, 203, 204, 205, 212, 214, 215, 216, 218, 224, 226, 244, 245, 247], "ieee": [155, 208], "ignor": [2, 18, 161, 174, 205, 209, 211, 216, 220, 227, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 248], "ignore_quant_ops_list": [12, 173, 186], "illustr": [186, 205, 207, 209, 210, 211, 212, 232, 233, 234, 245, 247], "ilsvrc": [188, 216, 218, 220, 230], "ilsvrc2012": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "ilsvrc2012_devkit_t12": [218, 220, 230], "ilsvrc2012_img_v": [218, 220, 230], "imag": [9, 168, 176, 180, 183, 186, 188, 189, 197, 199, 203, 216, 218, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "image_bw": 223, "image_net_config": [227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "image_net_data_load": [227, 229, 230, 231, 232, 234, 236, 238, 239, 240, 241, 242], "image_net_evalu": [227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "image_net_train": [232, 233, 234, 238, 240, 241], "image_rgb": 223, "image_s": [227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "imagefold": [235, 237], "imagenet": [185, 186, 188, 189, 190, 191, 197, 216, 218, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "imagenet_data": [186, 197, 228], "imagenet_dataset": [218, 220, 230, 237], "imagenetdataload": [227, 229, 230, 231, 232, 234, 236, 238, 239, 240, 241, 242], "imagenetdatapipelin": [227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "imagenetevalu": [227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "imagenettrain": [232, 233, 234, 238, 240, 241], "images_dir": 238, "images_mean": 235, "images_std": 235, "imbal": 191, "immedi": [229, 237, 239], "impact": [180, 207, 218, 221, 224, 225, 227, 235, 244], "implement": [6, 19, 158, 169, 188, 195, 196, 198, 199, 202, 203, 224, 226, 227, 235, 247, 248], "impli": [216, 227, 235], "import": [12, 19, 147, 148, 149, 150, 151, 153, 154, 155, 156, 159, 160, 161, 162, 164, 166, 167, 169, 170, 173, 177, 180, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 204, 205, 209, 211, 213, 215, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "impos": 204, "improp": 199, "improv": [181, 186, 187, 190, 192, 193, 198, 199, 204, 206, 208, 216, 217, 218, 220, 224, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 244, 246, 248], "in1": 219, "in2": 219, "in_channel": [159, 204, 215], "in_eval_mod": 237, "in_featur": [19, 160, 164, 170], "in_plac": [170, 177, 193, 195, 196, 198, 220, 221], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "includ": [2, 14, 157, 174, 175, 180, 184, 188, 190, 191, 198, 199, 201, 203, 207, 210, 213, 216, 221, 222, 244, 245, 248], "incompat": 199, "incorrect": [163, 199, 219], "incorrectli": [199, 248], "increas": [18, 192, 205, 207, 209, 211, 218, 221, 227, 235, 244, 245], "increment": 248, "incur": [203, 216], "independ": [161, 180, 224], "index": [9, 164, 168, 176, 183, 199, 203, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "index_0": [230, 242], "index_1": [230, 242], "index_n": [230, 242], "indic": [18, 98, 99, 100, 163, 164, 169, 170, 199, 205, 206, 209, 211, 212, 213, 219, 220, 221, 235, 237], "indirect": [216, 227, 235], "individu": [9, 168, 176, 181, 197, 203, 207, 214, 222], "induc": 245, "infer": [10, 12, 14, 170, 173, 175, 180, 185, 186, 187, 188, 190, 193, 197, 199, 203, 204, 206, 216, 218, 219, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 248], "inferencesess": [2, 9, 10, 166, 202, 203, 216, 218, 220, 227, 228, 229, 231], "influenc": 245, "info": [162, 199], "inform": [2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 160, 162, 174, 179, 181, 199, 213, 216, 220, 227, 235, 238], "inherit": [19, 89, 164, 199], "init": [190, 191, 198], "initi": [6, 7, 10, 150, 151, 155, 164, 169, 170, 177, 190, 191, 202, 218, 220, 227, 229, 230, 231, 235, 236, 240, 241, 242], "initial_accuraci": [188, 237], "initializd": 164, "inner": 224, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "inp_symmetri": [171, 178, 197], "inplac": [155, 162, 190, 191], "input": [1, 2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 180, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 203, 204, 205, 207, 208, 209, 211, 212, 213, 215, 216, 218, 220, 221, 222, 223, 227, 228, 229, 230, 231, 235, 238, 242, 243, 245], "input1": [35, 51, 94], "input2": [35, 51, 94], "input_": [150, 151, 152, 153, 154], "input_batch": 202, "input_channel": [8, 159, 204, 215], "input_data": [188, 220, 227, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "input_dlc": 243, "input_id": [193, 195, 196, 198], "input_inst": [6, 158, 202], "input_length": 37, "input_list": 243, "input_nam": [166, 170, 177, 185, 186, 190, 197, 216, 218, 220, 221, 227, 228, 229, 230, 231], "input_network": 243, "input_q": 169, "input_qdq": 169, "input_qtzr": 19, "input_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 163, 164, 170, 174, 216, 220, 221], "input_shap": [16, 17, 18, 161, 186, 190, 191, 197, 203, 205, 209, 211, 216, 218, 219, 220, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241], "input_tensor": 161, "inputs_batch": [220, 227, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "insert": [161, 192, 198, 216, 218, 220, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 239, 240, 241, 245, 247], "insid": [13, 19, 161, 164, 183, 187, 199, 219], "insight": [210, 224, 244], "inspect": 208, "instabl": 238, "instal": [180, 181, 199, 210, 226, 237, 239, 240, 241, 247], "instanc": [1, 6, 7, 11, 19, 158, 161, 162, 177, 186, 197, 202, 210, 218, 221, 244, 248], "instancenorm1d": 78, "instancenorm2d": 79, "instancenorm3d": 80, "instanti": [167, 169, 195, 196, 198, 199, 204, 210, 213, 216, 222, 223, 227, 230, 235, 238, 242], "instead": [155, 161, 162, 199, 204, 212, 213, 218, 220, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "instruct": [181, 184, 193, 198, 199, 204, 226, 243, 248], "int": [1, 2, 6, 7, 8, 9, 11, 12, 13, 14, 15, 18, 48, 49, 50, 98, 99, 100, 150, 151, 152, 153, 154, 155, 159, 160, 163, 165, 168, 170, 171, 173, 174, 175, 176, 177, 178, 186, 187, 188, 189, 193, 195, 196, 197, 199, 202, 203, 204, 205, 209, 211, 213, 215, 216, 218, 220, 221, 227, 232, 233, 234, 235, 237, 245], "int16": [7, 10, 163, 185, 188, 216, 218, 219, 220, 221, 227, 235, 244, 245, 247, 248], "int32": [166, 195, 196, 199, 213], "int4": [10, 163, 186, 188, 197, 199, 219, 220, 221, 247], "int8": [10, 148, 149, 163, 180, 185, 186, 188, 197, 216, 218, 219, 220, 221, 227, 228, 229, 231, 235, 236, 239, 240, 241, 244, 245, 247, 248], "int_multipli": 19, "integ": [153, 154, 159, 169, 177, 180, 186, 195, 199, 203, 204, 213, 215, 216, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244, 247, 248], "integr": [14, 167, 175, 187, 188], "intel": [182, 184, 185], "intellig": 208, "intend": [180, 203, 205, 209, 211, 213, 219], "intens": 221, "inter": 200, "interact": [157, 160, 180], "intercept": 245, "interdepend": 216, "interest": [9, 168, 176, 203], "interfac": [199, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "intermedi": [2, 6, 14, 158, 165, 170, 174, 175, 177, 180, 188, 193, 199, 200, 202, 216, 220, 221, 227, 235], "intern": [14, 160, 175, 177, 188, 198, 208, 222, 236], "interpol": [207, 227, 235], "interpret": 220, "intersect": 244, "introduc": [164, 187, 191, 192, 193, 199, 222, 245], "invalid": [161, 204], "invoc": [232, 233, 234], "invok": [9, 10, 164, 168, 170, 176, 177, 203, 208, 210, 220, 221], "involv": [2, 160, 174, 189, 194, 195, 196, 216, 224, 227, 235, 248], "io": [163, 219], "ip": 226, "ipynb": 226, "irrespect": [13, 187], "is_avail": [185, 186, 187, 188, 189, 190, 191, 193, 197, 198, 216, 219, 220, 221, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "is_bfloat16": 155, "is_float16": 155, "is_initi": [19, 150, 151, 155, 164, 169], "is_input_quant": 222, "is_leaf_modul": 161, "is_output_quant": 222, "is_quant": 222, "is_sym": 213, "is_symmetr": [184, 213, 222], "is_train": [227, 229, 230, 231, 232, 234, 236, 238, 239, 240, 241, 242], "is_unsigned_symmetr": 160, "isinst": [159, 195, 196, 204, 215], "islic": [186, 197, 218, 221, 228], "isol": [183, 199, 245], "issu": [162, 189, 199, 200, 202, 210, 219, 224, 248], "item": [155, 193, 195, 196, 198, 216, 220, 221, 230, 242, 243], "iter": [9, 10, 12, 13, 14, 18, 165, 173, 175, 182, 186, 187, 188, 193, 198, 199, 203, 205, 209, 211, 216, 220, 227, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 244], "itertool": [186, 195, 196, 197, 218, 221, 228], "its": [9, 19, 89, 147, 155, 163, 164, 180, 181, 187, 193, 195, 196, 203, 204, 205, 212, 216, 219, 221, 222, 226, 227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 245, 248], "itself": [202, 208, 209, 230, 238, 242], "j_": [150, 151, 152, 153, 154], "j_0": [150, 151, 152, 153, 154], "j_d": [150, 151, 152, 153, 154], "jaderberg": 208, "jan": 208, "jenkin": 183, "jian": 208, "jianhua": 208, "jit": [195, 196, 223], "job": [232, 233, 234, 238, 240, 241, 243], "join": [185, 186, 190, 191, 197, 205, 209, 211, 216, 218, 220, 235, 236, 237], "jointli": [240, 241], "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 230, 242, 243, 244, 245, 246, 247, 248, 249], "jupyt": [181, 199, 226], "just": [199, 212, 219, 227, 230, 235, 240, 241, 242, 245], "k": [169, 193, 195, 196, 198, 211, 237], "k_proj": 198, "kaim": 208, "kd": 199, "keep": [161, 191, 222, 231, 244, 248], "kei": [155, 163, 185, 193, 195, 196, 198, 213, 219, 228, 229, 231, 232, 233, 234, 236, 239, 240, 241, 244], "kept": [220, 224], "kera": 199, "kernel": [19, 199, 205, 209, 211, 215], "kernel_s": [161, 162, 170, 190, 191, 220, 221], "keyword": [19, 162], "kl": 180, "kldivloss": 81, "know": [19, 227, 235, 238], "knowledg": [187, 192, 193, 213], "known": [162, 198, 199, 204, 245], "kullback": 180, "kuzmin": 208, "kv": 199, "kwarg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 162, 163, 164, 166, 170, 199, 216, 220, 221], "l1": [171, 178, 197, 219], "l1loss": 82, "l2": 219, "lab": [180, 247], "label": [183, 186, 188, 189, 193, 195, 196, 197, 198, 203, 216, 218, 220, 221, 227, 228, 235, 237], "labeled_data": 188, "labeled_data_load": 188, "lambda": [14, 159, 163, 165, 174, 175, 188, 193, 199, 204, 215, 216, 230], "laptop": [180, 181], "larg": [167, 191, 192, 194, 199, 206, 208, 209, 211, 221, 248], "larger": [209, 211, 244], "last": [199, 219], "latenc": [182, 199, 206, 216, 217, 218, 244, 248], "later": [177, 182, 184, 185, 198, 199, 218, 220, 227, 230, 238, 242], "latest": [163, 182, 184, 185, 199, 219], "launch": 226, "layer": [1, 3, 4, 6, 7, 9, 11, 12, 14, 16, 17, 18, 19, 89, 157, 158, 159, 162, 164, 167, 168, 170, 171, 173, 175, 176, 177, 178, 180, 186, 187, 188, 189, 190, 193, 195, 196, 197, 198, 199, 201, 204, 205, 206, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 225, 228, 232, 233, 234, 244, 245, 247, 248], "layer1": [230, 242], "layer2": [230, 242], "layer_nam": [7, 9, 165, 168, 176, 193, 203, 218], "layer_output_util": [0, 5, 156, 202], "layer_sensitivity_dict": [7, 218], "layern": [230, 242], "layernorm": [87, 199], "layeroutpututil": [6, 158, 202], "layers_to_exclud": 162, "layout": [149, 161], "lazili": 199, "lceil": [150, 151, 153, 154, 155, 245], "lead": [9, 11, 197, 199, 203, 220, 224], "leaf": [161, 163, 187, 193, 199], "leakyrelu": [88, 199], "learn": [160, 177, 179, 180, 181, 182, 190, 191, 194, 195, 196, 199, 208, 221, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 244, 248], "learnabl": [150, 151, 187, 192, 193, 199, 221], "learnedgrid": 199, "learnedgridquant": 160, "learning_r": [232, 233, 234, 238, 240, 241], "learning_rate_schedul": [232, 233, 234, 238, 240, 241], "learnt": 193, "least": [174, 218, 228, 236], "leav": 196, "left": [150, 151, 152, 153, 154, 155, 169, 194, 204, 207, 212, 216, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 245], "leftarrow": 191, "legaci": 199, "leibler": 180, "len": [10, 12, 161, 173, 186, 188, 193, 195, 196, 198, 216, 218, 221, 235, 237], "length": [159, 164, 204, 213, 215], "leq": [150, 151, 152, 153, 154], "less": [180, 205, 207, 217, 222, 235, 245, 247], "lesser": [227, 235], "let": [161, 191, 193, 218, 220, 227, 235, 247], "level": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 159, 160, 161, 163, 165, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 181, 186, 187, 188, 189, 190, 191, 193, 194, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 209, 210, 211, 215, 216, 217, 218, 219, 220, 221, 224, 228, 231, 232, 233, 234, 236, 240, 241, 245], "leverag": 244, "lfloor": [150, 151, 152, 153, 154, 155], "libpymo": [160, 184, 199], "libpython": 199, "libqnnhtp": 243, "libqnnmodeldlc": 243, "librari": [10, 167, 180, 220, 243], "lie": 215, "light": [227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242], "lightweight": 194, "like": [160, 181, 199, 202, 203, 208, 213, 214, 215, 216, 218, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247], "limit": [193, 198, 199, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "limitedbatchdataload": [193, 198], "line": [220, 243], "linear": [3, 12, 16, 19, 89, 159, 160, 161, 162, 164, 167, 173, 186, 189, 190, 191, 192, 198, 199, 204, 205, 215], "linear1": [159, 204, 215], "linear10": 215, "linear_1": 204, "link": [199, 226], "list": [1, 2, 3, 6, 8, 10, 12, 14, 16, 17, 18, 48, 49, 50, 98, 99, 100, 153, 154, 155, 158, 159, 161, 162, 163, 164, 168, 171, 173, 174, 175, 176, 178, 186, 188, 190, 191, 193, 195, 196, 197, 198, 202, 203, 204, 205, 207, 209, 211, 213, 215, 219, 220, 222, 227, 228, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 248], "list_of_module_comp_ratio_pair": [18, 205, 209, 211], "listen": 210, "lite": [181, 190, 199, 202, 204, 213, 215, 216, 243, 245, 247, 248], "lite_mp": [0, 5, 218], "litemp": [199, 244], "liter": [163, 219], "littl": [160, 208, 228, 232, 233, 234, 236, 239, 240, 241, 248], "ll": [184, 226], "llama": [193, 198, 199], "llama3": 193, "llamadecoderlay": [187, 193], "llamaforcausallm": [193, 198], "llamamodel": [187, 193], "llm": [199, 247], "llm_configur": 199, "lm_head": 198, "load": [13, 161, 162, 177, 180, 181, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 205, 208, 209, 211, 216, 218, 220, 227, 228, 229, 231, 243, 247], "load_adapt": [195, 196], "load_checkpoint": 177, "load_dataset": [186, 188, 189, 193, 195, 196, 198, 216], "load_encod": [186, 220], "load_encodings_to_sim": [199, 202, 220], "load_model": [185, 186, 190, 191, 197, 216, 218, 220, 227, 228, 229, 230, 231], "load_state_dict": 155, "loader": [2, 9, 12, 14, 15, 163, 171, 173, 174, 175, 178, 186, 188, 189, 197, 203, 216, 218, 220, 221, 228, 229, 230, 231, 232, 234, 236, 238, 239, 240, 241, 242], "local": [197, 210, 218, 220, 226, 230], "localresponsenorm": 90, "locat": [198, 218, 220, 227, 230, 235, 238, 242, 248], "log": [162, 163, 203, 219], "log_2": 155, "log_fil": [163, 219], "log_input": 111, "log_prob": 37, "logger": 162, "logic": [2, 19, 89, 164, 174, 199, 216, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242], "logit": [195, 196, 216, 221, 235, 237], "logsigmoid": 91, "logsoftmax": 92, "long": [193, 195, 198], "longer": [160, 170, 204, 213, 220, 221, 232, 233, 234, 247], "look": [181, 213, 215, 227, 230, 235, 242, 243], "loop": [161, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "lora": [165, 167, 180, 193, 199], "lora_a": [195, 196], "lora_a_lay": 196, "lora_add_lay": 196, "lora_alpha": 167, "lora_b": [195, 196], "lora_b_lay": 196, "lora_config": 167, "lora_dropout": 167, "lora_mul_lay": 196, "loraconfig": 167, "loralay": [195, 196], "lose": 212, "loss": [9, 11, 12, 147, 168, 171, 173, 176, 178, 181, 186, 189, 195, 196, 197, 199, 208, 217, 221, 228, 229, 231, 236, 239, 240, 241, 245], "loss_fn": [171, 178, 189, 195, 196, 197, 221], "lost": [245, 248], "lot": 244, "low": [160, 169, 180, 192, 194, 199, 204, 208, 213, 220, 232, 234], "lower": [11, 12, 166, 173, 180, 186, 191, 197, 207, 215, 216, 217, 221, 224, 227, 235, 244, 247], "lowest": [205, 244], "lpbq": [159, 199, 204, 213], "lppool1d": 83, "lppool2d": 84, "lr": [189, 199, 221], "lstm": [85, 199], "lstmcell": 86, "lsvrc": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "lt": 184, "lwc": 193, "m": [183, 184, 211, 226], "mac": [18, 205, 206, 208, 209, 211, 214, 216, 227, 232, 233, 234, 235], "machin": [179, 180, 194, 199, 208, 248], "made": [161, 163, 180, 199, 219, 222, 235, 236, 239, 240, 241], "magnitud": 205, "mai": [2, 11, 14, 19, 147, 148, 149, 157, 160, 161, 164, 174, 175, 186, 188, 191, 197, 198, 199, 201, 203, 204, 208, 213, 216, 218, 220, 221, 227, 228, 229, 230, 235, 238, 239, 242, 244, 248], "main": [189, 221, 222], "maintain": [180, 188, 199, 207, 208, 236, 244], "major": [208, 213], "make": [163, 164, 167, 180, 191, 199, 216, 219, 222, 223, 238, 247, 248], "make_dummy_input": 185, "make_psnr_eval_fn": 218, "makedir": [227, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "manag": [10, 184, 199, 247], "mandatori": 202, "mani": [161, 199, 206, 227, 230, 232, 233, 234, 235, 242], "manner": [14, 175, 188, 191], "mantissa": [155, 159, 204], "mantissa_bit": [155, 159, 204], "manual": [18, 150, 151, 160, 163, 184, 188, 199, 205, 208, 209, 211, 213, 232, 233, 234, 244], "manual_param": [205, 209, 211], "manual_se": 235, "manualmodeparam": [18, 205, 209, 211], "manylinux_2_34_x86_64": 184, "map": [19, 148, 153, 154, 162, 164, 180, 193, 195, 196, 198, 199, 203, 207, 213, 220, 222, 245], "map_loc": 202, "marginrankingloss": 94, "mark": 198, "marku": 208, "mask": 19, "maskedadd": 19, "match": [155, 170, 177, 199, 203, 204, 205, 208, 209, 211, 212, 220, 221, 222, 224], "math": [188, 216, 218, 248], "mathemat": [169, 180, 191, 198, 223, 227, 235], "matmul": [8, 199, 215], "matmul_8": 162, "matric": 195, "matrix": 244, "matter": [19, 230, 242], "max": [9, 150, 151, 155, 157, 160, 168, 176, 184, 197, 199, 200, 201, 208, 213, 220, 231, 243, 247], "max_batch": 218, "max_epoch": [232, 233, 234, 238, 240, 241], "max_iter": [193, 195, 196], "maximum": [2, 14, 153, 154, 155, 174, 175, 188, 216, 227, 230, 235, 242, 245], "maxpool1d": 95, "maxpool2d": 96, "maxpool3d": 97, "maxunpool1d": 98, "maxunpool2d": 99, "maxunpool3d": 100, "mdoel": 189, "mean": [15, 164, 186, 188, 189, 197, 200, 216, 218, 220, 222, 227, 228, 230, 232, 233, 234, 235, 237, 238, 239, 242, 244, 245], "measur": [2, 9, 18, 168, 174, 176, 180, 203, 205, 209, 211, 216, 218, 220, 227, 232, 233, 234, 235, 244, 248], "mechan": [161, 169, 186], "meet": [14, 174, 175, 188, 206, 207, 216, 227, 235, 244, 248], "member": 222, "memori": [18, 149, 167, 180, 181, 199, 205, 206, 208, 209, 211, 212, 214, 217, 221, 232, 233, 234, 244, 248], "memory_format": 149, "merg": [180, 198], "met": [2, 174, 216, 227, 235], "meta": [193, 198], "metadata": [165, 193], "metapackag": 226, "method": [6, 13, 14, 19, 89, 149, 158, 160, 161, 163, 164, 170, 174, 175, 177, 180, 184, 187, 188, 191, 192, 195, 202, 207, 208, 216, 217, 219, 220, 221, 224, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 244, 245], "methodologi": 248, "metric": [18, 186, 199, 203, 205, 209, 211, 216, 227, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 244, 245, 248], "mha": 199, "middl": 248, "might": [2, 174, 191, 199, 203, 213, 216, 224, 227, 228, 230, 232, 233, 234, 235, 236, 239, 240, 241, 242, 244, 248], "migrat": [0, 156, 199, 220], "mimic": 247, "mimick": 160, "min": [9, 150, 151, 157, 160, 168, 176, 184, 198, 199, 200, 201, 213, 231, 243, 247], "min_max": [10, 186, 199, 218, 220, 227, 228, 231], "min_max_rang": [203, 230, 242], "miniconda": 183, "miniforg": 183, "minim": [171, 178, 180, 181, 191, 192, 197, 214, 216, 217, 227, 231, 235, 240, 241, 244, 245], "minima": 197, "minimum": [153, 154, 161, 187, 193, 197, 199, 230, 242, 245], "minmaxencodinganalyz": 155, "minor": [199, 213, 221], "miou": [227, 235, 244], "mish": 101, "mismatch": [169, 200, 202, 220], "miss": [155, 162, 199, 220, 243], "missing_kei": 155, "mistral": [198, 199], "mistralforcausallm": 198, "mix": [2, 14, 163, 174, 175, 182, 188, 199, 248], "mixed_precis": [0, 5, 156, 216, 219, 227, 235], "mixed_precision_algo": [2, 163, 174, 216, 227, 235], "mixedprecisionconfigur": [163, 219], "mixin": [19, 164], "mkdir": 226, "ml": [181, 208, 231, 240, 241], "mmp": 217, "mmp_log": [163, 219], "mnist": [205, 209, 211], "mnist_torch_model": 205, "mnist_trained_on_gpu": [205, 209, 211], "mnt": [183, 230], "mobil": [180, 181], "mobilenet": [180, 186, 189], "mobilenet_v2": [185, 186, 188, 189, 190, 191, 197, 216, 218, 219, 220, 221], "mobilenet_v2_weight": [186, 190, 191, 197, 216, 218, 220], "mobilenetv2": [186, 188, 190, 191, 218, 220], "mode": [14, 18, 161, 170, 175, 177, 188, 199, 205, 209, 211, 220, 221, 222, 232, 233, 234], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 221, 222, 225, 226, 243, 245, 246, 247, 249], "model_config": [193, 198], "model_id": [193, 195, 196, 198], "model_input": 162, "model_or_pipelin": 186, "model_prepar": [0, 156, 199, 235, 236, 238, 240, 241, 242], "model_prepare_requir": [14, 175, 188], "model_preparer_elementwise_add_exampl": 161, "model_preparer_functional_exampl": 161, "model_preparer_reused_exampl": 161, "model_torch": 202, "model_transform": 161, "model_valid": [0, 156], "modelcompressor": [18, 205, 209, 211, 232, 233, 234], "modeling_llama": [193, 198], "modeling_opt": [195, 196], "modelprepar": [161, 235, 236, 240, 241, 242], "modelproto": [3, 4, 6, 9, 10, 190, 191, 202, 203, 220], "modelvalid": 162, "modelwithconsecutivelinearblock": 187, "modelwithelementwiseaddop": 161, "modelwithfunctionallinear": 162, "modelwithfunctionalrelu": 161, "modelwithlinear": 187, "modelwithnontorchfunct": 161, "modelwithoutfunctionallinear": 162, "modelwithoutreusednod": 162, "modelwithreusednod": 162, "modelwithreusedrelu": 161, "modif": [235, 236, 239, 240, 241], "modifi": [13, 161, 170, 172, 177, 186, 187, 190, 191, 195, 196, 197, 198, 199, 202, 212, 216, 218, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 245, 247, 248], "modul": [2, 8, 12, 13, 14, 15, 16, 17, 18, 19, 89, 155, 157, 158, 159, 161, 162, 163, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 209, 211, 212, 215, 216, 219, 220, 221, 223, 227, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 248], "module_cl": 19, "module_classes_to_exclud": 161, "module_nam": [195, 196], "module_to_exclud": 161, "modulecompratiopair": [18, 205, 209, 211], "moduledict": [19, 160, 164, 170, 220, 221], "modulelist": [19, 160, 164, 170, 220, 221], "modules_to_exclud": [14, 161, 171, 175, 178, 188, 197], "modules_to_ignor": [18, 168, 176, 203, 205, 209, 211, 232, 233, 234], "momentum": [189, 190, 191], "monitor": 203, "monoton": [18, 205, 207, 209, 211], "more": [12, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 162, 164, 167, 173, 174, 177, 180, 186, 191, 199, 203, 205, 207, 208, 209, 210, 211, 216, 217, 219, 221, 222, 224, 227, 230, 235, 238, 242, 243, 244, 245, 247, 248], "most": [169, 180, 184, 188, 217, 218, 222, 244, 245], "move": [216, 226], "movement": 199, "mp": [163, 217, 218, 219], "mp_configur": 219, "mse": [9, 11, 160, 168, 171, 176, 178, 199, 203, 221, 245], "mseloss": 93, "much": [212, 232, 233, 234, 248], "mul_scal": 196, "multi": 199, "multigpu": 199, "multiheadattent": 199, "multilabelmarginloss": 102, "multilabelsoftmarginloss": 103, "multimarginloss": 104, "multipl": [14, 16, 17, 18, 149, 158, 161, 162, 163, 164, 167, 170, 175, 177, 188, 190, 191, 199, 202, 204, 205, 208, 209, 211, 215, 219, 220, 221, 230, 242, 244, 245], "multipli": [164, 206, 208, 209, 211, 214, 227, 235], "must": [149, 155, 159, 162, 164, 186, 187, 188, 189, 193, 197, 198, 203, 204, 206, 212, 215, 219, 220, 222, 228, 229, 230, 231, 232, 233, 234, 236, 239, 240, 241, 242, 247], "mutual": [155, 159, 204], "mymodel": 198, "n": [9, 168, 169, 176, 203, 204, 211, 221], "nagel": 208, "name": [1, 2, 8, 9, 19, 157, 158, 163, 164, 167, 168, 170, 174, 176, 177, 183, 186, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 210, 213, 215, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 242, 243, 245], "name_": [9, 168, 176, 203, 230, 242], "name_to_quantizer_dict": [2, 163, 174, 216], "named_modul": [195, 196], "named_paramet": 195, "namedtupl": 155, "namespac": [156, 160, 173, 174, 175, 176, 177, 178], "naming_schem": [158, 202], "namingschem": [158, 202], "nan": 199, "nativ": [164, 199], "navig": 226, "na\u00efv": 192, "nconv": 190, "ndarrai": [1, 2, 6, 9, 10, 11, 186, 188, 197, 202, 203, 216, 220, 228], "nearest": [14, 175, 177, 186, 188, 228, 236], "necessari": [147, 148, 149, 170, 183, 198, 203, 205, 209, 210, 211, 220, 221, 226, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242, 248], "necessarili": [227, 235], "need": [9, 10, 12, 13, 18, 158, 163, 168, 173, 176, 180, 184, 186, 187, 189, 190, 191, 193, 197, 198, 199, 202, 203, 205, 209, 211, 213, 216, 219, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "neg": [137, 138, 153, 154, 199, 221, 227, 235], "negat": 236, "nest": 199, "net": [218, 220, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "network": [164, 180, 199, 206, 207, 208, 210, 221, 224, 237, 245], "neural": [180, 206, 208, 221, 224, 227, 235, 237, 244, 245], "neuron": 180, "new": [149, 150, 151, 160, 161, 177, 181, 194, 195, 199, 204, 216, 219, 223, 235, 242], "new_empti": 149, "next": [191, 198, 199, 218, 220, 224, 227, 229, 235, 238, 245], "next_conv_weight": 191, "night": [228, 229, 230, 231, 236, 239, 240, 241, 242], "nllloss": 105, "nllloss2d": 106, "nmodel": [190, 191], "nn": [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 156, 157, 159, 160, 161, 162, 163, 166, 170, 186, 187, 188, 189, 195, 196, 197, 198, 199, 201, 203, 204, 205, 209, 211, 215, 216, 219, 220, 221, 223, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 248], "nncf": 180, "nnext": 191, "no_grad": [162, 170, 185, 186, 187, 195, 196, 197, 199, 216, 220, 221, 235, 236, 237, 239, 240, 241, 242], "node": [1, 8, 10, 14, 161, 170, 174, 175, 177, 180, 186, 188, 190, 191, 199, 215, 220, 221, 223, 227, 228, 229, 231, 235, 236, 239, 240, 241, 244, 247, 248], "node_names_to_optim": [1, 186], "noffset": 169, "nois": [191, 203, 214, 218, 221, 222, 247], "noisi": 238, "non": [10, 161, 163, 167, 171, 178, 187, 193, 197, 199, 210, 220, 237, 243, 245, 247], "none": [1, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 48, 49, 50, 60, 68, 69, 85, 86, 89, 98, 99, 100, 112, 113, 149, 152, 153, 154, 155, 157, 158, 159, 160, 161, 163, 164, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 184, 185, 186, 187, 188, 189, 190, 191, 197, 199, 201, 202, 203, 204, 205, 209, 210, 211, 215, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "norm": [191, 199, 203], "normal": [180, 186, 188, 190, 197, 199, 203, 216, 218, 220, 228, 237], "notabl": 213, "note": [2, 9, 13, 14, 160, 168, 169, 170, 171, 174, 175, 176, 177, 178, 184, 187, 188, 191, 197, 202, 203, 204, 205, 209, 210, 211, 216, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 244], "note1": [227, 230, 235, 238, 242], "note2": [227, 230, 235, 238, 242], "notebook": [199, 225, 228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241], "noth": [18, 205, 209, 211, 248], "notic": [157, 199, 201], "notimplementederror": 19, "now": [161, 162, 169, 170, 185, 186, 190, 191, 199, 213, 216, 220, 221, 227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "np": [1, 2, 10, 11, 184, 186, 188, 197, 203, 216, 218, 220, 228, 230], "nprev": 191, "nscale": 169, "nullptr": 199, "num": [153, 154], "num_batch": [9, 12, 15, 171, 173, 178, 186, 187, 188, 189, 197, 198, 203, 220, 228, 229, 230, 236, 237, 238, 239, 242], "num_calibration_sampl": [186, 197, 216, 218, 220, 228], "num_candid": [11, 171, 178, 197], "num_channel": 213, "num_comp_ratio_candid": [18, 205, 209, 211, 232, 233, 234], "num_epoch": [189, 199, 221], "num_eval_sampl": [186, 216, 228], "num_iter": [1, 13, 165, 186, 187, 193, 199], "num_of_sampl": 188, "num_reconstruction_sampl": [18, 205, 232, 234], "num_sampl": [2, 163, 174, 187, 216, 218, 235, 237], "num_samples_for_phase_1": [14, 175, 188], "num_samples_for_phase_2": [14, 175, 188], "num_step": [153, 154], "num_val_samples_per_class": 230, "num_work": [186, 188, 189, 197, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "number": [1, 2, 9, 11, 12, 13, 14, 15, 18, 19, 153, 154, 155, 159, 161, 163, 164, 165, 171, 173, 174, 175, 177, 178, 180, 186, 187, 188, 189, 193, 194, 197, 198, 199, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 245, 247, 248], "numer": [158, 202, 244], "numpi": [166, 184, 185, 186, 188, 197, 203, 216, 218, 220, 227, 228, 229, 230, 231], "numpy_help": [190, 191], "nvidia": [180, 182, 184, 185, 220], "o": [184, 185, 186, 190, 191, 197, 199, 205, 209, 211, 216, 218, 220, 227, 228, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "o_proj": 198, "object": [1, 2, 7, 8, 9, 10, 13, 14, 19, 147, 148, 149, 155, 157, 159, 162, 163, 164, 165, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 186, 187, 188, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 209, 211, 215, 216, 217, 218, 219, 220, 221, 227, 230, 235, 236, 237, 239, 240, 241, 242, 244, 245, 247, 248], "observ": [10, 19, 155, 164, 170, 199, 203, 208, 218, 220, 221, 238, 244, 245], "obtain": [6, 158, 203, 206, 213, 243], "obvious": [205, 209, 211], "occur": [157, 198, 201, 213, 216], "occurr": [9, 168, 176, 203, 205], "oct": 208, "off": [12, 173, 183, 186, 189, 216, 220, 227, 235, 248], "offer": [9, 168, 176, 188, 197, 200, 203, 204, 217, 237, 244], "offset": [9, 19, 60, 150, 151, 152, 153, 154, 168, 169, 176, 184, 199, 203, 213, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 243, 245, 247], "offset_": [150, 151, 152, 153, 154], "often": [186, 188, 190, 191, 208, 221, 244], "older": [184, 219], "omit": [207, 222], "omniqu": [0, 156, 187, 199], "onc": [19, 149, 162, 169, 199, 205, 208, 219, 223, 231, 232, 233, 234, 239, 240, 241, 243, 247, 248], "one": [10, 12, 14, 16, 161, 162, 163, 169, 170, 173, 174, 175, 177, 184, 186, 187, 188, 190, 193, 195, 196, 199, 204, 205, 208, 209, 210, 211, 213, 215, 216, 218, 219, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 247], "ones": [193, 198, 244, 245], "ones_lik": [150, 151], "onli": [2, 9, 10, 147, 148, 149, 153, 154, 157, 159, 160, 161, 163, 164, 166, 168, 169, 170, 172, 174, 176, 177, 182, 183, 184, 186, 190, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 208, 210, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244, 245, 248], "onnx": [0, 1, 2, 3, 5, 6, 8, 9, 10, 14, 156, 158, 163, 170, 175, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 197, 199, 201, 202, 203, 204, 213, 215, 216, 218, 219, 220, 221, 222, 223, 226, 230, 243, 244, 245, 247, 248], "onnx_data": [186, 228], "onnx_data_gener": 220, "onnx_encoding_path": 220, "onnx_export_arg": [14, 158, 170, 175, 177, 188, 202, 220, 221], "onnx_file_nam": 223, "onnx_model": [185, 203, 218, 230], "onnx_model_path": 230, "onnx_output": 166, "onnx_qdq": 10, "onnx_util": 202, "onnxexportapiarg": [14, 158, 170, 175, 177, 188, 202, 220, 221], "onnxmodel": [9, 203], "onnxruntim": [10, 166, 183, 184, 185, 186, 197, 199, 202, 203, 218, 220, 227, 228, 229, 231], "onnxsim": [186, 190, 191, 202, 203, 216, 218, 220, 227, 228, 229, 231], "onto": 247, "op": [2, 10, 12, 14, 162, 170, 173, 174, 175, 177, 186, 188, 198, 199, 216, 220, 221, 222, 227, 228, 229, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 243, 245], "op_list": 222, "op_typ": [2, 8, 10, 174, 215, 216, 222, 227, 235], "op_type_map": 162, "open": 180, "opencv": 199, "oper": [8, 19, 160, 161, 162, 164, 180, 185, 187, 191, 192, 193, 197, 198, 199, 202, 215, 216, 218, 220, 222, 223, 224, 227, 231, 235, 240, 241, 245, 247, 248], "oppos": [229, 239], "opset": [166, 199], "opset_vers": [166, 170, 177, 220, 221], "opt": [195, 196], "optim": [1, 2, 11, 12, 13, 14, 18, 155, 165, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 205, 208, 210, 214, 216, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248], "optimized_accuraci": [188, 237], "optimum": 221, "option": [2, 8, 9, 10, 12, 13, 14, 15, 18, 19, 150, 151, 152, 153, 154, 155, 159, 161, 163, 166, 167, 168, 170, 171, 173, 174, 175, 176, 177, 178, 184, 185, 186, 187, 188, 189, 197, 199, 203, 204, 205, 209, 211, 215, 216, 218, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245], "optlearnedpositionalembed": [195, 196], "optmiz": [198, 227, 235], "orang": 212, "order": [19, 89, 162, 164, 200, 205, 219, 222, 223, 228, 230, 232, 233, 234, 236, 238, 240, 241, 242, 243, 247], "org": [161, 172, 184, 187, 193, 198, 199, 218, 220, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "organ": [180, 246], "origin": [18, 19, 160, 161, 164, 171, 178, 180, 192, 197, 198, 199, 202, 203, 205, 206, 207, 209, 210, 211, 218, 227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244, 245, 248], "ort": [10, 166, 186, 197, 220, 227, 228, 229, 231], "ort_disable_al": 166, "oscil": 189, "other": [149, 159, 160, 161, 163, 181, 184, 185, 187, 193, 194, 195, 196, 198, 199, 204, 207, 215, 219, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246], "otherwis": [150, 151, 153, 154, 159, 162, 170, 177, 191, 204, 213, 215, 220, 221, 224, 228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241, 248], "our": [167, 238, 244], "out": [9, 150, 151, 152, 153, 154, 155, 161, 168, 169, 170, 176, 177, 184, 185, 199, 203, 220, 221, 225], "out1": 219, "out2": 219, "out3": 219, "out_": [150, 151, 152, 153, 154], "out_channel": [159, 204, 215], "out_featur": [19, 160, 164, 170], "outlier": [192, 198, 203, 245], "outlin": [184, 194, 206, 244], "output": [2, 6, 7, 8, 9, 10, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 153, 155, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 174, 175, 176, 177, 178, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 197, 199, 203, 204, 205, 208, 209, 211, 212, 213, 215, 216, 218, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245], "output_bw": [14, 175, 188], "output_dir": 243, "output_dir_path": 243, "output_dlc": 243, "output_encod": 19, "output_nam": [166, 170, 177, 186, 190, 197, 216, 218, 220, 221, 227, 228, 229, 230, 231], "output_path": [165, 193, 243], "output_qtzr": 19, "output_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 163, 164, 170, 174, 216, 220, 221], "output_s": [48, 49, 50, 98, 99, 100], "outsid": [198, 222], "over": [9, 14, 153, 154, 164, 169, 175, 188, 192, 199, 203, 205, 207, 208, 209, 211, 215, 216, 244, 245], "overal": [191, 206, 207, 216, 224], "overfit": 197, "overflow": 199, "overhead": [232, 234, 235, 245], "overlin": [151, 154], "overload": [10, 153, 154, 170, 220, 221], "overrid": [14, 149, 161, 170, 175, 177, 188, 219, 220, 221, 235, 236, 239, 240, 241], "overridden": [19, 164, 222], "override_precis": [7, 218], "overtax": 248, "overview": [160, 214, 244, 246], "overwri": 220, "overwriiten": 220, "overwritten": [160, 196, 220], "own": [188, 198, 203, 247], "p": [218, 220, 227, 230, 235], "p1": 219, "p2": 219, "pack": 225, "packag": [0, 156, 182, 199, 216, 226], "pad": [161, 162, 170, 190, 191, 220, 221], "page": [181, 183, 184, 185, 206, 245, 248], "pair": [3, 16, 18, 190, 198, 205, 209, 211], "pairwis": 193, "pairwisedist": 108, "pandoc": 183, "paper": 198, "param": [2, 12, 18, 19, 89, 162, 163, 164, 168, 171, 173, 174, 176, 178, 186, 195, 197, 199, 203, 204, 205, 209, 211, 213, 216, 219, 227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 247], "param_bitwidth": 213, "param_bw": [14, 175, 188], "param_bw_override_list": [12, 173, 186], "param_encod": [160, 213], "param_nam": [9, 168, 176, 203], "param_name_": [9, 168, 176, 203, 230, 242], "param_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 164, 170, 204, 220, 221], "param_typ": [10, 185, 186, 197, 218, 220, 227, 228, 229, 231], "paramet": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 208, 209, 211, 213, 214, 215, 216, 217, 218, 219, 220, 222, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 248], "parameter": [10, 220], "parameter_quant": [2, 163, 174, 216], "parent": [19, 89, 164, 199, 230], "pareto": [2, 14, 174, 175, 188, 227, 235], "pareto_front": 188, "pareto_front_list": [216, 227, 235], "pars": [12, 170, 173, 177, 186, 220, 221], "part": [9, 168, 176, 203, 208, 220, 247], "partial": [14, 161, 175, 188, 199, 220], "particular": [159, 162, 204, 215, 216, 222, 227, 235], "pass": [2, 9, 10, 12, 13, 14, 15, 19, 89, 155, 158, 160, 161, 162, 163, 164, 168, 170, 171, 173, 174, 175, 176, 177, 178, 180, 186, 187, 188, 189, 195, 196, 197, 199, 202, 203, 204, 210, 216, 218, 219, 220, 221, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 248], "pass_calibration_data": [220, 221, 227, 229, 231, 235, 236, 238, 239, 240, 241, 242], "past": [199, 226], "patch": 213, "path": [2, 9, 10, 12, 14, 18, 157, 161, 163, 165, 168, 170, 173, 174, 175, 176, 177, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 201, 202, 203, 205, 209, 211, 216, 218, 219, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "path_to_aimet_repo": 230, "path_to_imagenet": [216, 218, 220, 221], "pathlik": 220, "pattern": [199, 208], "pcq": [9, 168, 176, 203], "pdf": [9, 168, 172, 176, 198, 199, 203], "peak": 218, "peft": [0, 156, 165, 193, 195, 196, 199], "peft_model": 167, "peft_model_id": [195, 196], "peftmixedmodel": 167, "penalti": 216, "pend": [13, 187], "pendyam": 208, "per": [8, 9, 18, 158, 159, 164, 168, 176, 180, 189, 190, 192, 199, 200, 202, 204, 205, 209, 210, 211, 213, 214, 215, 216, 218, 222, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 244, 245], "per_block": 213, "per_block_int_scal": 213, "per_channel": 213, "per_channel_quant": [164, 213, 222], "per_layer_mse_loss": [203, 230, 242], "per_layer_quant_dis": [203, 230, 242], "per_layer_quant_en": [203, 230, 242], "per_sample_weight": 60, "per_tensor": 213, "percent_to_flip": [7, 218], "percentag": [7, 180, 217, 218, 228, 229, 230, 231, 236, 239, 240, 241, 242, 244], "perform": [2, 3, 4, 9, 12, 14, 15, 17, 19, 89, 150, 151, 155, 164, 167, 168, 171, 173, 174, 175, 176, 178, 180, 181, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 203, 205, 206, 207, 208, 209, 211, 213, 214, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 242, 245, 246, 247, 248], "perform_per_layer_analysis_by_disabling_quant": [9, 203], "perform_per_layer_analysis_by_disabling_quant_wrapp": [168, 176, 203], "perform_per_layer_analysis_by_enabling_quant": [9, 203], "perform_per_layer_analysis_by_enabling_quant_wrapp": [168, 176, 203], "perhap": [227, 230, 235, 238, 242], "period": [12, 173, 186], "perman": 199, "persist": 155, "person": 180, "perspect": [187, 193, 227, 235], "phase": [2, 14, 163, 174, 175, 188, 208, 227, 235], "phase1": [2, 174, 216, 227, 235], "phase1_optim": [2, 174, 216, 227, 235], "phase2": 174, "phase2_revers": 174, "phi": 199, "phone": [180, 181], "php": [226, 227, 230, 235, 238, 242], "pick": [2, 174, 206, 207, 216, 227, 235], "pickl": [18, 205, 209, 211], "pictur": [228, 229, 230, 231, 236, 239, 240, 241, 242], "piec": 161, "pin": [183, 199], "pin_memori": [149, 161, 235], "pink": 212, "pinpoint": 203, "pip": [184, 185, 199, 210, 226], "pipelin": [14, 175, 188, 199, 202, 224, 245, 247], "piptool": 183, "pitr": 208, "pixelshuffl": 109, "pixelunshuffl": 110, "place": [2, 4, 12, 13, 17, 170, 172, 173, 174, 177, 186, 187, 191, 198, 199, 216, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "place_model": [195, 196], "placehold": [195, 196], "placement": [198, 199, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242], "plan": [184, 199, 247], "platform": [179, 180, 184, 203, 228, 229, 231, 236, 239, 240, 241], "pleas": [160, 162, 167, 170, 177, 184, 199, 203, 205, 209, 211, 220, 221, 227, 230, 231, 235, 238, 242, 243], "plot": [2, 157, 174, 199, 201, 203, 216, 227, 230, 235, 242], "pmatrix": [150, 151, 152, 153, 154], "point": [2, 9, 10, 19, 147, 148, 159, 160, 168, 174, 176, 177, 180, 190, 191, 192, 198, 199, 203, 204, 213, 216, 218, 220, 221, 224, 227, 229, 232, 233, 234, 235, 236, 238, 239, 240, 241, 244, 245, 247, 248], "pointer": [230, 242], "poissonnllloss": 111, "polish": 199, "poor": 244, "popul": 213, "popular": 194, "port": [199, 210], "portabl": 180, "portion": 191, "posit": [137, 138, 153, 154, 199], "possibl": [2, 3, 14, 162, 166, 170, 174, 175, 177, 188, 190, 203, 207, 216, 220, 221, 222, 223, 224, 227, 235, 237, 248], "post": [14, 175, 180, 182, 185, 188, 191, 199, 203, 208, 213, 215, 221, 228, 229, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 245, 248], "post_training_tf": [9, 12, 157, 168, 173, 176, 186, 199, 201, 203, 213, 227, 235, 238, 239], "post_training_tf_enhanc": [9, 12, 14, 168, 170, 173, 175, 176, 177, 186, 188, 203, 213, 220, 221, 227, 229, 230, 235, 236, 238, 239, 240, 242], "potenti": [210, 238], "power": [180, 199, 204, 213, 217, 224, 244], "pp": 208, "practic": [204, 208, 216, 227, 230, 232, 233, 234, 235, 237, 242], "pre": [184, 187, 190, 193, 197, 198, 199, 200, 202, 203, 205, 208, 214, 218, 220, 226, 228], "preced": [189, 192, 212, 219], "precis": [2, 7, 14, 150, 151, 152, 153, 154, 155, 163, 174, 175, 180, 182, 188, 199, 213, 221, 224, 228, 229, 230, 231, 236, 238, 239, 240, 241, 242], "precomput": [195, 220], "precursor": 190, "pred": 235, "pred_label": [186, 188, 197, 216, 218, 220, 228], "pred_prob": [186, 188, 216, 218, 220, 228], "predefin": 237, "predict": [180, 220], "prefer": [160, 184, 226, 247], "prefix": [12, 164, 170, 173, 177, 186, 220, 221], "prelu": [107, 199], "prepar": [13, 14, 161, 167, 175, 187, 188, 190, 198, 199, 216, 223, 246, 248], "prepare_model": [161, 235, 236, 238, 240, 241, 242], "prepared_model": 161, "prepend": [164, 226], "preprocess": [186, 188, 197, 216, 228], "prequantize_const": [10, 166], "prerequisit": 210, "presenc": 221, "present": [7, 158, 160, 162, 167, 202, 213, 218, 238], "preserv": [155, 161, 186, 199, 216, 218], "preserve_format": 149, "pretrain": [186, 188, 189, 190, 191, 197, 203, 216, 219, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 245], "pretti": [205, 209, 211], "prev": 191, "prev_conv_weight": 191, "prevent": [160, 161, 196, 199, 205], "previou": [2, 18, 169, 174, 180, 205, 206, 209, 211, 216, 224, 227, 235, 243, 244], "print": [19, 153, 154, 160, 161, 162, 164, 169, 170, 184, 185, 186, 188, 190, 191, 197, 203, 205, 209, 211, 216, 218, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "prior": [2, 174, 199, 213, 216, 227, 235, 236, 239, 240, 241], "privileg": 226, "probabl": [180, 199], "problem": [161, 199, 224, 248], "problemat": [161, 224], "proce": [14, 175, 188, 190, 220, 244, 248], "procedur": [199, 207, 210, 229, 232, 234, 239], "proceed": [190, 191, 216, 227, 229, 231], "process": [180, 188, 199, 202, 205, 207, 208, 216, 219, 220, 227, 228, 229, 231, 235, 236, 239, 240, 241, 244, 245, 247], "processor": [180, 182, 184, 185], "produc": [14, 147, 148, 157, 161, 175, 180, 188, 191, 200, 201, 203, 207, 213, 216, 227, 230, 235, 242, 245], "product": [180, 181, 206], "profil": [163, 182, 183, 216, 218, 243, 244], "progress": [199, 210], "project": 180, "promot": [199, 216], "prone": [188, 219], "pronounc": 217, "propag": [199, 212, 219], "propagate_encod": [14, 170, 175, 177, 188, 220, 221], "proper": [198, 199], "properli": [150, 151, 199, 202], "properti": [155, 160], "provid": [0, 2, 5, 9, 10, 12, 14, 19, 155, 156, 160, 162, 167, 168, 170, 173, 174, 175, 176, 177, 180, 182, 183, 184, 186, 188, 195, 197, 199, 203, 204, 206, 207, 208, 210, 213, 216, 218, 219, 220, 221, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248], "proxi": 161, "prune": [18, 180, 199, 206, 207, 209, 211, 212, 214, 226, 233], "psnr": [199, 218], "psnr_eval_fn": 218, "pt_model": [186, 190, 191, 197, 216, 218, 220, 227, 228, 229, 231], "pth": [170, 177, 202, 205, 209, 211, 220, 221], "ptq": [14, 175, 180, 181, 182, 187, 188, 191, 192, 193, 198, 199, 203, 208, 220, 221, 237, 245, 247, 248], "public": [0, 156, 199, 220], "publish": 199, "pure": 223, "purpos": [180, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "put": [174, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "py38": 184, "pylint": 198, "pypi": [185, 199], "pyproject": 183, "pytest": 183, "python": [180, 182, 184, 185, 199], "python3": [183, 184, 199, 226], "pythonpath": [202, 226], "pytorch": [0, 14, 156, 158, 161, 162, 164, 169, 170, 171, 175, 177, 178, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 209, 211, 215, 216, 218, 219, 220, 221, 222, 226, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248], "q": [10, 19, 147, 148, 149, 150, 151, 153, 154, 155, 160, 164, 169, 195, 196, 245], "q_": 245, "q_modul": 160, "q_output": 19, "q_proj": 198, "qadd": 164, "qairt": 243, "qat": [12, 173, 180, 182, 186, 189, 195, 196, 199, 224, 227, 231, 235, 245, 247, 248], "qat2": 199, "qc": 199, "qc_op": 10, "qc_quantize_op": 199, "qcquantizeop": [2, 10, 199, 216], "qcquantizewrapp": [160, 199], "qdo": 180, "qdq": [151, 155, 195, 199, 244, 247, 248], "qlinear": [19, 160, 164], "qmax": [150, 151, 153, 154, 169, 170, 220, 221], "qmin": [150, 151, 153, 154, 169, 170, 220, 221], "qmodul": 160, "qmul": 164, "qmult": 19, "qnn": [182, 199, 243, 247], "qol": 199, "qsim": [13, 187], "qtype": [7, 10, 218, 220], "qtzr": [169, 195], "quad": [150, 151, 152, 153, 154], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 244, 245, 246, 247, 248, 249], "qualiti": [227, 235], "quant": [2, 9, 10, 12, 14, 168, 170, 171, 173, 175, 176, 177, 178, 186, 188, 197, 199, 203, 216, 220, 221, 227, 228, 235, 236, 237, 238, 239, 240, 241], "quant_analyz": [0, 5, 156, 160, 203, 230, 242], "quant_analyzer_result": 203, "quant_dequ": 147, "quant_schem": [9, 10, 14, 157, 168, 170, 175, 176, 177, 184, 186, 187, 188, 193, 198, 201, 203, 213, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "quant_sim": [165, 193, 219], "quant_sim_model": [177, 220], "quant_stats_visu": [157, 201], "quant_wrapp": 160, "quantanalyz": [9, 160, 168, 176, 199, 200, 218, 244], "quantiz": [0, 1, 2, 5, 8, 9, 10, 12, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 154, 155, 156, 157, 159, 161, 163, 166, 167, 168, 170, 171, 173, 174, 175, 176, 177, 178, 180, 182, 184, 186, 187, 189, 190, 191, 193, 196, 197, 198, 199, 201, 202, 206, 208, 210, 217, 218, 219, 222, 223, 225, 226, 232, 233, 234, 237], "quantizablemultiheadattent": 199, "quantizaiton": [8, 215], "quantization_overrid": 243, "quantization_tf": 184, "quantizationdatatyp": [2, 14, 160, 163, 170, 174, 175, 177, 188, 216, 220, 221, 227, 235], "quantizationmixin": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 164, 195, 196], "quantizationmod": 184, "quantizationsim": [227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 244], "quantizationsimmodel": [1, 2, 7, 8, 9, 10, 11, 13, 14, 19, 157, 159, 160, 163, 165, 166, 168, 169, 170, 171, 175, 176, 177, 178, 185, 186, 187, 188, 189, 193, 195, 197, 198, 199, 201, 202, 203, 204, 213, 215, 216, 218, 219, 220, 221, 222, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245], "quantizationsimmodelv1": 160, "quantizationsimmodelv2": 160, "quantized_": [186, 187, 193], "quantized_dlc": 243, "quantized_linear": 19, "quantized_mobilenet_v2": [186, 197, 220], "quantized_model": [170, 220, 221], "quantized_repr": [19, 147, 148, 149], "quantizedadd": 164, "quantizedconv2d": [160, 164, 170, 204, 220, 221], "quantizedequant": [147, 160, 164, 169, 170, 184, 195, 204, 220, 221], "quantizedlinear": [19, 160, 164, 170, 204], "quantizedmaskedadd": 19, "quantizedmultipli": [19, 164], "quantizedoptlearnedpositionalembed": [195, 196], "quantizedrelu": 160, "quantizedsoftmax": 164, "quantizedtensor": [19, 147, 149, 150, 169], "quantizelinear": [10, 166, 199, 244], "quantizer_arg": 213, "quantizer_group": [2, 163, 174, 216], "quantizer_info": [2, 216], "quantizerbas": [19, 164, 169, 196], "quantizergroup": [2, 14, 163, 174, 175, 188, 216], "quantschem": [9, 10, 12, 14, 157, 168, 170, 173, 175, 176, 177, 185, 186, 187, 188, 193, 198, 199, 201, 203, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "quantsim": [0, 5, 6, 9, 12, 156, 157, 158, 160, 163, 167, 168, 173, 176, 180, 185, 186, 187, 189, 190, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 213, 215, 216, 217, 219, 221, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241, 244, 247, 248], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "quantsim_layer_output": 202, "quantsim_layer_output_util": 202, "quantsimmodel": [187, 193, 198], "quatiz": 247, "quic": [180, 184, 226], "quick": [169, 181, 182, 199, 220, 232, 234], "quickli": [181, 218, 227, 230, 235, 242], "qwa": 199, "qwen": [193, 198], "qwen2": [193, 199], "qwen2forcausallm": 198, "r": [2, 9, 167, 168, 174, 176, 183, 203, 216], "r1": [172, 198, 199], "r1_fusion_pair": 198, "r1_placement": 198, "r2": 198, "r3": 198, "r4": 198, "radic": 248, "rais": [19, 155, 188, 198, 216], "rand": [162, 187, 202, 235, 236, 238, 239, 240, 241, 242], "randint": [193, 198], "randn": [19, 148, 149, 150, 151, 161, 162, 164, 170, 184, 185, 186, 188, 189, 190, 191, 197, 203, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 237], "random": [184, 185, 203, 205, 230, 237], "random_split": [218, 220], "randperm": 235, "rang": [9, 153, 154, 157, 161, 168, 169, 176, 177, 180, 189, 191, 192, 193, 195, 196, 198, 199, 200, 201, 207, 216, 220, 221, 224, 227, 228, 229, 231, 235, 236, 237, 238, 239, 240, 244, 245], "rank": [18, 192, 194, 209, 211, 218], "rank_select": 211, "rank_select_schem": [18, 211], "rankselectschem": [18, 205, 209, 211], "rapidli": [182, 217], "rare": [199, 244], "rate": [199, 208, 221, 232, 233, 234, 238, 240, 241], "rather": [161, 197, 222, 238], "ratio": [18, 205, 206, 209, 211, 218, 232, 233, 234], "raw": [193, 195, 196, 198], "rceil": [153, 154], "re": [2, 169, 171, 174, 178, 197, 199, 216, 226, 227, 235, 248], "reach": [188, 237], "read": [203, 215], "reader": [227, 230, 235, 238, 242], "readi": [169, 170, 177, 198, 220, 221, 224, 227, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 244, 247], "readili": [227, 230, 235, 238, 242], "real": [147, 148, 185, 210, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "realiz": [163, 219], "realli": [227, 230, 238, 242], "reason": [162, 220, 221, 228, 229, 231, 236, 239, 240, 241, 248], "reassess": 244, "recalcul": 238, "recalibr": 218, "receiv": 161, "recent": 247, "recip": 199, "recogn": [162, 245], "recommend": [1, 2, 9, 10, 12, 166, 169, 173, 182, 184, 185, 186, 187, 189, 190, 191, 193, 203, 206, 216, 220, 224, 227, 228, 229, 231, 236, 237, 244, 247, 248], "recomput": [199, 228], "reconfigur": 199, "reconstruct": [12, 173, 186, 232, 234], "record": [9, 168, 176, 203], "recov": [199, 218, 224, 229, 232, 233, 234, 239, 247, 248], "recoveri": 248, "recurr": 199, "recurs": 199, "redefin": 162, "redesign": 199, "reduc": [1, 11, 164, 167, 180, 181, 186, 189, 191, 192, 197, 198, 199, 208, 212, 214, 217, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245], "reduct": [206, 232, 233, 234, 248], "redund": [190, 208], "reestim": [15, 189], "reestimate_bn_stat": [15, 189, 238], "refer": [158, 160, 167, 171, 177, 178, 197, 202, 204, 213, 216, 218, 220, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243], "reflect": [240, 241, 245], "reflectionpad1d": 117, "reflectionpad2d": 118, "reflectionpad3d": 119, "regard": [2, 174, 216, 228, 229, 230, 231, 236, 239, 240, 241, 242], "regardless": [187, 193], "regist": [19, 155, 187, 195, 196, 198, 199, 227], "regress": 205, "regular": [12, 19, 169, 170, 173, 177, 186, 220, 221, 227, 228], "rel": [18, 205, 206, 209, 211, 216, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 244], "relat": [160, 205, 209, 211, 245], "relationship": 204, "releas": [183, 184, 193, 226], "release_tag": 226, "relev": [232, 233, 234, 247], "reli": [160, 197, 221, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "reliabl": 184, "reload": 239, "relu": [115, 160, 161, 162, 191, 199, 212, 222, 223], "relu1": [162, 223], "relu2": [162, 223], "relu6": [116, 190, 191, 199], "remain": [13, 160, 171, 178, 186, 187, 191, 193, 197, 198, 199, 218, 221], "remov": [14, 15, 161, 175, 180, 188, 189, 190, 196, 199, 205, 212, 213, 220, 245], "remove_all_quant": 196, "remove_column": [193, 195, 196, 198], "renam": 199, "render": 199, "reorgan": 199, "repeat": [167, 205, 244, 248], "repeatedli": 247, "replac": [10, 160, 161, 164, 167, 195, 199, 204, 218, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "replace_lora_layers_with_quantizable_lay": [167, 195, 196], "replicationpad1d": 120, "replicationpad2d": 121, "replicationpad3d": 122, "repo": 230, "report": [199, 218, 219, 244, 248], "repositori": 226, "repres": [2, 9, 10, 14, 147, 148, 149, 155, 163, 164, 168, 170, 174, 175, 176, 177, 180, 185, 188, 197, 199, 203, 205, 207, 209, 211, 213, 216, 218, 219, 220, 221, 223, 230, 242, 243, 245], "represent": [147, 148, 149, 155, 169, 180, 199, 243, 245], "requant": 190, "request": 219, "requir": [2, 18, 19, 158, 160, 161, 167, 169, 170, 177, 180, 183, 184, 186, 187, 190, 191, 193, 195, 197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 213, 216, 217, 218, 219, 220, 221, 222, 223, 227, 230, 232, 233, 234, 235, 236, 239, 240, 241, 242, 244, 245, 247, 248], "requires_grad": [148, 149, 161, 191, 199, 220], "requires_grad_": [160, 195, 196], "rerun": 162, "rescal": 191, "research": 180, "resid": 199, "residu": 205, "resiz": [186, 188, 197, 199, 216, 218, 220, 228, 235], "resnet": [170, 206, 220, 221, 227, 228, 229, 230, 231], "resnet18": [170, 203, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "resnet18_after_adaround": 228, "resnet18_after_cle_bc": [236, 237, 239], "resnet18_after_qat": [238, 240, 241], "resnet18_mixed_precis": [227, 235], "resnet18_quantsim_analysi": 230, "resolv": [162, 199], "resort": 224, "resourc": [180, 227, 235, 238, 248], "respecit": [227, 235], "respect": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 164, 169, 171, 178, 192, 197, 199, 203], "respond": [230, 242], "respons": [2, 174, 180, 208, 216, 227, 235, 248], "rest": [199, 224], "restor": [177, 199, 224, 232, 233, 234, 245, 247], "restrict": [204, 215], "result": [2, 9, 14, 19, 147, 148, 149, 168, 169, 170, 174, 175, 176, 177, 186, 187, 188, 189, 191, 193, 195, 196, 197, 198, 200, 203, 205, 206, 208, 209, 211, 216, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245], "results_dir": [2, 9, 14, 168, 174, 175, 176, 188, 203, 216, 227, 230, 235, 242], "retain": [180, 216], "retest": 248, "retrain": [180, 223, 247], "retriev": 19, "retrieve_context": 243, "return": [2, 3, 6, 9, 10, 12, 13, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 157, 158, 159, 161, 162, 163, 165, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 193, 195, 196, 197, 198, 201, 202, 203, 204, 205, 207, 209, 211, 215, 216, 218, 220, 221, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "return_dict": [193, 195, 196, 198], "reus": [161, 162, 223, 227, 235], "reveal": 224, "revert": 224, "revis": 213, "revisit": 206, "rework": 213, "rewrit": 162, "rfloor": [150, 151, 152, 153, 154, 155, 245], "rgb": [188, 216, 223], "rgb_output": 223, "right": [150, 151, 152, 153, 154, 155, 164, 169, 204, 212, 216, 227, 235, 237, 245], "rmsnorm": [198, 199], "rmsnorm_fusion_pair": 198, "rmsnorm_linear_pair": 198, "rmsnormal": 199, "rnn": [112, 199], "rnncell": 113, "robust": [166, 197, 199, 247], "root": [183, 186, 197, 199, 235, 237], "rotat": [172, 192, 198, 199], "rough": [227, 235], "roughli": [14, 163, 174, 175, 188, 216], "round": [1, 12, 14, 18, 173, 175, 177, 180, 188, 199, 203, 209, 211, 244, 245], "round_nearest": 184, "rounding_mod": [14, 170, 175, 177, 188, 199, 220, 221], "roundingmod": 184, "routin": [220, 227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "rrelu": 114, "rtype": [10, 147, 148, 149, 163, 174, 216], "rule": [199, 204, 222, 247], "run": [2, 9, 10, 13, 14, 15, 18, 150, 151, 160, 161, 162, 163, 164, 166, 168, 169, 170, 174, 175, 176, 177, 180, 182, 185, 186, 187, 188, 189, 197, 199, 202, 204, 205, 208, 209, 210, 211, 213, 215, 216, 217, 218, 219, 220, 225, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "run_forward_pass": [10, 170, 220, 221], "run_infer": [14, 175, 188, 237], "runnabl": 247, "runtim": [11, 19, 169, 170, 177, 180, 181, 182, 190, 197, 199, 202, 203, 204, 205, 206, 208, 209, 211, 213, 215, 216, 218, 220, 221, 227, 228, 229, 231, 235, 236, 239, 240, 241, 243, 244, 246, 247, 248], "runtimeerror": [155, 170, 220, 221], "s2": 187, "s3": 187, "s_1": 204, "s_2": 204, "s_n": 204, "safe": 149, "safetensor": [165, 193], "sai": [161, 206, 219, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "sake": [202, 229, 239], "same": [9, 14, 19, 89, 147, 148, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 173, 174, 175, 176, 177, 178, 188, 191, 192, 194, 198, 202, 203, 204, 215, 219, 220, 221, 222, 227, 229, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241], "sampl": [1, 2, 9, 10, 11, 13, 14, 157, 163, 164, 168, 171, 174, 175, 176, 177, 178, 179, 185, 186, 187, 188, 190, 191, 193, 195, 196, 197, 198, 199, 201, 203, 205, 216, 218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245], "sandeep": 208, "saniti": [169, 199], "satisfactori": [186, 187, 189, 193, 197, 224, 248], "satisfi": [161, 163, 188, 204, 219, 237], "saurabh": 208, "save": [2, 6, 9, 10, 12, 14, 18, 157, 158, 165, 168, 170, 173, 174, 175, 176, 177, 186, 188, 193, 199, 200, 201, 202, 203, 205, 209, 211, 216, 218, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247], "save_checkpoint": 177, "save_model_as_external_data": 199, "save_path": [157, 201], "saved_eval_scores_dict": [18, 205, 209, 211], "scalar": [9, 168, 176, 199, 203], "scale": [19, 148, 149, 150, 151, 152, 153, 154, 155, 165, 169, 180, 184, 191, 192, 193, 199, 203, 204, 213, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 243, 245, 247], "scale_": [150, 151, 152, 153, 154, 169], "scenario": [212, 228, 229, 230, 231, 236, 239, 240, 241, 242], "scene": 247, "schedul": [199, 232, 233, 234, 238, 240, 241], "schema": 199, "scheme": [9, 10, 12, 14, 18, 158, 168, 170, 173, 175, 176, 177, 180, 186, 188, 199, 202, 203, 204, 205, 208, 209, 211, 220, 221, 227, 230, 232, 233, 234, 235, 238, 242], "scope": 161, "score": [2, 9, 14, 18, 168, 174, 175, 176, 188, 203, 205, 207, 208, 209, 210, 211, 216, 227, 228, 229, 231, 237, 238], "script": 238, "sdk": [181, 227, 235, 244, 247], "search": [2, 14, 171, 174, 175, 178, 188, 192, 197, 199, 216, 221, 222, 227, 232, 233, 234, 235, 238, 240, 241, 244, 247], "searcher": 208, "sec": [227, 235], "second": [10, 12, 164, 167, 170, 171, 173, 178, 186, 190, 197, 204, 216, 220, 221, 230, 242], "secondari": 199, "section": [2, 162, 174, 195, 196, 199, 204, 214, 215, 216, 222, 225, 227, 235, 237, 244, 245, 246, 248], "see": [0, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 156, 177, 181, 184, 185, 191, 199, 204, 205, 208, 209, 210, 211, 215, 220, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245, 248], "seed": 205, "seem": 208, "seen": [203, 230, 242], "select": [14, 18, 175, 180, 184, 188, 195, 196, 203, 206, 209, 210, 211, 212, 215, 217, 218, 221, 222, 227, 235, 243, 244, 245], "select_param": [18, 211], "self": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 161, 162, 188, 193, 198, 216, 223, 237], "selu": 123, "sens": [164, 216], "sensit": [2, 7, 9, 168, 174, 176, 189, 199, 200, 207, 214, 217, 221, 227, 229, 230, 235, 239, 244, 245, 247, 248], "separ": [12, 160, 161, 162, 170, 173, 177, 186, 189, 199, 203, 220, 221, 224], "separableconv2d": 199, "seq_length": [193, 195, 196, 198], "seq_ms": [0, 156, 160, 197], "seqms": [192, 197, 199], "seqmseparam": [171, 178, 197], "sequenc": [170, 188, 192, 220, 221, 222, 237], "sequenti": [11, 160, 171, 178, 190, 191, 199, 221, 222, 224], "seri": [14, 163, 170, 175, 177, 188, 219, 220, 221, 237, 243], "serial": [220, 243], "serializetostr": [202, 218, 227, 228, 229, 231], "serv": [164, 188, 198, 203, 210], "servic": 219, "sess": [10, 166, 220, 227, 228, 229, 231], "sess_opt": 166, "session": [10, 185, 186, 197, 199, 202, 203, 216, 218, 220, 227, 228, 229, 230, 231], "sessionopt": 166, "set": [1, 2, 7, 8, 9, 10, 11, 14, 18, 19, 155, 157, 159, 160, 161, 162, 163, 164, 166, 168, 174, 175, 176, 177, 180, 185, 186, 188, 194, 195, 196, 197, 199, 201, 203, 204, 205, 206, 207, 208, 209, 211, 213, 215, 216, 217, 218, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248], "set_activation_quantizers_to_float": [159, 204], "set_adapt": 167, "set_adaround_param": [14, 175, 188, 237], "set_and_freeze_param_encod": 236, "set_blockwise_quantization_for_weight": [159, 204, 215], "set_default_kernel": 19, "set_export_param": [14, 175, 188], "set_extra_st": 155, "set_grouped_blockwise_quantization_for_weight": [0, 5, 159, 215], "set_kernel": 19, "set_mixed_precision_param": [14, 175, 188], "set_model_input_precis": [163, 219], "set_model_output_precis": [163, 219], "set_model_preparer_param": [14, 175, 188], "set_precis": [163, 219], "set_quant": 199, "set_quant_scheme_candid": [14, 175, 188], "set_quantizers_to_candid": [2, 163, 174, 216], "set_rang": [147, 148], "set_transform": [188, 216], "settabl": 245, "setup": [160, 183, 184, 185, 190, 216], "sever": [162, 164, 197, 199, 203, 206, 223, 244], "sgd": [189, 221], "sh": 183, "shall": 213, "shape": [10, 16, 17, 18, 147, 148, 149, 150, 151, 155, 159, 160, 161, 162, 164, 169, 170, 186, 190, 191, 195, 197, 203, 204, 205, 209, 211, 215, 216, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "share": [162, 164, 198, 204], "sharp": 208, "sharpli": [232, 233, 234, 248], "shift_label": [195, 196], "shift_logit": [195, 196], "should": [9, 13, 18, 19, 89, 149, 158, 160, 161, 164, 168, 170, 171, 176, 177, 178, 184, 185, 187, 197, 198, 202, 203, 205, 206, 209, 211, 213, 220, 221, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 248], "shouldn": 169, "show": [162, 167, 198, 200, 204, 207, 216, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "showcas": [226, 230, 242], "shown": [160, 167, 183, 184, 185, 188, 203, 212, 216, 223, 224, 230, 242], "shuffl": [186, 187, 193, 197, 198, 205, 216, 218, 220, 228], "side": 212, "sigmoid": [125, 161], "sign": [153, 154, 160, 169, 245], "signal": 218, "signatur": [10, 18, 19, 153, 154, 170, 188, 199, 203, 205, 209, 211, 220, 221, 232, 233, 234], "signific": [216, 224], "significantli": [199, 221, 244], "silu": 124, "sim": [1, 2, 7, 8, 9, 10, 11, 13, 157, 159, 160, 163, 166, 167, 168, 170, 171, 174, 176, 177, 178, 185, 186, 187, 188, 189, 193, 196, 197, 198, 199, 201, 203, 204, 213, 215, 216, 218, 219, 220, 221, 228, 237], "sim1": 160, "sim2": 160, "sim_model": [236, 238, 239, 240, 241, 242], "similar": [167, 169, 180, 184, 198, 215, 244, 245], "similarli": [224, 227, 230, 235, 238, 242, 244, 248], "simpl": [161, 203, 205, 209, 211, 230, 242], "simpler": 160, "simpli": [9, 161, 168, 176, 177, 203, 218, 220, 227, 235, 238], "simplic": [167, 202], "simplif": [191, 244], "simplifi": [1, 2, 9, 10, 160, 186, 190, 191, 202, 203, 216, 218, 220, 243], "simuat": [228, 229, 236, 239, 240, 241], "simul": [10, 155, 159, 164, 167, 170, 177, 180, 182, 186, 187, 189, 190, 193, 197, 198, 199, 200, 201, 202, 204, 213, 216, 217, 220, 221, 222, 223, 225, 232, 233, 234, 242, 244, 248], "sinc": [170, 198, 204, 206, 216, 220, 221, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 245], "singl": [2, 6, 14, 158, 161, 163, 174, 175, 188, 199, 202, 203, 204, 205, 207, 208, 209, 211, 213, 215, 216, 230, 242, 245], "singular": [208, 209, 211, 233, 234], "situat": 203, "six": 222, "size": [8, 9, 149, 150, 151, 152, 153, 154, 159, 162, 164, 169, 180, 187, 193, 203, 204, 209, 211, 213, 214, 215, 220, 236, 244, 247, 248], "skbuild_build_target": 183, "skew": [228, 229, 231, 236, 239, 240, 241], "skip": [12, 14, 171, 173, 175, 178, 183, 186, 188, 195, 197, 199, 205, 229, 239, 244], "skipped_optim": 190, "slightli": [194, 227, 235], "slim": 199, "slow": 208, "slower": 199, "small": [10, 170, 189, 194, 197, 204, 218, 220, 221, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 244], "smaller": [2, 12, 173, 174, 186, 191, 208, 209, 211, 216, 217, 224, 227, 235, 247], "smoothl1loss": 126, "snapdragon": [227, 235], "snippet": [161, 199, 227, 235], "snpe": 199, "so": [9, 10, 12, 13, 160, 161, 162, 164, 170, 173, 177, 180, 184, 186, 187, 190, 191, 195, 196, 198, 199, 203, 205, 207, 209, 211, 220, 221, 227, 230, 232, 233, 234, 235, 236, 238, 240, 241, 242, 243, 248], "softmarginloss": 127, "softmax": [128, 164, 235], "softmax2d": 129, "softmin": 130, "softplu": [131, 161], "softshrink": 132, "softsign": 133, "softwar": [180, 181, 199], "sole": 203, "solid": 221, "solut": [207, 216, 221, 224, 235], "some": [18, 160, 161, 164, 169, 191, 195, 196, 205, 207, 209, 211, 212, 216, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 244, 245, 247, 248], "someth": [177, 203, 208], "sometim": [203, 205, 208, 227, 228, 229, 231, 235, 236, 239, 240, 241, 248], "somewher": 248, "soon": 188, "sort": [13, 187, 218], "sourc": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 161, 162, 163, 164, 166, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 186, 188, 189, 190, 191, 197, 198, 199, 201, 202, 203, 204, 205, 209, 211, 215, 216, 218, 219, 220, 221, 224], "space": [18, 205, 209, 211, 216, 245], "spars": 199, "spatial": [18, 199, 205, 206, 207, 211, 214, 226], "spatial_svd": [18, 205, 209, 211, 233, 234], "spatial_svd_auto_mod": 209, "spatial_svd_manual_mod": 209, "spatialsvdparamet": [18, 205, 209, 211, 233, 234], "spconv": 199, "special": [14, 161, 175, 180, 188], "specif": [9, 14, 18, 19, 159, 162, 164, 168, 170, 175, 176, 177, 183, 188, 192, 194, 198, 199, 203, 204, 205, 209, 211, 215, 220, 221, 222, 227, 228, 235, 236, 238, 243, 244, 245, 248], "specifi": [2, 9, 12, 18, 150, 151, 152, 153, 154, 155, 163, 168, 170, 173, 174, 176, 177, 186, 188, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 216, 219, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "speed": [18, 190, 199, 205, 208, 209, 211, 214, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "speedup": [171, 178, 197, 227, 235], "spinquant": [0, 156, 199], "spinquant_optim": 198, "split": [171, 178, 186, 188, 189, 193, 195, 196, 197, 198, 216, 218, 220, 221, 228, 245], "sqnr": [2, 14, 163, 171, 174, 175, 178, 188, 197, 216, 227, 235, 245], "sqnr_metric": [7, 218], "sqrt": 161, "squar": [200, 245], "squeez": 235, "ssvd": 206, "ssvd_comp_stat": 234, "ssvd_compressed_model": 234, "ssvd_cp_compressed_model": 234, "ssvd_cp_finetuned_model": 234, "ssvd_finetuned_model": 234, "stabl": [161, 166, 221, 238], "stack": [179, 216, 230], "stand": 199, "standalon": 199, "standard": [155, 161, 164, 180, 231, 240, 241], "start": [2, 12, 153, 154, 161, 162, 167, 173, 174, 181, 186, 199, 206, 216, 221, 222, 225, 226, 227, 230, 232, 233, 234, 235, 238, 240, 241, 242, 244, 245, 248], "start_beta": [12, 173, 186], "stat": [9, 15, 157, 189, 201, 203, 205, 209, 211], "statatist": 238, "state": [155, 171, 178, 179, 182, 197, 216, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242], "state_dict": 155, "stateless": 223, "statement": 161, "static": [18, 161, 162, 199, 205, 209, 211], "staticgridperchannelquant": 160, "staticgridquant": 160, "staticgridquantwrapp": 160, "staticmethod": [161, 227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "statist": [9, 10, 15, 18, 150, 151, 155, 157, 164, 168, 170, 176, 189, 192, 200, 201, 205, 209, 211, 220, 221, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241], "std": [186, 188, 197, 216, 218, 220, 228, 235, 237], "step": [1, 9, 153, 154, 161, 163, 167, 168, 171, 176, 178, 183, 184, 185, 191, 194, 195, 196, 205, 206, 207, 208, 215, 217, 224, 226, 227, 229, 230, 231, 235, 238, 242, 243, 245, 246, 247], "still": [160, 204, 224, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 248], "stochast": 177, "stop": [12, 173, 186, 188, 244], "stopiter": [188, 198, 216], "storag": [214, 215, 245], "store": [12, 148, 163, 170, 173, 177, 186, 219, 220, 221, 227, 235], "str": [1, 2, 6, 7, 8, 9, 10, 11, 12, 14, 155, 157, 158, 161, 163, 165, 168, 170, 171, 173, 174, 175, 176, 177, 178, 186, 188, 193, 197, 201, 202, 203, 215, 216, 218, 219, 220, 221, 223, 228], "str_idx": 188, "straightforward": [184, 248], "strateg": 216, "strategi": 248, "stream": [186, 188, 189], "streamlin": 244, "strict": [8, 155, 163, 215, 219, 220, 222, 245], "strict_symmetr": [184, 222], "strict_valid": [14, 175, 188], "strictli": [155, 198, 218, 220], "stride": [149, 161, 162, 170, 190, 191, 220, 221], "strike": 208, "string": [10, 163, 170, 177, 213, 219, 220, 222], "strongli": [161, 169, 228, 236], "structur": [14, 161, 164, 175, 180, 188, 208, 219, 230, 234, 242], "stude": 225, "sub": [230, 242], "subbackward0": 169, "subclass": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 169, 199], "subdirectori": [227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "subfold": [227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "sublay": 162, "submit": 243, "submodul": 223, "subpackag": [19, 199], "subsequ": [147, 180, 186, 190, 191, 198, 199, 220, 222, 227, 229, 235], "subset": [9, 10, 168, 170, 176, 189, 203, 204, 212, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "subsetrandomsampl": 237, "subsidiari": 181, "substanti": 213, "success": 237, "successfulli": [170, 202, 220, 221], "sudo": 226, "suffic": [228, 229, 231, 236, 239, 240, 241], "suffici": [203, 220, 245, 247], "suggest": [177, 207, 208, 232, 234, 244, 248], "suit": [180, 184, 188, 195, 237], "suitabl": [180, 244], "sum": [186, 188, 197, 216, 218, 220, 221, 228, 235, 237], "summari": [181, 199, 208], "sun": 208, "super": [19, 161, 162, 188, 198, 199, 216], "supergroup": [199, 245], "suppli": [204, 215], "support": [1, 2, 7, 9, 12, 18, 19, 161, 162, 163, 166, 168, 170, 171, 172, 173, 174, 176, 177, 178, 181, 184, 185, 186, 187, 188, 189, 190, 193, 197, 198, 199, 201, 203, 204, 205, 206, 208, 209, 211, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 227, 235, 238, 240, 241, 244, 245, 248], "supported_kernel": [2, 174, 216, 227, 235], "supported_kernel_op": [163, 174, 216], "supported_module_dict": 198, "suppos": [2, 174, 191, 216, 227, 235], "suscept": 221, "svd": [18, 199, 205, 206, 207, 214, 226], "swap": 199, "sweep": [11, 197], "switch": [199, 204], "sy": 230, "symbol": 161, "symbolic_trac": [14, 161, 175, 188], "symfp": [171, 178, 197], "symmetr": [19, 147, 148, 149, 150, 151, 159, 160, 164, 169, 170, 195, 199, 204, 213, 215, 220, 221, 222, 245], "symmetri": [160, 171, 178, 197, 199], "symqt": [171, 178, 197], "syntax": 213, "system": 183, "systemat": 180, "t": [2, 10, 14, 19, 161, 169, 170, 171, 174, 175, 178, 183, 188, 193, 195, 196, 197, 198, 199, 203, 216, 220, 221, 222, 227, 230, 235, 238, 242, 247], "tabl": [157, 198, 201, 210, 213, 226], "tag": 226, "take": [1, 2, 10, 13, 14, 18, 19, 159, 162, 163, 166, 167, 169, 170, 174, 175, 186, 187, 188, 193, 198, 199, 204, 205, 207, 208, 209, 211, 212, 215, 216, 218, 219, 220, 221, 224, 225, 227, 230, 232, 233, 234, 235, 238, 242, 244, 247], "taken": [13, 187, 212], "tanh": [134, 199], "tanhshrink": 135, "tap": [168, 176, 203], "tar": [18, 211, 218, 220, 230], "target": [10, 18, 30, 31, 37, 51, 53, 70, 76, 77, 81, 82, 93, 94, 102, 103, 104, 105, 106, 111, 126, 127, 167, 170, 174, 177, 180, 187, 188, 190, 193, 197, 199, 200, 202, 205, 206, 207, 208, 209, 211, 213, 215, 216, 220, 221, 224, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 244, 245, 248], "target_comp_ratio": [18, 205, 209, 211, 232, 233, 234], "target_data": [235, 236, 238, 239, 240, 241, 242], "target_length": 37, "target_modul": 167, "task": [180, 210, 213, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 244], "taxonomi": 208, "tbd": 201, "techniqu": [14, 175, 180, 182, 186, 187, 188, 190, 191, 193, 194, 195, 197, 198, 199, 203, 205, 206, 209, 211, 221, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 245, 247, 248], "technologi": [180, 181], "tempfil": 199, "temporari": [10, 186, 190, 191, 216, 220], "temporarili": [160, 196], "tend": 221, "tensor": [1, 2, 10, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 158, 161, 162, 163, 164, 166, 168, 169, 170, 173, 174, 175, 176, 177, 186, 187, 188, 190, 191, 193, 195, 196, 199, 202, 203, 204, 205, 209, 211, 213, 214, 216, 219, 220, 221, 222, 223, 224, 227, 229, 230, 235, 239, 240, 241, 242, 245], "tensor_nam": 213, "tensor_quant": 160, "tensorflow": [180, 181, 190, 199, 202, 204, 213, 215, 216, 218, 226, 242, 243, 244, 245, 247, 248], "tensorquantizationsimforpython": 184, "term": [170, 181, 204, 209, 211, 214, 220, 221], "termin": 226, "test": [9, 168, 176, 193, 195, 196, 203, 247, 248], "test_dataload": [193, 195, 196], "test_dataset": [193, 195, 196], "text": [150, 151, 152, 153, 154, 193, 195, 196, 198], "tf": [2, 9, 168, 176, 199, 203, 216, 220, 227, 230, 235, 238, 242], "tf_enhanc": [10, 220, 227, 235, 238], "tfe": 199, "tfencod": 160, "tflite": [227, 235], "tfoplambda": 199, "than": [12, 18, 160, 161, 162, 173, 174, 177, 186, 197, 198, 199, 204, 205, 206, 209, 211, 215, 216, 219, 221, 222, 227, 228, 235, 236, 238, 244, 247], "thei": [161, 162, 194, 195, 196, 210, 215, 221, 224, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 244, 245, 247], "them": [14, 155, 160, 161, 162, 164, 175, 180, 188, 191, 203, 208, 214, 218, 219, 220, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "theme": 199, "therebi": [216, 227, 235], "therefor": [2, 174, 190, 206, 216, 227, 235], "theta_": [150, 151], "thi": [2, 6, 9, 10, 11, 12, 13, 14, 18, 19, 89, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 181, 183, 184, 185, 186, 187, 188, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241, 243, 244, 245, 246, 247, 248], "thing": [228, 229, 231, 232, 233, 234, 236, 239, 240, 241, 243], "those": [9, 168, 176, 195, 203, 218, 222, 244, 247], "though": [19, 169, 204, 222, 227, 235, 248], "three": [161, 206, 227, 232, 233, 234, 235, 247], "threshold": [136, 157, 188, 201, 244], "through": [2, 10, 149, 160, 161, 163, 164, 168, 174, 176, 184, 186, 191, 195, 196, 198, 203, 215, 216, 218, 219, 220, 221, 225, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 244, 245, 248], "throughout": [13, 180, 187, 221, 222], "throw": [19, 169, 170, 199, 220, 221], "thrown": [199, 220], "thu": [147, 148, 149, 216], "tie_word_embed": 198, "tijmen": 208, "till": [2, 174, 216, 227, 235, 244], "time": [18, 149, 161, 162, 167, 182, 188, 199, 203, 204, 205, 208, 209, 210, 211, 215, 218, 219, 221, 230, 237, 242, 244, 247], "tmp": [9, 14, 168, 175, 176, 183, 185, 186, 188, 190, 191, 203, 216, 242], "tmpdir": 167, "to_arrai": [190, 191], "to_list": [2, 163, 174, 216], "to_onnx_qdq": [10, 199], "todo": [163, 219], "toggl": 183, "token": [193, 195, 196, 198, 224], "tokenized_dummy_text": [195, 196], "toler": [188, 206, 237], "tolist": 235, "toml": 183, "too": [204, 232, 234], "tool": [168, 176, 180, 182, 183, 199, 203, 212, 243, 246, 248], "toolchain": 244, "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 243, 244, 245, 246, 247, 248, 249], "tooltip": 199, "top": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 159, 163, 165, 168, 171, 172, 173, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 191, 193, 197, 198, 199, 203, 204, 205, 209, 210, 211, 215, 216, 218, 219, 220, 221, 227, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242], "top1": 237, "top1_accuraci": 235, "topk": [235, 237], "torch": [14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 166, 169, 170, 174, 175, 177, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 202, 203, 204, 205, 209, 211, 215, 216, 218, 219, 220, 221, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "torch_stabl": 184, "torchscript": [14, 158, 170, 175, 177, 180, 188, 202, 220, 221], "torchvis": [170, 185, 186, 188, 189, 190, 191, 197, 203, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "total": [195, 196, 207, 216, 220, 245], "total_length": [193, 195, 196, 198], "total_sampl": [186, 197, 216, 218, 220, 228], "totensor": [186, 188, 197, 216, 218, 220, 228, 235, 237], "toward": 224, "tpu": 180, "tqdm": [186, 195, 196, 197, 216, 218, 220, 221, 228, 235, 237], "trace": [14, 157, 161, 175, 188, 201, 223], "traceabl": [161, 195, 196, 223], "traceback": 161, "traceerror": 161, "tracer": 161, "track": [203, 230, 242], "track_running_stat": [190, 191], "trade": [12, 173, 186, 216, 227, 235], "tradeoff": [180, 216, 227, 235, 248], "train": [9, 10, 14, 15, 18, 165, 168, 170, 175, 176, 180, 182, 185, 186, 187, 188, 189, 191, 193, 194, 197, 198, 199, 200, 202, 203, 205, 208, 209, 211, 215, 216, 218, 224, 226, 227, 235, 237, 245, 248], "train_dataload": [170, 193, 195, 196, 198, 220, 221], "train_dataset": [193, 195, 196, 198], "train_flag": [18, 205, 209, 211], "train_load": [205, 238], "train_model": [18, 205, 209, 211], "train_one_epoch": [195, 196], "trainabl": [187, 193, 194, 195, 196, 241], "trainer": [18, 205, 209, 211, 226, 232, 233, 234, 238, 240, 241], "training_range_learning_with_tf_init": [170, 187, 193, 198, 220, 221, 238, 241], "trainingextens": 183, "trainingmod": [227, 231], "transact": 208, "transform": [161, 186, 188, 193, 195, 196, 197, 198, 199, 204, 216, 218, 220, 228, 235, 236, 237, 240, 241, 242], "transpos": 169, "trap": 197, "travers": 219, "treat": 221, "tri": [182, 208, 232, 233, 234], "tripletmarginloss": 137, "tripletmarginwithdistanceloss": 138, "true": [2, 10, 14, 18, 19, 148, 149, 150, 151, 155, 159, 160, 161, 162, 163, 164, 166, 169, 170, 174, 175, 177, 184, 186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 203, 204, 205, 209, 211, 213, 215, 216, 219, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "true_quant": [195, 196], "truli": 161, "trust_remote_cod": [193, 198], "try": [18, 185, 188, 190, 191, 205, 206, 208, 209, 211, 216, 224, 227, 229, 231, 232, 233, 234, 235, 244, 247], "tune": [18, 167, 177, 181, 205, 206, 209, 211, 221, 228, 229, 231, 236, 237, 239, 240, 241, 244, 247, 248], "tuner": 195, "tupl": [2, 3, 6, 8, 9, 12, 13, 14, 16, 17, 18, 86, 150, 151, 152, 153, 154, 158, 159, 162, 163, 168, 169, 170, 173, 174, 175, 176, 177, 185, 186, 187, 188, 190, 191, 193, 198, 202, 203, 204, 205, 209, 211, 215, 216, 219, 220, 221, 223, 227, 230, 235, 242], "turn": [182, 199, 222], "tutori": 179, "tweak": [167, 244], "twice": 167, "two": [13, 18, 160, 161, 162, 163, 174, 180, 182, 187, 191, 198, 203, 207, 208, 209, 211, 216, 219, 221, 228, 229, 230, 231, 232, 233, 234, 236, 237, 239, 240, 241, 242, 243, 245, 247], "txt": [163, 183, 219, 243], "ty": 199, "type": [1, 2, 3, 8, 9, 10, 12, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 157, 159, 160, 161, 162, 163, 164, 168, 170, 171, 173, 174, 175, 176, 177, 178, 180, 186, 187, 188, 189, 190, 197, 198, 199, 201, 203, 204, 205, 208, 209, 210, 211, 215, 216, 220, 221, 222, 227, 232, 233, 234, 235, 237, 240, 241, 243, 247], "typeerror": 161, "typic": [19, 164, 180, 198, 203, 206, 216, 220, 227, 228, 230, 232, 234, 235, 236, 237, 238, 240, 241, 242, 244, 245, 247, 248], "ubuntu": [182, 184, 185], "ubuntu22": 199, "uint": 199, "uint16": 213, "uint32": 213, "uint8": [147, 213], "unaccept": 248, "unattain": 216, "uncalibr": 199, "unchang": [171, 178, 191, 197, 220], "uncompress": 206, "under": [1, 2, 10, 157, 160, 174, 186, 189, 201, 203, 216, 222, 224, 227, 235], "undergo": 180, "underli": [19, 169, 224], "understand": [160, 169, 226, 227, 230, 235, 238, 242], "undo": [15, 189], "uneven": 224, "unexpect": 155, "unexpected_kei": 155, "unflatten": 139, "unfold": 140, "unid": [14, 175, 188], "uniniti": [13, 186, 187, 193, 197, 198], "unintuit": [14, 175, 188], "union": [6, 8, 9, 12, 14, 16, 17, 18, 158, 159, 162, 163, 168, 170, 173, 174, 175, 176, 177, 186, 188, 190, 191, 202, 203, 204, 205, 209, 211, 215, 219, 220, 221, 244], "unit": 180, "unknown": 206, "unlabel": [9, 14, 175, 186, 188, 197, 203, 216, 227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 239, 240, 241, 242], "unlabeled_data": [188, 197], "unlabeled_data_load": [188, 203, 216, 230], "unlabeled_dataset_iter": [9, 203], "unlabeled_imagenet_data_load": 237, "unlabeled_imagenet_dataset": 237, "unlabeleddatasetwrapp": 237, "unlabelled_data_load": 188, "unless": [14, 19, 170, 175, 177, 188, 212, 220, 221, 248], "unlik": [155, 166, 190, 238], "unmodifi": [194, 207], "unnecessari": [190, 199, 212, 227, 235], "unpin": 199, "unrol": [161, 199], "unsign": [222, 245], "unsigned_symmetr": [184, 222], "unsigned_zero": 155, "unsimplifi": [190, 191, 216, 227, 229, 231], "unsqueez": [195, 196], "until": [14, 150, 151, 170, 175, 188, 189, 199, 220, 221, 237], "untouch": 220, "up": [12, 18, 163, 166, 173, 177, 180, 183, 186, 199, 203, 205, 208, 209, 211, 212, 216, 219, 220, 221, 222, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 247], "up_proj": 198, "updat": [1, 13, 155, 165, 172, 186, 187, 189, 193, 196, 198, 199, 210, 213, 220, 221, 226, 228, 230, 231, 236, 237, 239, 240, 241, 245], "update_lora_weight": [165, 193], "updatestat": 184, "upgrad": [0, 156, 199], "upon": [15, 19, 164, 189, 199, 243], "upsampl": 141, "upsamplingbilinear2d": 142, "upsamplingnearest2d": 143, "upstream": [205, 212], "upto": [227, 230, 235, 238, 242], "url": [18, 205, 209, 210, 211, 226], "us": [1, 2, 6, 8, 9, 10, 11, 12, 14, 15, 18, 19, 147, 148, 150, 151, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 167, 168, 169, 170, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 206, 210, 213, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248], "usabl": 213, "usag": [13, 162, 167, 169, 180, 187, 193, 199, 204, 213, 216, 221, 243, 244], "use_all_amp_candid": [2, 174, 216, 227, 235], "use_cach": [193, 195, 196, 198], "use_cuda": [18, 184, 199, 202, 205, 209, 211, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "use_embedded_encod": [170, 177, 220, 221], "use_fast": [193, 198], "use_monotonic_fit": [18, 205, 209, 211], "use_strict_symmetr": 160, "use_symmetric_encod": [160, 199], "user": [2, 10, 12, 13, 14, 18, 19, 160, 161, 163, 169, 170, 173, 174, 175, 177, 181, 183, 184, 186, 187, 188, 198, 199, 203, 205, 208, 209, 211, 213, 214, 216, 219, 220, 221, 223, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "user_onnx_lib": [10, 220], "userflow": [171, 178, 197], "usual": [189, 208, 221, 245, 248], "util": [6, 9, 160, 162, 168, 176, 185, 186, 187, 188, 189, 195, 196, 197, 199, 202, 203, 204, 215, 216, 218, 219, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "v": [2, 12, 173, 174, 183, 186, 204, 216, 227, 230, 235, 242, 245, 248], "v1": [0, 160, 193, 195, 196, 198, 199, 202, 216, 235, 236, 237, 238, 239, 240, 241, 242], "v2": [0, 19, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 163, 170, 187, 195, 196, 199, 201, 204, 215, 219, 220, 221], "v_proj": 198, "val": [186, 197, 218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "val_images_len": 230, "val_transform": 237, "valid": [2, 9, 162, 168, 174, 176, 179, 185, 186, 188, 199, 203, 204, 205, 209, 211, 213, 216, 220, 227, 230, 235, 237, 238, 242], "validate_example_model": 162, "validate_for_missing_modul": 162, "validate_for_reused_modul": 162, "validate_model": 162, "validation_check": 162, "valu": [1, 2, 9, 10, 12, 18, 19, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 161, 163, 168, 170, 173, 174, 176, 177, 180, 184, 185, 186, 191, 193, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 213, 215, 216, 219, 220, 221, 222, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 242, 245], "value_qtzr": 19, "var": [15, 70, 189], "vari": [2, 174, 206, 216], "variabl": [18, 150, 151, 161, 205, 207, 209, 211, 226], "varianc": 238, "variant": [180, 182, 183, 184, 185, 199], "varieti": 191, "variou": [2, 18, 163, 174, 182, 199, 205, 208, 209, 211, 214, 216, 224, 230, 237, 242, 244, 245], "vector": [199, 228, 236], "vedaldi": 208, "venic": 208, "venv": 183, "ver": 199, "ver_cuda": 183, "ver_onnxruntim": 183, "ver_python": 183, "ver_torch": 183, "veri": [13, 187, 203, 206, 208, 216, 228, 229, 230, 231, 236, 237, 239, 240, 241, 242, 244], "verifi": [161, 236, 239, 240, 241], "versa": [207, 224, 227, 235, 245], "version": [19, 89, 157, 160, 161, 164, 170, 177, 180, 182, 183, 185, 195, 196, 199, 201, 202, 220, 221, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245, 248], "via": [14, 175, 180, 182, 183, 184, 188, 199, 206, 218, 243, 245], "vice": [207, 224, 227, 235, 245], "view": [157, 162, 169, 181, 184, 195, 196, 200, 201, 210], "view_a": 237, "viewabl": 226, "virtual": 183, "vision": 208, "visit": 182, "visual": [18, 182, 199, 203, 205, 208, 209, 211, 216, 230, 242], "visualization_tool": [0, 156, 201], "visualization_url": [18, 205, 209, 211], "visualize_stat": [157, 201], "visualizecompress": 210, "vocab_s": [193, 195, 196, 198], "vol": 208, "volum": 212, "w": [2, 9, 168, 174, 176, 188, 199, 203, 211, 212, 216], "w1616": 244, "w16a16": [7, 199, 216, 218, 244], "w4a16": 199, "w4a8": [188, 199, 247, 248], "w4fp16": 199, "w8a16": [188, 216, 220, 244, 247, 248], "w8a8": [188, 216, 221, 244, 247, 248], "w8a8_accuraci": 218, "w8a8_mp_accuraci": 218, "w_1": 191, "w_2": 191, "wa": [18, 147, 169, 199, 202, 205, 208, 209, 211, 213, 216, 219, 227, 232, 234, 235, 236, 238, 244], "wai": [160, 169, 171, 177, 178, 184, 197, 202, 220, 227, 228, 229, 230, 231, 235, 236, 239, 240, 241, 242, 244, 247], "walk": [215, 225], "want": [2, 6, 19, 158, 161, 174, 177, 183, 195, 196, 202, 216, 227, 230, 235, 242, 248], "warm": [12, 173, 186], "warn": [13, 162, 187, 193], "wasn": 199, "we": [2, 6, 158, 160, 161, 162, 164, 167, 169, 174, 182, 186, 190, 191, 193, 195, 196, 198, 202, 204, 206, 208, 216, 220, 224, 227, 228, 230, 232, 234, 235, 236, 237, 238, 242, 244, 247, 248], "websit": [181, 206], "websocket": 210, "weight": [1, 2, 3, 8, 9, 11, 12, 13, 16, 18, 19, 157, 159, 160, 162, 163, 164, 165, 167, 168, 169, 170, 172, 173, 174, 176, 180, 182, 185, 186, 187, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 204, 206, 209, 214, 215, 216, 218, 219, 220, 221, 222, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 244, 245, 247, 248], "weight_decai": 189, "weight_q": 169, "weight_qdq": 169, "weight_svd": [18, 205, 209, 211], "weight_svd_auto_mod": 211, "weight_svd_manual_mod": 211, "weights_pdf": [203, 230, 242], "weightsvdparamet": [18, 205, 209, 211], "well": [147, 162, 198, 203, 208, 221, 227, 228, 229, 230, 231, 235, 236, 237, 239, 240, 241, 242, 244], "were": [199, 202, 206, 212, 213, 220, 221, 222, 228, 237], "weren": 161, "wget": [218, 220, 230], "what": [181, 198, 210, 244, 247], "wheel": 199, "when": [2, 9, 10, 14, 18, 19, 155, 161, 164, 168, 170, 171, 174, 175, 176, 177, 178, 187, 188, 189, 191, 193, 197, 198, 199, 203, 205, 208, 209, 210, 211, 212, 213, 216, 219, 220, 221, 222, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245], "whenev": [166, 199, 221, 223], "where": [2, 12, 18, 150, 151, 152, 153, 154, 155, 161, 169, 170, 173, 174, 177, 180, 186, 189, 198, 199, 203, 205, 207, 209, 210, 211, 212, 213, 216, 220, 221, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245], "wherea": [169, 174, 227, 235], "wherein": [6, 158, 202], "whether": [13, 14, 155, 161, 162, 163, 175, 187, 188, 202, 207, 219, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "which": [2, 6, 8, 12, 14, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 158, 159, 161, 162, 163, 164, 167, 169, 173, 174, 175, 177, 180, 182, 186, 187, 188, 190, 191, 192, 193, 194, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 219, 220, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247], "while": [8, 18, 155, 164, 194, 198, 199, 205, 207, 209, 211, 213, 215, 218, 220, 221, 224, 227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 244, 245, 247, 248], "whl": [183, 184], "who": 213, "whole": 245, "whose": [157, 158, 159, 161, 201, 202, 204, 215, 216, 222, 243], "why": [224, 227, 235], "wide": [180, 190, 191, 244], "width": [9, 168, 176, 190, 191, 203, 209, 211, 212, 213, 214, 215, 216, 217, 224, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 245, 247, 248], "wikitext": [193, 195, 196, 198], "wildcard": 169, "wise": [9, 168, 171, 176, 178, 180, 189, 197, 199, 203, 224, 244], "wish": [186, 190, 191, 197, 216, 219], "within": [19, 147, 148, 149, 164, 180, 183, 199, 203, 206, 219, 220, 245, 247], "without": [14, 147, 149, 155, 157, 167, 170, 172, 175, 177, 188, 197, 198, 199, 201, 212, 216, 220, 221, 228, 236, 237, 239, 240, 241, 247, 248], "won": [14, 161, 171, 175, 178, 188, 197, 222], "word": 149, "work": [160, 162, 177, 182, 184, 191, 194, 199, 204, 208, 226, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 244, 248], "workaround": 161, "workflow": [181, 192, 194, 199, 206, 215, 225, 243, 247], "workspac": [183, 226, 230], "world": 185, "wors": 208, "worth": 169, "would": [160, 163, 195, 199, 218, 219, 220, 222, 227, 230, 235, 238, 242, 244], "wq": 199, "wrap": [19, 160, 161, 199, 230, 242], "wrap_linear": 160, "wrapped_module_nam": [168, 176, 203], "wrapper": [168, 176, 199, 203, 205, 209, 211, 235, 236, 239, 240, 241], "write": [163, 219, 220, 229, 231, 236, 239, 240, 241], "written": [10, 170, 220, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "wrong": 199, "wsl2": 199, "www": 226, "x": [0, 147, 148, 149, 155, 156, 161, 162, 164, 166, 184, 190, 191, 199, 203, 206, 213, 218, 220, 223, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 245], "x1": [52, 108, 161], "x2": [52, 108, 161, 223], "x86": [182, 184, 185], "x_": 245, "x_c": 155, "x_dq": 148, "x_q": [148, 149], "x_qdq": 147, "xiangyu": 208, "xx": 213, "y": [161, 183, 203, 230, 242], "y_zero_point": 199, "ybelkada": [195, 196], "ye": 208, "yet": [166, 187, 193, 204, 227, 235], "yield": [2, 9, 12, 14, 15, 163, 171, 173, 174, 175, 178, 186, 188, 189, 197, 203, 206, 216, 220, 224, 238, 244, 245], "yihui": 208, "you": [9, 14, 18, 19, 160, 161, 168, 169, 175, 176, 177, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 195, 197, 198, 199, 203, 204, 205, 206, 208, 209, 210, 211, 215, 216, 218, 219, 220, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248], "your": [9, 14, 19, 160, 161, 162, 168, 175, 176, 183, 184, 185, 186, 188, 189, 195, 197, 200, 202, 203, 206, 208, 210, 214, 218, 220, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 245, 247, 248], "yy": 213, "zero": [12, 173, 186, 199, 245], "zero_grad": [189, 195, 196, 221], "zeropad1d": 144, "zeropad2d": 145, "zeropad3d": 146, "zhang": 208, "zisserman": 208, "zlib": 199, "zou": 208, "zz": 213, "\u00aa": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00b2": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00b3": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00b5": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00b9": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00ba": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00bc": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00bd": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u00be": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u03c9": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u210e": 209, "\u215b": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u215c": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u215d": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\u215e": [181, 190, 199, 202, 204, 213, 215, 216, 218, 243, 244, 245, 247, 248], "\ud835\udc58": [209, 211], "\ud835\udc5a": 209, "\ud835\udc5b": 209, "\ud835\udc64": 209}, "titles": ["AIMET API", "aimet_onnx.apply_adaround", "aimet_onnx.mixed_precision", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.lite_mp", "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.apply_seq_mse", "aimet_torch.adaround", "aimet_torch.experimental.adascale", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.experimental.omniquant", "aimet_torch.onnx.export (beta)", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.experimental.spinquant", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "External resources", "Glossary", "AIMET Documentation", "What is AIMET?", "Building from source", "Installation", "Quick Start", "Adaptive rounding", "AdaScale", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Post Training Quantization Techniques", "OmniQuant", "Quantized LoRa", "QW-LoRa", "QWA-LoRa", "Sequential MSE", "SpinQuant", "Release notes", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Blockwise Quantization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Encoding Format Specification", "Techniques", "Low-Power Blockwise Quantization (LPBQ)", "Automatic mixed precision", "Mixed precision", "Lite mixed precision", "Manual mixed precision", "Post Training Quantization", "Quantization-aware training", "Runtime configuration", "PyTorch model guidelines", "Quantization debugging guidelines", "Tutorials", "Example Notebooks", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quant Analyzer", "Quantization simulation", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "On-target inference", "Quantization workflow", "Quantization simulation guide", "Quantization user guide", "AIMET features", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [199, 213, 216], "1": [160, 186, 187, 188, 189, 190, 193, 197, 198, 199, 202, 203, 213, 216, 218, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "10": [183, 199], "11": 199, "12": 199, "13": 199, "14": 199, "15": 199, "16": 199, "17": 199, "18": 199, "19": 199, "2": [160, 186, 187, 188, 189, 190, 193, 197, 198, 199, 202, 203, 213, 216, 218, 219, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "20": 199, "21": 199, "22": 199, "23": 199, "24": 199, "25": 199, "26": 199, "27": 199, "28": 199, "29": 199, "3": [183, 186, 187, 188, 189, 190, 193, 197, 198, 199, 202, 203, 213, 216, 218, 220, 221, 222, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "30": 199, "31": 199, "32": 199, "33": 199, "34": 199, "35": 199, "4": [186, 187, 188, 189, 193, 197, 198, 199, 202, 203, 218, 220, 222, 224, 227, 228, 229, 235, 236, 238, 239, 240, 241, 244, 248], "5": [187, 188, 193, 197, 198, 199, 203, 218, 220, 222, 224, 238, 244], "6": [188, 199, 203, 213, 218, 224], "7": [188, 199, 218, 224], "8": [199, 224], "9": 199, "For": [228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241], "On": [243, 246], "accuraci": [227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 244, 247, 248], "activ": [203, 224], "adapt": [186, 192, 228, 236], "adaround": [12, 173, 228, 236], "adascal": [13, 187, 192], "adjust": 218, "advanc": 244, "affin": [160, 169], "ai": 243, "aimet": [0, 181, 182, 183, 185, 210, 245, 246, 247, 249], "aimet_onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "aimet_torch": [12, 13, 14, 15, 16, 17, 18, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178], "algorithm": [216, 227, 235], "also": 245, "altern": 184, "amp": [227, 235, 244], "an": [227, 228, 229, 231], "analysi": [200, 203, 214, 216, 218, 224, 230, 242], "analyz": [200, 203, 230, 242], "api": [0, 5, 156, 161, 162, 164, 167, 169, 181, 186, 187, 188, 189, 190, 191, 193, 197, 198, 201, 202, 203, 204, 205, 209, 211, 215, 216, 218, 219, 220, 221, 227, 235, 247], "appli": [215, 218, 219, 228, 229, 230, 236, 239, 242], "apply_adaround": 1, "apply_seq_ms": 11, "arg": 213, "auto_qu": [14, 175], "automat": [188, 192, 216, 217, 227, 235, 244], "autoqu": 237, "awar": [214, 221, 238, 240, 241, 244, 247], "base": [195, 218, 219], "baselin": [227, 228, 229, 231, 232, 233, 234, 235, 236, 239, 240, 241, 244], "batch": [189, 190, 192, 227, 229, 231, 235, 236, 239, 240, 241], "batch_norm_fold": [3, 16], "batchnorm": 238, "beta": 166, "block": 169, "blockwis": [204, 214, 215], "bn_reestim": 15, "bokeh": 210, "brows": 226, "build": [183, 184], "calibr": [196, 203, 220], "call": [227, 235, 245], "callback": [196, 203, 220, 227, 235], "case": [208, 216], "channel": [169, 205, 208, 232, 234], "check": 224, "choos": 184, "cle": [229, 239], "code": [160, 161, 205, 209, 211, 226], "compil": [183, 243], "compress": [18, 205, 206, 207, 208, 209, 210, 211, 214, 232, 233, 234], "comput": [164, 218, 220, 221, 227, 235], "conda": 183, "confid": 224, "config_util": 159, "configur": [164, 222, 245], "constant": 237, "contain": 183, "context": [186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 201, 202, 203, 205, 209, 211, 216, 219], "convers": 243, "convert": [216, 227, 228, 229, 231], "cp": 208, "creat": [183, 196, 203, 216, 218, 220, 227, 228, 229, 231, 235, 236, 238, 239, 240, 241], "cross": [191, 192, 229, 239], "cross_layer_equ": [4, 17], "cuda": 183, "data": 213, "dataset": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "debug": [224, 246], "default": 222, "defin": [227, 235, 237], "depend": 183, "deploi": [244, 248], "deploy": 247, "dequant": 152, "dequantizedtensor": 147, "descript": 203, "design": 210, "desir": 183, "detail": 244, "determin": [228, 229, 231, 236, 239, 240, 241, 245], "dictionari": 213, "direct": 243, "disabl": [230, 242], "docker": 183, "document": [181, 183, 249], "download": 226, "enabl": [203, 230, 242], "encod": [164, 203, 213, 218, 220, 227, 230, 235, 242, 245], "engin": 243, "environ": 183, "equal": [191, 192, 229, 239], "error": 203, "estim": [189, 192, 238], "evalu": [203, 218, 220, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "exampl": [160, 161, 181, 205, 209, 211, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "execut": [189, 191, 204, 243], "experiment": [13, 165, 172], "explor": 207, "export": [166, 220, 238, 245, 247], "extern": 179, "faq": 208, "featur": [206, 246, 247], "file": 222, "find": [216, 244], "fine": [208, 232, 233, 234], "fix": 224, "float": 160, "floatquantizedequant": 155, "flow": [167, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "fold": [190, 192, 227, 229, 231, 235, 236, 238, 239, 240, 241], "format": 213, "fp16": 248, "fp32": [224, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "from": [160, 183, 184], "front": 216, "function": [227, 235, 237], "gener": [200, 202, 248], "get": [182, 232, 233, 234, 235, 236, 239, 240, 241], "glossari": [180, 181], "granular": 245, "greedi": 207, "group": 216, "guid": [160, 245, 246], "guidebook": 206, "guidelin": [223, 224, 246, 248], "helper": 237, "histogram": 203, "how": [160, 207, 212, 222, 245], "hub": 243, "i": [182, 227, 230, 235, 238, 242], "import": [202, 203], "improv": 247, "individu": 224, "infer": [243, 246, 247], "inform": [228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241], "initi": 221, "input": [202, 219], "instal": [183, 184, 185], "instanti": [228, 229, 231, 232, 233, 234, 236, 239, 240, 241], "interact": [200, 201], "layer": [191, 192, 200, 202, 203, 207, 208, 219, 224, 227, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242], "layer_output_util": [6, 158], "leaf": 219, "learn": 241, "learnedgrid": 160, "level": 213, "librari": 203, "limit": 161, "list": 216, "lite": [217, 218, 244], "lite_mp": 7, "load": [202, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "local": 183, "lora": [192, 194, 195, 196], "loss": [203, 230, 242], "low": [214, 215], "lpbq": 215, "manual": [217, 219], "max": [203, 230, 242, 245], "mean": 203, "migrat": 160, "min": [203, 230, 242, 245], "mix": [214, 216, 217, 218, 219, 227, 235, 244, 247], "mixed_precis": [2, 163, 174], "mmp": 219, "model": [185, 195, 202, 203, 208, 218, 219, 220, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 248], "model_input": 222, "model_output": 222, "model_prepar": 161, "model_valid": 162, "modifi": 222, "modul": [160, 164], "more": [228, 229, 231, 232, 233, 234, 236, 237, 239, 240, 241], "move": 160, "mse": [192, 197, 230, 242], "new": 183, "next": [228, 232, 233, 234, 236, 237, 239, 240, 241, 244, 248], "nn": 164, "nois": 245, "non": 219, "norm": [189, 190, 192, 229, 239], "normal": [227, 231, 235, 236, 240, 241], "note": [181, 199, 208], "notebook": [181, 226, 227, 230, 235, 238, 242], "nvidia": 183, "obtain": 202, "old": 184, "omniqu": [165, 192, 193], "onnx": [166, 227, 228, 229, 231], "option": [208, 219], "output": [200, 202, 219], "overal": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "overhead": 216, "overview": [181, 207, 208, 210, 212, 222, 245], "packag": [183, 184], "param": 222, "paramet": [221, 227, 235, 245], "pareto": 216, "path": 247, "pdf": [230, 242], "peft": 167, "per": [169, 203, 207, 208, 224, 230, 242], "perform": [216, 218, 224, 238, 240, 241, 244], "phase": 216, "pip": 183, "pipelin": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "platform": [182, 185], "post": [181, 192, 214, 220, 244, 247], "power": [214, 215], "precis": [214, 216, 217, 218, 219, 227, 235, 244, 247, 248], "prepar": 203, "prerequisit": [184, 186, 187, 188, 189, 193, 197, 198, 203, 218, 219, 220], "pretrain": 237, "procedur": [187, 188, 190, 193, 197, 198, 205, 216, 248], "process": 160, "profil": 219, "prune": [205, 208, 232, 234], "ptq": [195, 244], "pypi": 184, "python": 183, "pytorch": [223, 227, 228, 229, 231], "qat": [221, 238, 240, 241, 244], "qualcomm": 243, "quant": [230, 242], "quant_analyz": [9, 168, 176], "quantanalyz": [203, 230, 242], "quantiz": [150, 153, 160, 164, 169, 181, 185, 188, 192, 194, 195, 200, 203, 204, 213, 214, 215, 216, 220, 221, 224, 227, 228, 229, 230, 231, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248], "quantizationmixin": 19, "quantizationsimmodel": 196, "quantize_dequant": 154, "quantizedadaptiveavgpool1d": 20, "quantizedadaptiveavgpool2d": 21, "quantizedadaptiveavgpool3d": 22, "quantizedadaptivemaxpool1d": 23, "quantizedadaptivemaxpool2d": 24, "quantizedadaptivemaxpool3d": 25, "quantizedalphadropout": 26, "quantizedavgpool1d": 27, "quantizedavgpool2d": 28, "quantizedavgpool3d": 29, "quantizedbatchnorm1d": 32, "quantizedbatchnorm2d": 33, "quantizedbatchnorm3d": 34, "quantizedbceloss": 30, "quantizedbcewithlogitsloss": 31, "quantizedbilinear": 35, "quantizedcelu": 36, "quantizedchannelshuffl": 38, "quantizedcircularpad1d": 39, "quantizedcircularpad2d": 40, "quantizedcircularpad3d": 41, "quantizedconstantpad1d": 42, "quantizedconstantpad2d": 43, "quantizedconstantpad3d": 44, "quantizedconv1d": 45, "quantizedconv2d": 46, "quantizedconv3d": 47, "quantizedconvtranspose1d": 48, "quantizedconvtranspose2d": 49, "quantizedconvtranspose3d": 50, "quantizedcosineembeddingloss": 51, "quantizedcosinesimilar": 52, "quantizedcrossentropyloss": 53, "quantizedctcloss": 37, "quantizeddropout": 54, "quantizeddropout1d": 55, "quantizeddropout2d": 56, "quantizeddropout3d": 57, "quantizedelu": 58, "quantizedembed": 59, "quantizedembeddingbag": 60, "quantizedequant": 151, "quantizedfeaturealphadropout": 61, "quantizedflatten": 62, "quantizedfold": 63, "quantizedfractionalmaxpool2d": 64, "quantizedfractionalmaxpool3d": 65, "quantizedgaussiannllloss": 70, "quantizedgelu": 66, "quantizedglu": 67, "quantizedgroupnorm": 71, "quantizedgru": 68, "quantizedgrucel": 69, "quantizedhardshrink": 72, "quantizedhardsigmoid": 73, "quantizedhardswish": 74, "quantizedhardtanh": 75, "quantizedhingeembeddingloss": 76, "quantizedhuberloss": 77, "quantizedinstancenorm1d": 78, "quantizedinstancenorm2d": 79, "quantizedinstancenorm3d": 80, "quantizedkldivloss": 81, "quantizedl1loss": 82, "quantizedlayernorm": 87, "quantizedleakyrelu": 88, "quantizedlinear": 89, "quantizedlocalresponsenorm": 90, "quantizedlogsigmoid": 91, "quantizedlogsoftmax": 92, "quantizedlppool1d": 83, "quantizedlppool2d": 84, "quantizedlstm": 85, "quantizedlstmcel": 86, "quantizedmarginrankingloss": 94, "quantizedmaxpool1d": 95, "quantizedmaxpool2d": 96, "quantizedmaxpool3d": 97, "quantizedmaxunpool1d": 98, "quantizedmaxunpool2d": 99, "quantizedmaxunpool3d": 100, "quantizedmish": 101, "quantizedmseloss": 93, "quantizedmultilabelmarginloss": 102, "quantizedmultilabelsoftmarginloss": 103, "quantizedmultimarginloss": 104, "quantizednllloss": 105, "quantizednllloss2d": 106, "quantizedpairwisedist": 108, "quantizedpixelshuffl": 109, "quantizedpixelunshuffl": 110, "quantizedpoissonnllloss": 111, "quantizedprelu": 107, "quantizedreflectionpad1d": 117, "quantizedreflectionpad2d": 118, "quantizedreflectionpad3d": 119, "quantizedrelu": 115, "quantizedrelu6": 116, "quantizedreplicationpad1d": 120, "quantizedreplicationpad2d": 121, "quantizedreplicationpad3d": 122, "quantizedrnn": 112, "quantizedrnncel": 113, "quantizedrrelu": 114, "quantizedselu": 123, "quantizedsigmoid": 125, "quantizedsilu": 124, "quantizedsmoothl1loss": 126, "quantizedsoftmarginloss": 127, "quantizedsoftmax": 128, "quantizedsoftmax2d": 129, "quantizedsoftmin": 130, "quantizedsoftplu": 131, "quantizedsoftshrink": 132, "quantizedsoftsign": 133, "quantizedtanh": 134, "quantizedtanhshrink": 135, "quantizedtensor": 148, "quantizedtensorbas": 149, "quantizedthreshold": 136, "quantizedtripletmarginloss": 137, "quantizedtripletmarginwithdistanceloss": 138, "quantizedunflatten": 139, "quantizedunfold": 140, "quantizedupsampl": 141, "quantizedupsamplingbilinear2d": 142, "quantizedupsamplingnearest2d": 143, "quantizedzeropad1d": 144, "quantizedzeropad2d": 145, "quantizedzeropad3d": 146, "quantsim": [8, 10, 159, 170, 177, 218, 220, 245], "quantwrapp": 160, "quick": 185, "quickli": 185, "qw": [194, 195], "qwa": [194, 196], "rang": [203, 230, 241, 242], "rank": 208, "ratio": [207, 208, 210], "re": [189, 192, 238], "recommend": 221, "recomput": 218, "reconstruct": 205, "reduc": [216, 248], "reestim": 238, "refer": [164, 169, 181, 208], "relat": 226, "releas": [181, 199], "resourc": 179, "restor": 248, "round": [186, 192, 208, 228, 236], "run": [183, 196, 203, 221, 226, 227, 235, 237], "runtim": [222, 245], "scheme": 245, "score": [232, 233, 234, 235, 236, 239, 240, 241], "sdk": 243, "select": [205, 207, 208], "sensit": [203, 216, 218, 224], "seq_ms": [171, 178], "sequenti": [192, 197], "server": [210, 226], "session": 210, "set": [183, 219], "set_grouped_blockwise_quantization_for_weight": 8, "setup": [186, 187, 189, 191, 193, 195, 196, 197, 198, 205, 209, 211, 219, 221], "signal": 245, "sim": [227, 229, 231, 235, 236, 238, 239, 240, 241], "simplifi": [227, 228, 229, 231], "simul": [227, 228, 229, 231, 235, 236, 238, 239, 240, 241, 245, 247], "small": 185, "sourc": [183, 184], "spatial": [208, 209, 233, 234], "specif": 213, "spinquant": [172, 192, 198], "squar": 203, "start": [182, 185, 210], "staticgrid": 160, "statist": [203, 230, 238, 242], "step": [186, 187, 188, 189, 190, 193, 197, 198, 202, 203, 216, 218, 219, 220, 221, 228, 232, 233, 234, 236, 237, 239, 240, 241, 244, 248], "structur": [213, 222], "summari": 238, "supergroup": 222, "support": [182, 183, 247], "svd": [208, 209, 211, 233, 234], "target": [243, 246, 247], "techniqu": [181, 192, 208, 214, 239, 244], "terminologi": 167, "test": [183, 185], "tf": 245, "thi": [227, 230, 235, 238, 242], "tool": [157, 200, 214, 247], "top": 213, "tradeoff": 244, "train": [181, 192, 195, 196, 214, 220, 221, 228, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 244, 247], "try": 248, "tune": [208, 232, 233, 234], "tutori": [181, 225], "type": [213, 219], "typic": 221, "unit": 183, "updat": 195, "us": [160, 205, 208, 209, 211, 216, 232, 233, 234, 244], "user": [167, 246], "v": [160, 244], "v1": [156, 173, 174, 175, 176, 177, 178], "valid": [228, 229, 231, 232, 233, 234, 236, 239, 240, 241], "variabl": 183, "variant": 221, "verifi": [184, 185, 248], "version": [184, 213, 249], "visual": [157, 200, 201, 210, 224], "visualization_tool": 157, "w16a16": 248, "w8a8": 218, "w8a8_mix": 218, "weight": [195, 203, 205, 208, 211, 224], "what": [182, 227, 230, 235, 238, 242], "wheel": 183, "winnow": [205, 212], "work": [207, 212, 245], "workflow": [186, 187, 188, 189, 190, 191, 193, 195, 196, 197, 198, 201, 202, 203, 205, 209, 211, 216, 218, 219, 220, 221, 224, 244, 245, 246, 248], "wrapper": [230, 242], "x": 160}})