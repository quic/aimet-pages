<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="Example Notebooks" href="notebooks.html"><link rel="prev" title="Quantization simulation guide" href="quantsim.html">

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.12.19 -->
        <title>Quantization recipes for LLMs - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=7bdb33bb" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/aimet-furo.css?v=58822075" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.22.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../overview/index.html">Overview</a><input aria-label="Toggle navigation of Overview" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../overview/install/quick-start.html">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/install/index.html">Install</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Tutorials</a><input aria-label="Toggle navigation of Tutorials" checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quantization_workflow.html">Quantization Workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="quantsim.html">Quantization Simulation</a><input aria-label="Toggle navigation of Quantization Simulation" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Quantization Recipes for LLMs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="notebooks.html">Example Notebooks</a><input aria-label="Toggle navigation of Example Notebooks" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="on_target_inference.html">Running Quantized Models on-device</a></li>
<li class="toctree-l2"><a class="reference internal" href="debugging_guidelines.html">Debugging Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../techniques/index.html">Techniques</a><input aria-label="Toggle navigation of Techniques" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../techniques/ptq.html">Post Training Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/qat.html">Quantization Aware Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/blockwise.html">Blockwise Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../techniques/lpbq.html">Low-Power Blockwise Quantization</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../techniques/mixed_precision/index.html">Mixed precision</a><input aria-label="Toggle navigation of Mixed precision" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../techniques/mixed_precision/litemp.html">Lite mixed precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/mixed_precision/mmp.html">Manual mixed precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/mixed_precision/amp.html">Automatic mixed precision</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../techniques/analysis_tools/index.html">Analysis tools</a><input aria-label="Toggle navigation of Analysis tools" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../techniques/analysis_tools/interactive_visualization.html">Interactive visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/analysis_tools/quant_analyzer.html">Quantization analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/analysis_tools/layer_output_generation.html">Layer output generation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../techniques/compression/index.html">Compression</a><input aria-label="Toggle navigation of Compression" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../techniques/compression/feature_guidebook.html">Compression guidebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/compression/greedy_compression_ratio_selection.html">Greedy compression ratio selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/compression/visualization_compression.html">Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/compression/weight_svd.html">Weight SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../techniques/compression/spatial_svd.html">Spatial SVD</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../techniques/compression/channel_pruning.html">Channel pruning</a><input aria-label="Toggle navigation of Channel pruning" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../techniques/compression/winnowing.html">Winnowing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ptq_techniques/index.html">PTQ Techniques</a><input aria-label="Toggle navigation of PTQ Techniques" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/adascale.html">AdaScale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/omniquant.html">OmniQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/autoquant.html">Automatic quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ptq_techniques/spinquant.html">SpinQuant</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../apiref/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../external/index.html">External Resources</a><input aria-label="Toggle navigation of External Resources" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference external" href="http://www.qualcomm.com/developer/artificial-intelligence#overview">Qualcomm AI Stack</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-models/">Qualcomm Hub Models</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-apps/">Qualcomm Hub Apps</a></li>
<li class="toctree-l2"><a class="reference external" href="https://aihub.qualcomm.com/">Qualcomm AI Hub</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="quantization-recipes-for-llms">
<span id="quantization-genai-recipe"></span><h1>Quantization recipes for LLMs<a class="headerlink" href="#quantization-recipes-for-llms" title="Link to this heading">¶</a></h1>
<p>This document presents the quantization and evaluation workflow for large language models (LLMs)
models using <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> and <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>. The objective is to communicate performance expectations through
two reference recipes applied to the following LLMs:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct">LLaMA 3.2 1B Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct">LLaMA 3.2 3B Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/Qwen/Qwen2.5-0.5B">Qwen 2.5 0.5B Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/Qwen/Qwen2.5-1.5B">Qwen 2.5 1.5B Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/Qwen/Qwen3-4B">Qwen 3 4B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/microsoft/Phi-3.5-mini-instruct">Phi 3.5 mini instruct</a></p></li>
</ul>
<p>The exported artifacts from the recipes are not directly compatible with <a class="reference external" href="https://www.qualcomm.com/developer/software/qualcomm-ai-engine-direct-sdk">Qualcomm® AI Engine Direct</a> (QAIRT) and require additional adaptation steps for deployment on the target hardware. Refer to the <a class="reference external" href="https://github.com/quic/ai-hub-models/blob/main/tutorials/llm/onboarding.md">model adaptation guide</a> for more details when deploying on the target hardware.</p>
<section id="system-requirements">
<h2>System Requirements<a class="headerlink" href="#system-requirements" title="Link to this heading">¶</a></h2>
<p>The quantization process requires a machine with:</p>
<ul class="simple">
<li><p>Operating System: Linux</p></li>
<li><p>Hardware: CUDA-enabled GPU</p></li>
</ul>
<p>GPU Memory Requirements:</p>
<ul class="simple">
<li><p>Minimum: 40GB VRAM</p></li>
</ul>
</section>
<section id="recipes">
<h2>Recipes<a class="headerlink" href="#recipes" title="Link to this heading">¶</a></h2>
<p>We present two recipes for INT4 weights, INT16 activations quantization using combinations of <a class="reference internal" href="../ptq_techniques/index.html#featureguide-index"><span class="std std-ref">Post-Training Quantization (PTQ)</span></a> techniques available in <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> and <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>PCQ + SpinQuant + AdaScale</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../techniques/ptq.html#techniques-ptq"><span class="std std-ref">Per-Channel Quantization (PCQ)</span></a> - Uses per-output channel scales for weights on linear layers.</p></li>
<li><p><a class="reference internal" href="../ptq_techniques/spinquant.html#ptq-spinquant"><span class="std std-ref">SpinQuant</span></a> - A PTQ technique that improves the accuracy by inserting rotations at specific points in the model to mitigate activation outliers.</p></li>
<li><p><a class="reference internal" href="../ptq_techniques/adascale.html#ptq-adascale"><span class="std std-ref">AdaScale</span></a> - A PTQ technique that enhances accuracy by introducing learnable parameters in the weight quantizers and performing Block-wise Knowledge Distillation (BKD) against FP outputs.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>LPBQ + SequentialMSE</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="../techniques/lpbq.html#techniques-lpbq"><span class="std std-ref">Low Power Blockwise Quantization (LPBQ)</span></a>  - Applies blockwise quantization (<code class="docutils literal notranslate"><span class="pre">block_size=64</span></code>) for weights on linear layers.</p></li>
<li><p><a class="reference internal" href="../ptq_techniques/seq_mse.html#ptq-seq-mse"><span class="std std-ref">SequentialMSE</span></a> - Calibrates layer-by-layer to minimize Mean Square Error (MSE) between quantized and FP outputs.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p>To maintain accuracy, activations are primarily kept at INT16, with a mixed-precision profile using INT8 activations selectively where feasible—such as for the KV cache.</p>
</section>
<section id="workflow-overview">
<h2>Workflow Overview<a class="headerlink" href="#workflow-overview" title="Link to this heading">¶</a></h2>
<ol class="arabic">
<li><dl class="simple">
<dt>Load the HuggingFace model</dt><dd><ul class="simple">
<li><p>Start by loading the pretrained model using HuggingFace <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Apply the selected Quantization recipe</dt><dd><ul>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> for <a class="reference external" href="https://pytorch.org/">PyTorch</a> based workflows or</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code> for <a class="reference external" href="https://onnx.ai/">ONNX</a> based workflows.</p></li>
<li><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a class="reference internal" href="../ptq_techniques/spinquant.html#ptq-spinquant"><span class="std std-ref">SpinQuant</span></a> technique is currently available only in <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code>. You can apply SpinQuant on the FP32 model before exporting it to ONNX, and then continue the workflow using <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>.</p>
</div>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Compute Activations encodings</dt><dd><ul class="simple">
<li><p>Both <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> and <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code> compute activation encodings using representative data. In this tutorial, we use <a class="reference external" href="https://github.com/quic/aimet/blob/develop/GenAITests/shared/helpers/datasets.py">WikiText (English)</a> for calibration.</p></li>
<li><dl class="simple">
<dt>For <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> only:</dt><dd><ul>
<li><p>Due to PyTorch limitations, certain functional operations (<code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code>) cannot have quantizers inserted. This makes implementing a mixed-precision profile (e.g., KV Cache in INT8) challenging.</p></li>
<li><p>To address this, include <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code> evaluation step within the <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> workflow. <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code> provides a static graph, ensuring correct quantizer insertion for all activations and delivering a more accurate quantization simulation.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Export for deployment</dt><dd><ul class="simple">
<li><p>Export the ONNX model along with the encodings file for the on-target inference.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</section>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading">¶</a></h2>
<p>This section provides a quick example of applying a quantization recipe using either <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> or <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>.</p>
<p>In this tutorial, we apply the quantization recipe to the <cite>Llama 3.2 1B</cite> model. The steps work for all fine-tuned variants that share the same tokenizer and network architecture.</p>
<p>The example scripts are designed to be <cite>flattened</cite>, so all AIMET API calls and HuggingFace API calls are visible at the top level.</p>
<p>To understand how this works under the hood for PyTorch and ONNX models using the same driver code, refer to the <a class="reference external" href="https://github.com/quic/aimet/tree/develop/GenAITests#how-it-all-works">Generator</a> class in <code class="docutils literal notranslate"><span class="pre">GenAITests</span></code>.</p>
<section id="quantize">
<h3>Quantize<a class="headerlink" href="#quantize" title="Link to this heading">¶</a></h3>
<p>Example: Apply Recipe 1 (pcq_spinquant_adascale)</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">Examples</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">quantize</span> \
 <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="nb">id</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span> \
 <span class="o">--</span><span class="n">recipe</span> <span class="s2">&quot;pcq_spinquant_adascale&quot;</span> \
 <span class="o">--</span><span class="n">export</span><span class="o">-</span><span class="n">path</span> <span class="s2">&quot;./torch_pcq&quot;</span> \
 <span class="o">--</span><span class="n">adascale</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">batches</span> <span class="mi">128</span> <span class="o">--</span><span class="n">adascale</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">iterations</span> <span class="mi">2048</span>
</pre></div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">Examples</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">quantize</span> \
 <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="nb">id</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span> \
 <span class="o">--</span><span class="n">recipe</span> <span class="s2">&quot;pcq_spinquant_adascale&quot;</span> \
 <span class="o">--</span><span class="n">export</span><span class="o">-</span><span class="n">path</span> <span class="s2">&quot;./onnx_pcq&quot;</span> \
 <span class="o">--</span><span class="n">adascale</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">batches</span> <span class="mi">128</span> <span class="o">--</span><span class="n">adascale</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">iterations</span> <span class="mi">2048</span>
</pre></div>
</div>
<p>Example: Apply Recipe 2 (lpbq_seqmse)</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">Examples</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">quantize</span> \
 <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="nb">id</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span> \
 <span class="o">--</span><span class="n">recipe</span> <span class="s2">&quot;lpbq_seqmse&quot;</span> \
 <span class="o">--</span><span class="n">export</span><span class="o">-</span><span class="n">path</span> <span class="s2">&quot;./torch_lpbq&quot;</span> \
 <span class="o">--</span><span class="n">seqmse</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">batches</span> <span class="mi">20</span>
</pre></div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">Examples</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">quantize</span> \
 <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="nb">id</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span> \
 <span class="o">--</span><span class="n">recipe</span> <span class="s2">&quot;lpbq_seqmse&quot;</span> \
 <span class="o">--</span><span class="n">export</span><span class="o">-</span><span class="n">path</span> <span class="s2">&quot;./onnx_lpbq&quot;</span> \
 <span class="o">--</span><span class="n">seqmse</span><span class="o">-</span><span class="n">num</span><span class="o">-</span><span class="n">batches</span> <span class="mi">20</span>
</pre></div>
</div>
</section>
<section id="evaluate">
<h3>Evaluate<a class="headerlink" href="#evaluate" title="Link to this heading">¶</a></h3>
<p>Use the checkpoint generated in the previous step to evaluate the quantized model.</p>
<ul class="simple">
<li><p>ONNX evaluation works for models quantized with either <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> or <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>.</p></li>
<li><p>PyTorch evaluation works only for models quantized with <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code>.</p></li>
</ul>
<p>Using <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">Examples</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">evaluate</span> \
 <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="nb">id</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span> \
 <span class="o">--</span><span class="n">checkpoint</span> <span class="s2">&quot;./torch_lpbq&quot;</span> \
 <span class="o">--</span><span class="nb">eval</span><span class="o">-</span><span class="n">ppl</span>
</pre></div>
</div>
<p>Now, we will go through the performance numbers for the selected LLMs.</p>
</section>
</section>
<section id="performance-summary">
<h2>Performance Summary<a class="headerlink" href="#performance-summary" title="Link to this heading">¶</a></h2>
<p>Once the model is quantized, it is essential to evaluate its accuracy to ensure it meets acceptable thresholds. The same evaluation can also be performed on the original (unquantized) model to establish a strong baseline.</p>
<p>We demonstrate quantitative evaluation using two key metrics:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Perplexity">Perplexity (PPL)</a> on WikiText (English)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/cais/mmlu">MMLU</a></p></li>
</ul>
<p>Additionally, we report:</p>
<ul class="simple">
<li><p>End-to-end runtime for each quantization recipe</p></li>
<li><p>Peak CUDA memory usage during quantization</p></li>
</ul>
<p>The consolidated performance tables summarize results for selected LLM models. You will find numbers for both recipes using <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> and <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For models quantized using <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code>, we include results from evaluation on <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>. This ensures accurate activation quantizer placement and mixed-precision simulation (e.g., INT8 KV Cache).</p>
<p>To avoid confusion, we explicitly report two fields for each result:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Quantized</span> <span class="pre">With</span></code> – the AIMET package used to create the quantized model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Evaluated</span> <span class="pre">On</span></code> – the AIMET package used to measure accuracy and performance</p></li>
</ul>
</div>
<p>During quantization and evaluation, we use a sequence length of <cite>2048 tokens</cite> (referred to as AR-2048) and the context length of <cite>4096 tokens</cite>.</p>
<section id="meta-llama-llama-3-2-1b-instruct">
<h3>1. meta-llama/Llama-3.2-1B-Instruct<a class="headerlink" href="#meta-llama-llama-3-2-1b-instruct" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Activations: INT16, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cache</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=2048</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>12.14</p></td>
<td><p>46.06</p></td>
<td><p>00:00:14</p></td>
<td><p>6.34</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.67</p></td>
<td><p>42.25</p></td>
<td><p>02:31:06</p></td>
<td><p>20.89</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.68</p></td>
<td><p>41.82</p></td>
<td><p>01:53:17</p></td>
<td><p>46.38</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>14.07</p></td>
<td><p>43.09</p></td>
<td><p>00:44:38</p></td>
<td><p>28.52</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.84</p></td>
<td><p>43.53</p></td>
<td><p>00:20:44</p></td>
<td><p>34.79</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="meta-llama-llama-3-2-3b-instruct">
<h3>2. meta-llama/Llama-3.2-3B-Instruct<a class="headerlink" href="#meta-llama-llama-3-2-3b-instruct" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Activations: INT16, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cache</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=1024</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>10.13</p></td>
<td><p>60.74</p></td>
<td><p>00:00:10</p></td>
<td><p>13.90</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>11.01</p></td>
<td><p>58.09</p></td>
<td><p>06:35:22</p></td>
<td><p>41.24</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>11.14</p></td>
<td><p>56.79</p></td>
<td><p>04:49:36</p></td>
<td><p>47.35</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>10.69</p></td>
<td><p>59.08</p></td>
<td><p>02:41:44</p></td>
<td><p>51.11</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>10.55</p></td>
<td><p>59.29</p></td>
<td><p>01:13:12</p></td>
<td><p>59.41</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="qwen-qwen2-5-0-5b-instruct">
<h3>3. Qwen/Qwen2.5-0.5B-Instruct<a class="headerlink" href="#qwen-qwen2-5-0-5b-instruct" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Activations: INT16</p></li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=2048</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>13.14</p></td>
<td><p>46.30</p></td>
<td><p>00:00:13</p></td>
<td><p>3.68</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.89</p></td>
<td><p>44.19</p></td>
<td><p>03:19:37</p></td>
<td><p>13.37</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.82</p></td>
<td><p>42.65</p></td>
<td><p>01:16:54</p></td>
<td><p>34.01</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>15.32</p></td>
<td><p>42.33</p></td>
<td><p>00:22:39</p></td>
<td><p>14.25</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>15.30</p></td>
<td><p>43.26</p></td>
<td><p>00:11:33</p></td>
<td><p>20.43</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="qwen-qwen2-5-1-5b-instruct">
<h3>4. Qwen/Qwen2.5-1.5B-Instruct<a class="headerlink" href="#qwen-qwen2-5-1-5b-instruct" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Activations: INT16</p></li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=1024</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>12.41</p></td>
<td><p>54.65</p></td>
<td><p>00:00:10</p></td>
<td><p>7.78</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.57</p></td>
<td><p>49.81</p></td>
<td><p>03:03:17</p></td>
<td><p>22.62</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.35</p></td>
<td><p>50.27</p></td>
<td><p>02:13:33</p></td>
<td><p>42.97</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>14.86</p></td>
<td><p>49.25</p></td>
<td><p>01:07:43</p></td>
<td><p>26.01</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>14.33</p></td>
<td><p>49.97</p></td>
<td><p>00:37:52</p></td>
<td><p>34.40</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="qwen-qwen3-4b">
<h3>5. Qwen/Qwen3-4B<a class="headerlink" href="#qwen-qwen3-4b" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Activations: INT16, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cache</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=512</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>12.41</p></td>
<td><p>70.06</p></td>
<td><p>00:00:10</p></td>
<td><p>17.02</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.85</p></td>
<td><p>65.07</p></td>
<td><p>06:41:32</p></td>
<td><p>47.71</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.79</p></td>
<td><p>62.33</p></td>
<td><p>04:34:22</p></td>
<td><p>71.3</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>13.10</p></td>
<td><p>65.66</p></td>
<td><p>02:41:48</p></td>
<td><p>39.42</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>12.77</p></td>
<td><p>65.36</p></td>
<td><p>01:35:29</p></td>
<td><p>63.61</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="microsoft-phi-3-5-mini-instruct">
<h3>6. microsoft/Phi-3.5-mini-instruct<a class="headerlink" href="#microsoft-phi-3-5-mini-instruct" title="Link to this heading">¶</a></h3>
<p>Precision settings:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Weights: INT4, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">LM</span> <span class="pre">Head</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Activations: INT16, except for:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cache</span></code>: INT8</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Hyperparameters:</p>
<ul class="simple">
<li><p>AdaScale: <code class="docutils literal notranslate"><span class="pre">num_batches=128</span></code>, <code class="docutils literal notranslate"><span class="pre">num_iterations=256</span></code></p></li>
<li><p>SequentialMSE: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
<li><p>Calibration: <code class="docutils literal notranslate"><span class="pre">num_batches=20</span></code></p></li>
</ul>
<div class="table-wrapper colwidths-given docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 3.0%" />
<col style="width: 3.0%" />
<col style="width: 5.0%" />
<col style="width: 3.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Quantized With</p></th>
<th class="head"><p>Evaluated On</p></th>
<th class="head"><p>PPL</p></th>
<th class="head"><p>MMLU</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>CUDA (GB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32</p></td>
<td><p>N/A</p></td>
<td><p>Both</p></td>
<td><p>5.77</p></td>
<td><p>68.89</p></td>
<td><p>00:00:08</p></td>
<td><p>16.17</p></td>
</tr>
<tr class="row-odd"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>6.58</p></td>
<td><p>62.62</p></td>
<td><p>04:16:53</p></td>
<td><p>48.03</p></td>
</tr>
<tr class="row-even"><td><p>PCQ + SpinQuant + AdaScale</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>6.50</p></td>
<td><p>62.51</p></td>
<td><p>01:51:43</p></td>
<td><p>61.85</p></td>
</tr>
<tr class="row-odd"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>6.45</p></td>
<td><p>64.63</p></td>
<td><p>02:03:41</p></td>
<td><p>37.64</p></td>
</tr>
<tr class="row-even"><td><p>LPBQ + SequentialMSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code></p></td>
<td><p>6.41</p></td>
<td><p>63.90</p></td>
<td><p>01:32:36</p></td>
<td><p>75.62</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="faqs">
<h2>FAQs<a class="headerlink" href="#faqs" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><dl class="simple">
<dt>When should I choose <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> vs <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code>?</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Choose <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> when:</dt><dd><ul>
<li><p>You want to apply quantization directly on a PyTorch model and keep the workflow within the PyTorch ecosystem.</p></li>
<li><p>You plan to apply <a class="reference internal" href="../techniques/qat.html#techniques-qat"><span class="std std-ref">Quantization-Aware Training (QAT)</span></a> or run calibration using PyTorch datasets and dataloaders.</p></li>
<li><p>You need flexibility for dynamic graph operations.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Choose <code class="docutils literal notranslate"><span class="pre">aimet-onnx</span></code> when:</dt><dd><ul>
<li><p>You need a static graph representation for deployment.</p></li>
<li><p>You want full quantization coverage, including functional operations that <code class="docutils literal notranslate"><span class="pre">aimet-torch</span></code> cannot instrument easily.</p></li>
<li><p>You are preparing the model for hardware adaptation (e.g. QAIRT) or other runtimes which consume ONNX graphs.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>When should I choose Recipe 1 vs Recipe 2?</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Choose Recipe 1: PCQ + SpinQuant + AdaScale</dt><dd><ul>
<li><p>Uses Per-channel Quantization (PCQ), which provides good granularity for weights.</p></li>
<li><p>Performance KPIs (token rate, time-to-first-tokens etc.) are better on the target device.</p></li>
<li><p>Recommended when you can afford longer calibration time and prioritize throughput over accuracy.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Choose Recipe 2: LPBQ + SequentialMSE</dt><dd><ul>
<li><p>Uses Blockwise quantization, which provides finer granularity than PCQ.</p></li>
<li><p>Recommended when the accuracy is the top priority.</p></li>
<li><p>Trade off: Slight impact on performance KPIs due to INT4 -&gt; INT8 decoding.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Can I run the artifacts generated from the recipes as-is on target hardware?</dt><dd><ul class="simple">
<li><p>No. The generated artifacts from the recipes are not directly compatible with QAIRT and require non-trivial adaptation steps for deployment on target hardware. Refer to the <a class="reference external" href="https://github.com/quic/ai-hub-models/blob/main/tutorials/llm/onboarding.md">model adaptation guide</a> for details.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Why does computing MMLU takes a long time?</dt><dd><ul class="simple">
<li><p>MMLU evaluation can be slow even on high-end GPUs because it involves thousands of questions across 57 subjects. You can trade off accuracy for speed by reducing the number of samples.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Why INT8 KV Cache is not “Good enough” for Qwen 2.5?</dt><dd><ul class="simple">
<li><p>Qwen 2.5 (0.5B and 1.5B) suffers with INT8 path (with only 256 discrete levels) for KV Cache activations due to wider dynamic range and INT16 offers 65,536 discrete levels which drastically reduces quantization error. So for Qwen 2.5, INT16, which doubles memory compared to INT8, maintains performance much closer to FP32, making it the better choice when quality matters and memory allows.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</section>
<section id="contact-us">
<h2>Contact Us<a class="headerlink" href="#contact-us" title="Link to this heading">¶</a></h2>
<p>Please reach out to us if you encounter any issue with this tutorial or applying recipes to similar models.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://qualcomm-ai-hub.slack.com/archives/C08JKBE0UHY">Slack Community</a></p></li>
</ul>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="notebooks.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Example Notebooks</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="quantsim.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Quantization simulation guide</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Quantization recipes for LLMs</a><ul>
<li><a class="reference internal" href="#system-requirements">System Requirements</a></li>
<li><a class="reference internal" href="#recipes">Recipes</a></li>
<li><a class="reference internal" href="#workflow-overview">Workflow Overview</a></li>
<li><a class="reference internal" href="#quick-start">Quick Start</a><ul>
<li><a class="reference internal" href="#quantize">Quantize</a></li>
<li><a class="reference internal" href="#evaluate">Evaluate</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-summary">Performance Summary</a><ul>
<li><a class="reference internal" href="#meta-llama-llama-3-2-1b-instruct">1. meta-llama/Llama-3.2-1B-Instruct</a></li>
<li><a class="reference internal" href="#meta-llama-llama-3-2-3b-instruct">2. meta-llama/Llama-3.2-3B-Instruct</a></li>
<li><a class="reference internal" href="#qwen-qwen2-5-0-5b-instruct">3. Qwen/Qwen2.5-0.5B-Instruct</a></li>
<li><a class="reference internal" href="#qwen-qwen2-5-1-5b-instruct">4. Qwen/Qwen2.5-1.5B-Instruct</a></li>
<li><a class="reference internal" href="#qwen-qwen3-4b">5. Qwen/Qwen3-4B</a></li>
<li><a class="reference internal" href="#microsoft-phi-3-5-mini-instruct">6. microsoft/Phi-3.5-mini-instruct</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faqs">FAQs</a></li>
<li><a class="reference internal" href="#contact-us">Contact Us</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>