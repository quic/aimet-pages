Search.setIndex({"alltitles": {"1. Define Constants and Helper functions": [[238, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[231, "1.-Example-evaluation-and-training-pipeline"], [239, "1.-Example-evaluation-and-training-pipeline"], [243, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[228, "1.-Example-evaluation-pipeline"], [236, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence checks": [[225, "fp32-confidence-checks"]], "1. Instantiate the example training and validation pipeline": [[229, "1.-Instantiate-the-example-training-and-validation-pipeline"], [230, "1.-Instantiate-the-example-training-and-validation-pipeline"], [232, "1.-Instantiate-the-example-training-and-validation-pipeline"], [233, "1.-Instantiate-the-example-training-and-validation-pipeline"], [234, "1.-Instantiate-the-example-training-and-validation-pipeline"], [235, "1.-Instantiate-the-example-training-and-validation-pipeline"], [237, "1.-Instantiate-the-example-training-and-validation-pipeline"], [240, "1.-Instantiate-the-example-training-and-validation-pipeline"], [241, "1.-Instantiate-the-example-training-and-validation-pipeline"], [242, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Run the notebook server": [[227, "run-the-notebook-server"]], "1. Sensitivity to weight and activation quantization": [[204, "sensitivity-to-weight-and-activation-quantization"]], "1. Set Random Seeds and Instantiate ImageNet Data Loaders": [[244, "1.-Set-Random-Seeds-and-Instantiate-ImageNet-Data-Loaders"]], "1. Versioning": [[214, "versioning"]], "1. defaults": [[223, "defaults"]], "1.13.0": [[200, "id57"]], "1.16.0": [[200, "id56"]], "1.16.1": [[200, "id55"]], "1.16.2": [[200, "id54"]], "1.17.0": [[200, "id53"]], "1.18.0": [[200, "id52"]], "1.19.1": [[200, "id51"]], "1.20.0": [[200, "id50"]], "1.21.0": [[200, "id49"]], "1.22.0": [[200, "id48"]], "1.22.1": [[200, "id47"]], "1.22.2": [[200, "id46"]], "1.23.0": [[200, "id45"]], "1.24.0": [[200, "id44"]], "1.25.0": [[200, "id43"]], "1.26.0": [[200, "id42"]], "1.27.0": [[200, "id41"]], "1.28.0": [[200, "id40"]], "1.29.0": [[200, "id39"]], "1.30.0": [[200, "id38"]], "1.31.0": [[200, "id37"]], "1.32.0": [[200, "id36"]], "1.33.0": [[200, "id35"]], "1.33.5": [[200, "id34"]], "1.34.0": [[200, "id33"]], "1.35.0": [[200, "id32"]], "1.35.1": [[200, "id31"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[228, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [229, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [230, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [232, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Create W4A8 QuantizationSimModel with Vision Transformer (ViT)": [[244, "2.-Create-W4A8-QuantizationSimModel-with-Vision-Transformer-(ViT)"]], "2. Download the example notebooks and related code": [[227, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[239, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[238, "2.-Load-a-pretrained-FP32-model"]], "2. Load the model": [[231, "2.-Load-the-model"], [243, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[233, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [234, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [235, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [236, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [237, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [240, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [241, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [242, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement": [[204, "per-layer-quantizer-enablement"]], "2. Version 0.6.1": [[214, "version-0-6-1"]], "2. Weights or activations quantization": [[225, "weights-or-activations-quantization"]], "2. params": [[223, "params"]], "2.0.0": [[200, "id30"]], "2.1. Encoding specification": [[214, "encoding-specification"]], "2.1.0": [[200, "id29"]], "2.10.0": [[200, "id16"]], "2.11.0": [[200, "id14"]], "2.12.0": [[200, "id13"]], "2.13.0": [[200, "id12"]], "2.14.0": [[200, "id10"]], "2.15.0": [[200, "id8"]], "2.16.0": [[200, "id7"]], "2.17.0": [[200, "id6"]], "2.18.0": [[200, "id5"]], "2.19.0": [[200, "id4"]], "2.2.0": [[200, "id28"]], "2.20.0": [[200, "id1"]], "2.3.0": [[200, "id27"]], "2.4.0": [[200, "id26"]], "2.5.0": [[200, "id25"]], "2.6.0": [[200, "id24"]], "2.7.0": [[200, "id23"]], "2.8.0": [[200, "id19"]], "2.9.0": [[200, "id17"]], "3. Apply QuantAnalyzer to the model": [[231, "3.-Apply-QuantAnalyzer-to-the-model"], [243, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[233, "3.-Compress-the-model-and-fine-tune"], [234, "3.-Compress-the-model-and-fine-tune"], [235, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[228, "3.-Create-a-quantization-simulation-model"], [236, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[239, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[229, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [230, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [232, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [237, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [240, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [241, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [242, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Evaluate Initial Accuracy Before QAT": [[244, "3.-Evaluate-Initial-Accuracy-Before-QAT"]], "3. Fixing weight quantization": [[225, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range": [[204, "per-layer-encodings-min-max-range"]], "3. Run AutoQuant": [[238, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[227, "run-the-notebooks"]], "3. Version 1.0.0": [[214, "version-1-0-0"]], "3. supergroups": [[223, "supergroups"]], "3.1. Encoding specification": [[214, "id1"]], "4. Apply AdaRound": [[229, "4.-Apply-AdaRound"]], "4. Apply Adaround": [[237, "4.-Apply-Adaround"]], "4. Apply CLE": [[230, "4.-Apply-CLE"], [240, "4.-Apply-CLE"]], "4. Fixing activation quantization": [[225, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[204, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[239, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[241, "4.-Perform-QAT"], [242, "4.-Perform-QAT"]], "4. Run AMP algorithm on the quantized model": [[228, "4.-Run-AMP-algorithm-on-the-quantized-model"], [236, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. Run QAT and Evaluate Post-QAT Accuracy": [[244, "4.-Run-QAT-and-Evaluate-Post-QAT-Accuracy"]], "4. model_input": [[223, "model-input"]], "5. Export Model": [[239, "5.-Export-Model"]], "5. Per-layer mean-square-error loss": [[204, "per-layer-mean-square-error-loss"]], "5. Performing per-layer analysis": [[225, "performing-per-layer-analysis"]], "5. model_output": [[223, "model-output"]], "6. Visualizing sensitive layers": [[225, "visualizing-sensitive-layers"]], "7. Fixing individual quantizers": [[225, "fixing-individual-quantizers"]], "8. Quantize the model": [[225, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[182, null]], "AIMET documentation versions": [[251, null]], "AIMET features": [[248, "aimet-features"], [249, null]], "AIMET visualization": [[211, null]], "API": [[161, "api"], [162, "api"], [167, "api"], [187, "api"], [188, "api"], [189, "api"], [190, "api"], [191, "api"], [192, "api"], [194, "api"], [198, "api"], [199, "api"], [202, "api"], [203, "api"], [204, "api"], [205, "api"], [206, "api"], [210, "api"], [212, "api"], [216, "api"], [217, "api"], [219, "api"], [220, "api"], [221, "api"], [222, "api"]], "API Reference": [[182, "api-reference"]], "API reference": [[164, "api-reference"], [169, "api-reference"]], "Accuracy improvement tools": [[249, "accuracy-improvement-tools"]], "Accuracy-vs-Performance Tradeoff": [[246, "accuracy-vs-performance-tradeoff"]], "AdaScale": [[188, null], [193, "adascale"]], "Adaptive Rounding (AdaRound)": [[229, null], [237, null]], "Adaptive rounding": [[187, null], [193, "adaptive-rounding"]], "Affine quantizers": [[169, "affine-quantizers"]], "Alternative packages": [[185, "alternative-packages"]], "Analysis descriptions": [[204, "analysis-descriptions"]], "Analysis tools": [[201, null], [215, "analysis-tools"]], "Apply LPBQ": [[216, "apply-lpbq"]], "AutoQuant": [[238, null]], "Automatic Mixed-Precision (AMP)": [[228, null], [236, null]], "Automatic mixed precision": [[217, null], [218, "automatic-mixed-precision"]], "Automatic quantization": [[189, null], [193, "automatic-quantization"]], "Batch norm folding": [[191, null], [193, "batch-norm-folding"]], "Batch norm re-estimation": [[190, null], [193, "batch-norm-re-estimation"]], "Blockwise Quantization": [[205, null], [215, "blockwise-quantization"]], "Browse the notebooks": [[227, "browse-the-notebooks"]], "Build AIMET documentation": [[184, "build-aimet-documentation"]], "Build AIMET wheel and run unit tests": [[184, "build-aimet-wheel-and-run-unit-tests"], [184, "id2"]], "Build and run docker container locally": [[184, "build-and-run-docker-container-locally"]], "Building from source": [[184, null], [185, "building-from-source"]], "CLE": [[240, "CLE"]], "Calibration Callback": [[197, "calibration-callback"]], "Call AMP API": [[228, "Call-AMP-API"], [236, "Call-AMP-API"]], "Channel pruning": [[206, null]], "Channel pruning (CP)": [[209, "channel-pruning-cp"]], "Channel selection": [[206, "channel-selection"]], "Choose and install a package": [[185, "choose-and-install-a-package"]], "Code Examples": [[160, "code-examples"], [161, "code-examples"]], "Code example": [[206, "code-example"], [210, "code-example"], [212, "code-example"]], "Compilation": [[245, "compilation"]], "Compile and install pip package dependencies": [[184, "compile-and-install-pip-package-dependencies"]], "Compressing using Spatial SVD": [[210, "compressing-using-spatial-svd"]], "Compression": [[209, null], [211, "compression"], [215, "compression"]], "Compression features Guidebook": [[207, null]], "Compression ratio selection": [[208, "compression-ratio-selection"], [209, "compression-ratio-selection"]], "Compression using Channel Pruning": [[206, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[212, "compression-using-weight-svd"]], "Compute Encodings": [[228, "Compute-Encodings"], [236, "Compute-Encodings"]], "Computing encodings": [[164, "computing-encodings"]], "Conda environment": [[184, "conda-environment"]], "Configuration": [[164, "configuration"]], "Configuration file structure": [[223, "configuration-file-structure"]], "Context": [[187, "context"], [188, "context"], [189, "context"], [190, "context"], [191, "context"], [192, "context"], [194, "context"], [196, "context"], [197, "context"], [198, "context"], [199, "context"], [202, "context"], [203, "context"], [204, "context"], [206, "context"], [210, "context"], [212, "context"], [217, "context"], [220, "context"]], "Conversion": [[245, "conversion"]], "Create Quantization Sim Model": [[228, "Create-Quantization-Sim-Model"], [236, "Create-Quantization-Sim-Model"], [239, "Create-Quantization-Sim-Model"]], "Create QuantizationSimModel": [[197, "create-quantizationsimmodel"]], "Create a new conda environment with Python 3.10": [[184, "create-a-new-conda-environment-with-python-3-10"]], "Create the Quantization Sim Model": [[230, "Create-the-Quantization-Sim-Model"], [232, "Create-the-Quantization-Sim-Model"], [237, "Create-the-Quantization-Sim-Model"], [240, "Create-the-Quantization-Sim-Model"], [241, "Create-the-Quantization-Sim-Model"], [242, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[230, null], [240, null]], "Cross-layer equalization": [[192, null], [193, "cross-layer-equalization"]], "Data type": [[214, "id5"]], "Dataset": [[228, "Dataset"], [229, "Dataset"], [230, "Dataset"], [231, "Dataset"], [232, "Dataset"], [233, "Dataset"], [234, "Dataset"], [235, "Dataset"], [236, "Dataset"], [237, "Dataset"], [238, "Dataset"], [239, "Dataset"], [240, "Dataset"], [241, "Dataset"], [242, "Dataset"], [243, "Dataset"]], "Debugging guidelines": [[248, "debugging-guidelines"]], "Debugging workflow": [[225, "debugging-workflow"]], "Define callback functions for AMP": [[228, "Define-callback-functions-for-AMP"], [236, "Define-callback-functions-for-AMP"]], "Deployment paths": [[249, "deployment-paths"]], "DequantizedTensor": [[147, null]], "Design": [[211, "design"]], "Detailed Workflow": [[246, "detailed-workflow"]], "Determine quantization parameters (encodings)": [[247, "determine-quantization-parameters-encodings"]], "Docker environment": [[184, "docker-environment"]], "Encoding Format Specification": [[214, null]], "Encoding dictionary structure": [[214, "id3"]], "Encoding min/max ranges": [[231, "Encoding-min/max-ranges"], [243, "Encoding-min/max-ranges"]], "Encoding type": [[214, "id4"]], "Example Notebooks": [[182, "example-notebooks"], [227, null]], "Executing blockwise quantization": [[205, "executing-blockwise-quantization"]], "Execution": [[190, "execution"], [192, "execution"], [245, "execution"]], "Export API": [[249, "export-api"]], "Export tools": [[249, "export-tools"]], "Exported Encodings": [[247, "exported-encodings"]], "External resources": [[180, null]], "FAQs": [[209, "faqs"]], "FloatQuantizeDequantize": [[155, null]], "Fold Batch Norm layers": [[230, "Fold-Batch-Norm-layers"], [240, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[228, "Fold-Batch-Normalization-layers"], [232, "Fold-Batch-Normalization-layers"], [236, "Fold-Batch-Normalization-layers"], [237, "Fold-Batch-Normalization-layers"], [241, "Fold-Batch-Normalization-layers"], [242, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[239, "Fold-BatchNorm-Layers"]], "For more information": [[229, "For-more-information"], [230, "For-more-information"], [232, "For-more-information"], [233, "For-more-information"], [234, "For-more-information"], [235, "For-more-information"], [237, "For-more-information"], [238, "For-more-information"], [240, "For-more-information"], [241, "For-more-information"], [242, "For-more-information"]], "General guidelines": [[250, "general-guidelines"]], "Get Started": [[183, "get-started"]], "Glossary": [[181, null], [182, "glossary"]], "Greedy compression ratio selection": [[208, null]], "How it works": [[208, "how-it-works"]], "How quantization simulation works": [[247, "how-quantization-simulation-works"]], "How to modify configuration file": [[223, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[160, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[213, "how-winnowing-works"]], "Installation": [[185, null]], "Installing AIMET": [[186, "installing-aimet"]], "Interactive visualization": [[201, "interactive-visualization"], [202, null]], "Layer output generation": [[201, "layer-output-generation"], [203, null]], "Limitations": [[161, "limitations"]], "Lite mixed precision": [[218, "lite-mixed-precision"], [219, null]], "LoRa Training": [[196, "lora-training"]], "Low-Power Blockwise Quantization": [[215, "low-power-blockwise-quantization"]], "Low-Power Blockwise Quantization (LPBQ)": [[216, null]], "Manual mixed precision": [[218, "manual-mixed-precision"], [220, null]], "Migration Process": [[160, "migration-process"]], "Migration guide": [[160, null]], "Min-Max (also called \u201cTF\u201d in AIMET)": [[247, "min-max-also-called-tf-in-aimet"]], "Mixed Precision": [[215, "mixed-precision"]], "Mixed Precision Algorithm": [[217, "mixed-precision-algorithm"]], "Mixed precision": [[218, null], [249, "mixed-precision"]], "Model compression": [[209, "model-compression"]], "Model compression using channel pruning": [[233, null]], "Model compression using spatial SVD": [[234, null]], "Model compression using spatial SVD and channel pruning": [[235, null]], "Model guidelines": [[224, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[160, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[160, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "NOTE": [[209, null]], "NVIDIA CUDA support": [[184, "nvidia-cuda-support"]], "Next steps": [[229, "Next-steps"], [233, "Next-steps"], [234, "Next-steps"], [235, "Next-steps"], [237, "Next-steps"], [238, "Next-steps"], [240, "Next-steps"], [241, "Next-steps"], [242, "Next-steps"]], "Next: Deploying the model": [[246, "next-deploying-the-model"]], "Next: deploying the model": [[250, "next-deploying-the-model"]], "Old versions": [[185, "old-versions"]], "OmniQuant": [[193, "omniquant"], [194, null]], "On-target inference": [[245, null], [248, "on-target-inference"]], "Optional techniques": [[209, "optional-techniques"]], "Overall flow": [[228, "Overall-flow"], [229, "Overall-flow"], [230, "Overall-flow"], [231, "Overall-flow"], [232, "Overall-flow"], [233, "Overall-flow"], [234, "Overall-flow"], [235, "Overall-flow"], [236, "Overall-flow"], [237, "Overall-flow"], [238, "Overall-flow"], [239, "Overall-flow"], [240, "Overall-flow"], [241, "Overall-flow"], [242, "Overall-flow"], [243, "Overall-flow"]], "Overview": [[182, "overview"], [208, "overview"], [209, "overview"], [211, "overview"], [213, "overview"], [223, "overview"], [247, "overview"]], "PDF of statistics": [[231, "PDF-of-statistics"], [243, "PDF-of-statistics"]], "PTQ": [[196, "ptq"]], "Parameters for AMP algorithm": [[228, "Parameters-for-AMP-algorithm"], [236, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[169, "per-block-quantization"]], "Per-channel quantization": [[169, "per-channel-quantization"]], "Per-layer MSE loss": [[231, "Per-layer-MSE-loss"], [243, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[231, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [243, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[208, "per-layer-exploration"]], "Per-layer fine-tuning": [[209, "per-layer-fine-tuning"]], "Perform QAT": [[239, "Perform-QAT"]], "Phase 0: Find quantizer groups": [[217, "phase-0-find-quantizer-groups"]], "Phase 1: Perform sensitivity analysis": [[217, "phase-1-perform-sensitivity-analysis"]], "Phase 2: Create a Pareto-front list": [[217, "phase-2-create-a-pareto-front-list"]], "Phase 3: Reduce Convert overhead": [[217, "phase-3-reduce-convert-overhead"]], "Post Training Quantization": [[215, "post-training-quantization"], [221, null]], "Post Training Quantization Techniques": [[182, "post-training-quantization-techniques"], [193, null]], "Post-training quantization": [[249, "post-training-quantization"]], "Prerequisites": [[185, "prerequisites"], [187, "prerequisites"], [188, "prerequisites"], [189, "prerequisites"], [190, "prerequisites"], [194, "prerequisites"], [198, "prerequisites"], [199, "prerequisites"], [204, "prerequisites"], [219, "prerequisites"], [220, "prerequisites"], [221, "prerequisites"]], "Procedure": [[188, "procedure"], [189, "procedure"], [191, "procedure"], [194, "procedure"], [198, "procedure"], [199, "procedure"], [206, "procedure"], [217, "procedure"], [250, "procedure"]], "PyPI": [[185, "pypi"]], "PyTorch model guidelines": [[224, null]], "QW-LoRa": [[195, "qw-lora"], [196, null]], "QWA-LoRa": [[195, "qwa-lora"], [197, null]], "Qualcomm\u00ae AI Engine Direct SDK": [[245, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[245, "qualcommreg-ai-hub"]], "Quant Analyzer": [[231, null], [243, null]], "QuantSim workflow": [[247, "quantsim-workflow"]], "Quantization": [[245, "quantization"]], "Quantization Aware Training": [[215, "quantization-aware-training"]], "Quantization analyzer": [[201, "quantization-analyzer"], [204, null]], "Quantization debugging guidelines": [[225, null]], "Quantization granularity": [[247, "quantization-granularity"]], "Quantization schemes": [[247, "quantization-schemes"]], "Quantization simulation": [[232, null], [249, "quantization-simulation"]], "Quantization simulation guide": [[247, null]], "Quantization user guide": [[248, null]], "Quantization workflow": [[246, null], [248, "quantization-workflow"], [250, null]], "Quantization-Aware Training (QAT)": [[244, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[239, null]], "Quantization-aware training": [[222, null], [241, null], [249, "quantization-aware-training"]], "Quantization-aware training with range learning": [[242, null]], "QuantizationMixin": [[19, null]], "Quantize": [[150, null]], "Quantize a small model quickly with AIMET": [[186, "quantize-a-small-model-quickly-with-aimet"]], "Quantize and Update Base Model Weights": [[196, "quantize-and-update-base-model-weights"]], "QuantizeDequantize": [[151, null]], "Quantized LoRa": [[193, "quantized-lora"], [195, null]], "Quantized modules": [[164, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[20, null]], "QuantizedAdaptiveAvgPool2d": [[21, null]], "QuantizedAdaptiveAvgPool3d": [[22, null]], "QuantizedAdaptiveMaxPool1d": [[23, null]], "QuantizedAdaptiveMaxPool2d": [[24, null]], "QuantizedAdaptiveMaxPool3d": [[25, null]], "QuantizedAlphaDropout": [[26, null]], "QuantizedAvgPool1d": [[27, null]], "QuantizedAvgPool2d": [[28, null]], "QuantizedAvgPool3d": [[29, null]], "QuantizedBCELoss": [[30, null]], "QuantizedBCEWithLogitsLoss": [[31, null]], "QuantizedBatchNorm1d": [[32, null]], "QuantizedBatchNorm2d": [[33, null]], "QuantizedBatchNorm3d": [[34, null]], "QuantizedBilinear": [[35, null]], "QuantizedCELU": [[36, null]], "QuantizedCTCLoss": [[37, null]], "QuantizedChannelShuffle": [[38, null]], "QuantizedCircularPad1d": [[39, null]], "QuantizedCircularPad2d": [[40, null]], "QuantizedCircularPad3d": [[41, null]], "QuantizedConstantPad1d": [[42, null]], "QuantizedConstantPad2d": [[43, null]], "QuantizedConstantPad3d": [[44, null]], "QuantizedConv1d": [[45, null]], "QuantizedConv2d": [[46, null]], "QuantizedConv3d": [[47, null]], "QuantizedConvTranspose1d": [[48, null]], "QuantizedConvTranspose2d": [[49, null]], "QuantizedConvTranspose3d": [[50, null]], "QuantizedCosineEmbeddingLoss": [[51, null]], "QuantizedCosineSimilarity": [[52, null]], "QuantizedCrossEntropyLoss": [[53, null]], "QuantizedDropout": [[54, null]], "QuantizedDropout1d": [[55, null]], "QuantizedDropout2d": [[56, null]], "QuantizedDropout3d": [[57, null]], "QuantizedELU": [[58, null]], "QuantizedEmbedding": [[59, null]], "QuantizedEmbeddingBag": [[60, null]], "QuantizedFeatureAlphaDropout": [[61, null]], "QuantizedFlatten": [[62, null]], "QuantizedFold": [[63, null]], "QuantizedFractionalMaxPool2d": [[64, null]], "QuantizedFractionalMaxPool3d": [[65, null]], "QuantizedGELU": [[66, null]], "QuantizedGLU": [[67, null]], "QuantizedGRU": [[68, null]], "QuantizedGRUCell": [[69, null]], "QuantizedGaussianNLLLoss": [[70, null]], "QuantizedGroupNorm": [[71, null]], "QuantizedHardshrink": [[72, null]], "QuantizedHardsigmoid": [[73, null]], "QuantizedHardswish": [[74, null]], "QuantizedHardtanh": [[75, null]], "QuantizedHingeEmbeddingLoss": [[76, null]], "QuantizedHuberLoss": [[77, null]], "QuantizedInstanceNorm1d": [[78, null]], "QuantizedInstanceNorm2d": [[79, null]], "QuantizedInstanceNorm3d": [[80, null]], "QuantizedKLDivLoss": [[81, null]], "QuantizedL1Loss": [[82, null]], "QuantizedLPPool1d": [[83, null]], "QuantizedLPPool2d": [[84, null]], "QuantizedLSTM": [[85, null]], "QuantizedLSTMCell": [[86, null]], "QuantizedLayerNorm": [[87, null]], "QuantizedLeakyReLU": [[88, null]], "QuantizedLinear": [[89, null]], "QuantizedLocalResponseNorm": [[90, null]], "QuantizedLogSigmoid": [[91, null]], "QuantizedLogSoftmax": [[92, null]], "QuantizedMSELoss": [[93, null]], "QuantizedMarginRankingLoss": [[94, null]], "QuantizedMaxPool1d": [[95, null]], "QuantizedMaxPool2d": [[96, null]], "QuantizedMaxPool3d": [[97, null]], "QuantizedMaxUnpool1d": [[98, null]], "QuantizedMaxUnpool2d": [[99, null]], "QuantizedMaxUnpool3d": [[100, null]], "QuantizedMish": [[101, null]], "QuantizedMultiLabelMarginLoss": [[102, null]], "QuantizedMultiLabelSoftMarginLoss": [[103, null]], "QuantizedMultiMarginLoss": [[104, null]], "QuantizedNLLLoss": [[105, null]], "QuantizedNLLLoss2d": [[106, null]], "QuantizedPReLU": [[107, null]], "QuantizedPairwiseDistance": [[108, null]], "QuantizedPixelShuffle": [[109, null]], "QuantizedPixelUnshuffle": [[110, null]], "QuantizedPoissonNLLLoss": [[111, null]], "QuantizedRNN": [[112, null]], "QuantizedRNNCell": [[113, null]], "QuantizedRReLU": [[114, null]], "QuantizedReLU": [[115, null]], "QuantizedReLU6": [[116, null]], "QuantizedReflectionPad1d": [[117, null]], "QuantizedReflectionPad2d": [[118, null]], "QuantizedReflectionPad3d": [[119, null]], "QuantizedReplicationPad1d": [[120, null]], "QuantizedReplicationPad2d": [[121, null]], "QuantizedReplicationPad3d": [[122, null]], "QuantizedSELU": [[123, null]], "QuantizedSiLU": [[124, null]], "QuantizedSigmoid": [[125, null]], "QuantizedSmoothL1Loss": [[126, null]], "QuantizedSoftMarginLoss": [[127, null]], "QuantizedSoftmax": [[128, null]], "QuantizedSoftmax2d": [[129, null]], "QuantizedSoftmin": [[130, null]], "QuantizedSoftplus": [[131, null]], "QuantizedSoftshrink": [[132, null]], "QuantizedSoftsign": [[133, null]], "QuantizedTanh": [[134, null]], "QuantizedTanhshrink": [[135, null]], "QuantizedTensor": [[148, null]], "QuantizedTensorBase": [[149, null]], "QuantizedThreshold": [[136, null]], "QuantizedTripletMarginLoss": [[137, null]], "QuantizedTripletMarginWithDistanceLoss": [[138, null]], "QuantizedUnflatten": [[139, null]], "QuantizedUnfold": [[140, null]], "QuantizedUpsample": [[141, null]], "QuantizedUpsamplingBilinear2d": [[142, null]], "QuantizedUpsamplingNearest2d": [[143, null]], "QuantizedZeroPad1d": [[144, null]], "QuantizedZeroPad2d": [[145, null]], "QuantizedZeroPad3d": [[146, null]], "Quantizer Args structure": [[214, "id6"]], "Quantizers": [[169, "quantizers"]], "Quick Start": [[186, null]], "Rank Rounding": [[209, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[239, "Re-estimate-BatchNorm-Statistics"]], "References": [[209, "references"]], "Release Notes": [[182, "release-notes"]], "Release notes": [[200, null]], "Run QWA-LoRa": [[197, "run-qwa-lora"]], "Running the notebooks": [[227, "running-the-notebooks"]], "Runtime configuration": [[223, null], [247, "runtime-configuration"]], "Sequential MSE": [[193, "sequential-mse"], [198, null]], "Set environment variables to build desired AIMET wheel": [[184, "set-environment-variables-to-build-desired-aimet-wheel"], [184, "id1"]], "Set model input precision": [[220, "set-model-input-precision"]], "Set model output precision": [[220, "set-model-output-precision"]], "Set precision based on layer type": [[220, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[220, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[220, "set-precision-of-a-non-leaf-layer"]], "Setup": [[187, "setup"], [188, "setup"], [190, "setup"], [192, "setup"], [194, "setup"], [196, "setup"], [197, "setup"], [198, "setup"], [199, "setup"], [206, "setup"], [210, "setup"], [212, "setup"], [220, "setup"]], "Signal-to-Quantization-Noise": [[247, "signal-to-quantization-noise"]], "Simulate quantization noise": [[247, "simulate-quantization-noise"]], "Spatial SVD": [[209, "spatial-svd"], [210, null]], "SpinQuant": [[193, "spinquant"], [199, null]], "Starting a Bokeh server session": [[211, "starting-a-bokeh-server-session"]], "Step 1": [[187, "step-1"], [188, "step-1"], [189, "step-1"], [190, "step-1"], [191, "step-1"], [194, "step-1"], [198, "step-1"], [199, "step-1"], [217, "step-1"]], "Step 1 Importing libraries": [[204, "step-1-importing-libraries"]], "Step 1: Applying MMP API options": [[220, "step-1-applying-mmp-api-options"]], "Step 1: Creating a QuantSim model": [[219, "step-1-creating-a-quantsim-model"], [221, "step-1-creating-a-quantsim-model"]], "Step 1: Find baseline precision": [[246, "step-1-find-baseline-precision"]], "Step 1: Importing the API": [[203, "step-1-importing-the-api"]], "Step 1: Setup": [[222, "step-1-setup"]], "Step 1: Trying FP16 precision (no quantization)": [[250, "step-1-trying-fp16-precision-no-quantization"]], "Step 2": [[187, "step-2"], [188, "step-2"], [189, "step-2"], [190, "step-2"], [191, "step-2"], [194, "step-2"], [198, "step-2"], [199, "step-2"], [217, "step-2"]], "Step 2 Preparing calibration callback": [[204, "step-2-preparing-calibration-callback"]], "Step 2: Applying the profile": [[220, "step-2-applying-the-profile"]], "Step 2: Compute initial quantization parameters": [[222, "step-2-compute-initial-quantization-parameters"]], "Step 2: Computing encodings": [[219, "step-2-computing-encodings"]], "Step 2: Creating a calibration callback": [[221, "step-2-creating-a-calibration-callback"]], "Step 2: Loading a model": [[203, "step-2-loading-a-model"]], "Step 2: Use lite mixed precision": [[246, "step-2-use-lite-mixed-precision"]], "Step 2: Verifying W16A16 quantization": [[250, "step-2-verifying-w16a16-quantization"]], "Step 3": [[187, "step-3"], [188, "step-3"], [189, "step-3"], [190, "step-3"], [191, "step-3"], [194, "step-3"], [198, "step-3"], [199, "step-3"]], "Step 3 Preparing evaluation callback": [[204, "step-3-preparing-evaluation-callback"]], "Step 3. Reducing precision": [[250, "step-3-reducing-precision"]], "Step 3: Computing encodings": [[221, "step-3-computing-encodings"]], "Step 3: Evaluation of w8a8 base precision": [[219, "step-3-evaluation-of-w8a8-base-precision"]], "Step 3: Obtaining inputs": [[203, "step-3-obtaining-inputs"]], "Step 3: Run quantization-aware training": [[222, "step-3-run-quantization-aware-training"]], "Step 3: Use Automatic Mixed Precision (AMP)": [[246, "step-3-use-automatic-mixed-precision-amp"]], "Step 4": [[187, "step-4"], [188, "step-4"], [189, "step-4"], [190, "step-4"], [194, "step-4"], [198, "step-4"], [199, "step-4"]], "Step 4 Preparing model": [[204, "step-4-preparing-model"]], "Step 4. Restoring accuracy": [[250, "step-4-restoring-accuracy"]], "Step 4: Evaluation": [[221, "step-4-evaluation"]], "Step 4: Generating layer outputs": [[203, "step-4-generating-layer-outputs"]], "Step 4: Perform sensitivity analysis": [[219, "step-4-perform-sensitivity-analysis"]], "Step 4: Use advanced Post-Training Quantization (PTQ) techniques": [[246, "step-4-use-advanced-post-training-quantization-ptq-techniques"]], "Step 5": [[188, "step-5"], [189, "step-5"], [194, "step-5"], [198, "step-5"], [199, "step-5"]], "Step 5 Creating QuantAnalyzer": [[204, "step-5-creating-quantanalyzer"]], "Step 5: Apply precision adjustment": [[219, "step-5-apply-precision-adjustment"]], "Step 5: Exporting the model": [[221, "step-5-exporting-the-model"]], "Step 5: Use Quantization-Aware Training (QAT)": [[246, "step-5-use-quantization-aware-training-qat"]], "Step 6": [[189, "step-6"]], "Step 6 Running the analysis": [[204, "step-6-running-the-analysis"]], "Step 6: Recompute encodings": [[219, "step-6-recompute-encodings"]], "Step 7": [[189, "step-7"]], "Step 7: Evaluation of w8a8_mixed precision": [[219, "step-7-evaluation-of-w8a8-mixed-precision"]], "Summary": [[239, "Summary"]], "Supported platform": [[183, "supported-platform"]], "Supported precisions for on-target inference": [[249, "supported-precisions-for-on-target-inference"]], "Techniques": [[182, "techniques"], [215, null], [240, "Techniques"]], "Terminology": [[167, "terminology"]], "Tested platform": [[186, "tested-platform"]], "Top level structure": [[214, "id2"]], "Training Callback": [[197, "training-callback"]], "Tutorials": [[182, "tutorials"], [226, null]], "Typical recommendations": [[222, "typical-recommendations"]], "Use Case": [[209, "use-case"]], "Use Cases": [[217, "use-cases"]], "User flow": [[167, "user-flow"]], "Variants of QAT": [[222, "variants-of-qat"]], "Verifying the installation": [[185, "verifying-the-installation"], [186, "verifying-the-installation"]], "Visualization Tools": [[157, "visualization-tools"]], "Visualizing compression ratios": [[211, "visualizing-compression-ratios"]], "Weight SVD": [[209, "weight-svd"], [212, null]], "Weight reconstruction": [[206, "weight-reconstruction"]], "What is AIMET?": [[183, null]], "What this notebook is not": [[228, "What-this-notebook-is-not"], [231, "What-this-notebook-is-not"], [236, "What-this-notebook-is-not"], [239, "What-this-notebook-is-not"], [243, "What-this-notebook-is-not"]], "Winnowing": [[206, "winnowing"], [213, null], [213, "id1"]], "Workflow": [[187, "workflow"], [187, "id2"], [188, "workflow"], [189, "workflow"], [190, "workflow"], [191, "workflow"], [192, "workflow"], [194, "workflow"], [196, "workflow"], [197, "workflow"], [198, "workflow"], [199, "workflow"], [202, "workflow"], [203, "workflow"], [204, "workflow"], [206, "workflow"], [210, "workflow"], [212, "workflow"], [217, "workflow"], [219, "workflow"], [220, "workflow"], [221, "workflow"], [222, "workflow"]], "aimet_onnx API": [[5, null]], "aimet_onnx.apply_adaround": [[1, null]], "aimet_onnx.apply_seq_mse": [[11, null]], "aimet_onnx.batch_norm_fold": [[3, null]], "aimet_onnx.cross_layer_equalization": [[4, null]], "aimet_onnx.layer_output_utils": [[6, null]], "aimet_onnx.lite_mp": [[7, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[9, null]], "aimet_onnx.quantsim": [[10, null]], "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights": [[8, null]], "aimet_torch": [[156, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[160, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[156, null]], "aimet_torch.adaround": [[12, null]], "aimet_torch.auto_quant": [[14, null]], "aimet_torch.batch_norm_fold": [[16, null]], "aimet_torch.bn_reestimation": [[15, null]], "aimet_torch.compress": [[18, null]], "aimet_torch.cross_layer_equalization": [[17, null]], "aimet_torch.experimental.adascale": [[13, null]], "aimet_torch.experimental.omniquant": [[165, null]], "aimet_torch.experimental.spinquant": [[172, null]], "aimet_torch.layer_output_utils": [[158, null]], "aimet_torch.mixed_precision": [[163, null]], "aimet_torch.model_preparer": [[161, null]], "aimet_torch.model_validator": [[162, null]], "aimet_torch.nn": [[164, null]], "aimet_torch.onnx.export": [[166, null]], "aimet_torch.peft": [[167, null]], "aimet_torch.quant_analyzer": [[168, null]], "aimet_torch.quantization": [[169, null]], "aimet_torch.quantsim": [[170, null]], "aimet_torch.quantsim.config_utils": [[159, null]], "aimet_torch.seq_mse": [[171, null]], "aimet_torch.utils": [[173, null]], "aimet_torch.v1": [[156, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[174, null]], "aimet_torch.v1.auto_quant": [[176, null]], "aimet_torch.v1.mixed_precision": [[175, null]], "aimet_torch.v1.quant_analyzer": [[177, null]], "aimet_torch.v1.quantsim": [[178, null]], "aimet_torch.v1.seq_mse": [[179, null]], "aimet_torch.visualization_tools": [[157, null]], "dequantize": [[152, null]], "quantize": [[153, null]], "quantize_dequantize": [[154, null]], "\ud83c\udfc1 Conclusion": [[244, "\ud83c\udfc1-Conclusion"]], "\ud83d\udcc1 Before Getting Started: Prepare the ImageNet Dataset": [[244, "\ud83d\udcc1-Before-Getting-Started:-Prepare-the-ImageNet-Dataset"]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/litemp", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/torch/adaround", "apiref/torch/adascale", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/omniquant", "apiref/torch/onnx", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/spinquant", "apiref/torch/utils", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "external/index", "glossary", "index", "overview/index", "overview/install/build_from_source", "overview/install/index", "overview/install/quick-start", "ptq_techniques/adaround", "ptq_techniques/adascale", "ptq_techniques/autoquant", "ptq_techniques/bn", "ptq_techniques/bnf", "ptq_techniques/cle", "ptq_techniques/index", "ptq_techniques/omniquant", "ptq_techniques/quantized_LoRa/index", "ptq_techniques/quantized_LoRa/qw_lora", "ptq_techniques/quantized_LoRa/qwa_lora", "ptq_techniques/seq_mse", "ptq_techniques/spinquant", "release_notes", "techniques/analysis_tools/index", "techniques/analysis_tools/interactive_visualization", "techniques/analysis_tools/layer_output_generation", "techniques/analysis_tools/quant_analyzer", "techniques/blockwise", "techniques/compression/channel_pruning", "techniques/compression/feature_guidebook", "techniques/compression/greedy_compression_ratio_selection", "techniques/compression/index", "techniques/compression/spatial_svd", "techniques/compression/visualization_compression", "techniques/compression/weight_svd", "techniques/compression/winnowing", "techniques/encoding_spec", "techniques/index", "techniques/lpbq", "techniques/mixed_precision/amp", "techniques/mixed_precision/index", "techniques/mixed_precision/litemp", "techniques/mixed_precision/mmp", "techniques/ptq", "techniques/qat", "techniques/runtime_config", "techniques/torch/model_guidelines", "tutorials/debugging_guidelines", "tutorials/index", "tutorials/notebooks", "tutorials/notebooks/onnx/quantization/AMP", "tutorials/notebooks/onnx/quantization/adaround", "tutorials/notebooks/onnx/quantization/cle", "tutorials/notebooks/onnx/quantization/quant_analyzer", "tutorials/notebooks/onnx/quantization/quantsim", "tutorials/notebooks/torch/compression/channel_pruning", "tutorials/notebooks/torch/compression/spatial_svd", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning", "tutorials/notebooks/torch/quantization/AMP", "tutorials/notebooks/torch/quantization/adaround", "tutorials/notebooks/torch/quantization/autoquant", "tutorials/notebooks/torch/quantization/bn_reestimation", "tutorials/notebooks/torch/quantization/cle", "tutorials/notebooks/torch/quantization/qat", "tutorials/notebooks/torch/quantization/qat_range_learning", "tutorials/notebooks/torch/quantization/quant_analyzer", "tutorials/notebooks/torch/v2/qat", "tutorials/on_target_inference", "tutorials/quantization_workflow", "tutorials/quantsim", "userguide/index", "userguide/quantization_tools", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/litemp.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/torch/adaround.rst", "apiref/torch/adascale.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/omniquant.rst", "apiref/torch/onnx.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/spinquant.rst", "apiref/torch/utils.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "external/index.rst", "glossary.rst", "index.rst", "overview/index.rst", "overview/install/build_from_source.rst", "overview/install/index.rst", "overview/install/quick-start.rst", "ptq_techniques/adaround.rst", "ptq_techniques/adascale.rst", "ptq_techniques/autoquant.rst", "ptq_techniques/bn.rst", "ptq_techniques/bnf.rst", "ptq_techniques/cle.rst", "ptq_techniques/index.rst", "ptq_techniques/omniquant.rst", "ptq_techniques/quantized_LoRa/index.rst", "ptq_techniques/quantized_LoRa/qw_lora.rst", "ptq_techniques/quantized_LoRa/qwa_lora.rst", "ptq_techniques/seq_mse.rst", "ptq_techniques/spinquant.rst", "release_notes.rst", "techniques/analysis_tools/index.rst", "techniques/analysis_tools/interactive_visualization.rst", "techniques/analysis_tools/layer_output_generation.rst", "techniques/analysis_tools/quant_analyzer.rst", "techniques/blockwise.rst", "techniques/compression/channel_pruning.rst", "techniques/compression/feature_guidebook.rst", "techniques/compression/greedy_compression_ratio_selection.rst", "techniques/compression/index.rst", "techniques/compression/spatial_svd.rst", "techniques/compression/visualization_compression.rst", "techniques/compression/weight_svd.rst", "techniques/compression/winnowing.rst", "techniques/encoding_spec.rst", "techniques/index.rst", "techniques/lpbq.rst", "techniques/mixed_precision/amp.rst", "techniques/mixed_precision/index.rst", "techniques/mixed_precision/litemp.rst", "techniques/mixed_precision/mmp.rst", "techniques/ptq.rst", "techniques/qat.rst", "techniques/runtime_config.rst", "techniques/torch/model_guidelines.rst", "tutorials/debugging_guidelines.rst", "tutorials/index.rst", "tutorials/notebooks.rst", "tutorials/notebooks/onnx/quantization/AMP.ipynb", "tutorials/notebooks/onnx/quantization/adaround.ipynb", "tutorials/notebooks/onnx/quantization/cle.ipynb", "tutorials/notebooks/onnx/quantization/quant_analyzer.ipynb", "tutorials/notebooks/onnx/quantization/quantsim.ipynb", "tutorials/notebooks/torch/compression/channel_pruning.ipynb", "tutorials/notebooks/torch/compression/spatial_svd.ipynb", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning.ipynb", "tutorials/notebooks/torch/quantization/AMP.ipynb", "tutorials/notebooks/torch/quantization/adaround.ipynb", "tutorials/notebooks/torch/quantization/autoquant.ipynb", "tutorials/notebooks/torch/quantization/bn_reestimation.ipynb", "tutorials/notebooks/torch/quantization/cle.ipynb", "tutorials/notebooks/torch/quantization/qat.ipynb", "tutorials/notebooks/torch/quantization/qat_range_learning.ipynb", "tutorials/notebooks/torch/quantization/quant_analyzer.ipynb", "tutorials/notebooks/torch/v2/qat.ipynb", "tutorials/on_target_inference.rst", "tutorials/quantization_workflow.rst", "tutorials/quantsim.rst", "userguide/index.rst", "userguide/quantization_tools.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[181, "term-Accelerator", true]], "accuracy": [[181, "term-Accuracy", true]], "activation": [[181, "term-Activation", true]], "activation quantization": [[181, "term-Activation-Quantization", true]], "adaround": [[181, "term-AdaRound", true]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[174, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[181, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[181, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [204, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [220, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[174, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adascale() (in module aimet_torch.experimental.adascale)": [[13, "aimet_torch.experimental.adascale.apply_adascale", false], [188, "aimet_torch.experimental.adascale.apply_adascale", false]], "apply_omniquant() (in module aimet_torch.experimental.omniquant)": [[165, "aimet_torch.experimental.omniquant.apply_omniquant", false], [194, "aimet_torch.experimental.omniquant.apply_omniquant", false]], "apply_seq_mse() (in module aimet_onnx)": [[11, "aimet_onnx.apply_seq_mse", false], [198, "aimet_onnx.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[171, "aimet_torch.seq_mse.apply_seq_mse", false], [198, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[179, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "apply_spinquant() (in module aimet_torch.experimental.spinquant)": [[172, "aimet_torch.experimental.spinquant.apply_spinquant", false], [199, "aimet_torch.experimental.spinquant.apply_spinquant", false]], "autoquant": [[181, "term-AutoQuant", true]], "batch normalization": [[181, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[181, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[181, "term-BN", true]], "callbackfunc (class in aimet_onnx.common.defs)": [[2, "aimet_onnx.common.defs.CallbackFunc", false], [217, "aimet_onnx.common.defs.CallbackFunc", false]], "callbackfunc (class in aimet_torch.common.defs)": [[163, "aimet_torch.common.defs.CallbackFunc", false], [175, "aimet_torch.common.defs.CallbackFunc", false], [217, "aimet_torch.common.defs.CallbackFunc", false]], "callbackfunc (class in aimet_torch.common.utils)": [[168, "aimet_torch.common.utils.CallbackFunc", false], [177, "aimet_torch.common.utils.CallbackFunc", false], [204, "aimet_torch.common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [217, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[163, "aimet_torch.mixed_precision.choose_mixed_precision", false], [217, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[175, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[181, "term-CNN", true]], "compression": [[181, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[178, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (in module aimet_onnx)": [[10, "aimet_onnx.compute_encodings", false]], "convolutional layer": [[181, "term-Convolutional-Layer", true]], "convolutional neural network": [[181, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[152, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[147, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[181, "term-Device", true]], "dlf": [[181, "term-DLF", true]], "dynamic layer fusion": [[181, "term-Dynamic-Layer-Fusion", true]], "edge device": [[181, "term-Edge-device", true]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [204, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[181, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[4, "aimet_onnx.cross_layer_equalization.equalize_model", false], [192, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[17, "aimet_torch.cross_layer_equalization.equalize_model", false], [192, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [217, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[163, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [175, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [217, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "exponent_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.exponent_bits", false]], "export() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.export", false]], "export() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.export", false]], "export() (aimet_torch.quantsim.quantizationsimmodelonnxexporter method)": [[170, "aimet_torch.quantsim.QuantizationSimModelOnnxExporter.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[178, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export() (in module aimet_torch.onnx)": [[166, "aimet_torch.onnx.export", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[16, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [191, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[3, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [191, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "fold_param_quantizers() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.fold_param_quantizers", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad1d method)": [[39, "aimet_torch.nn.QuantizedCircularPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad2d method)": [[40, "aimet_torch.nn.QuantizedCircularPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad3d method)": [[41, "aimet_torch.nn.QuantizedCircularPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad1d method)": [[42, "aimet_torch.nn.QuantizedConstantPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad2d method)": [[43, "aimet_torch.nn.QuantizedConstantPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad3d method)": [[44, "aimet_torch.nn.QuantizedConstantPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedhardsigmoid method)": [[73, "aimet_torch.nn.QuantizedHardsigmoid.forward", false]], "forward() (aimet_torch.nn.quantizedhardswish method)": [[74, "aimet_torch.nn.QuantizedHardswish.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[89, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad1d method)": [[117, "aimet_torch.nn.QuantizedReflectionPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad2d method)": [[118, "aimet_torch.nn.QuantizedReflectionPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad3d method)": [[119, "aimet_torch.nn.QuantizedReflectionPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad1d method)": [[120, "aimet_torch.nn.QuantizedReplicationPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad2d method)": [[121, "aimet_torch.nn.QuantizedReplicationPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad3d method)": [[122, "aimet_torch.nn.QuantizedReplicationPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedthreshold method)": [[136, "aimet_torch.nn.QuantizedThreshold.forward", false]], "forward() (aimet_torch.nn.quantizedtripletmarginwithdistanceloss method)": [[138, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.forward", false]], "forward() (aimet_torch.nn.quantizedunflatten method)": [[139, "aimet_torch.nn.QuantizedUnflatten.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad1d method)": [[144, "aimet_torch.nn.QuantizedZeroPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad2d method)": [[145, "aimet_torch.nn.QuantizedZeroPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad3d method)": [[146, "aimet_torch.nn.QuantizedZeroPad3d.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[150, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[171, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [198, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[179, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[181, "term-FP32", true]], "from_encodings() (aimet_torch.quantization.float.floatquantizedequantize class method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.from_encodings", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.from_module", false]], "from_str() (aimet_onnx.common.defs.quantscheme class method)": [[10, "aimet_onnx.common.defs.QuantScheme.from_str", false]], "from_str() (aimet_torch.common.defs.quantscheme class method)": [[170, "aimet_torch.common.defs.QuantScheme.from_str", false], [178, "aimet_torch.common.defs.QuantScheme.from_str", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [203, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[158, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [203, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_loss_fn() (aimet_torch.seq_mse.seqmseparams method)": [[171, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false], [198, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false]], "get_loss_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[179, "aimet_torch.v1.seq_mse.SeqMseParams.get_loss_fn", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "ignore() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.ignore", false]], "ignore_unknown_modules() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.ignore_unknown_modules", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[181, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[181, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[181, "term-KL-Divergence", true]], "layer": [[181, "term-Layer", true]], "layer-wise quantization": [[181, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [203, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[158, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [203, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[178, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lora mobilenet": [[181, "term-LoRA-MobileNet", true]], "mantissa_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.mantissa_bits", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [220, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[181, "term-Model", true]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "named_qmodules() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.named_qmodules", false]], "named_quantizer_parameters() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.named_quantizer_parameters", false]], "named_quantizers() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.named_quantizers", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[158, "aimet_torch.layer_output_utils.NamingScheme", false], [203, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[181, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[181, "term-NNCF", true]], "node": [[181, "term-Node", true]], "normalization": [[181, "term-Normalization", true]], "onnx": [[181, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [203, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "onnx (aimet_torch.quantizationsimmodel property)": [[170, "aimet_torch.QuantizationSimModel.onnx", false]], "open neural network exchange": [[181, "term-Open-Neural-Network-Exchange", true]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "per-channel quantization": [[181, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[181, "term-Post-Training-Quantization", true]], "prepare_model() (in module aimet_torch.model_preparer)": [[161, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[181, "term-Pruning", true]], "ptq": [[181, "term-PTQ", true]], "pytorch": [[181, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [203, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[181, "term-QAT", true]], "qdo": [[181, "term-QDO", true]], "qmodules() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.qmodules", false]], "qualcomm innovation center": [[181, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [204, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[168, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [204, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[177, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[181, "term-Quantization", true]], "quantization simulation": [[181, "term-Quantization-Simulation", true]], "quantization-aware training": [[181, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[19, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx)": [[10, "aimet_onnx.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch)": [[170, "aimet_torch.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[178, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantizationsimmodelonnxexporter (class in aimet_torch.quantsim)": [[170, "aimet_torch.quantsim.QuantizationSimModelOnnxExporter", false]], "quantize (class in aimet_torch.quantization.affine)": [[150, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[153, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[154, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[20, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[21, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[22, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[23, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[24, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[25, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[26, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[27, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[28, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[29, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[148, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[149, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizer_parameters() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.quantizer_parameters", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantizers() (aimet_torch.quantizationsimmodel method)": [[170, "aimet_torch.QuantizationSimModel.quantizers", false]], "quantscheme (class in aimet_onnx.common.defs)": [[10, "aimet_onnx.common.defs.QuantScheme", false]], "quantscheme (class in aimet_torch.common.defs)": [[170, "aimet_torch.common.defs.QuantScheme", false], [178, "aimet_torch.common.defs.QuantScheme", false]], "quantsim": [[181, "term-QuantSim", true]], "quic": [[181, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[15, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [190, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "remove_activation_quantizers() (in module aimet_torch.utils)": [[173, "aimet_torch.utils.remove_activation_quantizers", false]], "remove_all_quantizers() (in module aimet_torch.utils)": [[173, "aimet_torch.utils.remove_all_quantizers", false]], "remove_input_quantizers() (in module aimet_torch.utils)": [[173, "aimet_torch.utils.remove_input_quantizers", false]], "remove_output_quantizers() (in module aimet_torch.utils)": [[173, "aimet_torch.utils.remove_output_quantizers", false]], "remove_param_quantizers() (in module aimet_torch.utils)": [[173, "aimet_torch.utils.remove_param_quantizers", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[178, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[171, "aimet_torch.seq_mse.SeqMseParams", false], [198, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[179, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_onnx.quantsim)": [[8, "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[159, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [220, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [220, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[163, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [220, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [217, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[163, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [175, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [217, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[181, "term-Target-Hardware-Accelerator", true]], "target runtime": [[181, "term-Target-Runtime", true]], "tensorflow": [[181, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [217, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[163, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [175, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [217, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_onnx_qdq() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.to_onnx_qdq", false]], "torchscript": [[181, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[158, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [203, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[162, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[181, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[157, "aimet_torch.v2.visualization_tools.visualize_stats", false], [202, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[181, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_onnx": [[10, 0, 1, "", "QuantizationSimModel"], [198, 2, 1, "", "apply_seq_mse"], [10, 2, 1, "", "compute_encodings"]], "aimet_onnx.QuantizationSimModel": [[10, 1, 1, "", "compute_encodings"], [10, 1, 1, "", "export"], [10, 1, 1, "", "to_onnx_qdq"]], "aimet_onnx.amp.mixed_precision_algo": [[217, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[217, 1, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[217, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[217, 1, 1, "", "get_activation_quantizers"], [217, 1, 1, "", "get_active_quantizers"], [217, 1, 1, "", "get_candidate"], [217, 1, 1, "", "get_param_quantizers"], [217, 1, 1, "", "set_quantizers_to_candidate"], [217, 1, 1, "", "to_list"]], "aimet_onnx.batch_norm_fold": [[191, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.common.defs": [[217, 0, 1, "", "CallbackFunc"], [10, 0, 1, "", "QuantScheme"]], "aimet_onnx.common.defs.QuantScheme": [[10, 1, 1, "", "from_str"]], "aimet_onnx.cross_layer_equalization": [[192, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[203, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[203, 1, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[217, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[204, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[204, 1, 1, "", "analyze"], [204, 1, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[8, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch": [[170, 0, 1, "", "QuantizationSimModel"]], "aimet_torch.QuantizationSimModel": [[170, 1, 1, "", "compute_encodings"], [170, 1, 1, "", "export"], [170, 1, 1, "", "fold_param_quantizers"], [170, 1, 1, "", "named_qmodules"], [170, 1, 1, "", "named_quantizer_parameters"], [170, 1, 1, "", "named_quantizers"], [170, 3, 1, "", "onnx"], [170, 1, 1, "", "qmodules"], [170, 1, 1, "", "quantizer_parameters"], [170, 1, 1, "", "quantizers"]], "aimet_torch.amp.mixed_precision_algo": [[217, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[217, 1, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[217, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[217, 1, 1, "", "get_active_quantizers"], [217, 1, 1, "", "get_candidate"], [217, 1, 1, "", "get_input_quantizer_modules"], [217, 1, 1, "", "set_quantizers_to_candidate"], [217, 1, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[191, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[190, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.common.defs": [[217, 0, 1, "", "CallbackFunc"], [178, 0, 1, "", "QuantScheme"]], "aimet_torch.common.defs.QuantScheme": [[178, 1, 1, "", "from_str"]], "aimet_torch.common.utils": [[204, 0, 1, "", "CallbackFunc"]], "aimet_torch.cross_layer_equalization": [[192, 2, 1, "", "equalize_model"]], "aimet_torch.experimental.adascale": [[188, 2, 1, "", "apply_adascale"]], "aimet_torch.experimental.omniquant": [[194, 2, 1, "", "apply_omniquant"]], "aimet_torch.experimental.spinquant": [[199, 2, 1, "", "apply_spinquant"]], "aimet_torch.layer_output_utils": [[203, 0, 1, "", "LayerOutputUtil"], [203, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[203, 1, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[203, 4, 1, "", "ONNX"], [203, 4, 1, "", "PYTORCH"], [203, 4, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[217, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[161, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[162, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[162, 1, 1, "", "add_check"], [162, 1, 1, "", "validate_model"]], "aimet_torch.nn": [[19, 0, 1, "", "QuantizationMixin"], [20, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [21, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [22, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [23, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [24, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [25, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [26, 0, 1, "", "QuantizedAlphaDropout"], [27, 0, 1, "", "QuantizedAvgPool1d"], [28, 0, 1, "", "QuantizedAvgPool2d"], [29, 0, 1, "", "QuantizedAvgPool3d"], [30, 0, 1, "", "QuantizedBCELoss"], [31, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [32, 0, 1, "", "QuantizedBatchNorm1d"], [33, 0, 1, "", "QuantizedBatchNorm2d"], [34, 0, 1, "", "QuantizedBatchNorm3d"], [35, 0, 1, "", "QuantizedBilinear"], [36, 0, 1, "", "QuantizedCELU"], [37, 0, 1, "", "QuantizedCTCLoss"], [38, 0, 1, "", "QuantizedChannelShuffle"], [39, 0, 1, "", "QuantizedCircularPad1d"], [40, 0, 1, "", "QuantizedCircularPad2d"], [41, 0, 1, "", "QuantizedCircularPad3d"], [42, 0, 1, "", "QuantizedConstantPad1d"], [43, 0, 1, "", "QuantizedConstantPad2d"], [44, 0, 1, "", "QuantizedConstantPad3d"], [45, 0, 1, "", "QuantizedConv1d"], [46, 0, 1, "", "QuantizedConv2d"], [47, 0, 1, "", "QuantizedConv3d"], [48, 0, 1, "", "QuantizedConvTranspose1d"], [49, 0, 1, "", "QuantizedConvTranspose2d"], [50, 0, 1, "", "QuantizedConvTranspose3d"], [51, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [52, 0, 1, "", "QuantizedCosineSimilarity"], [53, 0, 1, "", "QuantizedCrossEntropyLoss"], [54, 0, 1, "", "QuantizedDropout"], [55, 0, 1, "", "QuantizedDropout1d"], [56, 0, 1, "", "QuantizedDropout2d"], [57, 0, 1, "", "QuantizedDropout3d"], [58, 0, 1, "", "QuantizedELU"], [59, 0, 1, "", "QuantizedEmbedding"], [60, 0, 1, "", "QuantizedEmbeddingBag"], [61, 0, 1, "", "QuantizedFeatureAlphaDropout"], [62, 0, 1, "", "QuantizedFlatten"], [63, 0, 1, "", "QuantizedFold"], [64, 0, 1, "", "QuantizedFractionalMaxPool2d"], [65, 0, 1, "", "QuantizedFractionalMaxPool3d"], [66, 0, 1, "", "QuantizedGELU"], [67, 0, 1, "", "QuantizedGLU"], [68, 0, 1, "", "QuantizedGRU"], [69, 0, 1, "", "QuantizedGRUCell"], [70, 0, 1, "", "QuantizedGaussianNLLLoss"], [71, 0, 1, "", "QuantizedGroupNorm"], [72, 0, 1, "", "QuantizedHardshrink"], [73, 0, 1, "", "QuantizedHardsigmoid"], [74, 0, 1, "", "QuantizedHardswish"], [75, 0, 1, "", "QuantizedHardtanh"], [76, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [77, 0, 1, "", "QuantizedHuberLoss"], [78, 0, 1, "", "QuantizedInstanceNorm1d"], [79, 0, 1, "", "QuantizedInstanceNorm2d"], [80, 0, 1, "", "QuantizedInstanceNorm3d"], [81, 0, 1, "", "QuantizedKLDivLoss"], [82, 0, 1, "", "QuantizedL1Loss"], [83, 0, 1, "", "QuantizedLPPool1d"], [84, 0, 1, "", "QuantizedLPPool2d"], [85, 0, 1, "", "QuantizedLSTM"], [86, 0, 1, "", "QuantizedLSTMCell"], [87, 0, 1, "", "QuantizedLayerNorm"], [88, 0, 1, "", "QuantizedLeakyReLU"], [89, 0, 1, "", "QuantizedLinear"], [90, 0, 1, "", "QuantizedLocalResponseNorm"], [91, 0, 1, "", "QuantizedLogSigmoid"], [92, 0, 1, "", "QuantizedLogSoftmax"], [93, 0, 1, "", "QuantizedMSELoss"], [94, 0, 1, "", "QuantizedMarginRankingLoss"], [95, 0, 1, "", "QuantizedMaxPool1d"], [96, 0, 1, "", "QuantizedMaxPool2d"], [97, 0, 1, "", "QuantizedMaxPool3d"], [98, 0, 1, "", "QuantizedMaxUnpool1d"], [99, 0, 1, "", "QuantizedMaxUnpool2d"], [100, 0, 1, "", "QuantizedMaxUnpool3d"], [101, 0, 1, "", "QuantizedMish"], [102, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [103, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [104, 0, 1, "", "QuantizedMultiMarginLoss"], [105, 0, 1, "", "QuantizedNLLLoss"], [106, 0, 1, "", "QuantizedNLLLoss2d"], [107, 0, 1, "", "QuantizedPReLU"], [108, 0, 1, "", "QuantizedPairwiseDistance"], [109, 0, 1, "", "QuantizedPixelShuffle"], [110, 0, 1, "", "QuantizedPixelUnshuffle"], [111, 0, 1, "", "QuantizedPoissonNLLLoss"], [112, 0, 1, "", "QuantizedRNN"], [113, 0, 1, "", "QuantizedRNNCell"], [114, 0, 1, "", "QuantizedRReLU"], [115, 0, 1, "", "QuantizedReLU"], [116, 0, 1, "", "QuantizedReLU6"], [117, 0, 1, "", "QuantizedReflectionPad1d"], [118, 0, 1, "", "QuantizedReflectionPad2d"], [119, 0, 1, "", "QuantizedReflectionPad3d"], [120, 0, 1, "", "QuantizedReplicationPad1d"], [121, 0, 1, "", "QuantizedReplicationPad2d"], [122, 0, 1, "", "QuantizedReplicationPad3d"], [123, 0, 1, "", "QuantizedSELU"], [124, 0, 1, "", "QuantizedSiLU"], [125, 0, 1, "", "QuantizedSigmoid"], [126, 0, 1, "", "QuantizedSmoothL1Loss"], [127, 0, 1, "", "QuantizedSoftMarginLoss"], [128, 0, 1, "", "QuantizedSoftmax"], [129, 0, 1, "", "QuantizedSoftmax2d"], [130, 0, 1, "", "QuantizedSoftmin"], [131, 0, 1, "", "QuantizedSoftplus"], [132, 0, 1, "", "QuantizedSoftshrink"], [133, 0, 1, "", "QuantizedSoftsign"], [134, 0, 1, "", "QuantizedTanh"], [135, 0, 1, "", "QuantizedTanhshrink"], [136, 0, 1, "", "QuantizedThreshold"], [137, 0, 1, "", "QuantizedTripletMarginLoss"], [138, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [139, 0, 1, "", "QuantizedUnflatten"], [140, 0, 1, "", "QuantizedUnfold"], [141, 0, 1, "", "QuantizedUpsample"], [142, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [143, 0, 1, "", "QuantizedUpsamplingNearest2d"], [144, 0, 1, "", "QuantizedZeroPad1d"], [145, 0, 1, "", "QuantizedZeroPad2d"], [146, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[19, 1, 1, "", "compute_encodings"], [19, 1, 1, "", "forward"], [19, 1, 1, "", "from_module"], [19, 1, 1, "", "get_default_kernel"], [19, 1, 1, "", "get_kernel"], [19, 1, 1, "", "ignore"], [19, 1, 1, "", "ignore_unknown_modules"], [19, 1, 1, "", "implements"], [19, 4, 1, "", "input_quantizers"], [19, 4, 1, "", "output_quantizers"], [19, 4, 1, "", "param_quantizers"], [19, 1, 1, "", "set_default_kernel"], [19, 1, 1, "", "set_kernel"], [19, 1, 1, "", "wrap"]], "aimet_torch.nn.QuantizedCircularPad1d": [[39, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedCircularPad2d": [[40, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedCircularPad3d": [[41, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad1d": [[42, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad2d": [[43, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad3d": [[44, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedHardsigmoid": [[73, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedHardswish": [[74, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedLinear": [[89, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad1d": [[117, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad2d": [[118, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad3d": [[119, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad1d": [[120, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad2d": [[121, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad3d": [[122, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedThreshold": [[136, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss": [[138, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedUnflatten": [[139, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad1d": [[144, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad2d": [[145, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad3d": [[146, 1, 1, "", "forward"]], "aimet_torch.onnx": [[166, 2, 1, "", "export"]], "aimet_torch.quant_analyzer": [[204, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[204, 1, 1, "", "analyze"], [204, 1, 1, "", "check_model_sensitivity_to_quantization"], [204, 1, 1, "", "export_per_layer_encoding_min_max_range"], [204, 1, 1, "", "export_per_layer_mse_loss"], [204, 1, 1, "", "export_per_layer_stats_histogram"], [204, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [204, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[147, 0, 1, "", "DequantizedTensor"], [148, 0, 1, "", "QuantizedTensor"], [149, 0, 1, "", "QuantizedTensorBase"]], "aimet_torch.quantization.DequantizedTensor": [[147, 1, 1, "", "dequantize"], [147, 1, 1, "", "quantize"], [147, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[148, 1, 1, "", "dequantize"], [148, 1, 1, "", "quantize"], [148, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[149, 1, 1, "", "clone"], [149, 1, 1, "", "dequantize"], [149, 1, 1, "", "detach"], [149, 1, 1, "", "new_empty"], [149, 1, 1, "", "quantize"], [149, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[150, 0, 1, "", "Quantize"], [151, 0, 1, "", "QuantizeDequantize"], [152, 2, 1, "", "dequantize"], [153, 2, 1, "", "quantize"], [154, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[150, 1, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[151, 1, 1, "", "forward"]], "aimet_torch.quantization.float": [[155, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[155, 3, 1, "", "bitwidth"], [155, 1, 1, "", "compute_encodings"], [155, 3, 1, "", "exponent_bits"], [155, 1, 1, "", "forward"], [155, 1, 1, "", "from_encodings"], [155, 1, 1, "", "get_encodings"], [155, 1, 1, "", "get_extra_state"], [155, 1, 1, "", "is_bfloat16"], [155, 1, 1, "", "is_float16"], [155, 1, 1, "", "load_state_dict"], [155, 3, 1, "", "mantissa_bits"], [155, 1, 1, "", "set_extra_state"]], "aimet_torch.quantsim": [[170, 0, 1, "", "QuantizationSimModelOnnxExporter"]], "aimet_torch.quantsim.QuantizationSimModelOnnxExporter": [[170, 1, 1, "", "export"]], "aimet_torch.seq_mse": [[198, 0, 1, "", "SeqMseParams"], [198, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[198, 1, 1, "", "forward_fn"], [198, 1, 1, "", "get_loss_fn"]], "aimet_torch.utils": [[173, 2, 1, "", "remove_activation_quantizers"], [173, 2, 1, "", "remove_all_quantizers"], [173, 2, 1, "", "remove_input_quantizers"], [173, 2, 1, "", "remove_output_quantizers"], [173, 2, 1, "", "remove_param_quantizers"]], "aimet_torch.v1.adaround.adaround_weight": [[174, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[174, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[175, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[177, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[177, 1, 1, "", "analyze"], [177, 1, 1, "", "check_model_sensitivity_to_quantization"], [177, 1, 1, "", "export_per_layer_encoding_min_max_range"], [177, 1, 1, "", "export_per_layer_mse_loss"], [177, 1, 1, "", "export_per_layer_stats_histogram"], [177, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [177, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[178, 0, 1, "", "QuantizationSimModel"], [178, 1, 1, "", "load_checkpoint"], [178, 1, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[178, 1, 1, "", "compute_encodings"], [178, 1, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[179, 0, 1, "", "SeqMseParams"], [179, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[179, 1, 1, "", "forward_fn"], [179, 1, 1, "", "get_loss_fn"]], "aimet_torch.v2.mixed_precision": [[220, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[220, 1, 1, "", "apply"], [220, 1, 1, "", "set_model_input_precision"], [220, 1, 1, "", "set_model_output_precision"], [220, 1, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[159, 2, 1, "", "set_activation_quantizers_to_float"], [159, 2, 1, "", "set_blockwise_quantization_for_weights"], [159, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[202, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "attribute", "Python attribute"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:attribute"}, "terms": {"": [2, 8, 9, 11, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 155, 159, 161, 163, 164, 168, 171, 175, 176, 177, 178, 179, 181, 184, 186, 187, 188, 189, 192, 194, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 219, 220, 221, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "00": [153, 154, 192, 233, 234, 235], "000": 247, "0000": [147, 154, 166, 169], "0000e": [153, 154], "001": [190, 228, 236, 244], "0014807": 185, "00183112e": 191, "00215936e": 191, "0022e": 192, "0030": 169, "00317": 188, "0032": 169, "0034": 169, "0035": 169, "0036": [151, 169], "0037": 169, "0038": 169, "0039": [151, 169], "0059": 169, "0063": 169, "0064": 169, "0068": 169, "0069": 169, "0073": 169, "0074": 169, "0078": 169, "0086": 186, "01": [12, 153, 154, 174, 187, 189, 191, 192, 238], "0115": 151, "0117": 169, "01392324": 186, "0142": 149, "0156": 169, "0158": 151, "01675645": 186, "0176": 151, "0182b7a": 200, "0195": 169, "02": [153, 154, 191, 192], "0234": [169, 186], "0235": 189, "0244e": 192, "0252": 186, "026354755942277083": 185, "02635476": 185, "0271e": 192, "0273": 169, "0278355": 185, "02887694e": 192, "0293162": 185, "0295": 151, "03": [191, 192], "0303": 186, "0312": 169, "03513189": 186, "0352": 169, "03658897": 186, "0375e": 192, "03798249e": 191, "0386": 151, "0391": 169, "04": [183, 185, 186], "04025269e": 191, "0406616e": 192, "0422": 186, "0424": 151, "0428": 190, "0430": 169, "0469": 169, "0471": 151, "05": [153, 154, 191, 192], "0500e": [153, 154], "0508": 169, "05270951": 185, "0541903": 185, "0549": 151, "0564": 151, "0597e": 192, "05f6810": 200, "06": 244, "0600": 186, "0639": 151, "0667": 154, "0680": 151, "0680e": 192, "07": 244, "074e85f": 200, "0784": 151, "07906426": 185, "08": [153, 154], "080545": 185, "0819": 151, "0820258": 185, "0859": 155, "0870": 186, "087e9b1": 200, "0882": 149, "0889": 155, "0891": 155, "0897725": 186, "08c17b8": 200, "0901e": 192, "09111059e": 191, "0932e": 192, "0947": 155, "0973e": 192, "0fe6701": 200, "0x7f127685a598": 162, "0x7f9dd9bd90d0": 162, "0x7ff5703eff28": 162, "0x7ff577373598": 162, "1": [0, 2, 10, 12, 13, 14, 18, 19, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 184, 185, 186, 192, 195, 196, 197, 205, 206, 208, 209, 210, 212, 213, 216, 247, 251], "10": [7, 10, 18, 19, 147, 150, 151, 153, 161, 162, 164, 169, 170, 178, 183, 185, 186, 205, 206, 208, 209, 210, 212, 217, 219, 221, 222, 233, 234, 235, 239, 241, 242, 251], "100": [2, 15, 160, 175, 181, 185, 190, 192, 217, 219, 239, 244], "1000": [9, 149, 168, 177, 204, 206, 210, 212, 219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "10000": [1, 187, 228, 229, 230, 231, 232, 236, 237], "1000e": [153, 154], "102": 169, "1024": [217, 221], "103": [150, 169], "104e7e8": 200, "10541902": 185, "106": 150, "1060": 151, "1068997": 185, "107": 169, "107b339": 200, "108": 169, "1080": 186, "109": 150, "10984787": 186, "10k": [1, 12, 174, 187], "11": [148, 149, 153, 169, 231, 235, 251], "110": [150, 169], "1108": 186, "111": [150, 169], "1128": 151, "11446196": 186, "1176": 151, "118": 169, "11899511": 186, "119": 169, "12": [153, 169, 184, 185, 205, 251], "12039044e": 192, "122": 150, "1228e": 192, "1232": 151, "1234": 244, "127": [148, 149, 169, 170, 173, 221, 222], "1279": 186, "128": [2, 14, 148, 149, 159, 161, 163, 169, 170, 173, 175, 176, 185, 189, 198, 205, 216, 217, 221, 222, 244], "129": 150, "13": [153, 169, 231, 251], "1307": 151, "131": 150, "13137": [188, 194, 200], "1316e": 192, "13177378": 185, "1333": 154, "1377e": 192, "1398": 209, "14": [153, 169, 231, 251], "1406": 209, "141": 150, "143": 150, "144": 150, "145": 150, "1450607": 186, "146": 150, "1489e": 192, "1493fe1": 200, "14c8e81": 200, "15": [153, 154, 169, 190, 209, 231, 239, 241, 242, 251], "150": 150, "1500": [13, 188], "15000": 187, "1500e": [153, 154], "152": 150, "153": 148, "155": 150, "15717569e": 191, "15812853": 185, "15c8b9b": 200, "15e": [233, 235], "15k": [1, 12, 174, 187], "16": [2, 19, 150, 155, 161, 163, 164, 167, 169, 170, 175, 178, 186, 188, 189, 192, 194, 196, 197, 199, 205, 217, 221, 222, 228, 231, 236, 244, 246, 250, 251], "1609": 186, "162": 150, "16406": [199, 200], "1647": 151, "1676": 186, "16839484e": 192, "16966406e": 192, "17": [150, 169, 192, 251], "1706e": 192, "1709": 151, "172": 150, "1727": 155, "1729": 155, "1741": 151, "178": 150, "179": 150, "18": [169, 228, 229, 230, 231, 232, 251], "181": 150, "18136823e": 191, "18448329": 185, "186": 150, "18673885e": 191, "187": 150, "188": 150, "1889": 151, "18dfedc": 200, "19": [150, 166, 169, 251], "192": 150, "1921e": [153, 154], "194": 150, "1943": 209, "1945": 149, "1955": 209, "1977": 151, "19778645e": 191, "19e5a4": 200, "1b": [194, 199], "1b99a39": 200, "1bf8b82": 200, "1e": [190, 191, 192, 222], "1k": [187, 189, 190, 217], "1m": [229, 230, 231, 232, 237, 240, 241, 242, 243], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 192, 193, 195, 196, 197, 201, 202, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 224, 226, 245, 247, 248, 249, 251], "20": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251], "200": [160, 238], "2000": [12, 148, 154, 174, 187, 237, 238, 244], "2000e": [153, 154], "2012": [219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "2014": 209, "2016": 209, "2017": 209, "2018e": 192, "2019": [230, 240], "2029": 186, "203": 150, "2048": [194, 199], "205": 148, "2050e": 192, "207": 150, "20722957": 186, "21": [149, 166, 244, 251], "21066449e": 191, "21083805": 185, "2118": 151, "2123188": 185, "21250950e": 191, "2137995": 185, "216": 150, "218": 150, "2196": 151, "21cddb6": 200, "22": [169, 183, 184, 185, 186, 251], "2205": 149, "2212": 151, "224": [170, 186, 187, 189, 190, 191, 192, 198, 203, 204, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "225": [187, 189, 198, 217, 219, 221, 229, 238, 244], "22583652e": 191, "2259": 186, "226": 150, "2260": 149, "2265": 151, "22884297": 186, "229": [187, 189, 198, 217, 219, 221, 229, 238, 244], "2298e": 192, "22b5c94": 200, "22cac5c": 200, "23": [148, 149, 251], "2306": 188, "2308": [188, 194, 200], "2353": 151, "2355": 151, "2363": 151, "237": 150, "23719281": 185, "23799022": 186, "238": 150, "24": 251, "240": 169, "2401543": 185, "2405": [199, 200], "241": 169, "242": 169, "24257803e": 191, "243": 169, "244": 169, "2443e": 192, "245": 169, "2458": 151, "246": 169, "24665177": 186, "247": 169, "248": 169, "249": 169, "2494": 186, "2494d90": 200, "25": [169, 251], "250": 169, "2500e": [153, 154], "251": 169, "252": 169, "253": 169, "254": 169, "2546": 151, "255": [147, 150, 169, 170, 173, 221, 222], "256": [9, 169, 187, 189, 198, 204, 217, 219, 221, 229, 236, 244], "2568": 151, "2592": 162, "26": [148, 169, 251], "26179108e": 191, "26354757": 185, "2650282": 185, "2667": 154, "27": 251, "27045077": 186, "273dd82": 200, "2771": 151, "28": [148, 149, 206, 210, 212, 251], "28065038": 186, "28238320e": 192, "283ecc1": 200, "28990233": 185, "28a7382": 200, "29": [169, 251], "291383": 185, "2921": 149, "2930528e": 192, "2998e": 192, "2b": 207, "2b7b548": 200, "2bc8c94": 200, "2c64eac": 200, "2c88364": 200, "2c8ae88": 200, "2d": [205, 206, 209, 213], "2d4e0eb": 200, "2ed8305": 200, "2f": 244, "2f05175": 200, "2gb": [166, 200], "2x": 200, "3": [2, 14, 18, 147, 149, 153, 154, 158, 160, 161, 162, 163, 167, 169, 170, 171, 173, 175, 176, 179, 183, 185, 186, 192, 196, 205, 206, 207, 210, 212, 213, 220, 251], "30": [150, 169, 251], "300": 236, "3000": 148, "3038": 151, "31": [9, 12, 168, 170, 174, 177, 178, 187, 204, 221, 222, 251], "31080866e": 191, "312": 148, "3136828": 200, "3137": 151, "31625706": 185, "3178": 186, "31ca7fd": 200, "32": [150, 161, 162, 169, 170, 178, 181, 187, 188, 189, 190, 191, 192, 198, 214, 217, 219, 221, 222, 225, 228, 229, 230, 233, 234, 235, 236, 237, 238, 240, 241, 242, 244, 251], "3209": 186, "32141271e": 191, "3216": 151, "3258": 186, "33": [150, 233, 234, 235, 251], "3333": 154, "339a225": 200, "34": 251, "34215236e": 192, "34261182": 185, "3434e": 192, "3435e": 192, "3451": 151, "3467390e": 192, "34694423e": 192, "347054": 185, "3470540046691895": 185, "3479e": 192, "35": 251, "350m": [196, 197], "35107604": 186, "35139937e": 192, "35536635": 186, "35602ea": 200, "3576329": 186, "3587": 186, "35ad990": 200, "36": 150, "3603": 186, "3657e": 192, "36678016": 186, "3687": 186, "36896658": 185, "37": [149, 150, 169], "3706": 186, "3724": 186, "3734087606114667": 185, "374e8db": 200, "3750526": 200, "37757687e": 192, "3792e": 192, "38": [148, 149, 209], "3851556": 186, "39": 147, "3938e": 192, "3992": 151, "3a8659b": 200, "3c92bb7": 200, "3d": 205, "3d4725f": 200, "3d5e0dd": 200, "4": [2, 8, 9, 12, 14, 148, 149, 153, 154, 159, 160, 161, 167, 168, 169, 170, 174, 176, 177, 178, 186, 191, 192, 196, 197, 205, 206, 208, 210, 212, 213, 214, 216, 217, 222, 232, 233, 234, 235, 238, 243, 251], "40": [148, 149, 194, 199], "4000": [147, 154], "406": [187, 189, 198, 217, 219, 221, 229, 238, 244], "4082": 186, "4094e": 192, "41": 147, "41059163e": 192, "4130": 186, "4132449": 186, "414cdde": 200, "4157": 151, "4186": 186, "42": 169, "42083430e": 191, "4216761": 185, "4231569": 185, "4236": 186, "4246376": 185, "42477691e": 191, "43": 169, "43178225": 186, "43477735": 186, "434ac6b": 200, "4392": 151, "4404": 186, "44632760e": 192, "4475": 151, "44803086": 185, "4495116": 185, "44993666e": 192, "4503196": 186, "45040053": 186, "4549": 151, "455": [183, 185, 186], "456": [187, 189, 198, 217, 219, 221, 229, 238, 244], "4578e": 192, "4585028e": 192, "4599525": 186, "45c2a65": 200, "46642041e": 192, "4667": 154, "46723792": 186, "4677236": 186, "4686e": 192, "4694": 151, "4706": 151, "47438562": 185, "4758663": 185, "4784": 151, "47fae94": 200, "48": 169, "485": [187, 189, 198, 217, 219, 221, 229, 238, 244], "4863": 151, "4881": 186, "49": [150, 169], "4933": 186, "4943e": 192, "495567f": 200, "499df9f": 200, "4ad0703": 200, "4b94ca9": 200, "4d": 205, "4ddbd66": 200, "4f": [189, 219, 221], "4febdd4": 200, "5": [149, 150, 151, 153, 154, 155, 160, 161, 164, 167, 169, 183, 185, 186, 190, 191, 192, 205, 206, 207, 210, 212, 217, 222, 230, 232, 233, 234, 235, 237, 240, 241, 242, 244, 251], "50": [207, 217, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "500": [204, 206, 221, 229, 230, 231, 232, 237, 238, 240, 241, 242, 243, 247], "5000": 238, "50000": [187, 217, 229], "5000e": [153, 154], "5006": 211, "50074035": 185, "501eebd": 200, "5022211": 185, "50366503": 186, "5091695": 186, "50f35dd": 200, "51": 147, "512": [196, 197], "5176e": 192, "5181860": 200, "51876003e": 191, "51f8990": 200, "52": 147, "5203": 186, "521": 147, "5220": 151, "5255": 151, "52709514": 185, "52974629e": 191, "5315115": 186, "5317543": 186, "5333": [151, 154], "54": [169, 244], "5430": 186, "55": [150, 169], "550c029": 200, "552ad83": 200, "55344987": 185, "5540": 151, "5549306": 185, "56632766e": 191, "56810045": 186, "5695": 151, "57": [148, 149, 169], "57021021e": 192, "57980466": 185, "5803": 186, "5825": 186, "5856506e": 192, "5876": 190, "5897": 151, "59": [150, 169], "59350af": 200, "59643137e": 191, "5985": 186, "5996e": 192, "59aac3": 200, "5a419f3": 200, "5aac9c5": 200, "5e": [234, 235, 239, 241, 242], "5e23ceb": 200, "6": [10, 149, 153, 154, 161, 169, 170, 191, 192, 205, 217, 221, 235, 237, 240, 241, 242, 251], "60": 169, "6000": [147, 148, 149, 154], "6014": 186, "6039": 151, "6054": 151, "6061594": 185, "6079": 151, "60808927": 186, "61": 169, "61087702e": 191, "6169e": 192, "6177": 151, "6196": 151, "62": 169, "6208e": 192, "6213797e": 192, "6216ca0": 200, "62274104": 186, "6247": 151, "62f5879": 200, "63": 150, "63172388e": 191, "6325141": 185, "64": [8, 9, 150, 159, 169, 170, 183, 185, 186, 188, 189, 204, 205, 216, 217, 221, 222], "6431": 151, "6463e": 192, "65": 207, "6529": 166, "654f4b1": 200, "6588689": 185, "658ec3c": 200, "66": [150, 207, 233, 234, 235], "6603496": 185, "6618e": 192, "6636515": 200, "6667": 154, "6676a6c": 200, "6695": 151, "66ccb45": 200, "68": 147, "68016": 222, "6836": 219, "68522364": 185, "6865": 219, "6867044": 185, "6885": 219, "6998855": 186, "69f96ff": 200, "6a37239": 200, "6c92a97": 200, "6c9f584": 200, "6d1664c": 200, "6d3aa97": 200, "6f670a4": 200, "6fe56b0": 200, "7": [149, 153, 154, 155, 169, 170, 191, 192, 213, 221, 222, 234, 235, 237, 239, 240, 241, 242, 251], "70": 169, "70029c5": 200, "70130579e": 192, "70838": 222, "71": 149, "7115784": 185, "7119e": 192, "7159e": 192, "7164": 189, "7173": 221, "72468403e": 192, "73242160e": 191, "7333": 154, "7336ead": 200, "7364b37": 200, "73793316": 185, "74": 150, "74478185e": 191, "75": 207, "754d030": 200, "755c54a": 200, "76": 169, "76428795": 185, "77": 169, "77213307e": 191, "7741": 151, "7765": 151, "7894": 151, "79": 244, "7932": 151, "7978e": 192, "7d4659d": 200, "7d63e66": 200, "7e5342b": 200, "8": [1, 2, 8, 9, 12, 14, 19, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 168, 169, 170, 174, 175, 176, 177, 178, 181, 185, 186, 187, 189, 191, 192, 198, 204, 206, 210, 212, 213, 216, 217, 220, 221, 222, 228, 229, 230, 231, 232, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 251], "80": 150, "800": [165, 194], "8000": [151, 154], "80053532e": 191, "8009871": 200, "8078": 151, "80cd141": 200, "81": 244, "81699747": 185, "81760912e": 191, "81884710e": 191, "8229": 151, "83": [150, 169], "832ea91": 200, "8347e": 192, "83640555e": 192, "8365e": 192, "836ab1": 200, "84": 169, "8433522": 185, "84edcf5": 200, "8560e13": 200, "86": 150, "8667": 154, "86945379e": 191, "8706": 151, "8711877": 185, "87656835e": 191, "8789e": 192, "8796": 151, "8836": 151, "88706ef": 200, "8874173": 200, "89": 150, "89074164e": 192, "89348674e": 191, "8945e": 192, "8984": 155, "8994": 155, "8998": 155, "8bit": 200, "9": [147, 153, 154, 161, 169, 190, 191, 192, 217, 221, 231, 233, 234, 235, 236, 237, 241, 242, 243, 251], "90": 217, "9086e": 192, "91": 169, "91109af": 200, "911af75": 200, "9157": 151, "9176": 151, "92": 169, "9216": 161, "92f63f5": 200, "93": 169, "9333": 154, "93787616e": 191, "94": [150, 169, 244], "9487": 151, "94877124": 185, "9490": 151, "95": [169, 244], "95260113e": 191, "9570": 169, "9585e": 192, "96": [169, 188], "9609": 169, "96155685e": 191, "9648": 169, "9688": 169, "97": 147, "9700": 151, "9727": 169, "9766": 169, "9805": 169, "9844": 169, "9883": 169, "99160d2": 200, "9922": 169, "9961": [151, 169], "9999": 166, "9a2a407": 200, "9b8c655": 200, "A": [2, 3, 9, 10, 12, 14, 16, 17, 18, 159, 163, 167, 168, 170, 174, 175, 176, 177, 178, 180, 181, 187, 189, 191, 192, 196, 198, 199, 204, 205, 206, 208, 209, 210, 212, 214, 216, 217, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 249], "And": [228, 236, 249], "As": [19, 189, 199, 204, 205, 207, 208, 224, 228, 230, 233, 234, 235, 236, 239, 240, 241, 242], "At": [7, 199, 207, 211, 219, 246], "But": [161, 209, 228, 231, 236, 239, 243], "By": [10, 18, 164, 167, 170, 187, 206, 209, 210, 212, 221, 222, 236, 237, 239, 240, 241, 242, 244, 247], "For": [9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 161, 162, 164, 166, 167, 168, 169, 170, 177, 182, 183, 185, 186, 189, 192, 199, 200, 203, 204, 205, 206, 207, 208, 209, 214, 217, 220, 221, 222, 223, 224, 227, 228, 231, 236, 239, 243, 244, 246, 247, 249, 250], "If": [1, 2, 9, 10, 12, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 152, 153, 154, 155, 160, 161, 162, 163, 164, 166, 168, 169, 170, 174, 175, 176, 177, 178, 187, 188, 189, 190, 194, 196, 198, 200, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 250], "In": [19, 149, 160, 161, 162, 164, 170, 175, 178, 186, 189, 190, 191, 196, 197, 199, 204, 205, 207, 208, 209, 211, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 249], "It": [1, 2, 6, 8, 9, 10, 160, 161, 170, 178, 187, 188, 189, 194, 198, 200, 201, 203, 204, 206, 208, 213, 214, 216, 217, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243], "Its": 213, "NOT": [14, 176, 189], "No": [162, 191, 249], "Not": [9, 168, 177, 188, 190, 194, 204, 205, 208, 220, 222, 227, 228, 231, 239, 243], "ON": 184, "OR": 160, "Of": [233, 234, 235, 241, 242], "On": [228, 236], "One": [204, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 247], "Or": [161, 178, 203, 204, 209], "Such": 161, "That": [213, 229, 230, 232, 237, 240, 241, 242], "The": [1, 2, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 153, 154, 155, 157, 158, 160, 161, 162, 163, 164, 166, 167, 169, 170, 171, 174, 175, 176, 178, 179, 181, 182, 185, 187, 188, 189, 190, 192, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250], "Then": [9, 168, 177, 192, 204, 209, 241, 242, 246], "There": [158, 160, 162, 178, 189, 203, 204, 222, 228, 229, 231, 236, 237, 239, 240, 241, 242, 243, 249, 250], "These": [14, 160, 162, 164, 176, 178, 187, 189, 194, 196, 197, 204, 206, 207, 223, 225, 228, 230, 236, 238, 240, 246, 247], "To": [2, 19, 156, 160, 163, 164, 167, 175, 182, 185, 186, 187, 188, 190, 191, 192, 194, 198, 199, 204, 205, 208, 211, 216, 217, 220, 221, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250], "With": [147, 183, 185, 222, 231, 243, 244], "_": [9, 10, 19, 150, 151, 154, 157, 164, 168, 169, 170, 177, 182, 185, 187, 188, 191, 192, 196, 198, 200, 202, 203, 204, 205, 214, 216, 217, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250], "__getitem__": 238, "__init__": [14, 161, 162, 164, 176, 189, 194, 199, 200, 217, 224, 238], "__iter__": [189, 194, 199, 217], "__len__": [9, 194, 199, 204, 238], "__next__": [189, 199, 217], "__quant_init__": [19, 164], "__torch_function__": 161, "__version__": 186, "_batch_index": 189, "_create_sampled_data_load": 238, "_current_iter": 217, "_data": [189, 217], "_dataset": 238, "_default_forward_fn": [14, 176, 189], "_default_r1_fusion_func": 199, "_default_rmsnorm_linear_pairs_func": 199, "_encodingmismatchinfo": 221, "_from_qnn_encoding_dict": 200, "_get_unlabled_data_load": 204, "_infer_activation_dtyp": 200, "_int": 200, "_is_encoding_frozen": 160, "_max": 155, "_module_to_wrap": 160, "_not_specifi": [170, 221, 222], "_q": 200, "_quantizationsimmodelinterfac": [168, 175, 177, 178, 204], "_quantschemepair": [14, 176, 189], "_remove_input_quant": 160, "_remove_output_quant": 160, "_remove_param_quant": 160, "_step": [153, 154], "_tie_quant": 200, "_unlabel": 217, "a2adae2": 200, "a8f32fc": 200, "a967b8f": 200, "a97354f": 200, "ab": [188, 194], "ab63866": 200, "abe8782": 200, "abil": [200, 250], "abl": [9, 14, 147, 148, 149, 161, 162, 176, 189, 204, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244, 246], "about": [147, 180, 182, 192, 214, 216, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 250], "abov": [161, 163, 204, 205, 206, 209, 210, 211, 212, 213, 220, 228, 231, 233, 234, 235, 236, 243, 246, 247], "absolut": [150, 151, 206, 210, 212, 217], "absolute_path_to_workspac": 227, "absorpt": [230, 240], "abstract": [19, 149, 164], "ac05d10": 200, "acc_top1": 189, "acceler": [170, 181, 185, 186, 200, 206, 209, 210, 212, 219, 221, 232, 241, 242], "accept": [171, 198, 200, 208, 217, 221, 225, 246, 249, 250], "access": [160, 199, 200, 236, 237, 240, 241, 242, 244], "accord": [10, 158, 170, 203, 221, 222], "accordingli": 200, "account": [13, 188, 200, 208, 225], "accumul": [207, 209, 210, 212, 215], "accur": [191, 244], "accuraci": [2, 11, 12, 14, 18, 167, 174, 175, 176, 181, 182, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 215, 217, 218, 219, 221, 222, 225, 226, 231, 238, 239, 243, 247], "achiev": [169, 181, 183, 207, 217, 218, 220, 229, 237, 244, 246, 249, 250], "acronym": 182, "across": [161, 181, 185, 192, 193, 200, 204, 246, 247], "act": [2, 9, 163, 168, 175, 177, 204, 217, 228], "action": [204, 213], "activ": [2, 9, 10, 13, 157, 159, 161, 163, 164, 167, 168, 171, 175, 177, 179, 181, 183, 184, 187, 188, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 205, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250], "activation_bitwidth": 214, "activation_encod": 214, "activation_quant": [2, 217], "activation_typ": [10, 186, 187, 198, 219, 221, 228, 229, 230, 232], "activations_pdf": [204, 231, 243], "actual": [201, 204, 228, 236, 239], "ad": [162, 167, 195, 199, 200, 214, 223, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242], "ada": 229, "ada_model": 237, "adamw": [196, 197, 244], "adapt": [12, 15, 167, 171, 174, 179, 181, 189, 190, 194, 195, 196, 197, 198, 200, 204, 227, 239, 246], "adapter1": 167, "adaptiveavgpool1d": 20, "adaptiveavgpool2d": 21, "adaptiveavgpool3d": 22, "adaptivemaxpool1d": 23, "adaptivemaxpool2d": 24, "adaptivemaxpool3d": 25, "adaptiveround": 200, "adaptor": [165, 194], "adaround": [0, 1, 14, 156, 160, 176, 181, 187, 189, 200, 204, 225, 227, 238, 246], "adaround_data_load": [189, 238], "adaround_dataset_s": [189, 238], "adaround_param": [14, 176, 189, 238], "adaround_weight": [12, 160, 174, 187, 189, 237, 238], "adarounded_model": 187, "adaroundparamet": [12, 14, 174, 176, 187, 189, 237, 238], "adascal": [0, 156, 200], "adascale_model_config_dict": 188, "adascale_optim": 188, "adascalemodelconfig": 188, "add": [161, 162, 164, 167, 178, 200, 213, 223, 224, 231, 232, 233, 234, 235, 241, 242, 247, 249, 250], "add_adapt": 167, "add_check": 162, "add_lora_to_r": 197, "addit": [19, 169, 170, 178, 190, 200, 214, 216, 221, 222, 223, 228, 231, 236, 239, 243, 245, 246], "address": [192, 200, 211, 225], "adequ": 249, "adher": 250, "adjac": [181, 191, 192, 193, 199, 223, 228, 229, 230, 232, 236, 237, 240, 241, 242], "adjust": [157, 193, 196, 197, 200, 202, 206, 207, 216, 222, 225, 246], "admin": 227, "advanc": [169, 181, 183, 200, 226], "advantag": [222, 246], "ae02aa8": 200, "ae981f7": 200, "affect": [12, 174, 181, 187, 216, 223, 229, 250], "affin": [8, 19, 147, 148, 149, 150, 151, 152, 153, 154, 159, 164, 191, 192, 196, 205, 216, 244], "affine_q": 160, "affine_qdq": 160, "affine_quant": 160, "affinequant": [155, 160], "affinequantizerbas": 244, "after": [1, 9, 155, 161, 162, 164, 168, 169, 177, 178, 181, 187, 189, 190, 191, 192, 199, 200, 204, 207, 209, 211, 219, 221, 222, 225, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 246], "afterward": [13, 188], "again": [230, 233, 234, 235, 240, 250], "against": [9, 168, 177, 204, 206, 239, 241, 242], "aggress": 181, "agre": 250, "ahead": 226, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 246, 247, 248, 249, 250, 251], "aim": [217, 233, 234, 235, 239, 246, 250], "aimet": [5, 18, 19, 156, 160, 161, 162, 164, 167, 169, 181, 185, 187, 188, 189, 190, 191, 192, 193, 194, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 215, 216, 217, 218, 219, 220, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 250], "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251], "aimet_export_artifact": 203, "aimet_exported_model": 245, "aimet_exported_model_path": 245, "aimet_omniquant_artifact": [165, 194], "aimet_onnx": [0, 185, 186, 187, 191, 192, 198, 200, 203, 204, 214, 216, 217, 219, 221, 228, 229, 230, 231, 232], "aimet_repo_path": 231, "aimet_tf": 185, "aimet_torch": [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 210, 212, 214, 216, 217, 220, 221, 222, 228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "algo": [175, 217], "algorithm": [2, 18, 175, 206, 207, 208, 209, 210, 212, 213, 214, 225, 246], "alia": [10, 170, 178, 200, 221, 222], "aliasbackward0": [147, 148, 149, 150, 151, 166, 169, 186], "align": [200, 221, 223], "all": [0, 1, 2, 3, 8, 9, 10, 16, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 156, 157, 159, 161, 162, 163, 164, 168, 169, 170, 171, 173, 175, 177, 178, 179, 184, 185, 186, 187, 188, 191, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 212, 214, 216, 217, 219, 220, 221, 222, 223, 225, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 249], "all_q_modul": 160, "all_quant_wrapp": 160, "allclos": 161, "allow": [2, 14, 18, 147, 148, 149, 159, 161, 175, 176, 189, 200, 201, 205, 206, 209, 210, 211, 212, 214, 216, 217, 224, 225, 228, 236, 238, 246], "allow_custom_downsample_op": [18, 206, 233, 235], "allow_overwrit": [160, 187, 197, 221], "allowed_accuracy_drop": [2, 14, 175, 176, 189, 217, 228, 236, 238], "alon": [167, 247], "along": [148, 167, 169, 170, 217, 246, 247], "alpha": [192, 217], "alphadropout": 26, "alreadi": [17, 19, 169, 170, 178, 192, 196, 199, 200, 217, 219, 221, 222, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246], "also": [12, 149, 161, 169, 170, 174, 175, 176, 177, 178, 179, 184, 185, 187, 188, 191, 194, 200, 201, 204, 205, 206, 209, 213, 214, 217, 219, 220, 221, 222, 223, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 245, 246, 249], "alter": [187, 237], "altern": [9, 168, 177, 204, 205, 216, 228, 233, 234, 235, 236], "alwai": [196, 197, 200, 208, 217], "am": [9, 168, 177, 204], "among": [12, 14, 174, 176, 187, 189, 200, 249], "amount": [198, 231, 243], "amp": [0, 2, 14, 156, 163, 175, 176, 189, 200, 217, 218, 227], "amp_search_algo": [2, 175, 217, 228, 236], "ampsearchalgo": [2, 175, 217, 228, 236], "an": [9, 14, 18, 19, 147, 148, 149, 155, 157, 159, 160, 161, 162, 164, 169, 170, 176, 178, 181, 184, 187, 189, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250], "analys": [204, 231, 243], "analysi": [2, 9, 18, 168, 175, 177, 200, 206, 209, 210, 212, 228, 233, 234, 235, 236], "analyt": 211, "analyz": [9, 18, 150, 151, 164, 168, 177, 185, 189, 193, 200, 206, 210, 211, 212, 220, 227, 238], "analyze_per_layer_sensit": [7, 200, 219], "anchor": [137, 138], "andrea": 209, "andrei": 209, "andrew": 209, "ani": [2, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 155, 161, 162, 163, 168, 170, 174, 175, 176, 177, 178, 185, 187, 188, 189, 190, 196, 200, 204, 205, 206, 210, 212, 216, 217, 220, 221, 222, 223, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 249], "anneal": [12, 174, 187], "anoth": [167, 170, 178, 181, 212, 213, 214, 217, 221, 222, 223], "anyth": [2, 175, 217, 228, 231, 236, 243], "api": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 157, 158, 159, 160, 163, 165, 166, 168, 170, 171, 172, 174, 175, 176, 177, 178, 179, 181, 196, 197, 200, 201, 211, 214, 224, 227, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 245], "app": 180, "appear": [18, 161, 162, 206, 210, 212], "appli": [1, 2, 8, 9, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 152, 153, 154, 159, 162, 163, 164, 171, 176, 179, 181, 182, 185, 187, 188, 189, 191, 192, 193, 194, 196, 198, 199, 200, 204, 205, 209, 211, 215, 217, 222, 223, 225, 227, 228, 233, 234, 235, 236, 238, 239, 244, 246, 247, 249, 250], "applic": [2, 175, 217, 228, 229, 236, 237, 240, 244, 245, 250], "apply_adaround": [0, 5, 12, 174, 187, 200, 229, 237], "apply_adascal": [13, 188], "apply_omniqu": [165, 194], "apply_seq_ms": [0, 5, 160, 171, 179, 198, 200], "apply_spinqu": [172, 199], "approach": [163, 196, 198, 200, 220, 222, 246, 247], "appropri": [12, 164, 170, 174, 175, 178, 185, 187, 196, 197, 208, 221, 222, 225, 228, 231, 236, 239, 243, 246, 247], "approx": 247, "approxim": [9, 198, 204, 207, 247, 250], "ar": [2, 7, 8, 9, 10, 12, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 155, 158, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 183, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 249, 250], "arang": [153, 154, 161, 169], "arbitrari": 205, "architectur": [181, 199, 200], "area": [204, 215], "aren": 184, "arg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 159, 163, 164, 166, 170, 171, 184, 198, 200, 205, 216, 217, 220, 221, 222], "argmax": [187, 189, 198, 217, 219, 221, 222, 229], "argument": [2, 10, 12, 13, 14, 19, 155, 159, 161, 162, 163, 165, 166, 168, 170, 171, 174, 175, 176, 177, 178, 179, 184, 187, 188, 189, 194, 198, 200, 203, 204, 205, 214, 216, 217, 221, 222, 231, 243, 245], "around": [183, 200, 204], "arrai": [159, 186, 205, 214, 216], "arrang": 211, "art": [180, 183, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243], "artifact": [10, 163, 167, 200, 218, 220, 221, 245, 249], "arxiv": [188, 194, 199, 200], "asic": 181, "ask": [19, 228, 231, 236, 239, 243], "assert": [160, 161, 169, 221, 236], "assess": 208, "assign": [19, 150, 151, 155, 164, 200, 216, 218, 246], "associ": [2, 14, 19, 149, 162, 163, 164, 169, 170, 175, 176, 189, 217, 231, 243, 245], "assum": [8, 14, 159, 176, 189, 205, 208, 216], "astyp": [204, 231], "asym": [171, 179, 198], "asymmetr": [150, 151, 169, 223, 247], "attention_mask": [194, 196, 197, 199], "attribut": [19, 160, 161, 163, 164, 169, 175, 199, 200, 204, 217], "attributeerror": 200, "augment": 211, "auto": [18, 206, 210, 212, 214, 228, 233, 234, 235, 236], "auto_param": [206, 210, 212, 233, 234, 235], "auto_qu": [189, 238], "autoconfig": [194, 196, 197, 199], "autograd": [147, 148, 149], "autom": [161, 181, 224, 236, 237, 241, 242, 243], "automat": [14, 18, 163, 176, 181, 200, 204, 205, 206, 207, 209, 210, 212, 215, 216, 219, 227, 238], "automodelforcausallm": [196, 197], "automodeparam": [18, 206, 210, 212, 233, 234, 235], "autoqu": [0, 14, 156, 176, 181, 189, 200, 227], "autoquantwithautomixedprecis": [14, 176, 189], "autotoken": [194, 196, 197, 199], "avail": [161, 171, 174, 175, 176, 177, 178, 179, 184, 185, 198, 200, 204, 219, 223, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 248], "avgpool1d": 27, "avgpool2d": [28, 220], "avgpool3d": 29, "avoid": [9, 168, 177, 178, 181, 200, 204, 207, 224, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243], "awai": 222, "awar": [19, 181, 183, 190, 193, 209, 225, 227, 247, 250], "axi": [169, 187, 189, 198, 200, 204, 214, 217, 219, 221, 229, 231, 243, 247], "b": [19, 150, 151, 152, 153, 154, 169, 205], "b1415bd": 200, "b1dafe6": 200, "b2350b2": 200, "b47a97": 200, "b5521f3": 200, "b55b058": 200, "b73bb71": 200, "b924107": 200, "b_": [150, 151, 152, 153, 154], "b_0": [150, 151, 152, 153, 154], "b_1": [150, 151, 152, 153, 154, 205], "b_2": 205, "b_d": [150, 151, 152, 153, 154], "b_n": 205, "ba10947": 200, "back": [2, 147, 148, 163, 168, 175, 177, 200, 204, 217, 223, 247], "backend": [10, 170, 178, 200, 221, 222, 245, 249], "backpropag": [147, 148, 149], "backslash": 200, "backward": [0, 156, 190, 191, 196, 197, 200, 221, 222, 244], "balanc": [181, 192, 209, 215, 219, 246, 250], "bandwidth": 207, "bar": 200, "base": [2, 7, 9, 19, 149, 150, 151, 153, 154, 155, 160, 164, 166, 167, 168, 169, 170, 175, 177, 185, 188, 194, 195, 197, 198, 200, 204, 205, 206, 207, 208, 211, 217, 218, 221, 228, 236, 237, 238, 239, 244, 245, 246, 249], "baselin": [2, 175, 208, 209, 217, 222, 238, 244], "basi": 200, "basic": [200, 217, 222, 228, 236, 239, 245, 246], "batch": [2, 9, 12, 14, 15, 155, 163, 171, 174, 175, 176, 179, 181, 187, 188, 189, 192, 194, 196, 197, 198, 199, 200, 204, 217, 221, 222, 231, 233, 234, 235, 239, 244], "batch_cntr": [189, 228, 230, 231, 232, 237, 239, 240, 241, 242, 243], "batch_data": 217, "batch_id": [194, 196, 197], "batch_idx": 198, "batch_norm": [3, 16, 191], "batch_norm_fold": [0, 5, 156, 190, 191, 217, 222, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242], "batch_siz": [12, 174, 187, 188, 189, 190, 191, 194, 196, 197, 198, 199, 204, 217, 219, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "batchnorm": [14, 15, 176, 189, 190, 191, 192, 200, 213, 227, 228, 229, 230, 232, 236, 237, 240, 241, 242], "batchnorm1d": [16, 32, 191], "batchnorm2d": [16, 33, 162, 191, 192], "batchnorm3d": 34, "batchnrom": 239, "bb93c76": 200, "bc": [230, 240], "bceloss": 30, "bcewithlogitsloss": 31, "becaus": [161, 205, 217, 239, 249], "becom": [200, 218], "becuas": 203, "been": [0, 19, 147, 156, 157, 162, 181, 186, 196, 200, 202, 213, 214, 238, 244, 245, 246, 247], "befor": [1, 2, 9, 10, 13, 19, 157, 160, 164, 169, 187, 188, 189, 190, 191, 192, 195, 196, 197, 199, 200, 202, 203, 204, 209, 217, 219, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 247, 249], "begin": [150, 151, 152, 153, 154, 161, 162, 182], "behav": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 200, 225], "behavior": [160, 161, 164, 191, 200, 223, 228, 236, 237, 240, 241, 242, 247, 249], "behind": 249, "being": [18, 160, 161, 162, 168, 177, 190, 200, 204, 205, 206, 210, 212, 214, 224], "below": [19, 150, 151, 153, 154, 160, 163, 164, 167, 169, 184, 185, 186, 196, 197, 199, 203, 205, 206, 208, 209, 211, 214, 219, 220, 221, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "benefici": [228, 231, 236, 243], "benefit": [198, 214, 216, 222, 246, 247], "bert": 225, "bespok": 196, "best": [14, 163, 176, 183, 185, 189, 193, 207, 209, 215, 220, 226, 238, 247], "beta": [12, 174, 187, 188, 200], "better": [189, 209, 217, 221, 222, 229, 233, 234, 235, 237, 239, 240, 241, 242, 249], "between": [2, 9, 12, 18, 158, 160, 163, 164, 168, 174, 175, 177, 181, 187, 192, 193, 198, 201, 203, 204, 205, 206, 210, 212, 217, 219, 221, 223, 228, 236, 246, 247, 250], "bfba557": 200, "bfloat16": [155, 200], "bia": [19, 155, 160, 161, 162, 164, 166, 167, 170, 173, 191, 192, 199, 200, 206, 221, 222, 223, 230, 240], "bias": [191, 228, 230, 236, 240], "bilinear": 35, "billion": [249, 250], "bin": [184, 245], "binari": [2, 175, 217, 228, 236, 245], "binary_fil": 245, "binary_file_nam": 245, "bit": [1, 2, 9, 12, 155, 159, 168, 169, 174, 175, 177, 181, 183, 185, 186, 187, 196, 200, 204, 205, 214, 215, 216, 217, 218, 222, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 247, 249, 250], "bitop": [2, 175, 217], "bitwidth": [1, 2, 8, 9, 12, 14, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 163, 164, 168, 169, 170, 174, 175, 176, 177, 178, 181, 185, 187, 189, 192, 196, 200, 204, 205, 214, 216, 217, 221, 222, 228, 236, 237, 247, 249], "biwidth": 237, "bkd": [13, 188, 193, 194], "blankevoort": 209, "block": [8, 14, 150, 151, 152, 153, 154, 159, 160, 165, 176, 188, 189, 194, 196, 197, 200, 205, 214, 215, 216, 247], "block_group": [159, 216], "block_siz": [8, 150, 151, 152, 153, 154, 159, 169, 205, 214, 216], "blockwis": [8, 159, 169, 188, 193, 194, 200, 214, 247], "bn": [3, 15, 16, 181, 190, 191, 193, 200, 204, 228, 229, 230, 232, 236, 237, 240, 241, 242], "bn1": 162, "bn_conv": 200, "bn_reestim": [0, 156, 190, 239], "bnf": [190, 191, 192], "bokeh": 217, "bool": [2, 10, 14, 18, 150, 151, 153, 154, 155, 159, 161, 162, 163, 166, 170, 175, 176, 178, 189, 205, 206, 210, 212, 214, 216, 217, 220, 221, 222, 231, 233, 234, 235, 237, 239, 240, 241, 242, 243], "boolean": [19, 163, 199, 200, 220], "both": [153, 154, 160, 161, 164, 168, 177, 183, 185, 186, 191, 195, 204, 209, 210, 211, 213, 215, 217, 221, 222, 223, 225, 228, 235, 236, 241, 242, 244, 245, 246, 247, 250], "bottleneck": 225, "box": [200, 226], "bq": [159, 200, 205, 216], "branch": [161, 223, 227], "break": [189, 194, 196, 197, 198, 200, 217, 221, 228, 230, 231, 232, 237, 239, 240, 241, 242, 243, 244], "bridg": 218, "brief": 215, "british": 209, "broad": 246, "broken": [195, 214], "browser": 227, "bruteforc": [228, 236], "buffer": 155, "bug": [166, 200, 214], "bugfix": 200, "build": [10, 160, 200, 221], "build_sess": 200, "buildx": 184, "built": [2, 19, 163, 164, 175, 184, 185, 217], "bw": [2, 8, 14, 159, 163, 170, 175, 176, 178, 189, 200, 214, 216, 217, 221, 222, 224], "bw_output": 224, "c": [169, 184, 207], "c0bdb46": 200, "c18fd05": 200, "c96894f": 200, "c_": 169, "cach": [2, 14, 165, 175, 176, 189, 194, 200, 217, 228, 236], "cache_id": [14, 176, 189], "calcul": [2, 14, 164, 175, 176, 178, 189, 195, 196, 200, 204, 208, 217, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 247], "calculate_and_fuse_encodings_into_weight": 196, "calibr": [9, 10, 150, 151, 157, 164, 167, 168, 170, 177, 178, 181, 186, 196, 200, 201, 202, 215, 216, 217, 218, 219, 222, 225, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 247, 249], "calibration_batch": 217, "calibration_callback": [196, 197], "calibration_data": 186, "calibration_data_load": [164, 219, 221], "calibration_dataset": [219, 221], "calibration_dataset_s": [189, 238], "calibration_wrapp": [194, 199], "call": [2, 9, 13, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 149, 155, 161, 163, 164, 168, 169, 170, 175, 176, 177, 188, 189, 200, 204, 205, 206, 210, 212, 216, 217, 219, 220, 221, 229, 230, 231, 232, 233, 234, 235, 237, 240, 241, 242, 243, 245, 249, 250], "call_funct": 161, "callabl": [2, 9, 10, 12, 13, 14, 15, 18, 19, 159, 161, 162, 163, 165, 168, 170, 171, 174, 175, 176, 177, 179, 187, 188, 189, 190, 194, 198, 204, 205, 206, 210, 212, 216, 217, 221, 222], "callal": [2, 175, 217], "callback": [2, 9, 18, 163, 168, 171, 175, 177, 178, 189, 196, 198, 206, 210, 212, 216, 217, 222, 231, 233, 234, 235, 243, 247], "callbackfunc": [2, 163, 168, 175, 177, 204, 217, 228, 231, 236, 243], "callbak": [228, 236], "can": [2, 6, 8, 9, 10, 11, 12, 16, 17, 147, 149, 150, 151, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 174, 175, 177, 178, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 196, 198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250], "candid": [2, 11, 14, 18, 163, 171, 175, 176, 179, 188, 189, 194, 198, 200, 206, 208, 209, 210, 212, 217, 228, 236, 246], "cannot": [150, 151, 161, 162, 228, 229, 230, 232, 236, 237, 240, 241, 242], "capabl": [164, 183, 185, 186, 211], "captur": [6, 158, 161, 170, 201, 203, 221, 222, 229, 230, 231, 232, 237, 240, 241, 242, 243], "card": [183, 185, 186], "care": 209, "carefulli": 181, "carri": [147, 148, 149], "case": [153, 154, 159, 160, 161, 162, 164, 186, 195, 199, 200, 204, 205, 208, 216, 220, 223, 224, 226, 229, 231, 243, 250], "cast": [155, 200], "cat": 169, "catch": 200, "caus": [200, 209, 217, 219, 223, 225, 233, 234, 235], "cbe67a": 200, "cd": [184, 227], "cdot": [150, 151, 152, 153, 154], "ce1ea63": 200, "ce68e75": 200, "ceil": [12, 174, 187, 189, 217, 219], "cell": [200, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "celu": 36, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "centercrop": [187, 189, 198, 217, 219, 221, 229, 236, 238, 244], "certain": [161, 168, 177, 200, 204, 209, 214, 217, 220, 221, 223, 224, 236, 237, 240, 241, 242, 243], "chain": [194, 196, 197, 199], "challeng": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "chang": [2, 13, 149, 157, 160, 161, 167, 169, 170, 175, 187, 188, 190, 191, 192, 194, 198, 199, 200, 202, 204, 209, 213, 214, 217, 218, 219, 220, 221, 222, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 249], "channel": [8, 9, 18, 159, 164, 168, 177, 181, 184, 190, 191, 192, 193, 200, 204, 205, 207, 208, 210, 212, 213, 214, 215, 216, 223, 225, 227, 228, 229, 230, 231, 232, 234, 236, 237, 239, 240, 241, 242, 243, 247], "channel_index": [9, 168, 177, 204], "channel_index_0": [231, 243], "channel_index_1": [231, 243], "channel_index_n": [231, 243], "channel_prun": [18, 206, 210, 212, 233, 235], "channel_pruning_auto_mod": 206, "channel_pruning_manual_mod": 206, "channelpruningparamet": [18, 206, 210, 212, 233, 235], "channelshuffl": 38, "characterist": [228, 231, 236, 239, 243], "chart": 225, "check": [14, 161, 162, 169, 176, 182, 186, 189, 200, 201, 203, 224, 228, 231, 236, 239, 243], "check_model_sensitivity_to_quant": [9, 168, 177, 204], "checker": [162, 200], "checkpoint": [171, 178, 179, 198], "checkpoints_config": [171, 179, 198], "chipset": 249, "choic": [185, 209, 214, 231, 243, 247], "choos": [178, 181, 187, 188, 194, 198, 200, 203, 206, 207, 209, 217, 223, 228, 233, 234, 235, 236], "choose_mixed_precis": [2, 163, 175, 217, 228, 236], "chose": 167, "chosen": [227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243], "chunk": 247, "ci": 184, "cin": 169, "circularpad1d": 39, "circularpad2d": 40, "circularpad3d": 41, "cl": [19, 200, 230, 240], "clamp": [150, 151, 153, 154, 155, 247], "class": [2, 6, 9, 10, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 155, 158, 160, 161, 162, 163, 164, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 187, 189, 194, 196, 197, 198, 199, 203, 204, 206, 210, 212, 217, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "classif": [181, 187, 209, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "classmethod": [10, 19, 155, 164, 170, 178, 221], "cle": [4, 17, 189, 192, 200, 204, 225, 227], "clean": [200, 217], "clean_start": [2, 175, 217, 228, 236], "clear": 200, "clearli": 200, "clip": [188, 191, 194, 223, 247], "clone": [149, 227], "close": [207, 247], "closer": [229, 237], "cloud": [181, 249], "cmake": 184, "cmake_arg": 184, "cnn": 181, "cnt": 200, "coars": 192, "code": [185, 187, 189, 192, 199, 200, 203, 205, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243], "codebas": 185, "collate_fn": [194, 196, 197, 199, 231], "collect": [1, 9, 11, 14, 157, 168, 176, 177, 180, 181, 187, 189, 198, 201, 202, 204, 208, 221, 228, 229, 230, 232, 236, 237, 240, 241, 242], "color": [200, 213], "column": 208, "com": [166, 185, 227], "combin": [2, 161, 175, 181, 189, 200, 205, 207, 209, 217, 228, 236], "come": [209, 218, 222, 246], "command": [185, 211, 227, 245], "common": [2, 10, 18, 160, 163, 168, 169, 170, 175, 177, 178, 186, 187, 188, 189, 190, 194, 199, 200, 204, 206, 210, 212, 216, 217, 219, 220, 221, 225, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "commonli": [12, 169, 174, 181, 187, 221], "comp": [18, 206, 210, 212], "comp_stat": [233, 234], "compar": [2, 161, 175, 181, 188, 194, 204, 205, 217, 218, 222, 228, 231, 236, 239, 243, 244, 246, 249], "comparison": [201, 203, 230, 232, 237, 240, 241, 242], "compat": [0, 156, 159, 183, 185, 186, 200, 205, 216, 221, 224, 229], "compil": [10, 183, 200, 221, 250], "complet": [2, 175, 217, 225, 228, 233, 234, 235, 236, 245, 246], "complex": [178, 204, 220], "compli": [236, 237, 241, 242, 243], "complic": 200, "compon": [160, 169, 194, 200, 219, 221], "compos": [163, 187, 189, 198, 217, 219, 220, 221, 229, 236, 238, 244], "comprehens": 246, "compress": [0, 156, 181, 182, 200, 213, 214, 227], "compress_model": [18, 206, 210, 211, 212, 233, 234, 235], "compress_schem": [18, 206, 210, 212, 233, 234, 235], "compressed_bw": 214, "compressed_model": [206, 210, 212, 233, 234], "compressionschem": [18, 206, 210, 212, 233, 234, 235], "compressionstat": [18, 206, 210, 212], "compressor": [18, 206, 210, 212], "compris": [194, 208, 228, 236], "compromis": [233, 234, 235], "compuat": [14, 176, 189], "comput": [2, 9, 10, 13, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 167, 168, 170, 171, 175, 176, 177, 178, 179, 181, 182, 183, 185, 186, 187, 188, 189, 191, 192, 194, 196, 197, 198, 199, 200, 203, 204, 206, 209, 210, 211, 212, 213, 214, 216, 217, 229, 230, 231, 232, 233, 234, 235, 237, 240, 241, 242, 243, 244, 247, 249, 250], "computation": 211, "compute_encod": [10, 13, 19, 148, 149, 150, 151, 155, 157, 160, 163, 164, 169, 170, 171, 178, 179, 186, 187, 188, 194, 196, 197, 198, 199, 200, 202, 217, 219, 220, 221, 222, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242, 244], "computeencod": 185, "concat": 200, "concatenated_exampl": [194, 196, 197, 199], "concept": [160, 205], "concis": 214, "conclus": 246, "concret": 161, "concrete_arg": [14, 161, 176, 189], "concurr": 167, "conda_env_nam": 184, "conda_install_dir": 184, "condit": [161, 162, 190], "confer": 209, "config": [2, 9, 163, 167, 168, 170, 171, 175, 177, 178, 179, 190, 194, 196, 197, 198, 199, 200, 204, 217, 220, 221, 222, 223, 228, 236, 244], "config_fil": [9, 10, 14, 168, 170, 176, 177, 178, 189, 190, 204, 219, 220, 221, 222, 231, 243, 244], "config_util": [0, 156, 205, 216], "configur": [9, 10, 12, 14, 18, 155, 163, 168, 169, 170, 174, 176, 177, 178, 187, 189, 196, 197, 198, 200, 204, 205, 206, 210, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242, 244, 246], "conflict": [163, 200, 220], "conform": [221, 250], "conjunct": 247, "connect": [181, 206, 209, 212], "connectedgraph": [162, 200], "consecut": [192, 217, 230, 240], "consid": [13, 188, 210, 212, 217, 225, 233, 235, 250], "consist": [158, 159, 160, 199, 200, 203, 205, 208, 216, 228, 236, 247], "consol": 201, "constant": [161, 200, 208, 232, 241, 242], "constantpad2d": [42, 43], "constantpad3d": 44, "constrain": [181, 205, 244], "constraint": [184, 200, 214], "constrast": 222, "construct": [200, 203], "constructor": [6, 158, 161, 203, 224], "consum": [189, 200, 209, 214, 218, 238], "consumpt": 246, "contain": [1, 2, 9, 18, 19, 147, 155, 157, 161, 162, 163, 164, 168, 175, 177, 181, 186, 187, 192, 202, 204, 206, 210, 212, 214, 217, 221, 223, 229, 230, 231, 232, 233, 234, 235, 237, 238, 240, 241, 242, 243, 245, 246, 247, 249], "content": 181, "context": [10, 19, 164, 200, 245, 250], "contigu": [188, 194, 196, 197, 200], "continu": [2, 162, 175, 178, 196, 200, 216, 217, 225, 228, 236, 246], "contrast": 160, "contribut": [185, 204, 225], "control": [14, 161, 164, 169, 176, 189, 247], "conv": [3, 8, 12, 16, 159, 161, 174, 187, 191, 192, 200, 205, 210, 212, 213, 216, 220, 223, 230, 240], "conv1": [8, 161, 162, 170, 192, 206, 210, 212, 216, 221, 222, 224, 233, 234, 235], "conv1d": [16, 45, 191, 200], "conv2": [159, 161, 192, 205, 206, 210, 212, 216, 224], "conv2d": [16, 19, 46, 159, 160, 161, 162, 164, 170, 191, 192, 200, 205, 206, 209, 213, 216, 221, 222], "conv2dnormactiv": [191, 192], "conv3d": [47, 200], "conv3dtranspos": 200, "conv_1": 205, "conv_weight": 191, "conv_weight_arrai": 191, "conv_weight_nam": 191, "conveni": 249, "convent": [200, 223], "converg": [181, 200, 222, 246], "convers": [200, 221, 224, 246], "convert": [2, 147, 160, 161, 163, 166, 170, 175, 181, 187, 189, 191, 192, 219, 221, 222, 236, 245, 246, 250], "convolut": [159, 181, 190, 191, 192, 193, 205, 206, 207, 209, 210, 212, 213, 216, 217, 225, 228, 229, 230, 232, 233, 235, 236, 237, 240, 241, 242], "convtranspos": 200, "convtranspose1d": [48, 200], "convtranspose2d": [16, 49, 191], "convtranspose3d": 50, "copi": [10, 149, 155, 178, 194, 196, 197, 199, 219, 227, 229, 236, 237, 240, 241, 242, 247], "copy_": 160, "correct": [14, 176, 181, 189, 217, 220, 221, 222, 230, 231, 238, 240, 243, 250], "correct_predict": [187, 198, 217, 219, 221, 229], "correctli": 200, "correl": [217, 228, 236], "correspond": [2, 3, 6, 9, 12, 16, 155, 158, 162, 163, 164, 168, 170, 174, 175, 177, 178, 179, 185, 187, 191, 193, 203, 204, 205, 206, 213, 217, 221, 222, 238, 246, 247], "cosine_similar": 166, "cosineembeddingloss": 51, "cosinesimilar": 52, "cost": [18, 206, 208, 209, 210, 212, 215, 218, 222, 233, 235, 247], "cost_metr": [18, 206, 210, 212, 233, 234, 235], "costmetr": [18, 206, 210, 212, 233, 234, 235], "could": [160, 178, 204, 213, 214, 221, 224, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 246], "counter": 239, "counterpart": [164, 246], "cours": [233, 234, 235, 241, 242], "cout": 169, "cover": [228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 247], "cp": [206, 207, 208, 213], "cp310": 185, "cp_comp_stat": 235, "cpu": [17, 161, 170, 178, 181, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 198, 199, 200, 203, 217, 220, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "cpuexecutionprovid": [10, 187, 198, 200, 219, 221, 228, 229, 230, 232], "creat": [2, 9, 10, 13, 14, 18, 19, 155, 157, 161, 163, 164, 167, 169, 175, 176, 178, 181, 183, 186, 187, 188, 189, 190, 194, 196, 198, 199, 200, 202, 203, 206, 209, 210, 212, 216, 218, 220, 227, 231, 233, 234, 235, 238, 243, 245, 247, 249, 250], "create_quantsim_and_encod": [9, 204], "creation": 200, "cross": [4, 14, 17, 176, 189, 204, 225, 227], "cross_entropi": 244, "cross_layer_equ": [0, 5, 156, 192, 230, 240], "crossentropyloss": [53, 190, 196, 197, 222], "crosslayerequ": 200, "ctcloss": 37, "ctivations_pdf": [9, 168, 177, 204], "cu121": 185, "cubla": 200, "cuda": [6, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 203, 204, 206, 217, 219, 220, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "cudaexecutionprovid": [187, 198, 219, 221, 228, 229, 230, 232], "cudnn": [183, 184, 185, 186], "cudnn_conv_algo_search": [228, 229, 230, 232], "cumul": 238, "curat": 182, "current": [13, 18, 19, 149, 155, 162, 163, 175, 188, 194, 199, 200, 206, 210, 212, 217, 236, 243, 251], "current_batch": 199, "curv": [2, 175, 208, 217, 228, 236], "custom": [10, 19, 161, 164, 181, 194, 199, 200, 214, 221, 222, 225, 228, 238, 247], "custom_function_not_to_be_trac": 161, "customdataload": [189, 217], "customdataset": 188, "custommodel": 161, "custommodul": 161, "customsilu": 200, "cycl": 249, "d": [150, 151, 152, 153, 154, 184, 194, 199, 214], "d33e98c": 200, "d57a934": 200, "dangl": 221, "dark": [228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243], "data": [2, 9, 12, 13, 14, 15, 18, 147, 148, 149, 157, 160, 161, 163, 165, 168, 170, 171, 174, 175, 176, 177, 178, 179, 181, 187, 188, 189, 190, 193, 194, 196, 197, 198, 200, 202, 204, 206, 210, 212, 215, 217, 219, 221, 222, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "data_load": [2, 12, 13, 14, 18, 157, 163, 171, 174, 175, 176, 179, 187, 188, 189, 190, 194, 198, 202, 206, 217, 219, 221, 222, 228, 230, 231, 232, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "data_set": [13, 188], "data_typ": [14, 160, 170, 176, 178, 189, 200, 221, 222], "dataload": [2, 12, 13, 14, 15, 163, 165, 174, 175, 176, 179, 187, 188, 189, 190, 194, 196, 197, 198, 199, 204, 217, 219, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "dataloader_wrapper_len": 194, "dataloadermnist": 206, "dataset": [2, 9, 10, 13, 14, 15, 168, 170, 175, 176, 177, 186, 187, 188, 189, 190, 196, 197, 198, 204, 217, 219, 221, 222, 227, 247, 249], "dataset_dir": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "dataset_path": 221, "dataset_root": [187, 198], "datasetfold": 244, "datatyp": [14, 176, 189, 214], "db99447": 200, "dc34fa4": 200, "dc8d978": 200, "de": [169, 247], "debug": [160, 200, 201, 203, 214, 226], "decai": 209, "decid": [171, 179, 188, 194, 198, 211, 217, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243], "decim": [206, 210, 212, 233, 234, 235], "decis": 250, "declar": 19, "decompos": [209, 210, 212], "decomposit": [209, 210, 212, 234, 235], "decompress": [8, 159, 216], "decompressed_bw": [8, 159, 216], "decor": 19, "decreas": [11, 181, 198], "dedic": 181, "deem": [217, 249], "deep": [181, 182, 183, 191, 209, 244], "deepcopi": [219, 229], "deeper": 227, "deepseek": 194, "deepspe": 200, "def": [2, 10, 18, 19, 160, 161, 162, 163, 164, 170, 175, 178, 186, 187, 188, 189, 194, 196, 197, 198, 199, 204, 206, 210, 212, 217, 219, 221, 222, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "default": [2, 9, 10, 12, 14, 18, 19, 149, 150, 151, 153, 154, 155, 157, 161, 163, 164, 166, 167, 168, 170, 171, 174, 175, 176, 177, 178, 179, 181, 186, 187, 189, 191, 192, 198, 200, 202, 204, 206, 208, 209, 210, 211, 212, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "default_activation_bw": [9, 204, 217, 231], "default_beta_rang": [12, 174, 187], "default_bitwidth": 217, "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "default_config_fil": [12, 174, 187], "default_data_col": [194, 196, 197, 199], "default_data_typ": [170, 178, 221, 222], "default_forward_fn": [171, 179, 188, 198], "default_new": 167, "default_num_iter": [12, 174, 187, 237, 238], "default_output_bw": [168, 170, 177, 178, 186, 188, 194, 196, 197, 198, 199, 204, 217, 220, 221, 222, 228, 230, 236, 237, 239, 240, 241, 242, 243, 244], "default_param_bw": [9, 12, 168, 170, 174, 177, 178, 186, 187, 188, 194, 196, 197, 198, 199, 204, 217, 220, 221, 222, 228, 230, 231, 236, 237, 239, 240, 241, 242, 243, 244], "default_quant_schem": [12, 174, 187, 237], "default_reg_param": [12, 174, 187], "default_warm_start": [12, 174, 187], "defer": 183, "defin": [2, 19, 161, 162, 164, 169, 175, 188, 194, 199, 200, 204, 206, 210, 212, 214, 217, 221, 223, 224, 231, 233, 234, 235, 237, 240, 241, 242, 243, 244, 246, 247], "definit": [2, 18, 19, 158, 160, 161, 163, 167, 175, 178, 203, 206, 210, 212, 217, 221, 224, 228, 231, 236, 237, 240, 241, 242, 243, 247], "degrad": [219, 229, 230, 232, 237, 240, 241, 242, 246, 249, 250], "degre": [210, 212], "deleg": 169, "delet": [2, 175, 200, 204, 217, 228, 236], "deliber": [228, 231, 236, 243], "delta": [9, 168, 177, 185, 200, 204, 247], "demand": 181, "demonstr": [231, 243, 244], "denable_cuda": 184, "denable_onnx": 184, "denable_test": 184, "denable_torch": 184, "denot": [199, 213, 214], "dens": 200, "depend": [147, 148, 149, 161, 200, 207, 223, 226, 227, 246], "deploi": [180, 247, 248, 249], "deploy": [180, 181, 182, 183, 221, 244, 245, 246, 248, 250], "deprec": [170, 200, 221, 222], "depth": [190, 200, 225], "depthwis": 200, "depthwiseconv": 200, "dequant": [147, 148, 149, 151, 154, 155, 169, 181, 185, 200, 249, 250], "dequantizedtensor": [148, 149, 151, 169, 170, 186], "dequantizelinear": [10, 166, 170, 200, 246], "deriv": [150, 151, 153, 154, 169, 187, 236, 247], "descend": 155, "describ": [155, 160, 184, 186, 199, 205, 209, 211, 214, 217, 223, 225, 247, 248, 249, 250], "descript": [164, 214, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "design": [162, 181, 187, 227, 228, 231, 236, 243, 246], "desir": [9, 149, 168, 169, 170, 177, 178, 204, 207, 209, 217, 218, 221, 222, 225, 228, 233, 234, 235, 236, 245, 246, 250], "detach": [149, 166], "detail": [14, 161, 170, 176, 182, 189, 206, 208, 211, 214, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247, 250], "detect": [209, 246], "determin": [9, 155, 159, 164, 168, 177, 178, 181, 187, 188, 189, 193, 194, 196, 197, 198, 199, 204, 205, 214, 216, 218, 219, 221, 228, 236, 238, 246], "determinist": 161, "dev": 184, "develop": [157, 180, 181, 183, 185, 202, 249], "devic": [6, 12, 14, 149, 161, 170, 174, 175, 176, 178, 180, 181, 182, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 201, 203, 217, 220, 221, 222, 226, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "df8b875": 200, "diable_missing_quant": 221, "diagnost": 225, "diagram": [196, 197], "dict": [1, 2, 7, 9, 10, 11, 13, 14, 18, 155, 158, 161, 163, 164, 168, 175, 176, 177, 178, 187, 188, 189, 198, 203, 204, 206, 210, 212, 214, 217, 219, 220, 221, 224, 229], "dictat": 249, "dictionari": [7, 9, 18, 168, 170, 177, 178, 199, 204, 206, 208, 209, 210, 212, 219, 221, 222, 223], "didn": 239, "diff": 200, "differ": [18, 160, 161, 167, 181, 193, 195, 198, 205, 206, 208, 209, 210, 212, 214, 216, 217, 218, 219, 220, 221, 225, 228, 231, 233, 234, 235, 236, 241, 242, 245, 246, 247, 250], "dilat": 200, "dim": [159, 169, 205, 216, 236], "dimens": [8, 159, 164, 205, 210, 212, 214, 216, 225, 247], "dir": [10, 221], "dir_path": [6, 158, 203], "direct": [1, 182, 187, 191, 200, 203, 205, 214, 216, 217, 219, 246, 247, 249, 250], "directli": [9, 183, 186, 196, 200, 204, 220, 221, 239, 247, 249], "directori": [2, 6, 9, 10, 14, 18, 158, 168, 175, 176, 177, 184, 187, 189, 191, 192, 198, 203, 204, 206, 210, 212, 217, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245], "disabl": [2, 9, 160, 164, 167, 168, 170, 171, 177, 178, 179, 191, 198, 199, 200, 204, 208, 209, 217, 221, 222, 223, 233, 235, 247], "disable_missing_quant": 221, "discard": 204, "discuss": [233, 234, 235], "disk": [6, 158, 200, 203], "dispatch": 200, "displai": [202, 211, 227], "display_comp_ratio_plot": 211, "display_eval_scor": 211, "dist": 184, "distict": 200, "distil": [188, 193, 194], "distinct": 162, "distort": 247, "distribut": [18, 181, 200, 206, 210, 212, 225, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 247], "divbackward0": 169, "dive": 227, "diverg": 181, "divid": [205, 207], "divis": [169, 181, 205, 216], "dlc": 245, "dlc_path": 245, "dlcompress": 200, "dlequal": 200, "dlf": 181, "do": [10, 12, 161, 162, 167, 170, 174, 185, 187, 191, 194, 196, 199, 204, 209, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 246, 247, 249, 250], "do_constant_fold": [191, 228, 230, 232], "do_not_trace_m": 161, "doc": [161, 167, 184, 200, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "dockerfil": 184, "docstr": 205, "document": [167, 185, 191, 200, 203, 205, 211, 214, 216, 217, 219, 227, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 249, 250], "doe": [19, 161, 175, 188, 194, 199, 200, 202, 204, 206, 208, 210, 212, 222, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 246, 247, 249, 250], "doesn": [19, 221, 228, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 249], "doesnt": [163, 220], "don": [19, 161, 184, 196, 197, 204, 231, 243], "done": [150, 151, 169, 200, 209, 221, 223, 239], "down": [195, 214], "down_proj": 199, "download": [182, 185, 219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "downsampl": [233, 235], "downstream": [200, 213, 214, 221, 246], "dq": 10, "dq_output": 19, "drastic": [208, 250], "draw": 246, "drawback": 247, "drift": 222, "driver": [183, 185, 186], "drop": [2, 14, 164, 175, 176, 189, 200, 204, 207, 209, 215, 217, 219, 222, 225, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 247], "dropout": 54, "dropout1d": 55, "dropout2d": 56, "dropout3d": 57, "dtype": [147, 148, 149, 155, 159, 160, 161, 163, 186, 194, 196, 197, 199, 200, 205, 214, 220], "due": [162, 166, 200, 205, 222, 246], "dummi": [9, 12, 14, 16, 17, 158, 162, 168, 170, 174, 175, 176, 177, 178, 187, 189, 191, 192, 196, 197, 203, 204, 221, 222, 228, 231, 236, 243], "dummy_attention_mask": [196, 197], "dummy_data": 204, "dummy_input": [9, 10, 12, 13, 14, 16, 17, 157, 158, 167, 168, 170, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 202, 203, 204, 217, 219, 220, 221, 222, 224, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 244], "dummy_input_dict": 203, "dummy_input_id": [196, 197], "dummy_model": [188, 194], "dummymodel": [13, 188], "dump": [194, 200], "duplic": [149, 161, 200], "dure": [1, 2, 10, 11, 12, 13, 14, 18, 19, 155, 157, 164, 174, 175, 176, 181, 182, 187, 188, 189, 198, 200, 201, 202, 206, 209, 210, 211, 212, 217, 219, 221, 222, 223, 231, 232, 233, 234, 235, 237, 239, 241, 242, 243, 244, 246, 247, 250], "dynam": [155, 161, 181, 192, 200, 222, 247], "dynamic_ax": [187, 191, 198, 217, 219, 221, 228, 229, 230, 231, 232], "dynamo": [166, 170, 186, 187, 191, 192, 198, 217, 219, 221, 228, 229, 230, 231, 232], "e": [9, 10, 165, 170, 178, 192, 193, 194, 204, 214, 217, 219, 221, 222, 228, 229, 231, 236, 239, 243, 246], "e026fd1": 200, "e49660c": 200, "e78dbec": 200, "e7d10c7": 200, "each": [1, 2, 9, 10, 11, 12, 19, 157, 162, 164, 165, 168, 169, 171, 174, 175, 177, 181, 187, 188, 191, 192, 194, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 214, 217, 221, 223, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247], "earli": [2, 175, 217], "easi": [200, 220, 249], "easier": 160, "easili": [169, 206, 210, 212], "eb1ac5c": 200, "ec22d86": 200, "echo": 184, "ecosystem": 180, "ed": 187, "edg": [181, 182, 249], "edit": [200, 214, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "ee949a2": 200, "effect": [12, 15, 164, 174, 181, 187, 190, 193, 199, 204, 220, 232, 241, 242, 244, 249, 250], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 239, 244, 245, 246, 247, 248, 249, 250, 251], "efficientnetb4": 200, "effort": [14, 163, 176, 189, 220, 238, 246, 249, 250], "eigen": 184, "eights_pdf": [9, 168, 177, 204], "either": [7, 18, 159, 163, 169, 199, 205, 206, 210, 212, 213, 216, 217, 219, 220, 224, 228, 233, 234, 235, 236, 246], "element": 214, "elementwis": [161, 164, 200], "elev": 246, "elimin": [181, 191, 204, 247], "els": [2, 161, 162, 175, 186, 187, 188, 189, 190, 191, 192, 194, 198, 199, 217, 220, 221, 222, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 244, 246], "elu": 58, "embed": [59, 161, 166, 170, 178, 200, 209, 221, 222, 225], "embed_token": 199, "embeddingbag": 60, "embodi": 181, "empir": [238, 246], "emploi": [181, 182], "empti": [8, 216, 223], "emul": 247, "enabl": [2, 8, 9, 14, 18, 155, 160, 163, 167, 168, 171, 175, 176, 177, 179, 181, 183, 189, 190, 191, 193, 198, 199, 200, 203, 205, 206, 210, 211, 212, 216, 217, 218, 219, 220, 223, 233, 235, 236, 242, 245, 247, 249], "enable_convert_op_reduct": [163, 175, 217, 236], "enable_onnx_check": [170, 178, 221, 222], "enable_per_layer_mse_loss": [9, 204, 231, 243], "enbl": 216, "enc": 19, "enc_typ": 214, "encapsul": [2, 163, 168, 175, 177, 204, 217], "encaptur": 167, "encod": [1, 2, 9, 10, 11, 12, 13, 14, 19, 147, 148, 149, 150, 151, 155, 160, 163, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 185, 187, 188, 189, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 216, 217, 220, 222, 229, 230, 232, 237, 238, 240, 241, 242, 244, 245, 246, 249], "encoding_analyz": [150, 151, 155], "encoding_path": [189, 237, 238], "encoding_vers": [10, 170, 221], "encodinganalyz": [150, 151, 155], "encodinganalyzerforpython": 185, "encodingbas": [147, 148, 155], "encodingmismatchinfo": 221, "encodingtyp": 214, "encount": 19, "encourag": [161, 224], "end": [13, 150, 151, 152, 153, 154, 161, 162, 182, 186, 188, 209, 217, 221, 227, 233, 234, 235, 239, 241, 242, 246, 249], "end_beta": [12, 174, 187], "end_idx": 189, "enforc": 155, "engin": [182, 191, 200, 203, 205, 214, 216, 217, 219, 246, 247, 249, 250], "enhanc": [9, 167, 168, 177, 200, 204, 221, 231, 243, 247], "enough": [231, 233, 234, 235, 250], "ensur": [164, 186, 190, 196, 197, 200, 203, 208, 217, 225, 244, 250], "enter": [19, 164, 189], "entir": [9, 168, 169, 177, 204, 206, 209, 210, 212, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "entri": [14, 170, 176, 178, 189, 214, 221, 222, 223], "enum": [2, 10, 12, 18, 158, 170, 174, 175, 178, 187, 203, 206, 210, 212, 214, 217, 221, 228, 236, 239], "enumer": [18, 158, 194, 196, 197, 198, 203, 206, 210, 212, 217, 219, 221, 244], "environ": [181, 185, 227, 228, 236, 243, 244], "ep": [191, 192, 200], "epoch": [190, 196, 197, 206, 209, 210, 212, 222, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242], "epsilon": 200, "equal": [4, 14, 17, 148, 149, 159, 169, 176, 189, 200, 204, 205, 207, 208, 216, 225, 227, 247], "equalize_model": [4, 17, 192, 230, 240], "equat": [150, 151, 152, 153, 154, 205, 247], "equival": [14, 19, 153, 154, 155, 159, 163, 170, 175, 176, 178, 188, 189, 194, 199, 205, 216, 217, 221, 222, 224, 228, 236, 237, 240, 241, 242, 243, 245], "error": [1, 14, 19, 161, 169, 176, 181, 187, 189, 192, 200, 201, 220, 221, 225, 247], "especi": [166, 181, 196, 222, 225, 228, 236, 249], "essenti": [228, 236, 239], "establish": 244, "estim": [200, 227, 247, 249], "esults_dir": [9, 168, 177, 204], "etc": [13, 181, 184, 188, 200, 207, 214, 217, 228, 236], "eval": [2, 9, 13, 14, 18, 161, 163, 168, 171, 175, 176, 177, 179, 186, 187, 188, 189, 190, 191, 192, 198, 204, 206, 208, 209, 210, 211, 212, 217, 220, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "eval_callback": [9, 14, 18, 168, 176, 177, 189, 204, 206, 210, 212, 231, 233, 234, 235, 238, 243], "eval_callback_factori": [228, 236], "eval_callback_fn": 204, "eval_callback_for_phase1": [2, 175, 217, 228, 236], "eval_callback_for_phase2": [2, 175, 217, 228, 236], "eval_callback_for_phase_1": 217, "eval_callback_for_phase_2": 217, "eval_data_load": [217, 219, 221], "eval_dataset": [219, 221], "eval_dataset_s": [189, 238], "eval_fn": 219, "eval_iter": [18, 206, 210, 212, 233, 234, 235], "eval_scor": [9, 18, 168, 177, 204, 206, 210, 212], "evalcallbackfactori": [2, 163, 175, 217, 228, 236], "evalu": [2, 9, 14, 18, 163, 168, 171, 175, 176, 177, 186, 187, 188, 189, 194, 198, 199, 206, 208, 209, 210, 211, 212, 216, 217, 222, 227, 238, 246, 247, 250], "evaluate_accuraci": 236, "evaluate_model": [206, 210, 212], "even": [19, 169, 217, 220, 222, 228, 236], "evenli": 205, "eventu": 220, "everi": [6, 9, 12, 158, 168, 169, 174, 177, 187, 188, 194, 201, 203, 204, 208, 209, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 239, 241, 242, 243, 247], "evid": 246, "ex": 167, "exactli": [13, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 164, 188, 231, 243, 247], "examin": 161, "exampl": [8, 10, 13, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 157, 159, 162, 163, 164, 166, 167, 169, 170, 171, 173, 179, 181, 184, 186, 187, 188, 189, 191, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204, 205, 207, 213, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 238, 244, 246, 247, 250], "exce": [157, 202], "exceed": [157, 202], "except": [155, 179, 191, 192, 200, 217, 220, 228, 230, 231, 232, 243], "exchang": 181, "exclud": [8, 11, 14, 161, 162, 168, 171, 176, 177, 179, 189, 198, 200, 204, 214, 216], "excluded_lay": 214, "excluded_nod": [8, 216], "exclus": [155, 159, 205], "execut": [2, 10, 14, 161, 170, 175, 176, 178, 189, 191, 200, 211, 217, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246], "exempt": [19, 164], "exercis": [228, 231, 236, 239, 243], "exhibit": 246, "exist": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 164, 170, 178, 205, 216, 217, 221, 222, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 249], "exist_ok": [228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "exit": [2, 14, 19, 164, 175, 176, 189, 217], "expand": 200, "expand_dim": 189, "expans": 209, "expect": [2, 9, 12, 14, 18, 155, 157, 161, 162, 163, 165, 168, 170, 171, 174, 175, 176, 177, 178, 179, 187, 189, 194, 198, 200, 202, 204, 206, 209, 210, 212, 217, 220, 221, 222, 228, 233, 234, 235, 236, 239, 244, 246], "experi": [18, 159, 183, 185, 206, 209, 210, 212, 216, 229, 233, 234, 235, 237, 240, 241, 242, 246, 250], "experiment": [0, 156, 183, 188, 194, 199, 200, 205, 209], "expert": 238, "explain": [200, 205, 206, 209, 228, 231, 236, 239, 243], "explan": [182, 229, 230, 232, 237, 240, 241, 242], "explicit": 200, "explicitli": [14, 176, 189, 213], "explor": [217, 246], "expon": [155, 159, 205], "exponent_bit": [155, 159, 205], "export": [9, 10, 13, 14, 158, 160, 163, 167, 168, 170, 171, 176, 177, 178, 179, 183, 184, 186, 187, 188, 189, 190, 191, 192, 194, 198, 200, 201, 203, 204, 209, 214, 216, 217, 218, 219, 220, 222, 224, 227, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 245, 246], "export_int32_bia": [166, 170], "export_model": [10, 167, 170, 178, 221, 222], "export_param": [228, 230, 232], "export_per_layer_encoding_min_max_rang": [9, 168, 177, 204], "export_per_layer_mse_loss": [9, 168, 177, 204], "export_per_layer_stats_histogram": [9, 168, 177, 204], "export_to_torchscript": [170, 178, 221, 222], "expos": [161, 199, 204], "express": [18, 196, 197, 206, 210, 212], "extend": [160, 200], "extens": [164, 227, 245], "extent": 246, "extern": 200, "extra": [155, 164, 184, 200], "extract": [181, 221, 230, 231, 232, 237, 240, 241, 242, 243], "extractor": 200, "extrem": [228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 247], "f": [161, 162, 166, 169, 170, 184, 185, 186, 187, 189, 191, 192, 198, 217, 219, 221, 222, 229, 238, 244], "f0": 207, "f0bc6c9": 200, "f333188": 200, "f39c0bf": 200, "f547a49": 200, "f7e700f": 200, "f94f3e2": 200, "f961ed4": 200, "f9d0d6c": 200, "facebook": [181, 196, 197], "facilit": 182, "fact": 196, "factor": [192, 200, 207, 209, 222, 233, 234, 235, 239, 241, 242], "factori": [2, 163, 175, 217], "fail": [161, 162, 163, 170, 189, 191, 192, 200, 217, 220, 221, 222, 228, 230, 232], "failur": 200, "fair": 181, "fairli": [231, 243], "fake": [151, 154, 155, 163, 175, 200, 217, 219, 221, 228, 229, 230, 232, 236, 237, 238, 239, 240, 241, 242, 244], "fakequ": [170, 178, 221, 222], "fall": [208, 223, 233, 234, 235, 250], "fallback": [200, 245], "fals": [2, 8, 10, 14, 18, 19, 147, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 166, 167, 169, 170, 173, 175, 176, 178, 185, 186, 187, 189, 191, 192, 194, 196, 197, 198, 199, 203, 205, 206, 210, 212, 214, 216, 217, 219, 220, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244], "famili": [181, 200, 220], "familiar": 227, "far": 239, "farther": [229, 237], "fast": 184, "faster": [200, 217, 228, 236, 238, 246], "fastest": 183, "fc": [209, 212], "fc1": 161, "fc2": 161, "fd7e40d": 200, "fe66376": 200, "fea395f": 200, "featur": [8, 13, 157, 160, 161, 162, 181, 182, 188, 189, 191, 192, 194, 199, 200, 202, 209, 211, 216, 217, 220, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247], "featurealphadropout": 61, "feed": 247, "feel": [220, 239], "feez": [171, 179, 198], "fefd504": 200, "few": [190, 207, 219, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244], "fewer": [209, 249], "field": [2, 155, 175, 214, 217, 228, 236], "figur": [187, 189, 206, 208, 210, 211, 212, 213, 225, 247], "file": [2, 9, 10, 12, 14, 18, 163, 168, 170, 171, 174, 175, 176, 177, 178, 179, 184, 185, 187, 189, 190, 198, 200, 204, 206, 210, 212, 214, 217, 220, 221, 222, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247, 249], "file_path": [178, 186, 187, 191, 192, 198, 217, 219, 221], "filenam": [9, 10, 12, 170, 174, 178, 187, 188, 190, 194, 204, 221, 222, 228, 229, 230, 232], "filename_prefix": [10, 12, 170, 174, 178, 187, 188, 190, 194, 198, 221, 222, 228, 229, 236, 237, 238, 239, 240, 241, 242], "filename_prefix_encod": [167, 170, 178, 221, 222], "fill": [14, 170, 176, 178, 189, 221, 222], "filter": 181, "final": [18, 162, 179, 196, 197, 199, 204, 205, 206, 207, 208, 210, 212, 217, 225, 228, 231, 236, 239, 243, 249], "find": [2, 162, 163, 171, 175, 178, 179, 183, 188, 194, 198, 204, 208, 218, 220, 222, 227, 228, 229, 233, 234, 235, 236, 237, 238, 240, 241, 242, 247, 250], "fine": [18, 167, 178, 182, 206, 207, 210, 212, 222, 229, 230, 232, 237, 238, 240, 241, 242, 244, 246, 249, 250], "finer": [155, 169, 205], "finess": 250, "finetun": [229, 230, 233, 234, 235, 237, 239, 241, 242], "finetuned_accuraci": [239, 241, 242], "finetuned_accuracy_bn_reestim": 239, "finetuned_model": [233, 234], "finish": [241, 242], "finit": 155, "first": [12, 161, 164, 167, 169, 171, 174, 179, 187, 198, 200, 209, 225, 231, 233, 234, 235, 239, 243, 249], "fit": [18, 206, 208, 210, 212, 239, 249], "five": [230, 240], "fix": [162, 200, 214, 217, 222, 228, 229, 230, 232, 247], "flag": [2, 14, 157, 160, 161, 162, 163, 170, 175, 176, 178, 184, 189, 202, 217, 220, 221, 222, 228, 236], "flatten": [62, 161, 196, 197, 214], "flexibl": [228, 236], "flexround": 188, "flip": [7, 219, 246], "flip_layers_to_higher_precis": [7, 219], "float": [2, 7, 9, 10, 12, 14, 18, 19, 70, 147, 148, 155, 159, 163, 166, 168, 170, 174, 175, 176, 177, 178, 181, 187, 189, 191, 192, 199, 200, 204, 205, 206, 210, 212, 214, 217, 219, 221, 222, 225, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250], "float16": [7, 10, 155, 159, 160, 200, 205, 219, 221, 246], "float32": [10, 186, 204, 221, 231], "float8": 200, "float_fallback": 245, "floatencod": [155, 200], "floatquant": [155, 160], "floatquantizedequant": 160, "flow": [14, 161, 176, 189, 225, 247], "fly": [166, 170], "focu": 239, "fold": [3, 14, 16, 63, 170, 176, 181, 189, 190, 192, 199, 200, 204, 229, 246], "fold_all_batch_norm": [16, 191, 217, 222, 236, 237, 240, 241, 242], "fold_all_batch_norms_to_scal": [190, 239], "fold_all_batch_norms_to_weight": [3, 191, 200, 228, 229, 230, 232], "fold_param_quant": 170, "folder": [204, 231, 243], "follow": [0, 5, 6, 9, 10, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 156, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 177, 178, 184, 185, 186, 187, 189, 190, 191, 192, 198, 200, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "footprint": [181, 182], "forall_": [150, 151, 152, 153, 154], "forc": [192, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243], "forg": 184, "form": [231, 243], "formal": 205, "format": [9, 10, 12, 149, 165, 168, 170, 174, 177, 178, 181, 182, 183, 187, 189, 194, 200, 204, 221, 222, 229, 245, 246, 247, 249], "former": 224, "forward": [2, 9, 10, 12, 13, 14, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 155, 161, 162, 163, 164, 165, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 187, 188, 189, 190, 191, 194, 196, 197, 198, 199, 200, 204, 217, 221, 222, 224, 225, 228, 231, 232, 236, 237, 239, 240, 241, 242, 243, 250], "forward_callback": 231, "forward_fn": [2, 12, 13, 14, 15, 163, 165, 171, 174, 175, 176, 179, 187, 188, 189, 190, 194, 198, 217, 228, 236, 239], "forward_one_batch": [228, 236], "forward_pass": [186, 187, 188, 196, 197, 198, 217], "forward_pass_arg": 221, "forward_pass_call_back": 217, "forward_pass_callback": [2, 9, 10, 168, 170, 175, 177, 178, 204, 217, 221, 222, 228, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "forward_pass_callback_arg": [10, 170, 178, 217, 221, 222, 236, 237, 239, 240, 241, 242], "forward_pass_callback_fn": 204, "found": [160, 246, 247], "four": [189, 217, 247], "fp": [193, 199, 222, 244], "fp16": [163, 200, 219, 220, 246, 249], "fp32": [2, 6, 9, 158, 163, 168, 171, 175, 177, 179, 181, 188, 194, 198, 200, 203, 204, 214, 217, 219, 231, 243, 245, 247, 249, 250], "fp32_layer_output": 203, "fp32_layer_output_util": 203, "fp32_output": [2, 217], "fp_accuraci": 219, "fp_input": 219, "fp_qdq": 160, "fp_quantiz": 160, "fp_session": 219, "fpt": 200, "fptquant": 200, "frac": [150, 151, 152, 153, 154, 155, 169, 192, 247], "fraction": [217, 250], "fractionalmaxpool2d": 64, "fractionalmaxpool3d": 65, "framework": [181, 182, 185, 186, 188, 191, 194, 199, 203, 221, 223, 227, 245, 247, 250], "free": [220, 230, 239, 240], "freez": [160, 188, 194, 198, 200, 237], "freeze_encod": 160, "friendli": [167, 189, 192, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 249], "from": [2, 7, 8, 10, 11, 12, 13, 14, 15, 19, 148, 149, 150, 151, 155, 156, 159, 161, 162, 163, 164, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 250], "from_encod": 155, "from_modul": 19, "from_numpi": 166, "from_pretrain": [194, 196, 197, 199], "from_str": [10, 170, 178, 221], "front": [2, 14, 175, 176, 189, 228, 236], "frozen": [1, 187, 196, 197, 221, 229], "full": [13, 181, 188, 218, 224, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242], "fulli": [0, 156, 200, 209, 212, 214, 221], "func": [2, 163, 168, 175, 177, 204, 217], "func_callback_arg": [2, 163, 168, 175, 177, 204, 217, 228, 236], "function": [0, 2, 5, 9, 10, 12, 13, 14, 15, 18, 19, 147, 148, 149, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 181, 187, 188, 189, 190, 192, 194, 196, 197, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 212, 216, 217, 221, 222, 224, 229, 230, 231, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244, 245, 247, 250], "function_nam": [233, 234, 235], "functional_op": 162, "fundament": 160, "furhter": 238, "furo": 200, "further": [147, 150, 151, 152, 153, 154, 161, 169, 206, 209, 222, 226, 235, 246, 247, 250], "fuse": [196, 199, 223, 247], "fuse_bn_into_conv": 191, "fusion": [181, 199, 200], "futur": [157, 199, 202], "fx": [14, 161, 176, 189], "g": [165, 178, 192, 194, 204, 214, 219, 228, 229, 231, 236, 239, 243, 246], "gain": [206, 207, 209, 229, 233, 234, 235, 237, 240, 241, 242, 246, 250], "gamma": 188, "gap": 218, "gate_proj": 199, "gaussiannllloss": 70, "gave": 189, "gelu": 66, "gemm": [8, 200, 216, 223], "gemma3": 200, "genai": 200, "gener": [6, 19, 150, 151, 152, 153, 154, 158, 166, 169, 170, 175, 181, 184, 187, 188, 190, 191, 192, 194, 196, 197, 198, 199, 200, 204, 205, 209, 214, 216, 217, 219, 220, 221, 222, 223, 225, 227, 228, 236, 238, 245, 246, 247, 249], "generate_calibration_callback": [196, 197], "generate_layer_output": [6, 158, 203], "get": [2, 18, 155, 161, 163, 167, 175, 182, 187, 197, 198, 200, 203, 209, 210, 212, 215, 217, 226, 227, 228, 229, 230, 232, 246], "get_activation_quant": [2, 217], "get_active_quant": [2, 163, 175, 217], "get_available_provid": [187, 198, 228, 229, 230, 232], "get_calibration_and_eval_data_load": [219, 221], "get_candid": [2, 163, 175, 217], "get_default_kernel": 19, "get_devic": 238, "get_encod": 155, "get_extra_st": 155, "get_input": [187, 217, 219, 228, 229, 230, 231, 232], "get_input_quantizer_modul": [163, 175, 217], "get_kernel": 19, "get_loss_fn": [171, 179, 198], "get_offset": 169, "get_param_quant": [2, 217], "get_path_for_per_channel_config": [186, 190, 220, 221], "get_peft_model": 167, "get_pre_processed_input": 203, "get_quant_scheme_candid": [14, 176, 189], "get_scal": [149, 169], "get_swap_module_params_on_convers": 155, "get_unlabeled_dataload": 231, "get_val_dataload": [228, 230, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243], "getenv": 244, "git": 227, "github": [166, 185, 227], "give": [200, 204, 209, 215, 231, 233, 234, 235, 243, 246, 248], "given": [2, 4, 7, 12, 14, 15, 17, 18, 19, 157, 163, 164, 170, 171, 174, 175, 176, 178, 179, 187, 189, 190, 192, 198, 199, 202, 206, 208, 209, 210, 212, 215, 217, 219, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 246, 250], "global": [181, 198, 225], "glu": 67, "go": [200, 228, 231, 236, 239, 243], "goal": [14, 176, 189, 204, 222, 228, 236, 249], "good": [167, 209, 233, 234, 235, 239, 241, 242, 246, 250], "googl": 181, "got": [3, 16, 161, 191], "gpu": [181, 183, 184, 185, 186, 200, 203, 206, 210, 212, 221, 231, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244], "grad_fn": [147, 148, 149, 150, 151, 166, 169, 186], "gradient": [147, 148, 149, 196, 197, 200, 221], "grant": 227, "granular": [18, 200, 205, 206, 209, 210, 212, 225, 233, 234, 235], "graph": [10, 12, 14, 149, 161, 166, 170, 174, 176, 178, 187, 189, 191, 192, 198, 200, 211, 214, 217, 219, 220, 221, 222, 228, 229, 230, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247, 249], "graph_optimization_level": 166, "graphmodul": 161, "graphoptimizationlevel": 166, "greater": [18, 206, 207, 208, 210, 212, 246], "greatest": 219, "greedi": [18, 206, 209, 210, 211, 212], "greedili": [228, 236], "greedy_param": [206, 210, 212, 233, 234, 235], "greedy_select_param": [18, 206, 210, 233, 234, 235], "greedymixedprecisionalgo": [163, 175, 217, 236], "greedyselectionparamet": [18, 206, 210, 212, 233, 234, 235], "green": 213, "grid": [169, 171, 179, 192, 198, 216, 247], "group": [2, 8, 159, 163, 175, 192, 200, 216, 223, 228, 236, 247], "groupnorm": 71, "gru": [68, 200], "grucel": 69, "guarante": 200, "guid": [0, 156, 182, 195, 200, 221, 226, 227, 228, 236, 237, 238, 240, 241, 242, 243, 246, 249], "guidebook": 209, "guidelin": [161, 221, 236, 237, 240, 241, 242, 243], "gz": [219, 221, 231], "h": [212, 213, 227], "ha": [0, 9, 12, 14, 19, 147, 156, 160, 161, 162, 170, 174, 175, 176, 178, 181, 183, 184, 187, 188, 189, 194, 195, 200, 204, 205, 207, 208, 211, 213, 216, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 249], "had": 224, "half": 207, "hand": [228, 236], "handl": [14, 15, 176, 185, 186, 189, 190, 200, 205, 247], "hard": 161, "hardshrink": 72, "hardsigmoid": 73, "hardswish": 74, "hardtanh": 75, "hardwar": [10, 170, 178, 181, 205, 221, 222, 244, 247, 249], "hat": 247, "have": [9, 19, 157, 158, 159, 160, 161, 162, 165, 168, 169, 177, 181, 183, 186, 192, 194, 196, 198, 199, 200, 202, 203, 204, 205, 209, 214, 216, 219, 221, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 250], "hba": [230, 240], "he": 209, "heavi": [157, 202, 211, 246], "height": [191, 192, 210, 212, 213, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "held": [19, 164, 241, 242], "help": [160, 162, 163, 170, 178, 180, 193, 199, 204, 205, 208, 209, 217, 218, 220, 221, 222, 223, 225, 227, 230, 231, 240, 243, 244, 249, 250], "helper": [163, 169, 170, 175, 196, 199, 217, 239], "hen": [14, 176, 189], "here": [12, 160, 167, 169, 174, 183, 187, 192, 196, 197, 219, 221, 223, 228, 230, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 249, 250], "heterogen": 249, "heurist": 238, "hexagon": 244, "hidden": 200, "hide": 220, "high": [2, 4, 17, 160, 175, 181, 182, 192, 193, 195, 200, 207, 208, 217, 222, 229, 230, 232, 233, 234, 235, 237, 240, 241, 242, 244, 246], "higher": [7, 12, 174, 175, 187, 200, 206, 208, 210, 212, 216, 217, 218, 219, 220, 222, 225, 228, 233, 234, 235, 236, 246, 247, 249], "highest": [14, 176, 189, 208, 219], "highli": [166, 246], "highlight": [211, 244], "hingeembeddingloss": 76, "histogram": [9, 168, 177, 200, 201, 231, 243, 247], "historgram": [9, 204, 231], "hold": [147, 148, 149, 164, 190, 223, 225], "honor": [206, 210, 212], "hood": 160, "hook": 247, "hope": [228, 236, 239], "host": [185, 200, 211], "hotspot": [9, 168, 177, 204, 215], "hover": 200, "how": [2, 162, 164, 167, 169, 170, 175, 181, 182, 184, 186, 187, 195, 199, 200, 204, 205, 209, 210, 212, 214, 215, 216, 217, 220, 221, 222, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 248, 250], "howev": [160, 185, 187, 195, 200, 205, 206, 210, 212, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 246, 247, 249, 250], "html": [9, 157, 161, 168, 177, 184, 200, 201, 202, 204, 227, 231, 243], "htp": [200, 244, 245], "htp_v66": [10, 170, 178, 221, 222], "htp_v68": [10, 170, 178, 221, 222], "htp_v69": [10, 170, 178, 221, 222], "htp_v73": [10, 170, 178, 221, 222], "htp_v75": [10, 170, 178, 221, 222, 231], "htp_v79": [10, 170, 178, 221, 222], "htp_v81": [10, 170, 178, 221, 222, 244], "http": [161, 166, 185, 188, 194, 199, 200, 211, 219, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "hub": [180, 182, 191, 200, 203, 205, 214, 216, 217, 219, 246, 247, 249, 250], "huberloss": 77, "huggingfac": [167, 196, 197, 199, 200], "hx": [68, 69, 85, 86, 112, 113], "hxwx5": 213, "hxwx8": 213, "hyper": [190, 229, 233, 234, 235, 237, 239, 240, 241, 242, 249], "hyperparamet": [222, 246], "i": [0, 1, 2, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18, 19, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 168, 169, 170, 174, 175, 176, 177, 178, 179, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250], "i_": [150, 151, 152, 153, 154], "i_0": [150, 151, 152, 153, 154], "i_d": [150, 151, 152, 153, 154], "iccv": [209, 230, 240], "id": [6, 14, 176, 189, 196, 197, 203, 211], "idea": 192, "ideal": [200, 231, 243], "idempot": 149, "ident": [191, 224], "identifi": [159, 162, 199, 200, 201, 204, 205, 206, 213, 215, 216, 217, 219, 225, 227, 246, 247, 249], "ieee": [155, 209], "ignor": [2, 18, 19, 161, 164, 175, 206, 210, 212, 217, 221, 228, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 250], "ignore_quant_ops_list": [12, 174, 187], "ignore_unknown_modul": [19, 164], "illustr": [187, 206, 208, 210, 211, 212, 213, 233, 234, 235, 247, 249], "ilsvrc": [189, 217, 219, 221, 231], "ilsvrc2012": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "ilsvrc2012_devkit_t12": [219, 221, 231], "ilsvrc2012_img_v": [219, 221, 231], "imag": [9, 168, 177, 181, 184, 187, 189, 190, 198, 200, 204, 217, 219, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "image1": 244, "image2": 244, "image_bw": 224, "image_net_config": [228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "image_net_data_load": [228, 230, 231, 232, 233, 235, 237, 239, 240, 241, 242, 243], "image_net_evalu": [228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "image_net_train": [233, 234, 235, 239, 241, 242], "image_rgb": 224, "image_s": [228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "imagefold": [236, 238, 244], "imagenet": [186, 187, 189, 190, 191, 192, 198, 217, 219, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "imagenet1k_v1": 244, "imagenet_data": [187, 198, 229], "imagenet_dataset": [219, 221, 231, 238, 244], "imagenet_dir": 244, "imagenetdataload": [228, 230, 231, 232, 233, 235, 237, 239, 240, 241, 242, 243], "imagenetdatapipelin": [228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "imagenetevalu": [228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "imagenettrain": [233, 234, 235, 239, 241, 242], "images_dir": 239, "images_mean": 236, "images_std": 236, "imbal": 192, "immedi": [230, 238, 240], "impact": [181, 208, 219, 222, 225, 226, 228, 236, 244, 246], "implement": [6, 19, 158, 169, 189, 196, 197, 199, 200, 203, 204, 225, 227, 228, 236, 249, 250], "impli": [217, 228, 236], "import": [12, 19, 147, 148, 149, 150, 151, 153, 154, 155, 156, 159, 160, 161, 162, 164, 166, 167, 169, 170, 174, 178, 181, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 205, 206, 210, 212, 216, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247], "impos": 205, "improp": 200, "improv": [182, 187, 188, 191, 193, 194, 199, 200, 205, 207, 209, 217, 218, 219, 221, 225, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 244, 246, 248, 250], "in1": 220, "in2": 220, "in_channel": [159, 205, 216], "in_eval_mod": 238, "in_featur": [19, 160, 164, 170], "in_plac": [170, 178, 194, 196, 197, 199, 221, 222, 244], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "includ": [2, 14, 157, 175, 176, 181, 185, 189, 191, 192, 199, 200, 202, 204, 208, 211, 214, 217, 222, 223, 246, 247, 250], "incompat": 200, "inconsist": 200, "incorrect": [163, 200, 220], "incorrectli": [200, 250], "increas": [18, 193, 206, 208, 210, 212, 219, 222, 228, 236, 246, 247], "increment": 250, "incur": [204, 217], "independ": [161, 181, 225], "index": [9, 164, 168, 177, 184, 200, 204, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "index_0": [231, 243], "index_1": [231, 243], "index_n": [231, 243], "indic": [18, 98, 99, 100, 163, 164, 169, 170, 200, 206, 207, 210, 212, 213, 214, 220, 221, 222, 236, 238, 244], "indirect": [217, 228, 236], "individu": [9, 168, 177, 182, 198, 204, 208, 215, 223], "induc": 247, "infer": [10, 12, 14, 170, 174, 176, 181, 186, 187, 188, 189, 191, 194, 198, 200, 204, 205, 207, 217, 219, 220, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 250], "inferencesess": [2, 9, 10, 166, 200, 203, 204, 217, 219, 221, 228, 229, 230, 232], "influenc": 247, "info": [162, 200], "inform": [2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 160, 162, 166, 175, 180, 182, 200, 214, 217, 221, 228, 236, 239, 244], "inherit": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 200], "init": [191, 192, 199], "initi": [6, 7, 10, 150, 151, 155, 164, 169, 170, 178, 191, 192, 203, 219, 221, 228, 230, 231, 232, 236, 237, 241, 242, 243], "initial_accuraci": [189, 238], "initializd": 164, "inner": 225, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "inp_symmetri": [171, 179, 198], "inplac": [162, 191, 192], "input": [1, 2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 181, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 212, 213, 214, 216, 217, 219, 221, 222, 223, 224, 228, 229, 230, 231, 232, 236, 239, 243, 245, 247], "input1": [35, 51, 94], "input2": [35, 51, 94], "input_": [150, 151, 152, 153, 154], "input_batch": 203, "input_channel": [8, 159, 205, 216], "input_data": [189, 221, 228, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "input_dlc": 245, "input_id": [194, 196, 197, 199], "input_inst": [6, 158, 203], "input_length": 37, "input_list": 245, "input_nam": [166, 170, 178, 186, 187, 191, 198, 217, 219, 221, 222, 228, 229, 230, 231, 232], "input_network": 245, "input_q": 169, "input_qdq": 169, "input_qtzr": 19, "input_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 163, 164, 170, 173, 175, 217, 221, 222], "input_shap": [16, 17, 18, 161, 187, 191, 192, 198, 204, 206, 210, 212, 217, 219, 220, 221, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242], "input_tensor": 161, "inputs_batch": [221, 228, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "insert": [161, 193, 199, 200, 217, 219, 221, 228, 229, 230, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 247, 249], "insid": [13, 19, 161, 164, 184, 188, 200, 220], "insight": [211, 225, 246], "inspect": 209, "instabl": 239, "instal": [181, 182, 200, 211, 227, 238, 240, 241, 242, 249], "instanc": [1, 6, 7, 11, 19, 158, 161, 162, 178, 187, 198, 203, 211, 219, 222, 246, 250], "instancenorm": 200, "instancenorm1d": 78, "instancenorm2d": 79, "instancenorm3d": 80, "instanti": [167, 169, 196, 197, 199, 200, 205, 211, 214, 217, 223, 224, 228, 231, 236, 239, 243], "instead": [161, 162, 200, 205, 213, 214, 219, 221, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "instruct": [182, 185, 194, 199, 200, 205, 227, 245, 250], "int": [1, 2, 6, 8, 9, 11, 12, 13, 14, 15, 18, 48, 49, 50, 98, 99, 100, 150, 151, 152, 153, 154, 155, 159, 160, 163, 165, 168, 170, 171, 174, 175, 176, 177, 178, 179, 187, 188, 189, 190, 194, 196, 197, 198, 200, 203, 204, 205, 206, 210, 212, 214, 216, 217, 219, 221, 222, 228, 233, 234, 235, 236, 238, 244, 247], "int16": [7, 10, 163, 186, 189, 200, 217, 219, 220, 221, 222, 228, 236, 246, 247, 249, 250], "int32": [166, 170, 196, 197, 200, 214], "int4": [10, 163, 187, 189, 198, 200, 220, 221, 222, 249], "int8": [10, 148, 149, 163, 181, 186, 187, 189, 198, 217, 219, 220, 221, 222, 228, 229, 230, 232, 236, 237, 240, 241, 242, 246, 247, 249, 250], "int_multipli": 19, "integ": [153, 154, 159, 169, 178, 181, 187, 196, 200, 204, 205, 214, 216, 217, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 246, 249, 250], "integr": [14, 167, 176, 188, 189], "intel": [183, 185, 186], "intellig": 209, "intend": [181, 204, 206, 210, 212, 214, 220], "intens": 222, "inter": 201, "interact": [157, 160, 181], "intercept": 247, "interdepend": 217, "interest": [9, 168, 177, 204], "interfac": [200, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "intermedi": [2, 6, 14, 158, 165, 170, 175, 176, 178, 181, 189, 194, 200, 201, 203, 217, 221, 222, 228, 236], "intern": [14, 160, 176, 178, 189, 199, 209, 223, 237], "interpol": [208, 228, 236], "interpret": 221, "intersect": 246, "introduc": [164, 188, 192, 193, 194, 200, 223, 247], "invalid": [161, 205], "invoc": [233, 234, 235], "invok": [9, 10, 164, 168, 170, 177, 178, 204, 209, 211, 221, 222], "involv": [2, 160, 175, 190, 195, 196, 197, 217, 225, 228, 236, 250], "io": [163, 220], "ip": 227, "ipynb": 227, "irrespect": [13, 188], "is_avail": [186, 187, 188, 189, 190, 191, 192, 194, 198, 199, 217, 220, 221, 222, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "is_bfloat16": 155, "is_float16": 155, "is_initi": [19, 150, 151, 155, 164, 169], "is_input_quant": 223, "is_leaf_modul": 161, "is_output_quant": 223, "is_quant": 223, "is_sym": 214, "is_symmetr": [185, 214, 223], "is_train": [228, 230, 231, 232, 233, 235, 237, 239, 240, 241, 242, 243], "is_unsigned_symmetr": 160, "isinst": [159, 196, 197, 205, 216, 244], "islic": [187, 198, 219, 222, 229], "isol": [184, 200, 247], "issu": [162, 166, 190, 200, 201, 203, 211, 220, 225, 250], "item": [194, 196, 197, 199, 217, 221, 222, 231, 243, 245], "iter": [9, 10, 12, 13, 14, 18, 165, 174, 176, 183, 187, 188, 189, 194, 199, 200, 204, 206, 210, 212, 217, 221, 228, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244, 246], "itertool": [187, 196, 197, 198, 219, 222, 229], "its": [9, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 155, 163, 164, 181, 182, 188, 194, 196, 197, 204, 205, 206, 213, 217, 220, 222, 223, 227, 228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 247, 250], "itself": [203, 209, 210, 231, 239, 243], "j_": [150, 151, 152, 153, 154], "j_0": [150, 151, 152, 153, 154], "j_d": [150, 151, 152, 153, 154], "jaderberg": 209, "jan": 209, "jenkin": 184, "jian": 209, "jianhua": 209, "jit": [196, 197, 224], "job": [233, 234, 235, 239, 241, 242, 245], "join": [186, 187, 191, 192, 198, 206, 210, 212, 217, 219, 221, 236, 237, 238, 244], "jointli": [241, 242], "jpeg": 244, "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 231, 243, 245, 246, 247, 248, 249, 250, 251], "jupyt": [182, 200, 227], "just": [200, 213, 220, 228, 231, 236, 241, 242, 243, 247], "k": [169, 194, 196, 197, 199, 212, 238], "k_proj": 199, "kaim": 209, "kd": 200, "keep": [161, 192, 223, 232, 246, 250], "kei": [155, 163, 186, 194, 196, 197, 199, 214, 220, 229, 230, 232, 233, 234, 235, 237, 240, 241, 242, 246], "kept": [221, 225], "kera": 200, "kernel": [19, 200, 206, 210, 212, 216], "kernel_s": [161, 162, 170, 173, 191, 192, 221, 222], "keyword": [19, 162], "kl": 181, "kldivloss": 81, "know": [19, 228, 236, 239], "knowledg": [188, 193, 194, 214], "known": [162, 166, 199, 200, 205, 247], "kullback": 181, "kuzmin": 209, "kv": 200, "kwarg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 162, 163, 164, 166, 170, 171, 198, 200, 217, 221, 222], "l1": [171, 179, 198, 220], "l1loss": 82, "l2": 220, "lab": [181, 249], "label": [184, 187, 189, 190, 194, 196, 197, 198, 199, 204, 217, 219, 221, 222, 228, 229, 236, 238, 244], "labeled_data": 189, "labeled_data_load": 189, "lambda": [14, 159, 163, 165, 175, 176, 189, 194, 200, 205, 216, 217, 231], "laptop": [181, 182], "larg": [166, 167, 192, 193, 195, 200, 207, 209, 210, 212, 222, 250], "larger": [210, 212, 246], "last": [200, 220], "latenc": [183, 200, 207, 217, 218, 219, 246, 250], "later": [178, 183, 185, 186, 199, 200, 219, 221, 228, 231, 239, 243, 244], "latest": [163, 183, 185, 186, 200, 220], "launch": 227, "layer": [1, 3, 4, 6, 7, 9, 11, 12, 14, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 157, 158, 159, 162, 164, 167, 168, 170, 171, 174, 176, 177, 178, 179, 181, 187, 188, 189, 190, 191, 194, 196, 197, 198, 199, 200, 202, 205, 206, 207, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 226, 227, 229, 233, 234, 235, 246, 247, 249, 250], "layer1": [231, 243], "layer2": [231, 243], "layer_nam": [7, 9, 165, 168, 177, 194, 204, 219], "layer_output_util": [0, 5, 156, 203], "layer_sensitivity_dict": [7, 219], "layern": [231, 243], "layernorm": [87, 200], "layeroutpututil": [6, 158, 203], "layers_to_exclud": 162, "layout": [149, 161], "lazili": 200, "lazyextractor": 200, "lceil": [150, 151, 153, 154, 155, 247], "lead": [9, 11, 198, 200, 204, 221, 225], "leaf": [161, 163, 188, 194, 200], "leakyrelu": [88, 200], "learn": [160, 178, 180, 181, 182, 183, 191, 192, 195, 196, 197, 200, 209, 222, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 244, 246, 250], "learnabl": [150, 151, 188, 193, 194, 200, 222], "learnedgrid": 200, "learnedgridquant": 160, "learning_r": [233, 234, 235, 239, 241, 242], "learning_rate_schedul": [233, 234, 235, 239, 241, 242], "learnt": 194, "least": [175, 219, 229, 237], "leav": 197, "left": [150, 151, 152, 153, 154, 155, 169, 195, 205, 208, 213, 217, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 247], "leftarrow": 192, "legaci": 200, "leibler": 181, "len": [10, 12, 161, 174, 187, 189, 194, 196, 197, 199, 217, 219, 222, 236, 238], "length": [159, 164, 205, 214, 216], "leq": [150, 151, 152, 153, 154], "less": [181, 206, 208, 218, 223, 236, 247, 249], "lesser": [228, 236], "let": [161, 192, 194, 219, 221, 228, 236, 249], "level": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 159, 160, 161, 163, 165, 168, 169, 171, 172, 174, 175, 176, 177, 178, 179, 182, 187, 188, 189, 190, 191, 192, 194, 195, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 210, 211, 212, 216, 217, 218, 219, 220, 221, 222, 225, 229, 232, 233, 234, 235, 237, 241, 242, 247], "leverag": 246, "lfloor": [150, 151, 152, 153, 154, 155], "libpymo": [160, 185, 200], "libpython": 200, "libqnnhtp": 245, "libqnnmodeldlc": 245, "librari": [10, 167, 181, 221, 245], "lie": 216, "light": [228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243], "lightweight": 195, "like": [160, 182, 200, 203, 204, 209, 214, 215, 216, 217, 219, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 249], "limit": [194, 199, 200, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "limitedbatchdataload": [194, 199], "line": [221, 245], "linear": [3, 12, 16, 19, 89, 159, 160, 161, 162, 164, 167, 174, 187, 190, 191, 192, 193, 199, 200, 205, 206, 216], "linear1": [159, 205, 216], "linear10": 216, "linear_1": 205, "link": [200, 227], "list": [1, 2, 3, 6, 8, 10, 11, 12, 14, 16, 17, 18, 48, 49, 50, 98, 99, 100, 153, 154, 155, 158, 159, 161, 162, 163, 164, 168, 171, 174, 175, 176, 177, 179, 187, 189, 191, 192, 194, 196, 197, 198, 199, 203, 204, 205, 206, 208, 210, 212, 214, 216, 220, 221, 223, 228, 229, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 250], "list_of_module_comp_ratio_pair": [18, 206, 210, 212], "listen": 211, "lite": [182, 191, 200, 203, 205, 214, 216, 217, 245, 247, 249, 250], "lite_mp": [0, 5, 219], "litemp": [200, 246], "liter": [163, 220], "littl": [160, 209, 229, 233, 234, 235, 237, 240, 241, 242, 250], "ll": [185, 227], "llama": [194, 199, 200], "llama3": 194, "llamadecoderlay": [188, 194], "llamaforcausallm": [194, 199], "llamamodel": [188, 194], "llm": [200, 249], "llm_configur": 200, "lm_head": 199, "load": [13, 161, 162, 178, 181, 182, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 206, 209, 210, 212, 217, 219, 221, 228, 229, 230, 232, 244, 245, 249], "load_adapt": [196, 197], "load_checkpoint": 178, "load_dataset": [187, 189, 190, 194, 196, 197, 199, 217], "load_encod": [187, 221], "load_encodings_to_sim": [200, 203, 221], "load_model": [186, 187, 191, 192, 198, 217, 219, 221, 228, 229, 230, 231, 232], "load_state_dict": 155, "loader": [2, 9, 12, 14, 15, 163, 171, 174, 175, 176, 179, 187, 189, 190, 198, 204, 217, 219, 221, 222, 229, 230, 231, 232, 233, 235, 237, 239, 240, 241, 242, 243], "local": [198, 200, 211, 219, 221, 227, 231, 244], "localresponsenorm": 90, "locat": [199, 219, 221, 228, 231, 236, 239, 243, 250], "log": [162, 163, 204, 220], "log_2": 155, "log_fil": [163, 220], "log_input": 111, "log_prob": 37, "logger": 162, "logic": [2, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 175, 200, 217, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243], "logit": [196, 197, 217, 222, 236, 238, 244], "logsigmoid": 91, "logsoftmax": 92, "long": [194, 196, 199], "longer": [160, 170, 205, 214, 221, 222, 233, 234, 235, 249], "look": [182, 214, 216, 228, 231, 236, 243, 245], "loop": [161, 222, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "lora": [165, 167, 181, 194, 200], "lora_a": [196, 197], "lora_a_lay": 197, "lora_add_lay": 197, "lora_alpha": 167, "lora_b": [196, 197], "lora_b_lay": 197, "lora_config": 167, "lora_dropout": 167, "lora_mul_lay": 197, "loraconfig": 167, "loralay": [196, 197], "lose": 213, "loss": [9, 11, 12, 147, 168, 171, 174, 177, 179, 182, 187, 190, 196, 197, 198, 200, 209, 218, 222, 229, 230, 232, 237, 240, 241, 242, 244, 247], "loss_fn": [171, 179, 190, 196, 197, 198, 222], "lost": [244, 247, 250], "lot": 246, "low": [160, 169, 181, 193, 195, 200, 205, 209, 214, 221, 233, 235], "lower": [11, 12, 174, 181, 187, 192, 198, 208, 216, 217, 218, 222, 225, 228, 236, 246, 249], "lowest": [206, 246], "lpai": 200, "lpbq": [159, 200, 205, 214], "lppool1d": 83, "lppool2d": 84, "lr": [190, 200, 222, 244], "lstm": [85, 200], "lstmcell": 86, "lsvrc": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "lt": 185, "lwc": 194, "m": [184, 185, 212, 227], "mac": [18, 206, 207, 209, 210, 212, 215, 217, 228, 233, 234, 235, 236], "machin": [180, 181, 195, 200, 209, 250], "made": [161, 163, 181, 200, 220, 223, 236, 237, 240, 241, 242, 244], "magnitud": 206, "mai": [2, 11, 14, 19, 147, 148, 149, 157, 160, 161, 164, 175, 176, 187, 189, 192, 198, 199, 200, 202, 204, 205, 209, 214, 217, 219, 221, 222, 228, 229, 230, 231, 236, 239, 240, 243, 246, 250], "main": [190, 222, 223], "maintain": [181, 189, 200, 208, 209, 237, 246], "major": [209, 214], "make": [163, 164, 167, 181, 192, 200, 217, 220, 224, 239, 249, 250], "make_dummy_input": 186, "make_psnr_eval_fn": 219, "makedir": [228, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "manag": [10, 185, 200, 249], "mandatori": 203, "mani": [161, 200, 207, 228, 231, 233, 234, 235, 236, 243], "manner": [14, 176, 189, 192], "mantissa": [155, 159, 205], "mantissa_bit": [155, 159, 205], "manual": [18, 150, 151, 160, 163, 185, 189, 200, 206, 209, 210, 212, 214, 233, 234, 235, 246], "manual_param": [206, 210, 212], "manual_se": [236, 244], "manualmodeparam": [18, 206, 210, 212], "manylinux_2_34_x86_64": 185, "map": [19, 148, 153, 154, 162, 164, 181, 194, 196, 197, 199, 200, 204, 208, 214, 221, 223, 247], "map_loc": 203, "marginrankingloss": 94, "mark": 199, "marku": 209, "mask": 19, "maskedadd": 19, "match": [155, 170, 178, 200, 204, 205, 206, 209, 210, 212, 213, 221, 222, 223, 225], "math": [189, 217, 219, 250], "mathemat": [169, 181, 192, 199, 224, 228, 236], "matmul": [8, 200, 216], "matmul_8": 162, "matric": 196, "matrix": 246, "matter": [19, 231, 243], "max": [9, 150, 151, 155, 157, 160, 168, 177, 185, 198, 200, 201, 202, 209, 214, 221, 232, 245, 249], "max_batch": 219, "max_epoch": [233, 234, 235, 239, 241, 242], "max_iter": [194, 196, 197], "maximum": [2, 14, 153, 154, 155, 175, 176, 189, 200, 217, 228, 231, 236, 243, 247], "maxpool1d": 95, "maxpool2d": 96, "maxpool3d": 97, "maxunpool1d": 98, "maxunpool2d": 99, "maxunpool3d": 100, "mdoel": 190, "mean": [15, 164, 187, 189, 190, 198, 201, 217, 219, 221, 223, 228, 229, 231, 233, 234, 235, 236, 238, 239, 240, 243, 244, 246, 247], "measur": [2, 9, 18, 168, 175, 177, 181, 204, 206, 210, 212, 217, 219, 221, 228, 233, 234, 235, 236, 246, 250], "mechan": [161, 169, 187, 200], "meet": [14, 175, 176, 189, 207, 208, 217, 228, 236, 246, 250], "member": 223, "memori": [18, 149, 167, 181, 182, 200, 206, 207, 209, 210, 212, 213, 215, 218, 222, 233, 234, 235, 246, 250], "memory_format": 149, "merg": [181, 199], "met": [2, 175, 217, 228, 236], "meta": [194, 199], "metadata": [165, 194], "metapackag": 227, "method": [6, 13, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 158, 160, 161, 163, 164, 170, 175, 176, 178, 181, 185, 188, 189, 192, 193, 196, 203, 208, 209, 217, 218, 220, 221, 222, 225, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 246, 247], "methodologi": 250, "metric": [18, 187, 200, 204, 206, 210, 212, 217, 228, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 246, 247, 250], "mha": 200, "middl": 250, "might": [2, 175, 192, 200, 204, 214, 217, 225, 228, 229, 231, 233, 234, 235, 236, 237, 240, 241, 242, 243, 246, 250], "migrat": [0, 156, 200, 221], "mimic": 249, "mimick": 160, "min": [9, 150, 151, 157, 160, 168, 177, 185, 199, 200, 201, 202, 214, 232, 245, 249], "min_max": [10, 187, 200, 219, 221, 228, 229, 232], "min_max_rang": [204, 231, 243], "miniconda": 184, "miniforg": 184, "minim": [171, 179, 181, 182, 192, 193, 198, 215, 217, 218, 228, 232, 236, 241, 242, 246, 247], "minima": 198, "minimum": [153, 154, 161, 188, 194, 198, 200, 231, 243, 247], "minmaxencodinganalyz": 155, "minor": [200, 214, 222], "miou": [228, 236, 246], "mish": 101, "misidentif": 200, "mismatch": [169, 201, 203, 221], "miss": [155, 162, 200, 221, 245], "missing_kei": 155, "mistral": [199, 200], "mistralforcausallm": 199, "mix": [2, 14, 163, 175, 176, 183, 189, 200, 227, 250], "mixed_precis": [0, 5, 156, 217, 220, 228, 236], "mixed_precision_algo": [2, 163, 175, 217, 228, 236], "mixedprecisionconfigur": [163, 220], "mixin": [19, 164], "mkdir": 227, "ml": [182, 209, 232, 241, 242], "mmp": [200, 218], "mmp_log": [163, 220], "mnist": [206, 210, 212], "mnist_torch_model": 206, "mnist_trained_on_gpu": [206, 210, 212], "mnt": [184, 231], "mobil": [181, 182], "mobilenet": [181, 187, 190], "mobilenet_v2": [186, 187, 189, 190, 191, 192, 198, 217, 219, 220, 221, 222], "mobilenet_v2_weight": [187, 191, 192, 198, 217, 219, 221], "mobilenetv2": [187, 189, 191, 192, 219, 221], "mode": [18, 161, 170, 176, 178, 200, 206, 210, 212, 221, 222, 223, 233, 234, 235], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 222, 223, 226, 227, 244, 245, 247, 248, 249, 251], "model_config": [194, 199], "model_id": [194, 196, 197, 199], "model_input": 162, "model_or_pipelin": 187, "model_prepar": [0, 156, 200, 236, 237, 239, 241, 242, 243], "model_prepare_requir": [14, 176, 189], "model_preparer_elementwise_add_exampl": 161, "model_preparer_functional_exampl": 161, "model_preparer_reused_exampl": 161, "model_torch": 203, "model_transform": 161, "model_valid": [0, 156], "modelcompressor": [18, 206, 210, 212, 233, 234, 235], "modeling_llama": [194, 199], "modeling_opt": [196, 197], "modelprepar": [161, 236, 237, 241, 242, 243], "modelproto": [3, 4, 6, 9, 10, 191, 192, 203, 204, 221], "modelvalid": 162, "modelwithconsecutivelinearblock": 188, "modelwithelementwiseaddop": 161, "modelwithfunctionallinear": 162, "modelwithfunctionalrelu": 161, "modelwithlinear": 188, "modelwithnontorchfunct": 161, "modelwithoutfunctionallinear": 162, "modelwithoutreusednod": 162, "modelwithreusednod": 162, "modelwithreusedrelu": 161, "modif": [236, 237, 240, 241, 242], "modifi": [13, 161, 170, 178, 187, 188, 191, 192, 196, 197, 198, 200, 203, 213, 217, 219, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 247, 249, 250], "modul": [2, 8, 12, 13, 14, 15, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 157, 158, 159, 161, 162, 163, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 210, 212, 213, 216, 217, 220, 221, 222, 224, 228, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 250], "module_cl": [19, 164], "module_classes_to_exclud": 161, "module_nam": [196, 197], "module_to_exclud": 161, "modulecompratiopair": [18, 206, 210, 212], "moduledict": [19, 160, 164, 170, 173, 221, 222], "modulelist": [19, 160, 164, 170, 173, 221, 222], "modules_to_exclud": [14, 161, 171, 176, 179, 189, 198], "modules_to_ignor": [18, 168, 177, 204, 206, 210, 212, 233, 234, 235], "momentum": [190, 191, 192], "monitor": 204, "monoton": [18, 206, 208, 210, 212], "more": [12, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 162, 164, 166, 167, 170, 174, 175, 178, 181, 187, 192, 200, 204, 206, 208, 209, 210, 211, 212, 217, 218, 220, 222, 223, 225, 228, 231, 236, 239, 243, 244, 245, 246, 247, 249, 250], "most": [169, 181, 185, 189, 218, 219, 244, 246, 247], "move": [217, 227], "movement": 200, "mp": [163, 218, 219, 220], "mp_configur": 220, "mse": [9, 11, 160, 168, 171, 177, 179, 200, 204, 222, 247], "mseloss": 93, "much": [213, 233, 234, 235, 250], "mul_scal": 197, "multi": 200, "multigpu": 200, "multiheadattent": 200, "multilabelmarginloss": 102, "multilabelsoftmarginloss": 103, "multimarginloss": 104, "multipl": [14, 16, 17, 18, 149, 158, 161, 162, 163, 164, 167, 170, 176, 178, 189, 191, 192, 200, 203, 205, 206, 209, 210, 212, 216, 220, 221, 222, 231, 243, 246, 247], "multipli": [164, 207, 209, 210, 212, 215, 228, 236], "must": [149, 155, 159, 162, 164, 187, 188, 189, 190, 194, 198, 199, 204, 205, 207, 213, 216, 220, 221, 223, 229, 230, 231, 232, 233, 234, 235, 237, 240, 241, 242, 243, 249], "mutual": [155, 159, 205], "mymodel": 199, "mymodul": [19, 164], "n": [9, 168, 169, 177, 204, 205, 212, 222, 244], "n01440764": 244, "n01443537": 244, "n_imag": 244, "n_iter": 244, "nagel": 209, "name": [1, 2, 8, 9, 11, 19, 157, 158, 163, 164, 167, 168, 170, 175, 177, 178, 184, 187, 191, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204, 211, 214, 216, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 243, 245, 247], "name_": [9, 168, 177, 204, 231, 243], "name_to_quantizer_dict": [2, 163, 175, 217], "named_modul": [196, 197], "named_paramet": 196, "named_qmodul": 170, "named_quant": 170, "named_quantizer_paramet": 170, "namedtupl": 155, "namespac": [156, 160, 174, 175, 176, 177, 178, 179], "naming_schem": [158, 203], "namingschem": [158, 203], "nan": 200, "nativ": [164, 200], "navig": 227, "na\u00efv": 193, "nconv": 191, "ndarrai": [1, 2, 6, 9, 10, 11, 187, 189, 198, 203, 204, 217, 221, 229], "nearest": [176, 178, 187, 229, 237], "necessari": [147, 148, 149, 170, 184, 199, 204, 206, 210, 211, 212, 221, 222, 227, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 250], "necessarili": [228, 236], "need": [9, 10, 12, 13, 18, 158, 163, 168, 174, 177, 181, 185, 187, 188, 190, 191, 192, 194, 198, 199, 200, 203, 204, 206, 210, 212, 214, 217, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250], "neg": [137, 138, 153, 154, 200, 222, 228, 236], "negat": 237, "nest": 200, "net": [219, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245], "network": [164, 181, 200, 207, 208, 209, 211, 222, 225, 238, 247], "neural": [181, 207, 209, 222, 225, 228, 236, 238, 246, 247], "neuron": 181, "new": [149, 150, 151, 160, 161, 178, 182, 195, 196, 200, 205, 217, 220, 224, 236, 243], "new_empti": 149, "next": [192, 199, 200, 219, 221, 225, 228, 230, 236, 239, 244, 247], "next_conv_weight": 192, "night": [229, 230, 231, 232, 237, 240, 241, 242, 243], "nllloss": 105, "nllloss2d": 106, "nmodel": [191, 192], "nn": [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 156, 157, 159, 160, 161, 162, 163, 166, 170, 187, 188, 189, 190, 196, 197, 198, 199, 200, 202, 204, 205, 206, 210, 212, 216, 217, 220, 221, 222, 224, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 250], "nncf": 181, "nnext": 192, "no_grad": [162, 170, 186, 187, 188, 196, 197, 198, 200, 217, 221, 222, 236, 237, 238, 240, 241, 242, 243, 244], "node": [1, 8, 10, 11, 14, 161, 166, 170, 175, 176, 178, 181, 187, 189, 191, 192, 198, 200, 216, 221, 222, 224, 228, 229, 230, 232, 236, 237, 240, 241, 242, 246, 249, 250], "node_names_to_optim": [1, 187], "nodes_to_exclud": [8, 11, 198, 216], "nodes_to_includ": [1, 187], "noffset": 169, "nois": [192, 204, 215, 219, 222, 223, 249], "noisi": 239, "non": [10, 161, 163, 167, 171, 179, 188, 194, 198, 200, 211, 221, 238, 245, 247, 249], "none": [1, 2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 48, 49, 50, 60, 68, 69, 73, 74, 85, 86, 89, 98, 99, 100, 112, 113, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 160, 161, 163, 164, 166, 167, 168, 169, 170, 173, 174, 175, 176, 177, 178, 179, 185, 186, 187, 188, 189, 190, 191, 192, 198, 200, 202, 203, 204, 205, 206, 210, 211, 212, 216, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "norm": [192, 200, 204], "normal": [181, 187, 189, 191, 198, 200, 204, 217, 219, 221, 229, 238, 244], "notabl": 214, "note": [2, 9, 13, 14, 160, 168, 169, 170, 171, 175, 176, 177, 178, 179, 185, 188, 189, 192, 198, 203, 204, 205, 206, 210, 211, 212, 217, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 246], "note1": [228, 231, 236, 239, 243], "note2": [228, 231, 236, 239, 243], "notebook": [200, 226, 229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242, 244], "noth": [18, 206, 210, 212, 250], "notic": [157, 200, 202], "notimplementederror": 19, "now": [161, 162, 169, 170, 186, 187, 191, 192, 200, 214, 217, 221, 222, 228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "np": [1, 2, 10, 11, 185, 187, 189, 198, 204, 217, 219, 221, 229, 231, 244], "nprev": 192, "npu": 244, "nscale": 169, "nullptr": 200, "num": [153, 154], "num_batch": [9, 12, 15, 171, 174, 179, 187, 188, 189, 190, 198, 199, 204, 221, 229, 230, 231, 237, 238, 239, 240, 243], "num_calibration_sampl": [187, 198, 217, 219, 221, 229], "num_candid": [11, 171, 179, 198], "num_channel": 214, "num_comp_ratio_candid": [18, 206, 210, 212, 233, 234, 235], "num_epoch": [190, 200, 222], "num_eval_sampl": [187, 217, 229], "num_iter": [1, 13, 165, 187, 188, 194, 200], "num_of_sampl": 189, "num_reconstruction_sampl": [18, 206, 233, 235], "num_sampl": [2, 163, 175, 188, 217, 219, 236, 238], "num_samples_for_phase_1": [14, 176, 189], "num_samples_for_phase_2": [14, 176, 189], "num_step": [153, 154], "num_val_samples_per_class": 231, "num_work": [187, 189, 190, 198, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "number": [1, 2, 9, 11, 12, 13, 14, 15, 18, 19, 153, 154, 155, 159, 161, 163, 164, 165, 171, 174, 175, 176, 178, 179, 181, 187, 188, 189, 190, 194, 195, 198, 199, 200, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 219, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 247, 249, 250], "numer": [158, 200, 203, 246], "numpi": [166, 185, 186, 187, 189, 198, 204, 217, 219, 221, 228, 229, 230, 231, 232, 244], "numpy_help": [191, 192], "nvidia": [181, 183, 185, 186, 221], "o": [185, 186, 187, 191, 192, 198, 200, 206, 210, 212, 217, 219, 221, 228, 229, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "o_proj": 199, "object": [1, 2, 7, 8, 9, 10, 13, 14, 19, 147, 148, 149, 155, 157, 159, 162, 163, 164, 165, 167, 168, 169, 170, 171, 175, 176, 177, 178, 179, 187, 188, 189, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 210, 212, 216, 217, 218, 219, 220, 221, 222, 228, 231, 236, 237, 238, 240, 241, 242, 243, 246, 247, 249, 250], "observ": [10, 19, 155, 164, 170, 200, 204, 209, 219, 221, 222, 239, 244, 246, 247], "obtain": [6, 158, 204, 207, 214, 245], "obvious": [206, 210, 212], "occur": [157, 199, 202, 214, 217], "occurr": [9, 168, 177, 204, 206], "oct": 209, "off": [12, 174, 184, 187, 190, 217, 221, 228, 236, 250], "offer": [9, 168, 177, 189, 198, 201, 204, 205, 218, 238, 246], "offset": [9, 19, 60, 150, 151, 152, 153, 154, 168, 169, 177, 185, 200, 204, 214, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 245, 247, 249], "offset_": [150, 151, 152, 153, 154], "often": [187, 189, 191, 192, 209, 222, 246], "older": [185, 220], "omit": [208, 223], "omniqu": [0, 156, 188, 200], "onc": [19, 149, 162, 169, 200, 206, 209, 220, 224, 232, 233, 234, 235, 240, 241, 242, 245, 249, 250], "one": [10, 12, 14, 16, 161, 162, 163, 169, 170, 174, 175, 176, 178, 185, 187, 188, 189, 191, 194, 196, 197, 200, 205, 206, 209, 210, 211, 212, 214, 216, 217, 219, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 249], "ones": [194, 199, 246, 247], "ones_lik": [150, 151], "onli": [2, 9, 10, 147, 148, 149, 153, 154, 155, 157, 159, 160, 161, 163, 164, 168, 169, 170, 175, 177, 178, 183, 184, 185, 187, 191, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 209, 211, 213, 214, 216, 217, 219, 220, 221, 222, 223, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 250], "onnx": [0, 1, 2, 3, 5, 6, 8, 9, 10, 14, 156, 158, 163, 170, 176, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 198, 200, 202, 203, 204, 205, 214, 216, 217, 219, 220, 221, 222, 223, 224, 227, 231, 245, 246, 247, 249, 250], "onnx_data": [187, 229], "onnx_data_gener": 221, "onnx_encoding_path": 221, "onnx_export_arg": [14, 158, 170, 176, 178, 189, 203, 221, 222], "onnx_file_nam": 224, "onnx_model": [186, 204, 219, 231], "onnx_model_path": 231, "onnx_output": 166, "onnx_qdq": 10, "onnx_util": 203, "onnxexportapiarg": [14, 158, 170, 176, 178, 189, 203, 221, 222], "onnxmodel": [9, 204], "onnxruntim": [10, 166, 184, 185, 186, 187, 198, 200, 203, 204, 219, 221, 228, 229, 230, 232], "onnxsim": [187, 191, 192, 203, 204, 217, 219, 221, 228, 229, 230, 232], "onto": 249, "op": [2, 10, 12, 14, 162, 170, 174, 175, 176, 178, 187, 189, 199, 200, 217, 221, 222, 223, 228, 229, 230, 232, 233, 235, 236, 237, 238, 239, 240, 241, 242, 245, 247], "op_list": 223, "op_typ": [2, 8, 10, 175, 216, 217, 223, 228, 236], "op_type_map": 162, "open": 181, "opencv": 200, "oper": [8, 19, 160, 161, 162, 164, 181, 186, 188, 192, 193, 194, 198, 199, 200, 203, 216, 217, 219, 221, 223, 224, 225, 228, 232, 236, 241, 242, 247, 249, 250], "oppos": [230, 240], "opset": 200, "opset_vers": [166, 170, 178, 221, 222], "opt": [196, 197], "optim": [1, 2, 11, 12, 13, 14, 18, 155, 165, 171, 174, 175, 176, 179, 180, 181, 182, 183, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 206, 209, 211, 215, 217, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250], "optimized_accuraci": [189, 238], "optimum": 222, "option": [2, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 150, 151, 152, 153, 154, 155, 159, 161, 163, 166, 167, 168, 170, 171, 174, 175, 176, 177, 178, 179, 185, 186, 187, 188, 189, 190, 198, 200, 204, 205, 206, 210, 212, 216, 217, 219, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247], "optlearnedpositionalembed": [196, 197], "optmiz": [199, 228, 236], "orang": 213, "order": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 162, 164, 200, 201, 206, 220, 223, 224, 229, 231, 233, 234, 235, 237, 239, 241, 242, 243, 245, 249], "org": [161, 188, 194, 199, 200, 219, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "organ": [181, 244, 248], "origin": [18, 19, 160, 161, 164, 179, 181, 193, 198, 199, 200, 203, 204, 206, 207, 208, 210, 211, 212, 219, 228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 246, 247, 250], "ort": [10, 166, 187, 198, 221, 228, 229, 230, 232], "ort_disable_al": 166, "oscil": 190, "other": [149, 159, 160, 161, 163, 182, 185, 186, 188, 194, 195, 196, 197, 199, 200, 205, 208, 216, 220, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 248], "otherwis": [150, 151, 153, 154, 159, 162, 170, 178, 192, 205, 214, 216, 221, 222, 225, 229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242, 250], "our": [167, 239, 246], "out": [9, 150, 151, 152, 153, 154, 155, 161, 168, 169, 170, 177, 178, 185, 186, 200, 204, 221, 222, 226], "out1": 220, "out2": 220, "out3": 220, "out_": [150, 151, 152, 153, 154], "out_channel": [159, 205, 216], "out_featur": [19, 160, 164, 170], "outlier": [193, 199, 204, 247], "outlin": [185, 195, 207, 246], "output": [2, 6, 7, 8, 9, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 153, 155, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 175, 176, 177, 178, 179, 181, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 200, 204, 205, 206, 209, 210, 212, 213, 214, 216, 217, 219, 221, 222, 223, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247], "output_bw": [14, 176, 189], "output_dir": 245, "output_dir_path": 245, "output_dlc": 245, "output_encod": 19, "output_nam": [166, 170, 178, 187, 191, 198, 217, 219, 221, 222, 228, 229, 230, 231, 232], "output_path": [165, 194, 245], "output_qtzr": 19, "output_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 163, 164, 170, 173, 175, 217, 221, 222], "output_s": [48, 49, 50, 98, 99, 100], "outsid": [199, 223], "over": [9, 14, 153, 154, 164, 169, 176, 189, 193, 200, 204, 206, 208, 209, 210, 212, 216, 217, 246, 247], "overal": [192, 207, 208, 217, 225], "overfit": 198, "overflow": 200, "overhead": [233, 235, 236, 247], "overlin": [151, 154], "overload": [10, 153, 154, 170, 221, 222], "overrid": [14, 149, 161, 170, 176, 178, 189, 220, 221, 222, 236, 237, 240, 241, 242], "overridden": [19, 164, 223], "override_precis": [7, 219], "overtax": 250, "overview": [160, 215, 246, 248], "overwri": 221, "overwriiten": 221, "overwrit": 244, "overwritten": [160, 197, 221], "own": [189, 199, 204, 249], "p": [219, 221, 228, 231, 236], "p1": 220, "p2": 220, "pack": 226, "packag": [0, 156, 183, 200, 217, 227], "pad": [161, 162, 170, 191, 192, 200, 221, 222], "page": [182, 184, 185, 186, 207, 247, 250], "pair": [3, 16, 18, 191, 199, 206, 210, 212], "pairwis": 194, "pairwisedist": 108, "pandoc": 184, "paper": 199, "param": [2, 12, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 162, 163, 164, 168, 170, 171, 174, 175, 177, 179, 187, 196, 198, 200, 204, 205, 206, 210, 212, 214, 217, 220, 228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244, 249], "param_bitwidth": 214, "param_bw": [14, 176, 189], "param_bw_override_list": [12, 174, 187], "param_encod": [160, 214], "param_nam": [9, 168, 177, 204], "param_name_": [9, 168, 177, 204, 231, 243], "param_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 160, 164, 170, 173, 205, 221, 222], "param_typ": [10, 186, 187, 198, 219, 221, 228, 229, 230, 232], "paramet": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 173, 174, 175, 176, 177, 178, 179, 181, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 209, 210, 212, 214, 215, 216, 217, 218, 219, 220, 221, 223, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 249, 250], "parameter": [10, 221], "parameter_quant": [2, 163, 175, 217], "parent": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 200, 231], "pareto": [2, 14, 175, 176, 189, 228, 236], "pareto_front": 189, "pareto_front_list": [217, 228, 236], "pars": [12, 170, 174, 178, 187, 221, 222], "part": [9, 168, 177, 204, 209, 221, 249], "partial": [14, 161, 176, 189, 200, 221], "particular": [159, 162, 205, 216, 217, 223, 228, 236], "pass": [2, 9, 10, 12, 13, 14, 15, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 158, 160, 161, 162, 163, 164, 168, 170, 171, 174, 175, 176, 177, 178, 179, 181, 187, 188, 189, 190, 196, 197, 198, 200, 203, 204, 205, 211, 217, 219, 220, 221, 222, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247, 250], "pass_calibration_data": [221, 222, 228, 230, 232, 236, 237, 239, 240, 241, 242, 243], "past": [200, 227], "patch": 214, "path": [2, 9, 10, 12, 14, 18, 157, 161, 163, 165, 168, 170, 174, 175, 176, 177, 178, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 202, 203, 204, 206, 210, 212, 217, 219, 220, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245], "path_to_aimet_repo": 231, "path_to_imagenet": [217, 219, 221, 222], "pathlik": 221, "pattern": [200, 209], "pbar": 244, "pcq": [9, 168, 177, 204], "pdf": [9, 168, 177, 199, 200, 204], "peak": 219, "peft": [0, 156, 165, 194, 196, 197, 200], "peft_model": 167, "peft_model_id": [196, 197], "peftmixedmodel": 167, "penalti": 217, "pend": [13, 188], "pendyam": 209, "per": [8, 9, 18, 158, 159, 164, 168, 177, 181, 190, 191, 193, 200, 201, 203, 205, 206, 210, 211, 212, 214, 215, 216, 217, 219, 223, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 246, 247], "per_block": 214, "per_block_int_scal": 214, "per_channel": 214, "per_channel_quant": [164, 214, 223], "per_layer_mse_loss": [204, 231, 243], "per_layer_quant_dis": [204, 231, 243], "per_layer_quant_en": [204, 231, 243], "per_sample_weight": 60, "per_tensor": 214, "percent_to_flip": [7, 219], "percentag": [7, 181, 200, 218, 219, 229, 230, 231, 232, 237, 240, 241, 242, 243, 246], "perform": [2, 3, 4, 9, 12, 14, 15, 17, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 155, 164, 167, 168, 171, 174, 175, 176, 177, 179, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 204, 206, 207, 208, 209, 210, 212, 214, 215, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 243, 244, 247, 248, 249, 250], "perform_per_layer_analysis_by_disabling_quant": [9, 204], "perform_per_layer_analysis_by_disabling_quant_wrapp": [168, 177, 204], "perform_per_layer_analysis_by_enabling_quant": [9, 204], "perform_per_layer_analysis_by_enabling_quant_wrapp": [168, 177, 204], "perhap": [228, 231, 236, 239, 243], "period": [12, 174, 187], "perman": 200, "persist": 155, "person": 181, "perspect": [188, 194, 228, 236], "phase": [2, 14, 163, 175, 176, 189, 209, 228, 236], "phase1": [2, 175, 217, 228, 236], "phase1_optim": [2, 175, 217, 228, 236], "phase2": 175, "phase2_revers": 175, "phi": 200, "phone": [181, 182], "php": [227, 228, 231, 236, 239, 243], "pick": [2, 175, 207, 208, 217, 228, 236], "pickl": [18, 206, 210, 212], "pictur": [229, 230, 231, 232, 237, 240, 241, 242, 243], "piec": 161, "pin": [184, 200], "pin_memori": [149, 161, 236], "pink": 213, "pinpoint": 204, "pip": [185, 186, 200, 211, 227], "pipelin": [14, 176, 189, 200, 203, 225, 247, 249], "piptool": 184, "pitr": 209, "pixelshuffl": 109, "pixelunshuffl": 110, "place": [2, 4, 12, 13, 17, 170, 174, 175, 178, 187, 188, 192, 199, 200, 217, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "place_model": [196, 197], "placehold": [196, 197], "placement": [199, 200, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243], "plan": [185, 200, 249], "platform": [180, 181, 185, 204, 229, 230, 232, 237, 240, 241, 242], "pleas": [160, 162, 167, 170, 178, 185, 200, 204, 206, 210, 212, 221, 222, 228, 231, 232, 236, 239, 243, 244, 245], "plot": [2, 157, 175, 200, 202, 204, 217, 228, 231, 236, 243], "pmatrix": [150, 151, 152, 153, 154], "point": [2, 9, 19, 147, 148, 159, 160, 168, 175, 177, 178, 181, 191, 192, 193, 199, 200, 204, 205, 214, 217, 219, 221, 222, 225, 228, 230, 233, 234, 235, 236, 237, 239, 240, 241, 242, 244, 246, 247, 249, 250], "pointer": [231, 243], "poissonnllloss": 111, "polish": 200, "poor": 246, "popul": 214, "popular": 195, "port": [200, 211], "portabl": 181, "portion": 192, "posit": [137, 138, 153, 154, 200], "possibl": [2, 3, 14, 162, 170, 175, 176, 178, 189, 191, 204, 208, 217, 221, 222, 223, 224, 225, 228, 236, 238, 250], "post": [14, 176, 181, 183, 186, 189, 192, 200, 204, 209, 214, 216, 222, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241, 242, 247, 250], "post_training_tf": [9, 12, 157, 168, 174, 177, 187, 200, 202, 204, 214, 228, 236, 239, 240], "post_training_tf_enhanc": [9, 12, 14, 168, 170, 174, 176, 177, 178, 187, 189, 204, 214, 221, 222, 228, 230, 231, 236, 237, 239, 240, 241, 243], "potenti": [211, 239], "power": [181, 200, 205, 214, 218, 225, 246], "pp": 209, "practic": [205, 209, 217, 228, 231, 233, 234, 235, 236, 238, 243], "pre": [185, 188, 191, 194, 198, 199, 200, 201, 203, 204, 206, 209, 215, 219, 221, 227, 229], "preced": [190, 193, 213, 220], "precis": [2, 7, 14, 150, 151, 152, 153, 154, 155, 163, 175, 176, 181, 183, 189, 200, 214, 222, 225, 227, 229, 230, 231, 232, 237, 239, 240, 241, 242, 243], "precomput": [196, 221], "precursor": 191, "pred": 236, "pred_label": [187, 189, 198, 217, 219, 221, 229], "pred_prob": [187, 189, 217, 219, 221, 229], "predefin": 238, "predict": [181, 200, 221], "prefer": [160, 185, 227, 249], "prefix": [12, 164, 170, 174, 178, 187, 221, 222], "prelu": [107, 200], "prepar": [13, 14, 161, 167, 176, 188, 189, 191, 199, 200, 217, 224, 248, 250], "prepare_model": [161, 236, 237, 239, 241, 242, 243], "prepared_model": 161, "prepend": [164, 227], "preprocess": [187, 189, 198, 217, 229, 244], "prequantize_const": [10, 166], "prerequisit": 211, "presenc": 222, "present": [7, 155, 158, 160, 162, 167, 203, 214, 219, 239], "preserv": [155, 161, 187, 200, 217, 219], "preserve_format": 149, "pretrain": [187, 189, 190, 191, 192, 198, 204, 217, 220, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 247], "pretti": [206, 210, 212], "prev": 192, "prev_conv_weight": 192, "prevent": [160, 161, 197, 200, 206], "previou": [2, 18, 169, 175, 181, 206, 207, 210, 212, 217, 225, 228, 236, 245, 246], "print": [19, 153, 154, 160, 161, 162, 164, 169, 170, 173, 185, 186, 187, 189, 191, 192, 198, 204, 206, 210, 212, 217, 219, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244], "prior": [2, 175, 200, 217, 228, 236, 237, 240, 241, 242], "prioriti": 200, "privileg": 227, "probabl": [181, 200], "problem": [161, 200, 225, 250], "problemat": [161, 225], "proce": [14, 176, 189, 191, 221, 246, 250], "procedur": [200, 208, 211, 230, 233, 235, 240], "proceed": [191, 192, 217, 228, 230, 232], "process": [181, 189, 200, 203, 206, 208, 209, 217, 220, 221, 228, 229, 230, 232, 236, 237, 240, 241, 242, 246, 247, 249], "processor": [181, 183, 185, 186], "produc": [14, 147, 148, 157, 161, 176, 181, 189, 192, 201, 202, 204, 208, 214, 217, 228, 231, 236, 243, 247], "product": [181, 182, 200, 207], "profil": [163, 183, 184, 217, 219, 245, 246], "progress": [200, 211], "project": 181, "promot": [200, 217], "prone": [189, 220], "pronounc": 218, "propag": [200, 213, 220], "propagate_encod": [14, 170, 176, 178, 189, 221, 222], "proper": [199, 200, 244], "properli": [150, 151, 200, 203], "properti": [155, 160, 170], "provid": [0, 2, 5, 9, 10, 12, 14, 19, 155, 156, 160, 162, 167, 168, 170, 174, 175, 176, 177, 178, 181, 183, 184, 185, 187, 189, 196, 198, 200, 204, 205, 207, 208, 209, 211, 214, 217, 219, 220, 221, 222, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "proxi": 161, "prune": [18, 181, 200, 207, 208, 210, 212, 213, 215, 227, 234], "psnr": [200, 219], "psnr_eval_fn": 219, "pt_model": [187, 191, 192, 198, 217, 219, 221, 228, 229, 230, 232], "pth": [170, 178, 203, 206, 210, 212, 221, 222], "ptq": [14, 176, 181, 182, 183, 188, 189, 192, 193, 194, 199, 200, 204, 209, 221, 222, 238, 247, 249, 250], "public": [0, 156, 200, 221], "publish": 200, "pure": 224, "purpos": [181, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "put": [175, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "py310": 185, "pylint": 199, "pypi": [186, 200], "pyproject": 184, "pytest": 184, "python": [181, 183, 185, 186, 200, 244], "python3": [184, 185, 200, 227], "pythonpath": [203, 227], "pytorch": [0, 14, 156, 158, 161, 162, 164, 169, 170, 171, 176, 178, 179, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 210, 212, 216, 217, 219, 220, 221, 222, 223, 227, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 250], "q": [19, 147, 148, 149, 150, 151, 153, 154, 155, 160, 164, 169, 196, 197, 247], "q_": 247, "q_modul": 160, "q_output": 19, "q_proj": 199, "qadd": 164, "qairt": [200, 245], "qat": [12, 174, 181, 183, 187, 190, 196, 197, 200, 225, 227, 228, 232, 236, 247, 249, 250], "qat2": 200, "qc": 200, "qc_op": 10, "qc_quantize_op": 200, "qcquantizeop": [2, 10, 200, 217], "qcquantizewrapp": [160, 200], "qdo": 181, "qdq": [151, 155, 196, 200, 246, 249, 250], "qlinear": [19, 160, 164], "qmax": [150, 151, 153, 154, 169, 170, 173, 221, 222], "qmin": [150, 151, 153, 154, 169, 170, 173, 221, 222], "qmodul": [160, 170], "qmul": 164, "qmult": 19, "qnn": [183, 200, 245, 249], "qol": 200, "qsim": [13, 188], "qtype": [7, 10, 219, 221], "qtzr": [169, 196], "quad": [150, 151, 152, 153, 154], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 244, 246, 247, 248, 249, 250, 251], "qualiti": [228, 236], "quant": [2, 9, 10, 12, 14, 168, 170, 171, 174, 176, 177, 178, 179, 187, 189, 198, 200, 204, 217, 221, 222, 227, 228, 229, 236, 237, 238, 239, 240, 241, 242], "quant_analyz": [0, 5, 156, 160, 204, 231, 243], "quant_analyzer_result": 204, "quant_dequ": 147, "quant_schem": [9, 10, 14, 157, 168, 170, 176, 177, 178, 185, 187, 188, 189, 194, 199, 202, 204, 214, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "quant_sim": [165, 194, 220], "quant_sim_model": [178, 221], "quant_stats_visu": [157, 202], "quant_wrapp": 160, "quantanalyz": [9, 160, 168, 177, 200, 201, 219, 246], "quantiz": [0, 1, 2, 5, 8, 9, 10, 12, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 154, 155, 156, 157, 159, 161, 163, 166, 167, 168, 170, 171, 173, 174, 175, 176, 177, 178, 179, 181, 183, 185, 187, 188, 190, 191, 192, 194, 197, 198, 199, 200, 202, 203, 207, 209, 211, 218, 219, 220, 223, 224, 226, 227, 233, 234, 235, 238], "quantizablemultiheadattent": 200, "quantizaiton": [8, 216], "quantization_overrid": 245, "quantization_tf": 185, "quantizationdatatyp": [2, 14, 160, 163, 170, 175, 176, 178, 189, 217, 221, 222, 228, 236], "quantizationmixin": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 164, 196, 197], "quantizationmod": 185, "quantizationsim": [228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 246], "quantizationsimmodel": [1, 2, 7, 8, 9, 10, 11, 13, 14, 19, 157, 159, 160, 163, 164, 165, 166, 168, 169, 170, 171, 176, 177, 178, 179, 186, 187, 188, 189, 190, 194, 196, 198, 199, 200, 202, 203, 204, 205, 214, 216, 217, 219, 220, 221, 222, 223, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247], "quantizationsimmodelonnxexport": 170, "quantizationsimmodelv1": 160, "quantizationsimmodelv2": 160, "quantized_": [187, 188, 194], "quantized_dlc": 245, "quantized_linear": 19, "quantized_mobilenet_v2": [187, 198, 221], "quantized_model": [170, 221, 222], "quantized_repr": [19, 147, 148, 149], "quantizedadd": 164, "quantizedconv2d": [160, 164, 170, 173, 205, 221, 222], "quantizedequant": [147, 160, 164, 169, 170, 173, 185, 196, 205, 221, 222], "quantizedlinear": [19, 160, 164, 170, 205], "quantizedmaskedadd": 19, "quantizedmultipli": [19, 164], "quantizedoptlearnedpositionalembed": [196, 197], "quantizedrelu": 160, "quantizedsoftmax": 164, "quantizedtensor": [19, 147, 149, 150, 169, 200], "quantizelinear": [10, 166, 170, 200, 246], "quantizer_arg": 214, "quantizer_group": [2, 163, 175, 217], "quantizer_info": [2, 217], "quantizer_paramet": 170, "quantizerbas": [19, 164, 169, 197], "quantizergroup": [2, 14, 163, 175, 176, 189, 217], "quantschem": [9, 10, 12, 14, 157, 168, 170, 174, 176, 177, 178, 186, 187, 188, 189, 194, 199, 200, 202, 204, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "quantsim": [0, 5, 6, 9, 12, 156, 157, 158, 160, 163, 167, 168, 174, 177, 181, 186, 187, 191, 196, 197, 200, 201, 202, 203, 204, 205, 216, 217, 218, 220, 222, 227, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242, 246, 249, 250], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "quantsim_layer_output": 203, "quantsim_layer_output_util": 203, "quantsimmodel": [188, 194, 199], "quatiz": 249, "quic": [181, 185, 227], "quick": [169, 182, 183, 200, 221, 233, 235], "quickli": [182, 219, 228, 231, 236, 243], "qwa": 200, "qwen": [194, 199], "qwen2": [194, 200], "qwen2forcausallm": 199, "r": [2, 9, 167, 168, 175, 177, 184, 204, 217], "r1": [199, 200], "r1_fusion_pair": 199, "r1_placement": 199, "r2": 199, "r3": 199, "r4": 199, "radic": 250, "rais": [19, 155, 189, 199, 217, 244], "rand": [162, 188, 203, 236, 237, 239, 240, 241, 242, 243], "randint": [194, 199], "randn": [19, 148, 149, 150, 151, 161, 162, 164, 170, 185, 186, 187, 189, 190, 191, 192, 198, 204, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 238], "random": [185, 186, 204, 206, 231, 238], "random_split": [219, 221], "randperm": 236, "rang": [9, 153, 154, 157, 161, 168, 169, 177, 178, 181, 190, 192, 193, 194, 196, 197, 199, 200, 201, 202, 208, 217, 221, 222, 225, 228, 229, 230, 232, 236, 237, 238, 239, 240, 241, 246, 247], "rank": [18, 193, 195, 210, 212, 219], "rank_select": 212, "rank_select_schem": [18, 212], "rankselectschem": [18, 206, 210, 212], "rapidli": [183, 218], "rare": [200, 246], "rate": [200, 209, 222, 233, 234, 235, 239, 241, 242], "rather": [161, 198, 223, 239], "ratio": [18, 206, 207, 210, 212, 219, 233, 234, 235], "raw": [194, 196, 197, 199], "rceil": [153, 154], "re": [2, 169, 171, 175, 179, 198, 200, 217, 227, 228, 236, 250], "reach": [189, 238], "read": [204, 216], "reader": [228, 231, 236, 239, 243], "readi": [169, 170, 178, 199, 221, 222, 225, 228, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 246, 249], "readili": [228, 231, 236, 239, 243], "real": [147, 148, 186, 211, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "realiz": [163, 220], "realli": [228, 231, 239, 243], "reason": [162, 221, 222, 229, 230, 232, 237, 240, 241, 242, 250], "reassess": 246, "rebin": 200, "recalcul": 239, "recalibr": 219, "receiv": 161, "recent": 249, "recip": 200, "recogn": [162, 247], "recommend": [1, 2, 9, 10, 12, 166, 169, 174, 183, 185, 186, 187, 188, 190, 191, 192, 194, 204, 207, 217, 221, 225, 228, 229, 230, 232, 237, 238, 246, 249, 250], "recomput": [200, 229], "reconfigur": 200, "reconstruct": [12, 174, 187, 233, 235], "record": [9, 168, 177, 204], "recov": [200, 219, 225, 230, 233, 234, 235, 240, 244, 249, 250], "recoveri": 250, "recurr": 200, "recurs": 200, "redefin": 162, "redesign": 200, "reduc": [1, 11, 164, 167, 181, 182, 187, 190, 192, 193, 198, 199, 200, 209, 213, 215, 218, 222, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247], "reduct": [207, 233, 234, 235, 250], "redund": [191, 209], "reestim": [15, 190], "reestimate_bn_stat": [15, 190, 239], "refactor": 200, "refer": [158, 160, 167, 178, 179, 203, 205, 214, 217, 219, 221, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245], "reflect": [241, 242, 247], "reflectionpad1d": 117, "reflectionpad2d": 118, "reflectionpad3d": 119, "regard": [2, 175, 217, 229, 230, 231, 232, 237, 240, 241, 242, 243], "regardless": [188, 194], "regist": [19, 155, 188, 196, 197, 199, 200, 228], "regress": 206, "regular": [12, 19, 169, 170, 174, 178, 187, 221, 222, 228, 229], "rel": [18, 206, 207, 210, 212, 217, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 246], "relat": [160, 206, 210, 212, 247], "relationship": 205, "releas": [184, 185, 194, 227], "release_tag": 227, "relev": [233, 234, 235, 249], "reli": [160, 198, 222, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "reliabl": 185, "reload": 240, "relu": [115, 160, 161, 162, 192, 200, 213, 223, 224], "relu1": [162, 224], "relu2": [162, 224], "relu6": [116, 191, 192, 200], "remain": [13, 160, 171, 179, 187, 188, 192, 194, 198, 199, 200, 219, 222], "remov": [14, 15, 161, 173, 176, 181, 189, 190, 191, 197, 200, 206, 213, 214, 221, 244, 247], "remove_activation_quant": 173, "remove_all_quant": [173, 197, 244], "remove_column": [194, 196, 197, 199], "remove_input_quant": 173, "remove_output_quant": 173, "remove_param_quant": 173, "renam": 200, "render": 200, "reorgan": 200, "repackag": 200, "repeat": [167, 206, 246, 250], "repeatedli": 249, "replac": [10, 160, 161, 164, 167, 196, 200, 205, 219, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "replace_lora_layers_with_quantizable_lay": [167, 196, 197], "replicationpad1d": 120, "replicationpad2d": 121, "replicationpad3d": 122, "repo": 231, "report": [200, 219, 220, 246, 250], "repositori": 227, "repres": [2, 9, 10, 14, 147, 148, 149, 155, 163, 164, 166, 168, 170, 175, 176, 177, 178, 181, 186, 189, 198, 200, 204, 206, 208, 210, 212, 214, 217, 219, 220, 221, 222, 224, 231, 243, 245, 247], "represent": [147, 148, 149, 155, 169, 181, 200, 245, 247], "reproduc": 244, "requant": 191, "request": 220, "requir": [2, 18, 19, 158, 160, 161, 167, 169, 170, 178, 181, 184, 185, 187, 188, 191, 192, 194, 196, 198, 199, 200, 203, 204, 206, 207, 209, 210, 211, 212, 214, 217, 218, 219, 220, 221, 222, 223, 224, 228, 231, 233, 234, 235, 236, 237, 240, 241, 242, 243, 246, 247, 249, 250], "requires_grad": [148, 149, 155, 161, 192, 200, 221], "requires_grad_": [160, 196, 197], "rerun": 162, "rescal": 192, "research": 181, "resid": 200, "residu": 206, "resiz": [187, 189, 198, 200, 217, 219, 221, 229, 236, 244], "resnet": [170, 207, 221, 222, 228, 229, 230, 231, 232], "resnet18": [170, 204, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "resnet18_after_adaround": 229, "resnet18_after_cle_bc": [237, 238, 240], "resnet18_after_qat": [239, 241, 242], "resnet18_mixed_precis": [228, 236], "resnet18_quantsim_analysi": 231, "resolv": [162, 200], "resort": 225, "resourc": [181, 228, 236, 239, 244, 250], "respecit": [228, 236], "respect": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 164, 169, 179, 193, 200, 204], "respond": [231, 243], "respons": [2, 175, 181, 209, 217, 228, 236, 250], "rest": [200, 225], "restor": [178, 200, 225, 233, 234, 235, 247, 249], "restrict": [205, 216], "result": [2, 9, 14, 19, 147, 148, 149, 168, 169, 170, 175, 176, 177, 178, 187, 188, 189, 190, 192, 194, 196, 197, 198, 199, 201, 204, 206, 207, 209, 210, 212, 217, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247], "results_dir": [2, 9, 14, 168, 175, 176, 177, 189, 204, 217, 228, 231, 236, 243], "retain": [181, 217], "retest": 250, "retrain": [181, 224, 249], "retriev": 19, "retrieve_context": 245, "return": [2, 3, 6, 9, 10, 12, 13, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 157, 158, 159, 161, 162, 163, 164, 165, 168, 169, 170, 171, 174, 175, 176, 177, 178, 179, 187, 188, 189, 190, 191, 194, 196, 197, 198, 199, 202, 203, 204, 205, 206, 208, 210, 212, 216, 217, 219, 221, 222, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247], "return_dict": [194, 196, 197, 199], "reus": [161, 162, 224, 228, 236], "reveal": 225, "revert": 225, "revis": 214, "revisit": 207, "rework": 214, "rewrit": 162, "rfloor": [150, 151, 152, 153, 154, 155, 247], "rgb": [189, 217, 224], "rgb_output": 224, "right": [150, 151, 152, 153, 154, 155, 164, 169, 205, 213, 217, 228, 236, 238, 247], "rmsnorm": [199, 200], "rmsnorm_fusion_pair": 199, "rmsnorm_linear_pair": 199, "rmsnormal": 200, "rnn": [112, 200], "rnncell": 113, "robust": [166, 198, 200, 249], "root": [184, 187, 198, 200, 236, 238, 244], "rotat": [193, 199, 200], "rough": [228, 236], "roughli": [14, 163, 175, 176, 189, 217], "round": [1, 12, 18, 174, 176, 178, 181, 189, 200, 204, 210, 212, 227, 246, 247], "round_nearest": 185, "rounding_mod": [176, 178, 200], "roundingmod": 185, "routin": [221, 228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "rrelu": 114, "rtype": [10, 138, 147, 148, 149, 163, 175, 217], "rule": [200, 205, 223, 249], "run": [2, 9, 10, 13, 14, 15, 18, 150, 151, 160, 161, 162, 163, 164, 166, 168, 169, 170, 175, 176, 177, 178, 181, 183, 186, 187, 188, 189, 190, 198, 200, 203, 205, 206, 209, 210, 211, 212, 214, 216, 217, 218, 219, 220, 221, 226, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 245, 246, 247, 249, 250], "run_forward_pass": [10, 170, 221, 222], "run_infer": [14, 176, 189, 238], "runnabl": 249, "runtim": [11, 19, 169, 170, 178, 181, 182, 183, 191, 198, 200, 203, 204, 205, 206, 207, 209, 210, 212, 214, 216, 217, 219, 221, 222, 228, 229, 230, 232, 236, 237, 240, 241, 242, 245, 246, 248, 249, 250], "runtimeerror": [155, 170, 221, 222, 244], "s2": 188, "s3": 188, "s_1": 205, "s_2": 205, "s_n": 205, "safe": 149, "safetensor": [165, 194], "sai": [161, 207, 220, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "sake": [203, 230, 240], "same": [9, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 158, 160, 161, 162, 163, 164, 166, 167, 168, 170, 174, 175, 176, 177, 178, 179, 189, 192, 193, 195, 199, 200, 203, 204, 205, 216, 220, 221, 222, 223, 228, 230, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242], "sampl": [1, 2, 9, 10, 11, 13, 14, 157, 163, 164, 168, 171, 175, 176, 177, 178, 179, 180, 186, 187, 188, 189, 191, 192, 194, 196, 197, 198, 199, 200, 202, 204, 206, 217, 219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247], "sandeep": 209, "saniti": [169, 200], "satisfactori": [187, 188, 190, 194, 198, 225, 250], "satisfi": [161, 163, 189, 205, 220, 238], "saurabh": 209, "save": [2, 6, 9, 10, 12, 14, 18, 157, 158, 165, 168, 170, 174, 175, 176, 177, 178, 187, 189, 194, 200, 201, 202, 203, 204, 206, 210, 212, 217, 219, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247, 249], "save_checkpoint": 178, "save_model_as_external_data": 200, "save_path": [157, 202], "saved_eval_scores_dict": [18, 206, 210, 212], "scalar": [9, 168, 177, 200, 204], "scale": [19, 148, 149, 150, 151, 152, 153, 154, 155, 165, 169, 181, 185, 192, 193, 194, 200, 204, 205, 214, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 245, 247, 249], "scale_": [150, 151, 152, 153, 154, 169], "scenario": [213, 229, 230, 231, 232, 237, 240, 241, 242, 243, 244], "scene": 249, "schedul": [200, 233, 234, 235, 239, 241, 242], "schema": 200, "scheme": [9, 10, 12, 14, 18, 158, 168, 170, 174, 176, 177, 178, 181, 187, 189, 200, 203, 204, 205, 206, 209, 210, 212, 221, 222, 228, 231, 233, 234, 235, 236, 239, 243], "scope": 161, "score": [2, 9, 14, 18, 168, 175, 176, 177, 189, 204, 206, 208, 209, 210, 211, 212, 217, 228, 229, 230, 232, 238, 239], "script": 239, "sdk": [182, 228, 236, 246, 249], "search": [2, 14, 171, 175, 176, 179, 189, 193, 198, 200, 217, 222, 223, 228, 233, 234, 235, 236, 239, 241, 242, 246, 249], "searcher": 209, "sec": [228, 236], "second": [10, 12, 164, 167, 170, 171, 174, 179, 187, 191, 198, 205, 217, 221, 222, 231, 243], "secondari": 200, "section": [2, 162, 175, 196, 197, 200, 205, 215, 216, 217, 223, 226, 228, 236, 238, 246, 247, 248, 250], "see": [0, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 156, 166, 170, 178, 182, 185, 186, 192, 200, 205, 206, 209, 210, 211, 212, 216, 221, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 250], "seed": 206, "seem": 209, "seen": [204, 231, 243], "select": [14, 18, 176, 181, 185, 189, 196, 197, 204, 207, 210, 211, 212, 213, 216, 218, 219, 222, 223, 228, 236, 245, 246, 247], "select_param": [18, 212], "self": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 161, 162, 164, 189, 194, 199, 217, 224, 238], "selu": 123, "sens": [164, 217], "sensit": [2, 7, 9, 168, 175, 177, 190, 200, 201, 208, 215, 218, 222, 228, 230, 231, 236, 240, 246, 247, 249, 250], "separ": [12, 160, 161, 162, 170, 174, 178, 187, 190, 200, 204, 221, 222, 225], "separableconv2d": 200, "seq": 200, "seq_length": [194, 196, 197, 199], "seq_ms": [0, 156, 160, 198], "seqms": [193, 198, 200], "seqmseparam": [171, 179, 198], "sequenc": [170, 189, 193, 200, 221, 222, 223, 238], "sequenti": [11, 19, 160, 164, 171, 173, 179, 191, 192, 200, 222, 223, 225], "seri": [14, 163, 170, 176, 178, 189, 220, 221, 222, 238, 245], "serial": [221, 245], "serializetostr": [203, 219, 228, 229, 230, 232], "serv": [164, 189, 199, 204, 211], "servic": 220, "sess": [10, 166, 221, 228, 229, 230, 232], "sess_opt": 166, "session": [10, 186, 187, 198, 200, 203, 204, 217, 219, 221, 228, 229, 230, 231, 232], "sessionopt": 166, "set": [1, 2, 7, 8, 9, 10, 11, 14, 18, 19, 155, 157, 159, 160, 161, 162, 163, 164, 166, 168, 170, 175, 176, 177, 178, 181, 186, 187, 189, 195, 196, 197, 198, 200, 202, 204, 205, 206, 207, 208, 209, 210, 212, 214, 216, 217, 218, 219, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 249, 250], "set_activation_quantizers_to_float": [159, 205], "set_adapt": 167, "set_adaround_param": [14, 176, 189, 238], "set_and_freeze_param_encod": 237, "set_blockwise_quantization_for_weight": [159, 205, 216], "set_default_kernel": 19, "set_descript": 244, "set_export_param": [14, 176, 189], "set_extra_st": 155, "set_grouped_blockwise_quantization_for_weight": [0, 5, 159, 216], "set_kernel": 19, "set_mixed_precision_param": [14, 176, 189], "set_model_input_precis": [163, 220], "set_model_output_precis": [163, 220], "set_model_preparer_param": [14, 176, 189], "set_precis": [163, 220], "set_quant": 200, "set_quant_scheme_candid": [14, 176, 189], "set_quantizers_to_candid": [2, 163, 175, 217], "set_rang": [147, 148], "set_transform": [189, 217], "settabl": 247, "setup": [160, 184, 185, 186, 191, 217], "sever": [162, 164, 198, 200, 204, 207, 224, 246], "sgd": [190, 222], "sh": 184, "shall": 214, "shape": [10, 16, 17, 18, 147, 148, 149, 150, 151, 155, 159, 160, 161, 162, 164, 169, 170, 173, 187, 191, 192, 196, 198, 204, 205, 206, 210, 212, 216, 217, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "share": [162, 164, 199, 200, 205], "sharp": 209, "sharpli": [233, 234, 235, 250], "shift_label": [196, 197], "shift_logit": [196, 197], "should": [9, 13, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 158, 160, 161, 164, 168, 170, 177, 178, 179, 185, 186, 188, 199, 203, 204, 206, 207, 210, 212, 214, 221, 222, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245, 250], "shouldn": 169, "show": [162, 167, 199, 201, 205, 208, 217, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246], "showcas": [227, 231, 243], "shown": [160, 167, 184, 185, 186, 189, 204, 213, 217, 224, 225, 231, 243], "shuffl": [187, 188, 194, 198, 199, 206, 217, 219, 221, 229, 244], "side": 213, "sigmoid": [125, 161], "sign": [153, 154, 160, 169, 247], "signal": 219, "signatur": [10, 18, 19, 153, 154, 170, 189, 200, 204, 206, 210, 212, 221, 222, 233, 234, 235], "signific": [217, 225, 244], "significantli": [200, 222, 246], "silu": 124, "sim": [1, 2, 7, 8, 9, 10, 11, 13, 19, 157, 159, 160, 163, 164, 166, 167, 168, 170, 171, 173, 175, 177, 178, 179, 186, 187, 188, 189, 190, 194, 197, 198, 199, 200, 202, 204, 205, 216, 217, 219, 220, 221, 222, 229, 238, 244], "sim1": 160, "sim2": 160, "sim_model": [237, 239, 240, 241, 242, 243], "similar": [167, 169, 181, 185, 199, 216, 246, 247], "similarli": [225, 228, 231, 236, 239, 243, 246, 250], "simpl": [161, 204, 206, 210, 212, 231, 243], "simpler": 160, "simpli": [9, 161, 168, 177, 178, 204, 219, 221, 228, 236, 239], "simplic": [167, 203], "simplif": [192, 246], "simplifi": [1, 2, 9, 10, 160, 187, 191, 192, 203, 204, 217, 219, 221, 245], "simuat": [229, 230, 237, 240, 241, 242], "simul": [10, 155, 159, 164, 167, 170, 178, 181, 183, 187, 188, 190, 191, 194, 198, 199, 200, 201, 202, 203, 205, 214, 217, 218, 221, 222, 223, 224, 226, 227, 233, 234, 235, 243, 244, 246, 250], "sinc": [170, 199, 205, 207, 217, 221, 222, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 247], "singl": [2, 6, 14, 158, 161, 163, 170, 175, 176, 189, 200, 203, 204, 205, 206, 208, 209, 210, 212, 214, 216, 217, 231, 243, 247], "singular": [209, 210, 212, 234, 235], "situat": 204, "six": 223, "size": [8, 9, 149, 150, 151, 152, 153, 154, 159, 162, 164, 169, 170, 181, 188, 194, 204, 205, 210, 212, 214, 215, 216, 221, 237, 244, 246, 249, 250], "skbuild_build_target": 184, "skew": [229, 230, 232, 237, 240, 241, 242], "skip": [12, 14, 171, 174, 176, 179, 184, 187, 189, 196, 198, 200, 206, 230, 240, 246], "skipped_optim": 191, "slice": 200, "slightli": [195, 228, 236], "slim": 200, "slow": 209, "slower": 200, "small": [10, 170, 190, 195, 198, 200, 205, 219, 221, 222, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 246], "smaller": [2, 12, 174, 175, 187, 192, 209, 210, 212, 217, 218, 225, 228, 236, 249], "smoothl1loss": 126, "snapdragon": [228, 236], "snippet": [161, 200, 228, 236], "snpe": 200, "so": [9, 10, 12, 13, 160, 161, 162, 164, 170, 174, 178, 181, 185, 187, 188, 191, 192, 196, 197, 199, 200, 204, 206, 208, 210, 212, 221, 222, 228, 231, 233, 234, 235, 236, 237, 239, 241, 242, 243, 245, 250], "softmarginloss": 127, "softmax": [128, 164, 236], "softmax2d": 129, "softmin": 130, "softplu": [131, 161], "softshrink": 132, "softsign": 133, "softwar": [181, 182, 200], "sole": 204, "solid": 222, "solut": [208, 217, 222, 225, 236], "some": [18, 160, 161, 164, 169, 192, 196, 197, 200, 206, 208, 210, 212, 213, 217, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 246, 247, 249, 250], "someth": [178, 204, 209], "sometim": [204, 206, 209, 228, 229, 230, 232, 236, 237, 240, 241, 242, 250], "somewher": 250, "soon": 189, "sort": [13, 188, 219], "sourc": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 161, 162, 163, 164, 166, 168, 170, 171, 173, 174, 175, 176, 177, 178, 179, 181, 187, 189, 190, 191, 192, 198, 200, 202, 203, 204, 205, 206, 210, 212, 216, 217, 219, 220, 221, 222, 225], "space": [18, 206, 210, 212, 217, 247], "spars": 200, "spatial": [18, 200, 206, 207, 208, 212, 215, 227], "spatial_svd": [18, 206, 210, 212, 234, 235], "spatial_svd_auto_mod": 210, "spatial_svd_manual_mod": 210, "spatialsvdparamet": [18, 206, 210, 212, 234, 235], "spconv": 200, "special": [14, 161, 176, 181, 189], "specif": [9, 14, 18, 19, 159, 162, 164, 168, 170, 176, 177, 178, 184, 189, 193, 195, 199, 200, 204, 205, 206, 210, 212, 216, 221, 222, 223, 228, 229, 236, 237, 239, 244, 245, 246, 247, 250], "specifi": [2, 9, 12, 18, 150, 151, 152, 153, 154, 155, 163, 168, 170, 174, 175, 177, 178, 187, 189, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 217, 220, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "speed": [18, 191, 200, 206, 209, 210, 212, 215, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "speedup": [171, 179, 198, 228, 236], "spinquant": [0, 156, 200], "spinquant_optim": 199, "split": [171, 179, 187, 189, 190, 194, 196, 197, 198, 199, 217, 219, 221, 222, 229, 244, 247], "sqnr": [2, 14, 163, 171, 175, 176, 179, 189, 198, 217, 228, 236, 247], "sqnr_metric": [7, 219], "sqrt": 161, "squar": [201, 247], "squeez": 236, "ssvd": 207, "ssvd_comp_stat": 235, "ssvd_compressed_model": 235, "ssvd_cp_compressed_model": 235, "ssvd_cp_finetuned_model": 235, "ssvd_finetuned_model": 235, "stabl": [161, 222, 239], "stack": [180, 200, 217, 231], "stand": 200, "standalon": 200, "standard": [155, 161, 164, 181, 232, 241, 242, 244], "start": [2, 12, 153, 154, 161, 162, 167, 174, 175, 182, 187, 200, 207, 217, 222, 223, 226, 227, 228, 231, 233, 234, 235, 236, 239, 241, 242, 243, 246, 247, 250], "start_beta": [12, 174, 187], "stat": [9, 15, 157, 190, 200, 202, 204, 206, 210, 212], "statatist": 239, "state": [155, 179, 180, 183, 200, 217, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243], "state_dict": 155, "stateless": 224, "statement": 161, "static": [18, 161, 162, 200, 206, 210, 212], "staticgridperchannelquant": 160, "staticgridquant": 160, "staticgridquantwrapp": 160, "staticmethod": [161, 228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243], "statist": [9, 10, 15, 18, 150, 151, 155, 157, 164, 168, 170, 177, 190, 193, 201, 202, 206, 210, 212, 221, 222, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242], "std": [187, 189, 198, 217, 219, 221, 229, 236, 238, 244], "step": [1, 9, 153, 154, 161, 163, 167, 168, 171, 177, 179, 184, 185, 186, 192, 195, 196, 197, 206, 207, 208, 209, 216, 218, 225, 227, 228, 230, 231, 232, 236, 239, 243, 244, 245, 247, 248, 249], "still": [160, 205, 225, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 250], "stochast": 178, "stop": [12, 174, 187, 189, 246], "stopiter": [189, 199, 217], "storag": [215, 216, 247], "store": [12, 148, 163, 170, 174, 178, 187, 220, 221, 222, 228, 236], "str": [1, 2, 6, 7, 8, 9, 10, 11, 12, 14, 155, 157, 158, 161, 163, 165, 168, 170, 171, 174, 175, 176, 177, 178, 179, 187, 189, 194, 198, 202, 203, 204, 216, 217, 219, 220, 221, 222, 224, 229, 244], "str_idx": 189, "straightforward": [185, 250], "strateg": 217, "strategi": 250, "stream": [187, 189, 190], "streamlin": 246, "strict": [8, 155, 163, 216, 220, 221, 223, 247], "strict_symmetr": [185, 223], "strict_valid": [14, 176, 189], "strictli": [155, 199, 219, 221], "stride": [149, 161, 162, 170, 173, 191, 192, 200, 221, 222], "strike": 209, "string": [10, 163, 170, 178, 214, 220, 221, 223], "strongli": [161, 169, 229, 237], "structur": [14, 161, 164, 176, 181, 189, 209, 220, 231, 235, 243, 244], "stude": 226, "sub": [231, 243], "subbackward0": 169, "subclass": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 169, 200], "subdirectori": [228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "subfold": [228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "subgraph": 200, "sublay": 162, "submit": 245, "submodul": 224, "subpackag": [19, 200], "subsequ": [147, 181, 187, 191, 192, 199, 200, 221, 223, 228, 230, 236], "subset": [9, 10, 168, 170, 177, 190, 204, 205, 213, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246], "subsetrandomsampl": 238, "subsidiari": 182, "substanti": 214, "success": 238, "successfulli": [170, 203, 221, 222, 244], "sudo": 227, "suffic": [229, 230, 232, 237, 240, 241, 242], "suffici": [204, 221, 247, 249], "suggest": [178, 208, 209, 233, 235, 246, 250], "suit": [181, 185, 189, 196, 238], "suitabl": [181, 246], "sum": [187, 189, 198, 217, 219, 221, 222, 229, 236, 238, 244], "summari": [182, 200, 209], "sun": 209, "super": [19, 161, 162, 189, 199, 200, 217], "supergroup": [200, 247], "suppli": [205, 216], "support": [1, 2, 7, 9, 10, 11, 12, 18, 19, 161, 162, 163, 166, 168, 170, 171, 174, 175, 177, 178, 179, 182, 185, 186, 187, 188, 189, 190, 191, 194, 198, 199, 200, 202, 204, 205, 206, 207, 209, 210, 212, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 227, 228, 236, 239, 241, 242, 246, 247, 250], "supported_kernel": [2, 175, 217, 228, 236], "supported_kernel_op": [163, 175, 217], "supported_module_dict": 199, "suppos": [2, 175, 192, 217, 228, 236], "suscept": 222, "svd": [18, 200, 206, 207, 208, 215, 227], "swap": 200, "sweep": [11, 198], "switch": [200, 205], "sy": 231, "symbol": 161, "symbolic_trac": [14, 161, 176, 189], "symfp": [171, 179, 198], "symmetr": [19, 147, 148, 149, 150, 151, 159, 160, 164, 169, 170, 173, 196, 200, 205, 214, 216, 221, 222, 223, 247], "symmetri": [160, 171, 179, 198, 200], "symqt": [171, 179, 198], "syntax": 214, "system": 184, "systemat": 181, "t": [2, 10, 14, 19, 161, 169, 170, 171, 175, 176, 179, 184, 189, 194, 196, 197, 198, 199, 200, 204, 217, 221, 222, 228, 231, 236, 239, 243, 249], "tabl": [157, 199, 202, 211, 214, 227], "tag": 227, "take": [1, 2, 10, 13, 14, 18, 19, 159, 162, 163, 166, 167, 169, 170, 175, 176, 187, 188, 189, 194, 199, 200, 205, 206, 208, 209, 210, 212, 213, 216, 217, 219, 220, 221, 222, 225, 226, 228, 231, 233, 234, 235, 236, 239, 243, 246, 249], "taken": [13, 188, 213], "tanh": [134, 200], "tanhshrink": 135, "tap": [168, 177, 204], "tar": [18, 212, 219, 221, 231], "target": [10, 18, 30, 31, 37, 51, 53, 70, 76, 77, 81, 82, 93, 94, 102, 103, 104, 105, 106, 111, 126, 127, 167, 170, 175, 178, 181, 188, 189, 191, 194, 198, 200, 201, 203, 206, 207, 208, 209, 210, 212, 214, 216, 217, 221, 222, 225, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 246, 247, 250], "target_comp_ratio": [18, 206, 210, 212, 233, 234, 235], "target_data": [236, 237, 239, 240, 241, 242, 243], "target_length": 37, "target_modul": 167, "task": [181, 211, 214, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 246], "taxonomi": 209, "tbd": 202, "techniqu": [14, 176, 181, 183, 187, 188, 189, 191, 192, 194, 195, 196, 198, 199, 200, 204, 206, 207, 210, 212, 222, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 247, 249, 250], "technologi": [181, 182], "tempfil": 200, "temporari": [10, 187, 191, 192, 217, 221], "temporarili": [160, 173, 197, 244], "tend": 222, "tensor": [1, 2, 10, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 158, 161, 162, 163, 164, 166, 168, 169, 170, 174, 175, 176, 177, 178, 187, 188, 189, 191, 192, 194, 196, 197, 200, 203, 204, 205, 206, 210, 212, 214, 215, 217, 220, 221, 222, 223, 224, 225, 228, 230, 231, 236, 240, 241, 242, 243, 247], "tensor_nam": 214, "tensor_quant": 160, "tensorflow": [181, 182, 191, 200, 203, 205, 214, 216, 217, 219, 227, 243, 245, 246, 247, 249, 250], "tensorquantizationsimforpython": 185, "term": [170, 182, 205, 210, 212, 215, 221, 222], "termin": 227, "test": [9, 168, 177, 194, 196, 197, 204, 244, 249, 250], "test_data_load": 244, "test_dataload": [194, 196, 197], "test_dataset": [194, 196, 197], "text": [150, 151, 152, 153, 154, 194, 196, 197, 199], "tf": [2, 9, 168, 177, 200, 204, 217, 221, 228, 231, 236, 239, 243], "tf_enhanc": [10, 221, 228, 236, 239], "tfe": 200, "tfencod": 160, "tflite": [228, 236], "tfoplambda": 200, "than": [12, 18, 160, 161, 162, 174, 175, 178, 187, 198, 199, 200, 205, 206, 207, 210, 212, 216, 217, 220, 222, 223, 228, 229, 236, 237, 239, 246, 249], "thei": [161, 162, 195, 196, 197, 211, 216, 222, 225, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 246, 247, 249], "them": [14, 160, 161, 162, 164, 176, 181, 189, 192, 204, 209, 215, 219, 220, 221, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "theme": 200, "therebi": [217, 228, 236], "therefor": [2, 175, 191, 207, 217, 228, 236], "theta_": [150, 151], "thi": [2, 6, 9, 10, 11, 12, 13, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 174, 175, 176, 177, 178, 179, 182, 184, 185, 186, 187, 188, 189, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250], "thing": [229, 230, 232, 233, 234, 235, 237, 240, 241, 242, 245], "those": [9, 168, 177, 196, 204, 219, 223, 246, 249], "though": [19, 169, 205, 223, 228, 236, 250], "three": [161, 207, 228, 233, 234, 235, 236, 249], "threshold": [136, 157, 189, 202, 246], "through": [2, 10, 149, 160, 161, 163, 164, 168, 175, 177, 185, 187, 192, 196, 197, 199, 204, 216, 217, 219, 220, 221, 222, 226, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247, 250], "throughout": [13, 181, 188, 222, 223], "throw": [19, 169, 200], "thrown": [200, 221], "thu": [147, 148, 149, 217], "tie": 200, "tie_word_embed": 199, "tijmen": 209, "till": [2, 175, 217, 228, 236, 246], "time": [18, 149, 161, 162, 167, 183, 189, 200, 204, 205, 206, 209, 210, 211, 212, 216, 219, 220, 222, 231, 238, 243, 246, 249], "tmp": [9, 14, 168, 176, 177, 184, 186, 187, 189, 191, 192, 204, 217, 243], "tmpdir": 167, "to_arrai": [191, 192], "to_list": [2, 163, 175, 217], "to_onnx_qdq": [10, 200], "todo": [163, 220, 244], "toggl": 184, "token": [194, 196, 197, 199, 225], "tokenized_dummy_text": [196, 197], "toler": [189, 207, 238], "tolist": 236, "toml": 184, "too": [205, 233, 235], "tool": [168, 177, 181, 183, 184, 200, 204, 213, 245, 248, 250], "toolchain": 246, "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 245, 246, 247, 248, 249, 250, 251], "tooltip": 200, "top": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 159, 163, 165, 168, 171, 172, 174, 175, 176, 177, 178, 179, 187, 188, 189, 190, 191, 192, 194, 198, 199, 200, 204, 205, 206, 210, 211, 212, 216, 217, 219, 220, 221, 222, 228, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244], "top1": [238, 244], "top1_accuraci": [236, 244], "top5": 244, "top5_accuraci": 244, "topk": [236, 238, 244], "torch": [14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 159, 160, 161, 162, 163, 164, 166, 169, 170, 175, 176, 178, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 203, 204, 205, 206, 210, 212, 216, 217, 219, 220, 221, 222, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 250], "torchscript": [14, 158, 170, 176, 178, 181, 189, 203, 221, 222], "torchvis": [170, 186, 187, 189, 190, 191, 192, 198, 204, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "total": [196, 197, 208, 217, 221, 244, 247], "total_length": [194, 196, 197, 199], "total_sampl": [187, 198, 217, 219, 221, 229], "totensor": [187, 189, 198, 217, 219, 221, 229, 236, 238, 244], "toward": 225, "tpu": 181, "tqdm": [187, 196, 197, 198, 217, 219, 221, 222, 229, 236, 238, 244], "trace": [14, 157, 161, 176, 189, 202, 224], "traceabl": [161, 196, 197, 224], "traceback": 161, "traceerror": 161, "tracer": 161, "track": [204, 231, 243], "track_running_stat": [191, 192], "trade": [12, 174, 187, 217, 228, 236], "tradeoff": [181, 217, 228, 236, 250], "train": [9, 10, 14, 15, 18, 165, 168, 170, 176, 177, 181, 183, 186, 187, 188, 189, 190, 192, 194, 195, 198, 199, 200, 201, 203, 204, 206, 209, 210, 212, 216, 217, 219, 225, 227, 228, 236, 238, 247, 250], "train_data_load": 244, "train_dataload": [170, 194, 196, 197, 199, 221, 222], "train_dataset": [194, 196, 197, 199], "train_flag": [18, 206, 210, 212], "train_load": [206, 239], "train_model": [18, 206, 210, 212], "train_one_epoch": [196, 197], "trainabl": [188, 194, 195, 196, 197, 242], "trainer": [18, 206, 210, 212, 227, 233, 234, 235, 239, 241, 242], "training_range_learning_with_tf_init": [170, 188, 194, 199, 221, 222, 239, 242], "trainingextens": 184, "trainingmod": [228, 232], "transact": 209, "transform": [161, 187, 189, 194, 196, 197, 198, 199, 200, 205, 217, 219, 221, 229, 236, 237, 238, 241, 242, 243], "transpos": 169, "trap": 198, "travers": 220, "treat": 222, "tri": [183, 209, 233, 234, 235], "tripletmarginloss": 137, "tripletmarginwithdistanceloss": 138, "trivial": 200, "true": [2, 10, 14, 18, 19, 148, 149, 150, 151, 155, 159, 160, 161, 162, 163, 164, 166, 169, 170, 173, 175, 176, 178, 185, 187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 204, 205, 206, 210, 212, 214, 216, 217, 220, 221, 222, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "true_quant": 196, "truli": 161, "trust_remote_cod": [194, 199], "try": [18, 186, 189, 191, 192, 206, 207, 209, 210, 212, 217, 225, 228, 230, 232, 233, 234, 235, 236, 246, 249], "tune": [18, 167, 178, 182, 206, 207, 210, 212, 222, 229, 230, 232, 237, 238, 240, 241, 242, 244, 246, 249, 250], "tuner": 196, "tupl": [2, 3, 6, 8, 9, 12, 13, 14, 16, 17, 18, 86, 150, 151, 152, 153, 154, 158, 159, 162, 163, 168, 169, 170, 174, 175, 176, 177, 178, 186, 187, 188, 189, 191, 192, 194, 199, 200, 203, 204, 205, 206, 210, 212, 216, 217, 220, 221, 222, 224, 228, 231, 236, 243], "turn": [183, 200, 223], "tutori": 180, "tweak": [167, 246], "twice": 167, "two": [13, 18, 160, 161, 162, 163, 175, 181, 183, 188, 192, 199, 204, 208, 209, 210, 212, 217, 220, 222, 229, 230, 231, 232, 233, 234, 235, 237, 238, 240, 241, 242, 243, 244, 245, 247, 249], "txt": [163, 184, 220, 245], "ty": 200, "type": [1, 2, 3, 8, 9, 10, 12, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 157, 159, 160, 161, 162, 163, 164, 168, 170, 171, 174, 175, 176, 177, 178, 179, 181, 187, 188, 189, 190, 191, 198, 199, 200, 202, 204, 205, 206, 209, 210, 211, 212, 216, 217, 221, 222, 223, 228, 233, 234, 235, 236, 238, 241, 242, 244, 245, 249], "typeerror": 161, "typic": [19, 164, 181, 199, 204, 207, 217, 221, 228, 229, 231, 233, 235, 236, 237, 238, 239, 241, 242, 243, 246, 247, 249, 250], "ubuntu": [183, 185, 186], "ubuntu22": 200, "uint": 200, "uint16": 214, "uint32": 214, "uint8": [147, 214], "unaccept": 250, "unattain": 217, "uncalibr": 200, "unchang": [179, 192, 221], "uncompress": 207, "under": [1, 2, 10, 157, 160, 175, 187, 190, 202, 204, 217, 223, 225, 228, 236], "undergo": 181, "underli": [19, 169, 225], "understand": [160, 169, 227, 228, 231, 236, 239, 243], "undo": [15, 190], "uneven": 225, "unexpected_kei": 155, "unflatten": 139, "unfold": 140, "unid": [14, 176, 189], "uniniti": [13, 187, 188, 194, 198, 199], "unintuit": [14, 176, 189], "union": [6, 8, 9, 12, 14, 16, 17, 18, 158, 159, 162, 163, 168, 170, 174, 175, 176, 177, 178, 187, 189, 191, 192, 203, 204, 205, 206, 210, 212, 216, 220, 221, 222, 246], "unit": 181, "unknown": [200, 207], "unkown": [19, 164], "unlabel": [9, 14, 176, 187, 189, 198, 204, 217, 228, 229, 230, 231, 232, 233, 235, 236, 237, 238, 240, 241, 242, 243], "unlabeled_data": [189, 198], "unlabeled_data_load": [189, 204, 217, 231], "unlabeled_dataset_iter": [9, 204], "unlabeled_imagenet_data_load": 238, "unlabeled_imagenet_dataset": 238, "unlabeleddatasetwrapp": 238, "unlabelled_data_load": 189, "unless": [14, 19, 155, 170, 176, 178, 189, 213, 221, 222, 250], "unlik": [155, 191, 239], "unmodifi": [195, 208], "unnecessari": [191, 200, 213, 228, 236], "unpin": 200, "unrol": [161, 200], "unsign": [223, 247], "unsigned_symmetr": [185, 223], "unsigned_zero": 155, "unsimplifi": [191, 192, 217, 228, 230, 232], "unsqueez": [196, 197, 244], "until": [14, 150, 151, 170, 176, 189, 190, 200, 221, 222, 238], "untouch": 221, "up": [12, 18, 163, 174, 178, 181, 184, 187, 200, 204, 206, 209, 210, 212, 213, 217, 220, 221, 222, 223, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 249], "up_proj": 199, "updat": [1, 13, 155, 165, 187, 188, 190, 194, 197, 199, 200, 211, 214, 221, 222, 227, 229, 231, 232, 237, 238, 240, 241, 242, 247], "update_lora_weight": [165, 194], "updatestat": 185, "upgrad": [0, 156, 200], "upon": [15, 19, 164, 190, 200, 245], "upsampl": 141, "upsamplingbilinear2d": 142, "upsamplingnearest2d": 143, "upstream": [200, 206, 213], "upto": [228, 231, 236, 239, 243], "url": [18, 206, 210, 211, 212, 227], "us": [1, 2, 6, 8, 9, 10, 11, 12, 14, 15, 18, 19, 147, 148, 150, 151, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 167, 168, 169, 170, 174, 175, 176, 177, 178, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 207, 211, 214, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250], "usabl": 214, "usag": [13, 162, 167, 169, 181, 188, 194, 200, 205, 214, 217, 222, 245, 246], "use_all_amp_candid": [2, 175, 217, 228, 236], "use_cach": [194, 196, 197, 199], "use_cuda": [18, 185, 200, 203, 206, 210, 212, 231, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "use_embedded_encod": [170, 178, 221, 222], "use_fast": [194, 199], "use_monotonic_fit": [18, 206, 210, 212], "use_strict_symmetr": 160, "use_symmetric_encod": [160, 200], "user": [2, 10, 12, 13, 14, 18, 19, 160, 161, 163, 169, 170, 174, 175, 176, 178, 182, 184, 185, 187, 188, 189, 199, 200, 204, 206, 209, 210, 212, 214, 215, 217, 220, 221, 222, 224, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246], "user_onnx_lib": [10, 221], "userflow": [171, 179, 198], "usual": [190, 209, 222, 247, 250], "util": [0, 6, 9, 156, 160, 162, 168, 177, 186, 187, 188, 189, 190, 196, 197, 198, 200, 203, 204, 205, 216, 217, 219, 220, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "v": [2, 12, 174, 175, 184, 187, 205, 217, 228, 231, 236, 243, 247, 250], "v1": [0, 160, 194, 196, 197, 199, 200, 203, 217, 236, 237, 238, 239, 240, 241, 242, 243], "v2": [0, 19, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 163, 170, 188, 196, 200, 202, 205, 216, 220, 221, 222, 244], "v73": 200, "v81": 244, "v_proj": 199, "val": [187, 198, 219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "val_images_len": 231, "val_transform": 238, "valid": [2, 9, 162, 168, 175, 177, 180, 186, 187, 189, 200, 204, 205, 206, 210, 212, 214, 217, 221, 228, 231, 236, 238, 239, 243], "validate_example_model": 162, "validate_for_missing_modul": 162, "validate_for_reused_modul": 162, "validate_model": 162, "validation_check": 162, "validationerror": 200, "valu": [1, 2, 9, 10, 12, 18, 19, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 161, 163, 168, 170, 174, 175, 177, 178, 181, 185, 186, 187, 192, 194, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 216, 217, 220, 221, 222, 223, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 243, 247], "value_qtzr": 19, "var": [15, 70, 190], "vari": [2, 175, 207, 217], "variabl": [18, 150, 151, 161, 206, 208, 210, 212, 227, 244], "varianc": 239, "variant": [181, 183, 184, 185, 186, 200], "varieti": 192, "variou": [2, 18, 163, 175, 183, 200, 206, 209, 210, 212, 215, 217, 225, 231, 238, 243, 246, 247], "vector": [200, 229, 237], "vedaldi": 209, "venic": 209, "venv": 184, "ver": 200, "ver_cuda": 184, "ver_onnxruntim": 184, "ver_python": 184, "ver_torch": 184, "veri": [13, 188, 204, 207, 209, 217, 229, 230, 231, 232, 237, 238, 240, 241, 242, 243, 246], "verifi": [161, 237, 240, 241, 242], "versa": [208, 225, 228, 236, 247], "version": [10, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 157, 160, 161, 164, 166, 170, 178, 181, 183, 184, 186, 196, 197, 200, 202, 203, 221, 222, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 247, 250], "via": [14, 176, 181, 183, 184, 185, 189, 200, 207, 219, 244, 245, 247], "vice": [208, 225, 228, 236, 247], "view": [157, 162, 169, 182, 185, 196, 197, 201, 202, 211], "view_a": 238, "viewabl": 227, "virtual": 184, "vision": 209, "visit": 183, "visual": [18, 183, 200, 204, 206, 209, 210, 212, 217, 231, 243], "visualization_tool": [0, 156, 202], "visualization_url": [18, 206, 210, 212], "visualize_stat": [157, 202], "visualizecompress": 211, "vit_b_16": 244, "vocab_s": [194, 196, 197, 199], "vol": 209, "volum": 213, "w": [2, 9, 168, 175, 177, 189, 200, 204, 212, 213, 217], "w1616": 246, "w16a16": [7, 200, 217, 219, 246], "w4a16": 200, "w4a8": [189, 200, 249, 250], "w4fp16": 200, "w8a16": [189, 217, 221, 246, 249, 250], "w8a8": [189, 217, 222, 246, 249, 250], "w8a8_accuraci": 219, "w8a8_mp_accuraci": 219, "w_1": 192, "w_2": 192, "wa": [18, 147, 169, 200, 203, 206, 209, 210, 212, 214, 217, 220, 228, 233, 235, 236, 237, 239, 246], "wai": [160, 169, 171, 178, 179, 185, 198, 203, 221, 228, 229, 230, 231, 232, 236, 237, 240, 241, 242, 243, 246, 249], "walk": [216, 226], "want": [2, 6, 19, 158, 161, 175, 178, 184, 196, 197, 203, 217, 228, 231, 236, 243, 250], "warm": [12, 174, 187], "warn": [13, 162, 188, 194], "wasn": 200, "wast": 200, "we": [2, 6, 158, 160, 161, 162, 164, 167, 169, 175, 183, 187, 191, 192, 194, 196, 197, 199, 203, 205, 207, 209, 217, 221, 225, 228, 229, 231, 233, 235, 236, 237, 238, 239, 243, 244, 246, 249, 250], "websit": [182, 207], "websocket": 211, "weight": [1, 2, 3, 8, 9, 10, 11, 12, 13, 16, 18, 19, 157, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 175, 177, 181, 183, 186, 187, 188, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 205, 207, 210, 215, 216, 217, 219, 220, 221, 222, 223, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 244, 246, 247, 249, 250], "weight_decai": 190, "weight_q": 169, "weight_qdq": 169, "weight_svd": [18, 206, 210, 212], "weight_svd_auto_mod": 212, "weight_svd_manual_mod": 212, "weights_pdf": [204, 231, 243], "weightsvdparamet": [18, 206, 210, 212], "well": [147, 162, 199, 204, 209, 222, 228, 229, 230, 231, 232, 236, 237, 238, 240, 241, 242, 243, 246], "were": [200, 203, 207, 213, 214, 221, 222, 223, 229, 238, 244], "weren": 161, "wget": [219, 221, 231], "what": [182, 199, 211, 246, 249], "wheel": 200, "when": [2, 9, 10, 14, 18, 19, 155, 161, 164, 166, 168, 170, 171, 175, 176, 177, 178, 179, 188, 189, 190, 192, 194, 198, 199, 200, 204, 206, 209, 210, 211, 212, 213, 214, 217, 220, 221, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247], "whenev": [200, 222, 224], "where": [2, 12, 18, 150, 151, 152, 153, 154, 155, 161, 169, 170, 174, 175, 178, 181, 187, 190, 199, 200, 204, 206, 208, 210, 211, 212, 213, 214, 217, 221, 222, 224, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247], "wherea": [155, 169, 175, 228, 236], "wherein": [6, 158, 203], "whether": [13, 14, 155, 161, 162, 163, 176, 188, 189, 203, 208, 220, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "which": [2, 6, 8, 12, 14, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 158, 159, 161, 162, 163, 164, 167, 169, 170, 174, 175, 176, 178, 181, 183, 187, 188, 189, 191, 192, 193, 194, 195, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 220, 221, 223, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 245, 246, 247, 249], "while": [8, 18, 164, 195, 199, 200, 206, 208, 210, 212, 214, 216, 219, 221, 222, 225, 228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 244, 246, 247, 249, 250], "whl": [184, 185], "who": 214, "whole": 247, "whose": [157, 158, 159, 161, 202, 203, 205, 216, 217, 223, 245], "why": [225, 228, 236], "wide": [181, 191, 192, 246], "width": [9, 168, 177, 191, 192, 204, 210, 212, 213, 214, 215, 216, 217, 218, 225, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 247, 249, 250], "wikitext": [194, 196, 197, 199], "wildcard": 169, "wise": [9, 168, 171, 177, 179, 181, 190, 198, 200, 204, 225, 246], "wish": [187, 191, 192, 198, 217, 220], "within": [19, 147, 148, 149, 164, 181, 184, 200, 204, 207, 220, 221, 247, 249], "without": [14, 147, 149, 155, 157, 167, 170, 176, 178, 189, 198, 199, 200, 202, 213, 217, 221, 222, 229, 237, 238, 240, 241, 242, 249, 250], "won": [14, 161, 171, 176, 179, 189, 198], "word": 149, "work": [160, 162, 178, 183, 185, 192, 195, 200, 205, 209, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 246, 250], "workaround": 161, "workflow": [182, 193, 195, 200, 207, 216, 226, 244, 245, 249], "workspac": [184, 227, 231], "world": [186, 244], "wors": 209, "worth": 169, "would": [160, 163, 196, 200, 219, 220, 221, 223, 228, 231, 236, 239, 243, 246], "wq": 200, "wrap": [19, 160, 161, 200, 231, 243], "wrap_linear": 160, "wrapped_module_nam": [168, 177, 204], "wrapper": [168, 177, 200, 204, 206, 210, 212, 236, 237, 240, 241, 242], "write": [163, 220, 221, 230, 232, 237, 240, 241, 242], "written": [10, 170, 221, 222, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "wrong": 200, "wsl2": 200, "www": 227, "x": [0, 19, 147, 148, 149, 155, 156, 161, 162, 164, 166, 170, 185, 191, 192, 200, 204, 207, 214, 219, 221, 224, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 247], "x1": [52, 108, 161], "x2": [52, 108, 161, 224], "x86": [183, 185, 186], "x_": 247, "x_c": 155, "x_dq": 148, "x_q": [148, 149], "x_qdq": 147, "xiangyu": 209, "xx": 214, "y": [161, 184, 204, 231, 243], "y_zero_point": 200, "ybelkada": [196, 197], "ye": 209, "yet": [166, 170, 188, 194, 205, 228, 236], "yield": [2, 9, 12, 14, 15, 163, 170, 171, 174, 175, 176, 179, 187, 189, 190, 198, 204, 207, 217, 221, 225, 239, 246, 247], "yihui": 209, "you": [9, 14, 18, 19, 160, 161, 168, 169, 176, 177, 178, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 198, 199, 200, 204, 205, 206, 207, 209, 210, 211, 212, 216, 217, 219, 220, 221, 223, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250], "your": [9, 14, 19, 160, 161, 162, 168, 176, 177, 184, 185, 186, 187, 189, 190, 196, 198, 201, 203, 204, 207, 209, 211, 215, 219, 221, 226, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 247, 249, 250], "yy": 214, "zero": [12, 174, 187, 200, 247], "zero_grad": [190, 196, 197, 222, 244], "zeropad1d": 144, "zeropad2d": 145, "zeropad3d": 146, "zhang": 209, "zisserman": 209, "zlib": 200, "zou": 209, "zz": 214, "\u00aa": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00b2": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00b3": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00b5": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00b9": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00ba": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00bc": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00bd": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u00be": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u03c9": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u210e": 210, "\u215b": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u215c": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u215d": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\u215e": [182, 191, 200, 203, 205, 214, 216, 217, 219, 245, 246, 247, 249, 250], "\ud835\udc58": [210, 212], "\ud835\udc5a": 210, "\ud835\udc5b": 210, "\ud835\udc64": 210}, "titles": ["AIMET API", "aimet_onnx.apply_adaround", "aimet_onnx.mixed_precision", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.lite_mp", "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.apply_seq_mse", "aimet_torch.adaround", "aimet_torch.experimental.adascale", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.experimental.omniquant", "aimet_torch.onnx.export", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.experimental.spinquant", "aimet_torch.utils", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "External resources", "Glossary", "AIMET Documentation", "What is AIMET?", "Building from source", "Installation", "Quick Start", "Adaptive rounding", "AdaScale", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Post Training Quantization Techniques", "OmniQuant", "Quantized LoRa", "QW-LoRa", "QWA-LoRa", "Sequential MSE", "SpinQuant", "Release notes", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Blockwise Quantization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Encoding Format Specification", "Techniques", "Low-Power Blockwise Quantization (LPBQ)", "Automatic mixed precision", "Mixed precision", "Lite mixed precision", "Manual mixed precision", "Post Training Quantization", "Quantization-aware training", "Runtime configuration", "PyTorch model guidelines", "Quantization debugging guidelines", "Tutorials", "Example Notebooks", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quant Analyzer", "Quantization simulation", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "Quantization-Aware Training (QAT)", "On-target inference", "Quantization workflow", "Quantization simulation guide", "Quantization user guide", "AIMET features", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [200, 214, 217], "1": [160, 187, 188, 189, 190, 191, 194, 198, 199, 200, 203, 204, 214, 217, 219, 220, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 250], "10": [184, 200], "11": 200, "12": 200, "13": 200, "14": 200, "15": 200, "16": 200, "17": 200, "18": 200, "19": 200, "2": [160, 187, 188, 189, 190, 191, 194, 198, 199, 200, 203, 204, 214, 217, 219, 220, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 250], "20": 200, "21": 200, "22": 200, "23": 200, "24": 200, "25": 200, "26": 200, "27": 200, "28": 200, "29": 200, "3": [184, 187, 188, 189, 190, 191, 194, 198, 199, 200, 203, 204, 214, 217, 219, 221, 222, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 250], "30": 200, "31": 200, "32": 200, "33": 200, "34": 200, "35": 200, "4": [187, 188, 189, 190, 194, 198, 199, 200, 203, 204, 219, 221, 223, 225, 228, 229, 230, 236, 237, 239, 240, 241, 242, 244, 246, 250], "5": [188, 189, 194, 198, 199, 200, 204, 219, 221, 223, 225, 239, 246], "6": [189, 200, 204, 214, 219, 225], "7": [189, 200, 219, 225], "8": [200, 225], "9": 200, "For": [229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242], "On": [245, 248], "accuraci": [228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 244, 246, 249, 250], "activ": [204, 225], "adapt": [187, 193, 229, 237], "adaround": [12, 174, 229, 237], "adascal": [13, 188, 193], "adjust": 219, "advanc": 246, "affin": [160, 169], "ai": 245, "aimet": [0, 182, 183, 184, 186, 211, 247, 248, 249, 251], "aimet_onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "aimet_torch": [12, 13, 14, 15, 16, 17, 18, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], "algorithm": [217, 228, 236], "also": 247, "altern": 185, "amp": [228, 236, 246], "an": [228, 229, 230, 232], "analysi": [201, 204, 215, 217, 219, 225, 231, 243], "analyz": [201, 204, 231, 243], "api": [0, 5, 156, 161, 162, 164, 167, 169, 182, 187, 188, 189, 190, 191, 192, 194, 198, 199, 202, 203, 204, 205, 206, 210, 212, 216, 217, 219, 220, 221, 222, 228, 236, 249], "appli": [216, 219, 220, 229, 230, 231, 237, 240, 243], "apply_adaround": 1, "apply_seq_ms": 11, "arg": 214, "auto_qu": [14, 176], "automat": [189, 193, 217, 218, 228, 236, 246], "autoqu": 238, "awar": [215, 222, 239, 241, 242, 244, 246, 249], "base": [196, 219, 220], "baselin": [228, 229, 230, 232, 233, 234, 235, 236, 237, 240, 241, 242, 246], "batch": [190, 191, 193, 228, 230, 232, 236, 237, 240, 241, 242], "batch_norm_fold": [3, 16], "batchnorm": 239, "befor": 244, "block": 169, "blockwis": [205, 215, 216], "bn_reestim": 15, "bokeh": 211, "brows": 227, "build": [184, 185], "calibr": [197, 204, 221], "call": [228, 236, 247], "callback": [197, 204, 221, 228, 236], "case": [209, 217], "channel": [169, 206, 209, 233, 235], "check": 225, "choos": 185, "cle": [230, 240], "code": [160, 161, 206, 210, 212, 227], "compil": [184, 245], "compress": [18, 206, 207, 208, 209, 210, 211, 212, 215, 233, 234, 235], "comput": [164, 219, 221, 222, 228, 236], "conclus": 244, "conda": 184, "confid": 225, "config_util": 159, "configur": [164, 223, 247], "constant": 238, "contain": 184, "context": [187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 202, 203, 204, 206, 210, 212, 217, 220], "convers": 245, "convert": [217, 228, 229, 230, 232], "cp": 209, "creat": [184, 197, 204, 217, 219, 221, 228, 229, 230, 232, 236, 237, 239, 240, 241, 242, 244], "cross": [192, 193, 230, 240], "cross_layer_equ": [4, 17], "cuda": 184, "data": [214, 244], "dataset": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244], "debug": [225, 248], "default": 223, "defin": [228, 236, 238], "depend": 184, "deploi": [246, 250], "deploy": 249, "dequant": 152, "dequantizedtensor": 147, "descript": 204, "design": 211, "desir": 184, "detail": 246, "determin": [229, 230, 232, 237, 240, 241, 242, 247], "dictionari": 214, "direct": 245, "disabl": [231, 243], "docker": 184, "document": [182, 184, 251], "download": 227, "enabl": [204, 231, 243], "encod": [164, 204, 214, 219, 221, 228, 231, 236, 243, 247], "engin": 245, "environ": 184, "equal": [192, 193, 230, 240], "error": 204, "estim": [190, 193, 239], "evalu": [204, 219, 221, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244], "exampl": [160, 161, 182, 206, 210, 212, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "execut": [190, 192, 205, 245], "experiment": [13, 165, 172], "explor": 208, "export": [166, 221, 239, 247, 249], "extern": 180, "faq": 209, "featur": [207, 248, 249], "file": 223, "find": [217, 246], "fine": [209, 233, 234, 235], "fix": 225, "float": 160, "floatquantizedequant": 155, "flow": [167, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "fold": [191, 193, 228, 230, 232, 236, 237, 239, 240, 241, 242], "format": 214, "fp16": 250, "fp32": [225, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "from": [160, 184, 185], "front": 217, "function": [228, 236, 238], "gener": [201, 203, 250], "get": [183, 233, 234, 235, 236, 237, 240, 241, 242, 244], "glossari": [181, 182], "granular": 247, "greedi": 208, "group": 217, "guid": [160, 247, 248], "guidebook": 207, "guidelin": [224, 225, 248, 250], "helper": 238, "histogram": 204, "how": [160, 208, 213, 223, 247], "hub": 245, "i": [183, 228, 231, 236, 239, 243], "imagenet": 244, "import": [203, 204], "improv": 249, "individu": 225, "infer": [245, 248, 249], "inform": [229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242], "initi": [222, 244], "input": [203, 220], "instal": [184, 185, 186], "instanti": [229, 230, 232, 233, 234, 235, 237, 240, 241, 242, 244], "interact": [201, 202], "layer": [192, 193, 201, 203, 204, 208, 209, 220, 225, 228, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243], "layer_output_util": [6, 158], "leaf": 220, "learn": 242, "learnedgrid": 160, "level": 214, "librari": 204, "limit": 161, "list": 217, "lite": [218, 219, 246], "lite_mp": 7, "load": [203, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "loader": 244, "local": 184, "lora": [193, 195, 196, 197], "loss": [204, 231, 243], "low": [215, 216], "lpbq": 216, "manual": [218, 220], "max": [204, 231, 243, 247], "mean": 204, "migrat": 160, "min": [204, 231, 243, 247], "mix": [215, 217, 218, 219, 220, 228, 236, 246, 249], "mixed_precis": [2, 163, 175], "mmp": 220, "model": [186, 196, 203, 204, 209, 219, 220, 221, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 246, 250], "model_input": 223, "model_output": 223, "model_prepar": 161, "model_valid": 162, "modifi": 223, "modul": [160, 164], "more": [229, 230, 232, 233, 234, 235, 237, 238, 240, 241, 242], "move": 160, "mse": [193, 198, 231, 243], "new": 184, "next": [229, 233, 234, 235, 237, 238, 240, 241, 242, 246, 250], "nn": 164, "nois": 247, "non": 220, "norm": [190, 191, 193, 230, 240], "normal": [228, 232, 236, 237, 241, 242], "note": [182, 200, 209], "notebook": [182, 227, 228, 231, 236, 239, 243], "nvidia": 184, "obtain": 203, "old": 185, "omniqu": [165, 193, 194], "onnx": [166, 228, 229, 230, 232], "option": [209, 220], "output": [201, 203, 220], "overal": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], "overhead": 217, "overview": [182, 208, 209, 211, 213, 223, 247], "packag": [184, 185], "param": 223, "paramet": [222, 228, 236, 247], "pareto": 217, "path": 249, "pdf": [231, 243], "peft": 167, "per": [169, 204, 208, 209, 225, 231, 243], "perform": [217, 219, 225, 239, 241, 242, 246], "phase": 217, "pip": 184, "pipelin": [228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243], "platform": [183, 186], "post": [182, 193, 215, 221, 244, 246, 249], "power": [215, 216], "precis": [215, 217, 218, 219, 220, 228, 236, 246, 249, 250], "prepar": [204, 244], "prerequisit": [185, 187, 188, 189, 190, 194, 198, 199, 204, 219, 220, 221], "pretrain": 238, "procedur": [188, 189, 191, 194, 198, 199, 206, 217, 250], "process": 160, "profil": 220, "prune": [206, 209, 233, 235], "ptq": [196, 246], "pypi": 185, "python": 184, "pytorch": [224, 228, 229, 230, 232], "qat": [222, 239, 241, 242, 244, 246], "qualcomm": 245, "quant": [231, 243], "quant_analyz": [9, 168, 177], "quantanalyz": [204, 231, 243], "quantiz": [150, 153, 160, 164, 169, 182, 186, 189, 193, 195, 196, 201, 204, 205, 214, 215, 216, 217, 221, 222, 225, 228, 229, 230, 231, 232, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "quantizationmixin": 19, "quantizationsimmodel": [197, 244], "quantize_dequant": 154, "quantizedadaptiveavgpool1d": 20, "quantizedadaptiveavgpool2d": 21, "quantizedadaptiveavgpool3d": 22, "quantizedadaptivemaxpool1d": 23, "quantizedadaptivemaxpool2d": 24, "quantizedadaptivemaxpool3d": 25, "quantizedalphadropout": 26, "quantizedavgpool1d": 27, "quantizedavgpool2d": 28, "quantizedavgpool3d": 29, "quantizedbatchnorm1d": 32, "quantizedbatchnorm2d": 33, "quantizedbatchnorm3d": 34, "quantizedbceloss": 30, "quantizedbcewithlogitsloss": 31, "quantizedbilinear": 35, "quantizedcelu": 36, "quantizedchannelshuffl": 38, "quantizedcircularpad1d": 39, "quantizedcircularpad2d": 40, "quantizedcircularpad3d": 41, "quantizedconstantpad1d": 42, "quantizedconstantpad2d": 43, "quantizedconstantpad3d": 44, "quantizedconv1d": 45, "quantizedconv2d": 46, "quantizedconv3d": 47, "quantizedconvtranspose1d": 48, "quantizedconvtranspose2d": 49, "quantizedconvtranspose3d": 50, "quantizedcosineembeddingloss": 51, "quantizedcosinesimilar": 52, "quantizedcrossentropyloss": 53, "quantizedctcloss": 37, "quantizeddropout": 54, "quantizeddropout1d": 55, "quantizeddropout2d": 56, "quantizeddropout3d": 57, "quantizedelu": 58, "quantizedembed": 59, "quantizedembeddingbag": 60, "quantizedequant": 151, "quantizedfeaturealphadropout": 61, "quantizedflatten": 62, "quantizedfold": 63, "quantizedfractionalmaxpool2d": 64, "quantizedfractionalmaxpool3d": 65, "quantizedgaussiannllloss": 70, "quantizedgelu": 66, "quantizedglu": 67, "quantizedgroupnorm": 71, "quantizedgru": 68, "quantizedgrucel": 69, "quantizedhardshrink": 72, "quantizedhardsigmoid": 73, "quantizedhardswish": 74, "quantizedhardtanh": 75, "quantizedhingeembeddingloss": 76, "quantizedhuberloss": 77, "quantizedinstancenorm1d": 78, "quantizedinstancenorm2d": 79, "quantizedinstancenorm3d": 80, "quantizedkldivloss": 81, "quantizedl1loss": 82, "quantizedlayernorm": 87, "quantizedleakyrelu": 88, "quantizedlinear": 89, "quantizedlocalresponsenorm": 90, "quantizedlogsigmoid": 91, "quantizedlogsoftmax": 92, "quantizedlppool1d": 83, "quantizedlppool2d": 84, "quantizedlstm": 85, "quantizedlstmcel": 86, "quantizedmarginrankingloss": 94, "quantizedmaxpool1d": 95, "quantizedmaxpool2d": 96, "quantizedmaxpool3d": 97, "quantizedmaxunpool1d": 98, "quantizedmaxunpool2d": 99, "quantizedmaxunpool3d": 100, "quantizedmish": 101, "quantizedmseloss": 93, "quantizedmultilabelmarginloss": 102, "quantizedmultilabelsoftmarginloss": 103, "quantizedmultimarginloss": 104, "quantizednllloss": 105, "quantizednllloss2d": 106, "quantizedpairwisedist": 108, "quantizedpixelshuffl": 109, "quantizedpixelunshuffl": 110, "quantizedpoissonnllloss": 111, "quantizedprelu": 107, "quantizedreflectionpad1d": 117, "quantizedreflectionpad2d": 118, "quantizedreflectionpad3d": 119, "quantizedrelu": 115, "quantizedrelu6": 116, "quantizedreplicationpad1d": 120, "quantizedreplicationpad2d": 121, "quantizedreplicationpad3d": 122, "quantizedrnn": 112, "quantizedrnncel": 113, "quantizedrrelu": 114, "quantizedselu": 123, "quantizedsigmoid": 125, "quantizedsilu": 124, "quantizedsmoothl1loss": 126, "quantizedsoftmarginloss": 127, "quantizedsoftmax": 128, "quantizedsoftmax2d": 129, "quantizedsoftmin": 130, "quantizedsoftplu": 131, "quantizedsoftshrink": 132, "quantizedsoftsign": 133, "quantizedtanh": 134, "quantizedtanhshrink": 135, "quantizedtensor": 148, "quantizedtensorbas": 149, "quantizedthreshold": 136, "quantizedtripletmarginloss": 137, "quantizedtripletmarginwithdistanceloss": 138, "quantizedunflatten": 139, "quantizedunfold": 140, "quantizedupsampl": 141, "quantizedupsamplingbilinear2d": 142, "quantizedupsamplingnearest2d": 143, "quantizedzeropad1d": 144, "quantizedzeropad2d": 145, "quantizedzeropad3d": 146, "quantsim": [8, 10, 159, 170, 178, 219, 221, 247], "quantwrapp": 160, "quick": 186, "quickli": 186, "qw": [195, 196], "qwa": [195, 197], "random": 244, "rang": [204, 231, 242, 243], "rank": 209, "ratio": [208, 209, 211], "re": [190, 193, 239], "recommend": 222, "recomput": 219, "reconstruct": 206, "reduc": [217, 250], "reestim": 239, "refer": [164, 169, 182, 209], "relat": 227, "releas": [182, 200], "resourc": 180, "restor": 250, "round": [187, 193, 209, 229, 237], "run": [184, 197, 204, 222, 227, 228, 236, 238, 244], "runtim": [223, 247], "scheme": 247, "score": [233, 234, 235, 236, 237, 240, 241, 242], "sdk": 245, "seed": 244, "select": [206, 208, 209], "sensit": [204, 217, 219, 225], "seq_ms": [171, 179], "sequenti": [193, 198], "server": [211, 227], "session": 211, "set": [184, 220, 244], "set_grouped_blockwise_quantization_for_weight": 8, "setup": [187, 188, 190, 192, 194, 196, 197, 198, 199, 206, 210, 212, 220, 222], "signal": 247, "sim": [228, 230, 232, 236, 237, 239, 240, 241, 242], "simplifi": [228, 229, 230, 232], "simul": [228, 229, 230, 232, 236, 237, 239, 240, 241, 242, 247, 249], "small": 186, "sourc": [184, 185], "spatial": [209, 210, 234, 235], "specif": 214, "spinquant": [172, 193, 199], "squar": 204, "start": [183, 186, 211, 244], "staticgrid": 160, "statist": [204, 231, 239, 243], "step": [187, 188, 189, 190, 191, 194, 198, 199, 203, 204, 217, 219, 220, 221, 222, 229, 233, 234, 235, 237, 238, 240, 241, 242, 246, 250], "structur": [214, 223], "summari": 239, "supergroup": 223, "support": [183, 184, 249], "svd": [209, 210, 212, 234, 235], "target": [245, 248, 249], "techniqu": [182, 193, 209, 215, 240, 246], "terminologi": 167, "test": [184, 186], "tf": 247, "thi": [228, 231, 236, 239, 243], "tool": [157, 201, 215, 249], "top": 214, "tradeoff": 246, "train": [182, 193, 196, 197, 215, 221, 222, 229, 230, 231, 232, 233, 234, 235, 237, 239, 240, 241, 242, 243, 244, 246, 249], "transform": 244, "try": 250, "tune": [209, 233, 234, 235], "tutori": [182, 226], "type": [214, 220], "typic": 222, "unit": 184, "updat": 196, "us": [160, 206, 209, 210, 212, 217, 233, 234, 235, 246], "user": [167, 248], "util": 173, "v": [160, 246], "v1": [156, 174, 175, 176, 177, 178, 179], "valid": [229, 230, 232, 233, 234, 235, 237, 240, 241, 242], "variabl": 184, "variant": 222, "verifi": [185, 186, 250], "version": [185, 214, 251], "vision": 244, "visual": [157, 201, 202, 211, 225], "visualization_tool": 157, "vit": 244, "w16a16": 250, "w4a8": 244, "w8a8": 219, "w8a8_mix": 219, "weight": [196, 204, 206, 209, 212, 225], "what": [183, 228, 231, 236, 239, 243], "wheel": 184, "winnow": [206, 213], "work": [208, 213, 247], "workflow": [187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 199, 202, 203, 204, 206, 210, 212, 217, 219, 220, 221, 222, 225, 246, 247, 248, 250], "wrapper": [231, 243], "x": 160}})