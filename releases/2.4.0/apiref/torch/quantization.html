<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="QuantizedTensorBase" href="generated/aimet_torch.quantization.QuantizedTensorBase.html" /><link rel="prev" title="QuantizedZeroPad3d" href="generated/aimet_torch.nn.QuantizedZeroPad3d.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>aimet_torch.quantization - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../../_static/aimet-furo.css?v=22b0637d" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.4.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/quick-start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../userguide/index.html">User Guide</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of User Guide</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/quantization_tools.html">AIMET features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/quantization_workflow.html">Quantization workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/debugging_guidelines.html">Debugging guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/on_target_inference.html">On-target inference</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../quantsim/index.html">Quantization Simulation Guide</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Quantization Simulation Guide</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/calibration.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/qat.html">QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/blockwise.html">Blockwise quantization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../featureguide/index.html">Feature Guide</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Feature Guide</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../featureguide/mixed%20precision/index.html">Mixed precision</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Mixed precision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/mixed%20precision/mmp.html">Manual mixed precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/mixed%20precision/amp.html">Automatic mixed precision</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/autoquant.html">Automatic quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../featureguide/analysis%20tools/index.html">Analysis tools</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Analysis tools</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/analysis%20tools/interactive_visualization.html">Interactive visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/analysis%20tools/quant_analyzer.html">Quantization analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/analysis%20tools/layer_output_generation.html">Layer output generation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../featureguide/compression/index.html">Compression</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Compression</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/compression/feature_guidebook.html">Compression guidebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/compression/greedy_compression_ratio_selection.html">Greedy compression ratio selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/compression/visualization_compression.html">Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/compression/weight_svd.html">Weight SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/compression/spatial_svd.html">Spatial SVD</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../featureguide/compression/channel_pruning.html">Channel pruning</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of Channel pruning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../featureguide/compression/winnowing.html">Winnowing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../featureguide/quantized%20LoRa/index.html">Quantized LoRa</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Quantized LoRa</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/quantized%20LoRa/qw_lora.html">QW-LoRa</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../featureguide/quantized%20LoRa/qwa_lora.html">QWA-LoRa</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../index.html">API Reference</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of API Reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">aimet_torch</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of aimet_torch</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="migration_guide.html">Migrate to aimet_torch 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantsim.html">aimet_torch.quantsim</a></li>
<li class="toctree-l3"><a class="reference internal" href="adaround.html">aimet_torch.adaround</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="nn.html">aimet_torch.nn</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of aimet_torch.nn</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html">QuantizationMixin</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.html">QuantizedAdaptiveAvgPool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.html">QuantizedAdaptiveAvgPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.html">QuantizedAdaptiveAvgPool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.html">QuantizedAdaptiveMaxPool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.html">QuantizedAdaptiveMaxPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.html">QuantizedAdaptiveMaxPool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAlphaDropout.html">QuantizedAlphaDropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool1d.html">QuantizedAvgPool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool2d.html">QuantizedAvgPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool3d.html">QuantizedAvgPool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBCELoss.html">QuantizedBCELoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.html">QuantizedBCEWithLogitsLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm1d.html">QuantizedBatchNorm1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm2d.html">QuantizedBatchNorm2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm3d.html">QuantizedBatchNorm3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBilinear.html">QuantizedBilinear</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCELU.html">QuantizedCELU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCTCLoss.html">QuantizedCTCLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedChannelShuffle.html">QuantizedChannelShuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad1d.html">QuantizedCircularPad1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad2d.html">QuantizedCircularPad2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad3d.html">QuantizedCircularPad3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad1d.html">QuantizedConstantPad1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad2d.html">QuantizedConstantPad2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad3d.html">QuantizedConstantPad3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv1d.html">QuantizedConv1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv2d.html">QuantizedConv2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv3d.html">QuantizedConv3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose1d.html">QuantizedConvTranspose1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose2d.html">QuantizedConvTranspose2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose3d.html">QuantizedConvTranspose3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.html">QuantizedCosineEmbeddingLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCosineSimilarity.html">QuantizedCosineSimilarity</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCrossEntropyLoss.html">QuantizedCrossEntropyLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout.html">QuantizedDropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout1d.html">QuantizedDropout1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout2d.html">QuantizedDropout2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout3d.html">QuantizedDropout3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedELU.html">QuantizedELU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedEmbedding.html">QuantizedEmbedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedEmbeddingBag.html">QuantizedEmbeddingBag</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.html">QuantizedFeatureAlphaDropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFlatten.html">QuantizedFlatten</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFold.html">QuantizedFold</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.html">QuantizedFractionalMaxPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.html">QuantizedFractionalMaxPool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGELU.html">QuantizedGELU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGLU.html">QuantizedGLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGRU.html">QuantizedGRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGRUCell.html">QuantizedGRUCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGaussianNLLLoss.html">QuantizedGaussianNLLLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGroupNorm.html">QuantizedGroupNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardshrink.html">QuantizedHardshrink</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardsigmoid.html">QuantizedHardsigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardswish.html">QuantizedHardswish</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardtanh.html">QuantizedHardtanh</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.html">QuantizedHingeEmbeddingLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHuberLoss.html">QuantizedHuberLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm1d.html">QuantizedInstanceNorm1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm2d.html">QuantizedInstanceNorm2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm3d.html">QuantizedInstanceNorm3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedKLDivLoss.html">QuantizedKLDivLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedL1Loss.html">QuantizedL1Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLPPool1d.html">QuantizedLPPool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLPPool2d.html">QuantizedLPPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLSTM.html">QuantizedLSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLSTMCell.html">QuantizedLSTMCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLayerNorm.html">QuantizedLayerNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLeakyReLU.html">QuantizedLeakyReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLinear.html">QuantizedLinear</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLocalResponseNorm.html">QuantizedLocalResponseNorm</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLogSigmoid.html">QuantizedLogSigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLogSoftmax.html">QuantizedLogSoftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMSELoss.html">QuantizedMSELoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMarginRankingLoss.html">QuantizedMarginRankingLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool1d.html">QuantizedMaxPool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool2d.html">QuantizedMaxPool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool3d.html">QuantizedMaxPool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool1d.html">QuantizedMaxUnpool1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool2d.html">QuantizedMaxUnpool2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool3d.html">QuantizedMaxUnpool3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMish.html">QuantizedMish</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.html">QuantizedMultiLabelMarginLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.html">QuantizedMultiLabelSoftMarginLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiMarginLoss.html">QuantizedMultiMarginLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedNLLLoss.html">QuantizedNLLLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedNLLLoss2d.html">QuantizedNLLLoss2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPReLU.html">QuantizedPReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPairwiseDistance.html">QuantizedPairwiseDistance</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPixelShuffle.html">QuantizedPixelShuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPixelUnshuffle.html">QuantizedPixelUnshuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPoissonNLLLoss.html">QuantizedPoissonNLLLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRNN.html">QuantizedRNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRNNCell.html">QuantizedRNNCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRReLU.html">QuantizedRReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReLU.html">QuantizedReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReLU6.html">QuantizedReLU6</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad1d.html">QuantizedReflectionPad1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad2d.html">QuantizedReflectionPad2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad3d.html">QuantizedReflectionPad3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad1d.html">QuantizedReplicationPad1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad2d.html">QuantizedReplicationPad2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad3d.html">QuantizedReplicationPad3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSELU.html">QuantizedSELU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSiLU.html">QuantizedSiLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSigmoid.html">QuantizedSigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSmoothL1Loss.html">QuantizedSmoothL1Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftMarginLoss.html">QuantizedSoftMarginLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmax.html">QuantizedSoftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmax2d.html">QuantizedSoftmax2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmin.html">QuantizedSoftmin</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftplus.html">QuantizedSoftplus</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftshrink.html">QuantizedSoftshrink</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftsign.html">QuantizedSoftsign</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTanh.html">QuantizedTanh</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTanhshrink.html">QuantizedTanhshrink</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedThreshold.html">QuantizedThreshold</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTripletMarginLoss.html">QuantizedTripletMarginLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.html">QuantizedTripletMarginWithDistanceLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUnflatten.html">QuantizedUnflatten</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUnfold.html">QuantizedUnfold</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsample.html">QuantizedUpsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.html">QuantizedUpsamplingBilinear2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.html">QuantizedUpsamplingNearest2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad1d.html">QuantizedZeroPad1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad2d.html">QuantizedZeroPad2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad3d.html">QuantizedZeroPad3d</a></li>
</ul>
</li>
<li class="toctree-l3 current has-children current-page"><a class="current reference internal" href="#">aimet_torch.quantization</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle navigation of aimet_torch.quantization</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensorBase.html">QuantizedTensorBase</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensor.html">QuantizedTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.DequantizedTensor.html">DequantizedTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.affine.Quantize.html">Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.affine.QuantizeDequantize.html">QuantizeDequantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.float.FloatQuantizeDequantize.html">FloatQuantizeDequantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.affine.quantize.html">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.affine.quantize_dequantize.html">quantize_dequantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/aimet_torch.quantization.affine.dequantize.html">dequantize</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="seq_mse.html">aimet_torch.seq_mse</a></li>
<li class="toctree-l3"><a class="reference internal" href="lpbq.html">aimet_torch.quantsim.config_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="bnf.html">aimet_torch.batch_norm_fold</a></li>
<li class="toctree-l3"><a class="reference internal" href="cle.html">aimet_torch.cross_layer_equalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_preparer.html">aimet_torch.model_preparer</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_validator.html">aimet_torch.model_validator</a></li>
<li class="toctree-l3"><a class="reference internal" href="mp.html">aimet_torch.mixed_precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="quant_analyzer.html">aimet_torch.quant_analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="autoquant.html">aimet_torch.autoquant</a></li>
<li class="toctree-l3"><a class="reference internal" href="bn.html">aimet_torch.bn_reestimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="interactive_visualization.html">aimet_torch.visualization_tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="layer_output_generation.html">aimet_torch.layer_output_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="peft_lora.html">aimet_torch.peft</a></li>
<li class="toctree-l3"><a class="reference internal" href="compress.html">aimet_torch.compress</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/quantsim.html">aimet_torch.v1.quantsim</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/adaround.html">aimet_torch.v1.adaround</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/seq_mse.html">aimet_torch.v1.seq_mse</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/quant_analyzer.html">aimet_torch.v1.quant_analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/autoquant.html">aimet_torch.v1.autoquant</a></li>
<li class="toctree-l3"><a class="reference internal" href="v1/amp.html">aimet_torch.v1.amp</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../tensorflow/index.html">aimet_tensorflow</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle navigation of aimet_tensorflow</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/quantsim.html">aimet_tensorflow.quantsim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/adaround.html">aimet_tensorflow.adaround</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/bnf.html">aimet_tensorflow.batch_norm_fold</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/cle.html">aimet_tensorflow.cross_layer_equalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/amp.html">aimet_tensorflow.mixed_precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/quant_analyzer.html">aimet_tensorflow.quant_analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/autoquant.html">aimet_tensorflow.auto_quant_v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/layer_output_generation.html">aimet_tensorflow.layer_output_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/model_preparer.html">aimet_tensorflow.model_preparer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tensorflow/compress.html">aimet_tensorflow.compress</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../onnx/index.html">aimet_onnx</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle navigation of aimet_onnx</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../onnx/quantsim.html">aimet_onnx.quantsim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/adaround.html">aimet_onnx.adaround</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/seq_mse.html">aimet_onnx.seq_mse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/lpbq.html">aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/bnf.html">aimet_onnx.batch_norm_fold</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/cle.html">aimet_onnx.cross_layer_equalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/amp.html">aimet_onnx.mixed_precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/quant_analyzer.html">aimet_onnx.quant_analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../onnx/layer_output_generation.html">aimet_onnx.layer_output_utils</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="aimet-torch-quantization">
<span id="apiref-torch-quantization"></span><h1>aimet_torch.quantization<a class="headerlink" href="#aimet-torch-quantization" title="Link to this heading">¶</a></h1>
<section id="quantizers">
<h2>Quantizers<a class="headerlink" href="#quantizers" title="Link to this heading">¶</a></h2>
<p>AIMET quantizers are the low-level components of <span class="xref std std-ref">quantized modules</span> that
implement the quantization mechanism for PyTorch tensors.</p>
<p>AIMET quantizers are PyTorch modules that take a torch.Tensor as input and return
a <a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensor.html#aimet_torch.quantization.QuantizedTensor" title="aimet_torch.v2.quantization.tensor.QuantizedTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedTensor</span></code></a>
or <a class="reference internal" href="generated/aimet_torch.quantization.DequantizedTensor.html#aimet_torch.quantization.DequantizedTensor" title="aimet_torch.v2.quantization.tensor.DequantizedTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DequantizedTensors</span></code></a>,
a subclass of regular torch.Tensor with some additional attributes and helper functions for quantization.
All quantizers are derived from the base class <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> defined as below.</p>
<section id="affine-quantizers">
<h3>Affine quantizers<a class="headerlink" href="#affine-quantizers" title="Link to this heading">¶</a></h3>
<p>Even though it is <strong>strongly recommended</strong> for most users to delegate the instantiation and configuration of quantizers to <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code>,
it is worth understanding the underlying mechanism of quantizers for finer control over the quantized model.</p>
<p>The most commonly used quantizers are the affine quantizers such as <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizeDequantize</span></code>.
Here is a quick example of how to create an 8-bit asymmetric affine quantizer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>
<span class="n">qtzr</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qtzr</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)
</pre></div>
</div>
<p>Once you have created a quantizer object, you are first required to initialize the range of the input tensors
from which the quantization scale and offset will be derived. The most common way and recommended way to achieve
this is by using <code class="xref py py-meth docutils literal notranslate"><span class="pre">QuantizerBase.compute_encodings()</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Before compute_encodings:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * is_initialized: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * scale: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">get_scale</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * offset: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">get_offset</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span> <span class="o">/</span> <span class="mi">256</span> <span class="c1"># [0, 1/256, 2/256, ..., 255/256]</span>

<span class="k">with</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;After compute_encodings:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * is_initialized: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * scale: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">get_scale</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * offset: </span><span class="si">{</span><span class="n">qtzr</span><span class="o">.</span><span class="n">get_offset</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Quantizer encoding initialized. Now we&#39;re ready to run forward</span>
<span class="n">input_qdq</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Before compute_encodings:
  * is_initialized: False
  * scale: None
  * offset: None

After compute_encodings:
  * is_initialized: True
  * scale: tensor(0.0039, grad_fn=&lt;DivBackward0&gt;)
  * offset: tensor(0., grad_fn=&lt;SubBackward0&gt;)
</pre></div>
</div>
<p>Note that the output of the quantizer is either a <a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensor.html#aimet_torch.quantization.QuantizedTensor" title="aimet_torch.v2.quantization.tensor.QuantizedTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedTensor</span></code></a> or <a class="reference internal" href="generated/aimet_torch.quantization.DequantizedTensor.html#aimet_torch.quantization.DequantizedTensor" title="aimet_torch.v2.quantization.tensor.DequantizedTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DequantizedTensors</span></code></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output (dequantized representation):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_qdq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * scale: </span><span class="si">{</span><span class="n">input_qdq</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * offset: </span><span class="si">{</span><span class="n">input_qdq</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">offset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * bitwidth: </span><span class="si">{</span><span class="n">input_qdq</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * signed: </span><span class="si">{</span><span class="n">input_qdq</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">signed</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">input_q</span> <span class="o">=</span> <span class="n">input_qdq</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span> <span class="c1"># Integer representation of input_qdq</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output (quantized representation):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_q</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * scale: </span><span class="si">{</span><span class="n">input_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * offset: </span><span class="si">{</span><span class="n">input_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">offset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * bitwidth: </span><span class="si">{</span><span class="n">input_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">bitwidth</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  * signed: </span><span class="si">{</span><span class="n">input_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">signed</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Sanity checks</span>
<span class="c1"># 1. Quantizing and dequantizing input_qdq shouldn&#39;t change the result</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">input_qdq</span><span class="p">,</span> <span class="n">input_q</span><span class="o">.</span><span class="n">dequantize</span><span class="p">())</span>
<span class="c1"># 2. (De-)Quantizing an already (de-)quantized tensor shouldn&#39;t change the result</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">input_qdq</span><span class="p">,</span> <span class="n">input_qdq</span><span class="o">.</span><span class="n">dequantize</span><span class="p">())</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">input_q</span><span class="p">,</span> <span class="n">input_q</span><span class="o">.</span><span class="n">quantize</span><span class="p">())</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Output (dequantized representation):
DequantizedTensor([0.0000, 0.0039, 0.0078, 0.0117, 0.0156, 0.0195, 0.0234,
                   0.0273, 0.0312, 0.0352, 0.0391, 0.0430, 0.0469, 0.0508,
                   ...,
                   0.9570, 0.9609, 0.9648, 0.9688, 0.9727, 0.9766, 0.9805,
                   0.9844, 0.9883, 0.9922, 0.9961], grad_fn=&lt;AliasBackward0&gt;)
  * scale: tensor(0.0039, grad_fn=&lt;DivBackward0&gt;)
  * offset: tensor(0., grad_fn=&lt;SubBackward0&gt;)
  * bitwidth: 8
  * signed: False

Output (quantized representation):
QuantizedTensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,
                  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,
                 ...,
                 240., 241., 242., 243., 244., 245., 246., 247., 248., 249.,
                 250., 251., 252., 253., 254., 255.], grad_fn=&lt;AliasBackward0&gt;)
  * scale: tensor(0.0039, grad_fn=&lt;DivBackward0&gt;)
  * offset: tensor(0., grad_fn=&lt;SubBackward0&gt;)
  * bitwidth: 8
  * signed: False
</pre></div>
</div>
</section>
<section id="per-channel-quantization">
<h3>Per-channel quantization<a class="headerlink" href="#per-channel-quantization" title="Link to this heading">¶</a></h3>
<p><em>Per-channel quantization</em> is one of the advanced usages of affine quantizers where
one scale and offset will be associated with only one channel of the input tensor,
whereas one scale and offset was associated with the entire tensor in the previous example.</p>
<p>Per-channel quantization can be easily done by creating the quantizer with the desired shape of scale and offset.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>
<span class="n">Cout</span><span class="p">,</span> <span class="n">Cin</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span>

<span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span> <span class="o">/</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">Cin</span><span class="p">,</span> <span class="n">Cout</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Per-channel quantization along the output channel axis (Cout) of the weight</span>
<span class="n">qtzr</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">Cout</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantizer:</span><span class="se">\n</span><span class="si">{</span><span class="n">qtzr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">get_scale</span><span class="p">()</span>
<span class="n">offset</span> <span class="o">=</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">get_offset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scale:</span><span class="se">\n</span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2"> (shape: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Offset:</span><span class="se">\n</span><span class="si">{</span><span class="n">offset</span><span class="si">}</span><span class="s2"> (shape: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">offset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Quantizer:
QuantizeDequantize(shape=(8, 1), qmin=-128, qmax=127, symmetric=True)

Scale:
tensor([[0.0039],
        [0.0038],
        [0.0037],
        [0.0035],
        [0.0034],
        [0.0036],
        [0.0037],
        [0.0038]], grad_fn=&lt;DivBackward0&gt;) (shape: (8, 1))

Offset:
tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]]) (shape: (8, 1))
</pre></div>
</div>
<p>Note that:</p>
<ul class="simple">
<li><p>The shape <span class="math notranslate nohighlight">\((C_{out}, 1)\)</span> of scale and offset is equal to that of the quantizer</p></li>
<li><p>Every channel <span class="math notranslate nohighlight">\(c \in [0, C_{out})\)</span> of the quantized tensor is in the quantization grid of <span class="math notranslate nohighlight">\([-128, 127]\)</span>, associated with <span class="math notranslate nohighlight">\(scale_{c, :}\)</span> respectively</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">weight_qdq</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="n">weight_q</span> <span class="o">=</span> <span class="n">weight_qdq</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span> <span class="c1"># Integer representation of weight_qdq</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output (quantized representation):</span><span class="se">\n</span><span class="si">{weight_q}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scale:</span><span class="se">\n</span><span class="si">{</span><span class="n">weight_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Offset:</span><span class="se">\n</span><span class="si">{</span><span class="n">weight_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">offset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Output (quantized representation):
QuantizedTensor([[-128.,  -96.,  -64.,  -32.,    0.,   32.,   64.,   96.],
                 [-128.,  -95.,  -62.,  -29.,    4.,   37.,   70.,  103.],
                 [-128.,  -94.,  -60.,  -26.,    9.,   43.,   77.,  111.],
                 [-128.,  -93.,  -57.,  -22.,   13.,   49.,   84.,  119.],
                 [-127.,  -91.,  -54.,  -18.,   18.,   54.,   91.,  127.],
                 [-118.,  -83.,  -48.,  -13.,   22.,   57.,   92.,  127.],
                 [-110.,  -76.,  -42.,   -8.,   25.,   59.,   93.,  127.],
                 [-102.,  -70.,  -37.,   -4.,   29.,   61.,   94.,  127.]],
                grad_fn=&lt;AliasBackward0&gt;)

Scale:
tensor([[0.0039],
        [0.0038],
        [0.0037],
        [0.0035],
        [0.0034],
        [0.0036],
        [0.0037],
        [0.0038]], grad_fn=&lt;DivBackward0&gt;)

Offset:
tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]])
</pre></div>
</div>
</section>
<section id="per-block-quantization">
<h3>Per-block quantization<a class="headerlink" href="#per-block-quantization" title="Link to this heading">¶</a></h3>
<p><em>Per-block quantization</em> (also called <em>blockwise quantization</em>) is a further mathematical generalization of per-channel quantization,
similar to how per-channel quantization is a mathematical generalization of per-tensor quantization.</p>
<p>Blockwise quantization can be also easily done by creating a quantizer with the desired shape and block size.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>
<span class="n">Cout</span><span class="p">,</span> <span class="n">Cin</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># block size</span>

<span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
    <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="o">/</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">Cout</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">Cout</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Blockwise quantization with block size B</span>
<span class="n">qtzr</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">Cout</span><span class="p">,</span> <span class="n">Cin</span> <span class="o">//</span> <span class="n">B</span><span class="p">),</span>
                                   <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">),</span> <span class="c1"># NOTE: -1 indicates wildcard block size</span>
                                   <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantizer:</span><span class="se">\n</span><span class="si">{</span><span class="n">qtzr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">get_scale</span><span class="p">()</span>
<span class="n">offset</span> <span class="o">=</span> <span class="n">qtzr</span><span class="o">.</span><span class="n">get_offset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scale:</span><span class="se">\n</span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2"> (shape: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Offset:</span><span class="se">\n</span><span class="si">{</span><span class="n">offset</span><span class="si">}</span><span class="s2"> (shape: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">offset</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Quantizer:
QuantizeDequantize(shape=(8, 2), block_size=(-1, 4), qmin=-128, qmax=127, symmetric=True)

Scale:
tensor([[0.0039, 0.0078],
        [0.0037, 0.0073],
        [0.0034, 0.0068],
        [0.0032, 0.0063],
        [0.0030, 0.0059],
        [0.0032, 0.0064],
        [0.0034, 0.0069],
        [0.0037, 0.0074]], grad_fn=&lt;DivBackward0&gt;) (shape: (8, 2))

Offset:
tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]]) (shape: (8, 2))
</pre></div>
</div>
<p>Note that:</p>
<ul class="simple">
<li><p>The shape <span class="math notranslate nohighlight">\(\left(C_{out}, \frac{C_{in}}{B}\right) = (8, 2)\)</span> of scale and offset is equal to that of the quantizer</p></li>
<li><p>For every channel <span class="math notranslate nohighlight">\(c \in [0, C_{out})\)</span>, each block <span class="math notranslate nohighlight">\(k \in \left[0, \frac{C_{in}}{B}\right)\)</span> is in the quantization grid of <span class="math notranslate nohighlight">\([-128, 127]\)</span>, associated with <span class="math notranslate nohighlight">\(scale_{c, k:k+B}\)</span> respectively</p></li>
<li><p>If <span class="math notranslate nohighlight">\(C_{in}\)</span> is not divisible by block size <span class="math notranslate nohighlight">\(B\)</span>, the quantizer will throw shape mismatch error in runtime.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">weight_qdq</span> <span class="o">=</span> <span class="n">qtzr</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="n">weight_q</span> <span class="o">=</span> <span class="n">weight_qdq</span><span class="o">.</span><span class="n">quantize</span><span class="p">()</span> <span class="c1"># Integer representation of weight_qdq</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output (quantized representation):</span><span class="se">\n</span><span class="si">{weight_q}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scale:</span><span class="se">\n</span><span class="si">{</span><span class="n">weight_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Offset:</span><span class="se">\n</span><span class="si">{</span><span class="n">weight_q</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">offset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>Output (quantized representation):
QuantizedTensor([[-128.,  -64.,    0.,   64., -128.,  -64.,    0.,   64.],
                 [-128.,  -60.,    9.,   77., -128.,  -60.,    9.,   77.],
                 [-128.,  -55.,   18.,   91., -128.,  -55.,   18.,   91.],
                 [-128.,  -49.,   30.,  108., -128.,  -49.,   30.,  108.],
                 [-127.,  -42.,   42.,  127., -127.,  -42.,   42.,  127.],
                 [-107.,  -29.,   49.,  127., -107.,  -29.,   49.,  127.],
                 [ -91.,  -18.,   54.,  127.,  -91.,  -18.,   54.,  127.],
                 [ -76.,   -8.,   59.,  127.,  -76.,   -8.,   59.,  127.]],
                grad_fn=&lt;AliasBackward0&gt;)

Scale:
tensor([[0.0039, 0.0078],
        [0.0037, 0.0073],
        [0.0034, 0.0068],
        [0.0032, 0.0063],
        [0.0030, 0.0059],
        [0.0032, 0.0064],
        [0.0034, 0.0069],
        [0.0037, 0.0074]], grad_fn=&lt;DivBackward0&gt;)

Offset:
tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]])
</pre></div>
</div>
</section>
</section>
<section id="api-reference">
<h2>API reference<a class="headerlink" href="#api-reference" title="Link to this heading">¶</a></h2>
<p><strong>Quantized tensors</strong></p>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensorBase.html#aimet_torch.quantization.QuantizedTensorBase" title="aimet_torch.quantization.QuantizedTensorBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTensorBase</span></code></a></p></td>
<td><p>Abstract base class for quantized tensors.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.QuantizedTensor.html#aimet_torch.quantization.QuantizedTensor" title="aimet_torch.quantization.QuantizedTensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTensor</span></code></a></p></td>
<td><p>Represents a quantized tensor object.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.DequantizedTensor.html#aimet_torch.quantization.DequantizedTensor" title="aimet_torch.quantization.DequantizedTensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DequantizedTensor</span></code></a></p></td>
<td><p>Represents a tensor which has been quantized and subsequently dequantized.</p></td>
</tr>
</tbody>
</table>
</div>
<p id="api-beta-quantizers"><strong>Quantizers</strong></p>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.affine.Quantize.html#aimet_torch.quantization.affine.Quantize" title="aimet_torch.quantization.affine.Quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">affine.Quantize</span></code></a></p></td>
<td><p>Applies quantization to the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.affine.QuantizeDequantize.html#aimet_torch.quantization.affine.QuantizeDequantize" title="aimet_torch.quantization.affine.QuantizeDequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">affine.QuantizeDequantize</span></code></a></p></td>
<td><p>Applies fake-quantization by quantizing and dequantizing the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.float.FloatQuantizeDequantize.html#aimet_torch.quantization.float.FloatQuantizeDequantize" title="aimet_torch.quantization.float.FloatQuantizeDequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float.FloatQuantizeDequantize</span></code></a></p></td>
<td><p>Simulates quantization by fake-casting the input</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Functional APIs</strong></p>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.affine.quantize.html#aimet_torch.quantization.affine.quantize" title="aimet_torch.quantization.affine.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">affine.quantize</span></code></a></p></td>
<td><p>Applies quantization to the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.affine.quantize_dequantize.html#aimet_torch.quantization.affine.quantize_dequantize" title="aimet_torch.quantization.affine.quantize_dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">affine.quantize_dequantize</span></code></a></p></td>
<td><p>Applies fake-quantization by quantizing and dequantizing the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.quantization.affine.dequantize.html#aimet_torch.quantization.affine.dequantize" title="aimet_torch.quantization.affine.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">affine.dequantize</span></code></a></p></td>
<td><p>Applies dequantization to the input.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="generated/aimet_torch.quantization.QuantizedTensorBase.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">QuantizedTensorBase</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="generated/aimet_torch.nn.QuantizedZeroPad3d.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">QuantizedZeroPad3d</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">aimet_torch.quantization</a><ul>
<li><a class="reference internal" href="#quantizers">Quantizers</a><ul>
<li><a class="reference internal" href="#affine-quantizers">Affine quantizers</a></li>
<li><a class="reference internal" href="#per-channel-quantization">Per-channel quantization</a></li>
<li><a class="reference internal" href="#per-block-quantization">Per-block quantization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#api-reference">API reference</a><ul>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>