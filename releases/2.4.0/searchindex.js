Search.setIndex({"alltitles": {"1) Find layer groups": [[231, "find-layer-groups"]], "1. Creating a Keras model with subclass layers": [[196, "1.-Creating-a-Keras-model-with-subclass-layers"]], "1. Define Constants and Helper functions": [[207, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[199, "1.-Example-evaluation-and-training-pipeline"], [200, "1.-Example-evaluation-and-training-pipeline"], [208, "1.-Example-evaluation-and-training-pipeline"], [212, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[187, "1.-Example-evaluation-pipeline"], [205, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence checks": [[253, "fp32-confidence-checks"]], "1. Instantiate the example evaluation and training datasets": [[197, "1.-Instantiate-the-example-evaluation-and-training-datasets"], [198, "1.-Instantiate-the-example-evaluation-and-training-datasets"]], "1. Instantiate the example evaluation and training pipeline": [[194, "1.-Instantiate-the-example-evaluation-and-training-pipeline"]], "1. Instantiate the example evaluation method": [[191, "1.-Instantiate-the-example-evaluation-method"]], "1. Instantiate the example training and validation pipeline": [[188, "1.-Instantiate-the-example-training-and-validation-pipeline"], [189, "1.-Instantiate-the-example-training-and-validation-pipeline"], [190, "1.-Instantiate-the-example-training-and-validation-pipeline"], [192, "1.-Instantiate-the-example-training-and-validation-pipeline"], [193, "1.-Instantiate-the-example-training-and-validation-pipeline"], [201, "1.-Instantiate-the-example-training-and-validation-pipeline"], [202, "1.-Instantiate-the-example-training-and-validation-pipeline"], [203, "1.-Instantiate-the-example-training-and-validation-pipeline"], [204, "1.-Instantiate-the-example-training-and-validation-pipeline"], [206, "1.-Instantiate-the-example-training-and-validation-pipeline"], [209, "1.-Instantiate-the-example-training-and-validation-pipeline"], [210, "1.-Instantiate-the-example-training-and-validation-pipeline"], [211, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Run the notebook server": [[186, "run-the-notebook-server"]], "1. Sensitivity to weight and activation quantization": [[217, "sensitivity-to-weight-and-activation-quantization"]], "1. Versioning": [[246, "versioning"]], "1. defaults": [[249, "defaults"]], "1.13.0": [[252, "id32"]], "1.16.0": [[252, "id31"]], "1.16.1": [[252, "id30"]], "1.16.2": [[252, "id29"]], "1.17.0": [[252, "id28"]], "1.18.0": [[252, "id27"]], "1.19.1": [[252, "id26"]], "1.20.0": [[252, "id25"]], "1.21.0": [[252, "id24"]], "1.22.0": [[252, "id23"]], "1.22.1": [[252, "id22"]], "1.22.2": [[252, "id21"]], "1.23.0": [[252, "id20"]], "1.24.0": [[252, "id19"]], "1.25.0": [[252, "id18"]], "1.26.0": [[252, "id17"]], "1.27.0": [[252, "id16"]], "1.28.0": [[252, "id15"]], "1.29.0": [[252, "id14"]], "1.30.0": [[252, "id13"]], "1.31.0": [[252, "id12"]], "1.32.0": [[252, "id11"]], "1.33.0": [[252, "id10"]], "1.33.5": [[252, "id9"]], "1.34.0": [[252, "id8"]], "1.35.0": [[252, "id7"]], "1.35.1": [[252, "id6"]], "2) Perform sensitivity analysis (Phase 1)": [[231, "perform-sensitivity-analysis-phase-1"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[187, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [188, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [189, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [190, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Converting the Keras model with subclass layers to a Keras model with functional layers": [[196, "2.-Converting-the-Keras-model-with-subclass-layers-to-a-Keras-model-with-functional-layers"]], "2. Create the model in Keras": [[194, "2.-Create-the-model-in-Keras"]], "2. Define Constants and Datasets Prepare": [[194, "2.-Define-Constants-and-Datasets-Prepare"]], "2. Download the example notebooks and related code": [[186, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[208, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[193, "2.-Load-a-pretrained-FP32-model"], [199, "2.-Load-a-pretrained-FP32-model"], [207, "2.-Load-a-pretrained-FP32-model"]], "2. Load the FP32 model and evaluate the model to find the baseline FP32 accuracy": [[191, "2.-Load-the-FP32-model-and-evaluate-the-model-to-find-the-baseline-FP32-accuracy"]], "2. Load the model": [[212, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[192, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [197, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [198, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [200, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [201, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [202, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [203, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [204, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [205, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [206, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [209, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [210, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [211, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement": [[217, "per-layer-quantizer-enablement"]], "2. Version 0.6.1": [[246, "version-0-6-1"]], "2. Weights or activations quantization": [[253, "weights-or-activations-quantization"]], "2. params": [[249, "params"]], "2.0.0": [[252, "id5"]], "2.1. Encoding specification": [[246, "encoding-specification"]], "2.1.0": [[252, "id4"]], "2.2.0": [[252, "id3"]], "2.3.0": [[252, "id2"]], "2.4.0": [[252, "id1"]], "3) Create a Pareto-front list (Phase 2)": [[231, "create-a-pareto-front-list-phase-2"]], "3. Apply QuantAnalyzer to the model": [[199, "3.-Apply-QuantAnalyzer-to-the-model"], [212, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[202, "3.-Compress-the-model-and-fine-tune"], [203, "3.-Compress-the-model-and-fine-tune"], [204, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[187, "3.-Create-a-quantization-simulation-model"], [205, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[208, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[188, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [189, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [190, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [192, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [197, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [198, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [200, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [201, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [206, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [209, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [210, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [211, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Determine the baseline FP32 accuracy": [[193, "3.-Determine-the-baseline-FP32-accuracy"]], "3. Fixing weight quantization": [[253, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range": [[217, "per-layer-encodings-min-max-range"]], "3. Run AutoQuant": [[207, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[186, "run-the-notebooks"]], "3. Showing similarities and differences between the original and converted models": [[196, "3.-Showing-similarities-and-differences-between-the-original-and-converted-models"]], "3. Train and evaluate the model": [[194, "3.-Train-and-evaluate-the-model"]], "3. Version 1.0.0": [[246, "version-1-0-0"]], "3. supergroups": [[249, "supergroups"]], "3.1. Encoding specification": [[246, "id1"]], "3.Create a quantization simulation model (with fake quantization ops inserted)": [[191, "3.Create-a-quantization-simulation-model-(with-fake-quantization-ops-inserted)"]], "4) Reduce Bit-width Convert Op Overhead (Phase 3)": [[231, "reduce-bit-width-convert-op-overhead-phase-3"]], "4. Apply Adaround": [[188, "4.-Apply-Adaround"], [192, "4.-Apply-Adaround"], [200, "4.-Apply-Adaround"], [206, "4.-Apply-Adaround"]], "4. Apply CLE": [[189, "4.-Apply-CLE"], [201, "4.-Apply-CLE"]], "4. Apply CLE and BC": [[209, "4.-Apply-CLE-and-BC"]], "4. Create a QuantizationSim Model": [[194, "4.-Create-a-QuantizationSim-Model"]], "4. Define constants and helper functions": [[193, "4.-Define-constants-and-helper-functions"]], "4. Discussing the limitations of the Keras Model Preparer": [[196, "4.-Discussing-the-limitations-of-the-Keras-Model-Preparer"]], "4. Fixing activation quantization": [[253, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[217, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[208, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[197, "4.-Perform-QAT"], [198, "4.-Perform-QAT"], [210, "4.-Perform-QAT"], [211, "4.-Perform-QAT"]], "4. Run AMP algorithm on the quantized model": [[187, "4.-Run-AMP-algorithm-on-the-quantized-model"], [191, "4.-Run-AMP-algorithm-on-the-quantized-model"], [205, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. model_input": [[249, "model-input"]], "5. Apply AutoQuant": [[193, "5.-Apply-AutoQuant"]], "5. Export Model": [[194, "5.-Export-Model"], [208, "5.-Export-Model"]], "5. Per-layer mean-square-error loss": [[217, "per-layer-mean-square-error-loss"]], "5. Perform QAT": [[194, "5.-Perform-QAT"]], "5. Performing per-layer analysis": [[253, "performing-per-layer-analysis"]], "5. model_output": [[249, "model-output"]], "6. Visualizing sensitive layers": [[253, "visualizing-sensitive-layers"]], "7. Fixing individual quantizers": [[253, "fixing-individual-quantizers"]], "8. Quantize the model": [[253, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[239, null]], "AIMET documentation versions": [[258, null]], "AIMET features": [[254, "aimet-features"], [256, null]], "AIMET installation in Docker": [[241, null]], "AIMET manual installation and setup": [[242, null]], "AIMET visualization": [[227, null]], "API": [[20, "api"], [171, "api"], [172, "api"], [175, "api"], [213, "api"], [215, "api"], [216, "api"], [217, "api"], [218, "api"], [219, "api"], [220, "api"], [221, "api"], [222, "api"], [226, "api"], [228, "api"], [231, "api"], [233, "api"], [237, "api"], [244, "api"], [245, "api"], [248, "api"]], "API Call for Regular AMP": [[191, "API-Call-for-Regular-AMP"]], "API Reference": [[239, "api-reference"]], "API reference": [[174, "api-reference"], [177, "api-reference"]], "Accuracy improvement tools": [[256, "accuracy-improvement-tools"]], "Adaptive Rounding (AdaRound)": [[188, null], [192, null], [206, null]], "Adaptive rounding": [[213, null], [230, "adaptive-rounding"]], "Affine quantizers": [[177, "affine-quantizers"]], "Alternative packages": [[240, "alternative-packages"]], "Analysis descriptions": [[217, "analysis-descriptions"]], "Analysis tools": [[214, null], [230, "analysis-tools"]], "Apply the profile": [[233, "apply-the-profile"]], "AutoQuant": [[193, null], [207, null]], "Automatic Mixed-Precision (AMP)": [[187, null], [191, null], [205, null]], "Automatic mixed precision": [[231, null], [232, "automatic-mixed-precision"]], "Automatic quantization": [[218, null], [230, "automatic-quantization"]], "Batch norm folding": [[220, null], [230, "batch-norm-folding"]], "Batch norm re-estimation": [[219, null], [230, "batch-norm-re-estimation"]], "Bias Correction": [[209, "Bias-Correction"]], "Browse the notebooks": [[186, "browse-the-notebooks"]], "Build a Docker image": [[241, "build-a-docker-image"]], "Building from source": [[240, "building-from-source"]], "CLE": [[209, "CLE"]], "Calibration": [[245, null]], "Calibration Callback": [[236, "calibration-callback"]], "Call AMP API": [[187, "Call-AMP-API"], [205, "Call-AMP-API"]], "Channel pruning": [[222, null]], "Channel pruning (CP)": [[225, "channel-pruning-cp"]], "Channel selection": [[222, "channel-selection"]], "Choose and install a package": [[240, "choose-and-install-a-package"]], "Choose to download or build an image": [[241, "choose-to-download-or-build-an-image"]], "Choose your AIMET variant": [[241, "choose-your-aimet-variant"]], "Code Examples": [[20, "code-examples"], [170, "code-examples"], [171, "code-examples"]], "Code example": [[222, "code-example"], [226, "code-example"], [228, "code-example"], [231, "code-example"]], "Compilation": [[255, "compilation"]], "Complementary techniques": [[213, "complementary-techniques"]], "Compressing using Spatial SVD": [[226, "compressing-using-spatial-svd"]], "Compression": [[225, null], [227, "compression"], [230, "compression"]], "Compression features Guidebook": [[223, null]], "Compression ratio selection": [[224, "compression-ratio-selection"], [225, "compression-ratio-selection"]], "Compression using Channel Pruning": [[222, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[228, "compression-using-weight-svd"]], "Compute Encodings": [[187, "Compute-Encodings"], [191, "Compute-Encodings"], [205, "Compute-Encodings"]], "Computing encodings": [[174, "computing-encodings"]], "Configuration": [[174, "configuration"]], "Configuration file structure": [[249, "configuration-file-structure"]], "Context": [[213, "context"], [215, "context"], [216, "context"], [217, "context"], [218, "context"], [219, "context"], [220, "context"], [221, "context"], [222, "context"], [226, "context"], [228, "context"], [231, "context"], [233, "context"], [235, "context"], [236, "context"], [237, "context"]], "Conversion": [[255, "conversion"]], "Create Quantization Sim Model": [[187, "Create-Quantization-Sim-Model"], [205, "Create-Quantization-Sim-Model"], [208, "Create-Quantization-Sim-Model"]], "Create QuantizationSimModel": [[236, "create-quantizationsimmodel"]], "Create the Quantization Sim Model": [[188, "Create-the-Quantization-Sim-Model"], [189, "Create-the-Quantization-Sim-Model"], [190, "Create-the-Quantization-Sim-Model"], [192, "Create-the-Quantization-Sim-Model"], [197, "Create-the-Quantization-Sim-Model"], [198, "Create-the-Quantization-Sim-Model"], [201, "Create-the-Quantization-Sim-Model"], [206, "Create-the-Quantization-Sim-Model"], [209, "Create-the-Quantization-Sim-Model"], [210, "Create-the-Quantization-Sim-Model"], [211, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[189, null]], "Cross-Layer Equalization and Bias Correction": [[209, null]], "Cross-Layer Equalization with QuantSim": [[201, null]], "Cross-layer equalization": [[221, null], [230, "cross-layer-equalization"]], "Data type": [[246, "id5"]], "Dataset": [[187, "Dataset"], [188, "Dataset"], [189, "Dataset"], [190, "Dataset"], [191, "Dataset"], [192, "Dataset"], [193, "Dataset"], [194, "Dataset"], [197, "Dataset"], [198, "Dataset"], [199, "Dataset"], [200, "Dataset"], [201, "Dataset"], [202, "Dataset"], [203, "Dataset"], [204, "Dataset"], [205, "Dataset"], [206, "Dataset"], [207, "Dataset"], [208, "Dataset"], [209, "Dataset"], [210, "Dataset"], [211, "Dataset"], [212, "Dataset"]], "Debugging guidelines": [[254, "debugging-guidelines"]], "Debugging workflow": [[253, "debugging-workflow"]], "Define callback functions for AMP": [[187, "Define-callback-functions-for-AMP"], [205, "Define-callback-functions-for-AMP"]], "Deployment paths": [[256, "deployment-paths"]], "DequantizedTensor": [[157, null]], "Design": [[227, "design"]], "Determine quantization parameters (encodings)": [[247, "determine-quantization-parameters-encodings"]], "Docker install": [[240, "docker-install"]], "Download a prebuilt Docker image": [[241, "download-a-prebuilt-docker-image"]], "Encoding Format Specification": [[246, null]], "Encoding dictionary structure": [[246, "id3"]], "Encoding min/max ranges": [[199, "Encoding-min/max-ranges"], [212, "Encoding-min/max-ranges"]], "Encoding type": [[246, "id4"]], "Enum Definition": [[22, "enum-definition"]], "Environment setup": [[241, "environment-setup"]], "Example Notebooks": [[239, "example-notebooks"]], "Examples": [[186, null]], "Executing per-block quantization": [[244, "executing-per-block-quantization"]], "Execution": [[219, "execution"], [221, "execution"], [255, "execution"]], "Export API": [[256, "export-api"]], "Export tools": [[256, "export-tools"]], "Exported Encodings": [[247, "exported-encodings"]], "Exporting blockwise-quantized models": [[244, "exporting-blockwise-quantized-models"]], "FAQs": [[225, "faqs"]], "Fast AMP (AMP 2.0)": [[191, "Fast-AMP-(AMP-2.0)"]], "Feature Guide": [[239, "feature-guide"]], "FloatQuantizeDequantize": [[165, null]], "Fold Batch Norm layers": [[189, "Fold-Batch-Norm-layers"], [209, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[187, "Fold-Batch-Normalization-layers"], [188, "Fold-Batch-Normalization-layers"], [190, "Fold-Batch-Normalization-layers"], [192, "Fold-Batch-Normalization-layers"], [197, "Fold-Batch-Normalization-layers"], [198, "Fold-Batch-Normalization-layers"], [200, "Fold-Batch-Normalization-layers"], [201, "Fold-Batch-Normalization-layers"], [205, "Fold-Batch-Normalization-layers"], [206, "Fold-Batch-Normalization-layers"], [210, "Fold-Batch-Normalization-layers"], [211, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[194, "Fold-BatchNorm-Layers"], [208, "Fold-BatchNorm-Layers"]], "For more information": [[188, "For-more-information"], [189, "For-more-information"], [190, "For-more-information"], [192, "For-more-information"], [193, "For-more-information"], [197, "For-more-information"], [198, "For-more-information"], [201, "For-more-information"], [202, "For-more-information"], [203, "For-more-information"], [204, "For-more-information"], [206, "For-more-information"], [207, "For-more-information"], [209, "For-more-information"], [210, "For-more-information"], [211, "For-more-information"]], "General guidelines": [[257, "general-guidelines"]], "Glossary": [[238, null], [239, "glossary"]], "Greedy compression ratio selection": [[224, null]], "Host install from scratch": [[240, "host-install-from-scratch"]], "How it works": [[224, "how-it-works"]], "How quantization simulation works": [[247, "how-quantization-simulation-works"]], "How to modify configuration file": [[249, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[170, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[229, "how-winnowing-works"]], "Hyper parameters": [[213, "hyper-parameters"]], "Install AIMET packages": [[241, "install-aimet-packages"]], "Installation": [[239, "installation"], [240, null]], "Installing AIMET": [[241, "installing-aimet"], [242, "installing-aimet"], [243, "installing-aimet"]], "Interactive visualization": [[214, "interactive-visualization"], [215, null]], "Keras Model Preparer": [[196, null]], "Layer output generation": [[214, "layer-output-generation"], [216, null]], "Limitations": [[20, "limitations"], [171, "limitations"]], "LoRa Training": [[235, "lora-training"]], "Low power blockwise quantization": [[244, "low-power-blockwise-quantization"]], "MMP API options": [[233, "mmp-api-options"]], "Manual mixed precision": [[232, "manual-mixed-precision"], [233, null]], "Migration Process": [[170, "migration-process"]], "Migration guide": [[170, null]], "Min-Max (also called \u201cTF\u201d in AIMET)": [[247, "min-max-also-called-tf-in-aimet"]], "Mixed Precision Algorithm": [[231, "mixed-precision-algorithm"]], "Mixed precision": [[230, "mixed-precision"], [232, null], [256, "mixed-precision"]], "Model compression": [[225, "model-compression"]], "Model compression using channel pruning": [[202, null]], "Model compression using spatial SVD": [[203, null]], "Model compression using spatial SVD and channel pruning": [[204, null]], "Model guidelines": [[251, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[170, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[170, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "Multi-GPU support": [[248, "multi-gpu-support"]], "NOTE": [[225, null]], "Next steps": [[188, "Next-steps"], [192, "Next-steps"], [197, "Next-steps"], [198, "Next-steps"], [201, "Next-steps"], [202, "Next-steps"], [203, "Next-steps"], [204, "Next-steps"], [206, "Next-steps"], [207, "Next-steps"], [209, "Next-steps"], [210, "Next-steps"], [211, "Next-steps"]], "Next: deploying the model": [[257, "next-deploying-the-model"]], "Old versions": [[240, "old-versions"]], "On-target inference": [[254, "on-target-inference"], [255, null]], "Optimization techniques": [[230, null]], "Optional techniques": [[225, "optional-techniques"]], "Overall flow": [[187, "Overall-flow"], [188, "Overall-flow"], [189, "Overall-flow"], [190, "Overall-flow"], [191, "Overall-flow"], [192, "Overall-flow"], [193, "Overall-flow"], [194, "Overall-flow"], [195, "Overall-flow"], [196, "Overall-flow"], [197, "Overall-flow"], [198, "Overall-flow"], [199, "Overall-flow"], [200, "Overall-flow"], [201, "Overall-flow"], [202, "Overall-flow"], [203, "Overall-flow"], [204, "Overall-flow"], [205, "Overall-flow"], [206, "Overall-flow"], [207, "Overall-flow"], [208, "Overall-flow"], [209, "Overall-flow"], [210, "Overall-flow"], [211, "Overall-flow"], [212, "Overall-flow"]], "Overview": [[224, "overview"], [225, "overview"], [227, "overview"], [229, "overview"], [247, "overview"], [249, "overview"]], "PDF of statistics": [[199, "PDF-of-statistics"], [212, "PDF-of-statistics"]], "PTQ": [[235, "ptq"]], "Parameters for AMP algorithm": [[187, "Parameters-for-AMP-algorithm"], [191, "Parameters-for-AMP-algorithm"], [205, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[177, "per-block-quantization"], [244, null]], "Per-channel quantization": [[177, "per-channel-quantization"]], "Per-layer MSE loss": [[199, "Per-layer-MSE-loss"], [212, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[199, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [212, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[224, "per-layer-exploration"]], "Per-layer fine-tuning": [[225, "per-layer-fine-tuning"]], "Perform QAT": [[208, "Perform-QAT"]], "Post-training quantization": [[256, "post-training-quantization"]], "Prepare the evaluation callback function": [[194, "Prepare-the-evaluation-callback-function"]], "Prerequisites": [[213, "prerequisites"], [217, "prerequisites"], [218, "prerequisites"], [219, "prerequisites"], [237, "prerequisites"], [240, "prerequisites"], [241, "prerequisites"], [242, "prerequisites"], [245, "prerequisites"], [248, "prerequisites"]], "Procedure": [[218, "procedure"], [220, "procedure"], [222, "procedure"], [237, "procedure"], [257, "procedure"]], "PyPI": [[240, "pypi"]], "PyTorch model guidelines": [[251, null]], "QAT modes": [[248, "qat-modes"]], "QAT recommendations": [[248, "qat-recommendations"]], "QW-LoRa": [[234, "qw-lora"], [235, null]], "QWA-LoRa": [[234, "qwa-lora"], [236, null]], "Qualcomm\u00ae AI Engine Direct SDK": [[255, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[255, "qualcommreg-ai-hub"]], "Quant Analyzer": [[199, null], [212, null]], "QuantSim workflow": [[247, "quantsim-workflow"]], "Quantization": [[255, "quantization"]], "Quantization Simulation Guide": [[239, "quantization-simulation-guide"]], "Quantization analyzer": [[214, "quantization-analyzer"], [217, null]], "Quantization debugging guidelines": [[253, null]], "Quantization granularity": [[247, "quantization-granularity"]], "Quantization schemes": [[247, "quantization-schemes"]], "Quantization simulation": [[190, null], [256, "quantization-simulation"]], "Quantization simulation guide": [[247, null]], "Quantization user guide": [[254, null]], "Quantization workflow": [[254, "quantization-workflow"], [257, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[194, null], [208, null]], "Quantization-Aware Training with a Keras Transformer Model": [[195, null]], "Quantization-Aware training with range learning": [[198, null]], "Quantization-aware training": [[197, null], [210, null], [248, null], [256, "quantization-aware-training"]], "Quantization-aware training with range learning": [[211, null]], "QuantizationMixin": [[29, null]], "Quantize": [[160, null]], "Quantize and Update Base Model Weights": [[235, "quantize-and-update-base-model-weights"]], "QuantizeDequantize": [[161, null]], "Quantized LoRa": [[230, "quantized-lora"], [234, null]], "Quantized modules": [[174, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[30, null]], "QuantizedAdaptiveAvgPool2d": [[31, null]], "QuantizedAdaptiveAvgPool3d": [[32, null]], "QuantizedAdaptiveMaxPool1d": [[33, null]], "QuantizedAdaptiveMaxPool2d": [[34, null]], "QuantizedAdaptiveMaxPool3d": [[35, null]], "QuantizedAlphaDropout": [[36, null]], "QuantizedAvgPool1d": [[37, null]], "QuantizedAvgPool2d": [[38, null]], "QuantizedAvgPool3d": [[39, null]], "QuantizedBCELoss": [[40, null]], "QuantizedBCEWithLogitsLoss": [[41, null]], "QuantizedBatchNorm1d": [[42, null]], "QuantizedBatchNorm2d": [[43, null]], "QuantizedBatchNorm3d": [[44, null]], "QuantizedBilinear": [[45, null]], "QuantizedCELU": [[46, null]], "QuantizedCTCLoss": [[47, null]], "QuantizedChannelShuffle": [[48, null]], "QuantizedCircularPad1d": [[49, null]], "QuantizedCircularPad2d": [[50, null]], "QuantizedCircularPad3d": [[51, null]], "QuantizedConstantPad1d": [[52, null]], "QuantizedConstantPad2d": [[53, null]], "QuantizedConstantPad3d": [[54, null]], "QuantizedConv1d": [[55, null]], "QuantizedConv2d": [[56, null]], "QuantizedConv3d": [[57, null]], "QuantizedConvTranspose1d": [[58, null]], "QuantizedConvTranspose2d": [[59, null]], "QuantizedConvTranspose3d": [[60, null]], "QuantizedCosineEmbeddingLoss": [[61, null]], "QuantizedCosineSimilarity": [[62, null]], "QuantizedCrossEntropyLoss": [[63, null]], "QuantizedDropout": [[64, null]], "QuantizedDropout1d": [[65, null]], "QuantizedDropout2d": [[66, null]], "QuantizedDropout3d": [[67, null]], "QuantizedELU": [[68, null]], "QuantizedEmbedding": [[69, null]], "QuantizedEmbeddingBag": [[70, null]], "QuantizedFeatureAlphaDropout": [[71, null]], "QuantizedFlatten": [[72, null]], "QuantizedFold": [[73, null]], "QuantizedFractionalMaxPool2d": [[74, null]], "QuantizedFractionalMaxPool3d": [[75, null]], "QuantizedGELU": [[76, null]], "QuantizedGLU": [[77, null]], "QuantizedGRU": [[78, null]], "QuantizedGRUCell": [[79, null]], "QuantizedGaussianNLLLoss": [[80, null]], "QuantizedGroupNorm": [[81, null]], "QuantizedHardshrink": [[82, null]], "QuantizedHardsigmoid": [[83, null]], "QuantizedHardswish": [[84, null]], "QuantizedHardtanh": [[85, null]], "QuantizedHingeEmbeddingLoss": [[86, null]], "QuantizedHuberLoss": [[87, null]], "QuantizedInstanceNorm1d": [[88, null]], "QuantizedInstanceNorm2d": [[89, null]], "QuantizedInstanceNorm3d": [[90, null]], "QuantizedKLDivLoss": [[91, null]], "QuantizedL1Loss": [[92, null]], "QuantizedLPPool1d": [[93, null]], "QuantizedLPPool2d": [[94, null]], "QuantizedLSTM": [[95, null]], "QuantizedLSTMCell": [[96, null]], "QuantizedLayerNorm": [[97, null]], "QuantizedLeakyReLU": [[98, null]], "QuantizedLinear": [[99, null]], "QuantizedLocalResponseNorm": [[100, null]], "QuantizedLogSigmoid": [[101, null]], "QuantizedLogSoftmax": [[102, null]], "QuantizedMSELoss": [[103, null]], "QuantizedMarginRankingLoss": [[104, null]], "QuantizedMaxPool1d": [[105, null]], "QuantizedMaxPool2d": [[106, null]], "QuantizedMaxPool3d": [[107, null]], "QuantizedMaxUnpool1d": [[108, null]], "QuantizedMaxUnpool2d": [[109, null]], "QuantizedMaxUnpool3d": [[110, null]], "QuantizedMish": [[111, null]], "QuantizedMultiLabelMarginLoss": [[112, null]], "QuantizedMultiLabelSoftMarginLoss": [[113, null]], "QuantizedMultiMarginLoss": [[114, null]], "QuantizedNLLLoss": [[115, null]], "QuantizedNLLLoss2d": [[116, null]], "QuantizedPReLU": [[117, null]], "QuantizedPairwiseDistance": [[118, null]], "QuantizedPixelShuffle": [[119, null]], "QuantizedPixelUnshuffle": [[120, null]], "QuantizedPoissonNLLLoss": [[121, null]], "QuantizedRNN": [[122, null]], "QuantizedRNNCell": [[123, null]], "QuantizedRReLU": [[124, null]], "QuantizedReLU": [[125, null]], "QuantizedReLU6": [[126, null]], "QuantizedReflectionPad1d": [[127, null]], "QuantizedReflectionPad2d": [[128, null]], "QuantizedReflectionPad3d": [[129, null]], "QuantizedReplicationPad1d": [[130, null]], "QuantizedReplicationPad2d": [[131, null]], "QuantizedReplicationPad3d": [[132, null]], "QuantizedSELU": [[133, null]], "QuantizedSiLU": [[134, null]], "QuantizedSigmoid": [[135, null]], "QuantizedSmoothL1Loss": [[136, null]], "QuantizedSoftMarginLoss": [[137, null]], "QuantizedSoftmax": [[138, null]], "QuantizedSoftmax2d": [[139, null]], "QuantizedSoftmin": [[140, null]], "QuantizedSoftplus": [[141, null]], "QuantizedSoftshrink": [[142, null]], "QuantizedSoftsign": [[143, null]], "QuantizedTanh": [[144, null]], "QuantizedTanhshrink": [[145, null]], "QuantizedTensor": [[158, null]], "QuantizedTensorBase": [[159, null]], "QuantizedThreshold": [[146, null]], "QuantizedTripletMarginLoss": [[147, null]], "QuantizedTripletMarginWithDistanceLoss": [[148, null]], "QuantizedUnflatten": [[149, null]], "QuantizedUnfold": [[150, null]], "QuantizedUpsample": [[151, null]], "QuantizedUpsamplingBilinear2d": [[152, null]], "QuantizedUpsamplingNearest2d": [[153, null]], "QuantizedZeroPad1d": [[154, null]], "QuantizedZeroPad2d": [[155, null]], "QuantizedZeroPad3d": [[156, null]], "Quantizer Args structure": [[246, "id6"]], "Quantizers": [[177, "quantizers"]], "Quantsim and Adaround - Per Channel Quantization (PCQ)": [[200, null]], "Quick Start": [[239, "quick-start"]], "Quick Start (PyTorch)": [[243, null]], "Rank Rounding": [[225, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[208, "Re-estimate-BatchNorm-Statistics"]], "References": [[225, "references"]], "Regular AMP": [[191, "Regular-AMP"]], "Release Notes": [[239, "release-notes"]], "Release notes": [[252, null]], "Run QWA-LoRa": [[236, "run-qwa-lora"]], "Running a quick example": [[243, "running-a-quick-example"]], "Running the notebooks": [[186, "running-the-notebooks"]], "Runtime configuration": [[247, "runtime-configuration"], [249, null]], "Sequential MSE": [[230, "sequential-mse"], [237, null]], "Set model input precision": [[233, "set-model-input-precision"]], "Set model output precision": [[233, "set-model-output-precision"]], "Set precision based on layer type": [[233, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[233, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[233, "set-precision-of-a-non-leaf-layer"]], "Setup": [[213, "setup"], [219, "setup"], [221, "setup"], [222, "setup"], [226, "setup"], [228, "setup"], [233, "setup"], [235, "setup"], [236, "setup"], [237, "setup"]], "Signal-to-Quantization-Noise": [[247, "signal-to-quantization-noise"]], "Simulate quantization noise": [[247, "simulate-quantization-noise"]], "Spatial SVD": [[225, "spatial-svd"], [226, null]], "Start the docker container": [[241, "start-the-docker-container"]], "Starting a Bokeh server session": [[227, "starting-a-bokeh-server-session"]], "Step 1": [[213, "step-1"], [218, "step-1"], [219, "step-1"], [220, "step-1"], [231, "step-1"], [237, "step-1"]], "Step 1 Importing libraries": [[217, "step-1-importing-libraries"]], "Step 1: Creating a QuantSim model": [[245, "step-1-creating-a-quantsim-model"]], "Step 1: Importing the API": [[216, "step-1-importing-the-api"]], "Step 1: Setup": [[248, "step-1-setup"]], "Step 1: Trying FP16 precision (no quantization)": [[257, "step-1-trying-fp16-precision-no-quantization"]], "Step 2": [[213, "step-2"], [218, "step-2"], [219, "step-2"], [220, "step-2"], [231, "step-2"], [237, "step-2"]], "Step 2 Preparing calibration callback": [[217, "step-2-preparing-calibration-callback"]], "Step 2: Computing the initial quantization parameters": [[248, "step-2-computing-the-initial-quantization-parameters"]], "Step 2: Creating a calibration callback": [[245, "step-2-creating-a-calibration-callback"]], "Step 2: Loading a model": [[216, "step-2-loading-a-model"]], "Step 2: Verifying W16A16 quantization": [[257, "step-2-verifying-w16a16-quantization"]], "Step 3": [[213, "step-3"], [218, "step-3"], [219, "step-3"], [220, "step-3"], [237, "step-3"]], "Step 3 Preparing evaluation callback": [[217, "step-3-preparing-evaluation-callback"]], "Step 3. Reducing precision": [[257, "step-3-reducing-precision"]], "Step 3: Calibrate the quantized model": [[248, "step-3-calibrate-the-quantized-model"]], "Step 3: Computing encodings": [[245, "step-3-computing-encodings"]], "Step 3: Obtaining inputs": [[216, "step-3-obtaining-inputs"]], "Step 4": [[213, "step-4"], [218, "step-4"], [219, "step-4"], [237, "step-4"]], "Step 4 Preparing model": [[217, "step-4-preparing-model"]], "Step 4. Restoring accuracy": [[257, "step-4-restoring-accuracy"]], "Step 4: Evaluating the model": [[248, "step-4-evaluating-the-model"]], "Step 4: Evaluation": [[245, "step-4-evaluation"]], "Step 4: Generating layer outputs": [[216, "step-4-generating-layer-outputs"]], "Step 5": [[218, "step-5"], [237, "step-5"]], "Step 5 Creating QuantAnalyzer": [[217, "step-5-creating-quantanalyzer"]], "Step 5: Exporting the model": [[245, "step-5-exporting-the-model"], [248, "step-5-exporting-the-model"]], "Step 6": [[218, "step-6"]], "Step 6 Running the analysis": [[217, "step-6-running-the-analysis"]], "Step 7": [[218, "step-7"]], "Summary": [[191, "Summary"], [194, "Summary"], [196, "Summary"], [200, "Summary"], [208, "Summary"]], "Supported precisions for on-target inference": [[256, "supported-precisions-for-on-target-inference"]], "Techniques": [[209, "Techniques"]], "TensorFlow model guidelines": [[250, null]], "Tested platform": [[243, "tested-platform"]], "Top level structure": [[246, "id2"]], "Training Callback": [[236, "training-callback"]], "Use Case": [[225, "use-case"]], "Use Cases": [[231, "use-cases"]], "User Guide": [[239, "user-guide"]], "User flow": [[175, "user-flow"]], "Verifying the installation": [[240, "verifying-the-installation"], [243, "verifying-the-installation"]], "Visualization Tools": [[167, "visualization-tools"]], "Visualizing compression ratios": [[227, "visualizing-compression-ratios"]], "Weight SVD": [[225, "weight-svd"], [228, null]], "Weight reconstruction": [[222, "weight-reconstruction"]], "What this notebook is not": [[187, "What-this-notebook-is-not"], [191, "What-this-notebook-is-not"], [199, "What-this-notebook-is-not"], [200, "What-this-notebook-is-not"], [205, "What-this-notebook-is-not"], [208, "What-this-notebook-is-not"], [212, "What-this-notebook-is-not"]], "Winnowing": [[222, "winnowing"], [229, null], [229, "id1"]], "Workflow": [[213, "workflow"], [213, "id2"], [215, "workflow"], [216, "workflow"], [217, "workflow"], [218, "workflow"], [219, "workflow"], [220, "workflow"], [221, "workflow"], [222, "workflow"], [226, "workflow"], [228, "workflow"], [231, "workflow"], [233, "workflow"], [235, "workflow"], [236, "workflow"], [237, "workflow"], [245, "workflow"], [248, "workflow"]], "aimet_onnx": [[0, "aimet-onnx"]], "aimet_onnx API": [[5, null]], "aimet_onnx.adaround": [[1, null]], "aimet_onnx.batch_norm_fold": [[3, null]], "aimet_onnx.cross_layer_equalization": [[4, null]], "aimet_onnx.layer_output_utils": [[6, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[8, null]], "aimet_onnx.quantsim": [[9, null]], "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights": [[7, null]], "aimet_onnx.seq_mse": [[10, null]], "aimet_tensorflow": [[0, "aimet-tensorflow"]], "aimet_tensorflow API": [[18, null]], "aimet_tensorflow.adaround": [[11, null]], "aimet_tensorflow.auto_quant_v2": [[13, null]], "aimet_tensorflow.batch_norm_fold": [[15, null]], "aimet_tensorflow.compress": [[17, null]], "aimet_tensorflow.cross_layer_equalization": [[16, null]], "aimet_tensorflow.keras.bn_reestimation": [[14, null]], "aimet_tensorflow.layer_output_utils": [[19, null]], "aimet_tensorflow.mixed_precision": [[12, null]], "aimet_tensorflow.model_preparer": [[20, null]], "aimet_tensorflow.quant_analyzer": [[21, null]], "aimet_tensorflow.quantsim": [[22, null]], "aimet_torch": [[0, "aimet-torch"], [166, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[170, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[166, null]], "aimet_torch.adaround": [[23, null]], "aimet_torch.auto_quant": [[24, null]], "aimet_torch.batch_norm_fold": [[26, null]], "aimet_torch.bn_reestimation": [[25, null]], "aimet_torch.compress": [[28, null]], "aimet_torch.cross_layer_equalization": [[27, null]], "aimet_torch.layer_output_utils": [[168, null]], "aimet_torch.mixed_precision": [[173, null]], "aimet_torch.model_preparer": [[171, null]], "aimet_torch.model_validator": [[172, null]], "aimet_torch.nn": [[174, null]], "aimet_torch.peft": [[175, null]], "aimet_torch.quant_analyzer": [[176, null]], "aimet_torch.quantization": [[177, null]], "aimet_torch.quantsim": [[178, null]], "aimet_torch.quantsim.config_utils": [[169, null]], "aimet_torch.seq_mse": [[179, null]], "aimet_torch.v1": [[166, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[180, null]], "aimet_torch.v1.auto_quant": [[182, null]], "aimet_torch.v1.mixed_precision": [[181, null]], "aimet_torch.v1.quant_analyzer": [[183, null]], "aimet_torch.v1.quantsim": [[184, null]], "aimet_torch.v1.seq_mse": [[185, null]], "aimet_torch.visualization_tools": [[167, null]], "dequantize": [[162, null]], "quantize": [[163, null]], "quantize_dequantize": [[164, null]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/tensorflow/adaround", "apiref/tensorflow/amp", "apiref/tensorflow/autoquant", "apiref/tensorflow/bn", "apiref/tensorflow/bnf", "apiref/tensorflow/cle", "apiref/tensorflow/compress", "apiref/tensorflow/index", "apiref/tensorflow/layer_output_generation", "apiref/tensorflow/model_preparer", "apiref/tensorflow/quant_analyzer", "apiref/tensorflow/quantsim", "apiref/torch/adaround", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "examples/index", "examples/onnx/quantization/AMP", "examples/onnx/quantization/adaround", "examples/onnx/quantization/cle", "examples/onnx/quantization/quantsim", "examples/tensorflow/quantization/keras/KerasAMP", "examples/tensorflow/quantization/keras/adaround", "examples/tensorflow/quantization/keras/autoquant", "examples/tensorflow/quantization/keras/bn_reestimation", "examples/tensorflow/quantization/keras/keras_transformer_qat", "examples/tensorflow/quantization/keras/model_preparer", "examples/tensorflow/quantization/keras/qat", "examples/tensorflow/quantization/keras/qat_range_learning", "examples/tensorflow/quantization/keras/quant_analyzer", "examples/tensorflow/quantization/keras/quantsim_adaround_pcq", "examples/tensorflow/quantization/keras/quantsim_cle", "examples/torch/compression/channel_pruning", "examples/torch/compression/spatial_svd", "examples/torch/compression/spatial_svd_channel_pruning", "examples/torch/quantization/AMP", "examples/torch/quantization/adaround", "examples/torch/quantization/autoquant", "examples/torch/quantization/bn_reestimation", "examples/torch/quantization/cle_bc", "examples/torch/quantization/qat", "examples/torch/quantization/qat_range_learning", "examples/torch/quantization/quant_analyzer", "featureguide/adaround", "featureguide/analysis tools/index", "featureguide/analysis tools/interactive_visualization", "featureguide/analysis tools/layer_output_generation", "featureguide/analysis tools/quant_analyzer", "featureguide/autoquant", "featureguide/bn", "featureguide/bnf", "featureguide/cle", "featureguide/compression/channel_pruning", "featureguide/compression/feature_guidebook", "featureguide/compression/greedy_compression_ratio_selection", "featureguide/compression/index", "featureguide/compression/spatial_svd", "featureguide/compression/visualization_compression", "featureguide/compression/weight_svd", "featureguide/compression/winnowing", "featureguide/index", "featureguide/mixed precision/amp", "featureguide/mixed precision/index", "featureguide/mixed precision/mmp", "featureguide/quantized LoRa/index", "featureguide/quantized LoRa/qw_lora", "featureguide/quantized LoRa/qwa_lora", "featureguide/seq_mse", "glossary", "index", "install/index", "install/install_docker", "install/install_host", "install/quick-start", "quantsim/blockwise", "quantsim/calibration", "quantsim/encoding_spec", "quantsim/index", "quantsim/qat", "quantsim/runtime_config", "quantsim/tensorflow/model_guidelines", "quantsim/torch/model_guidelines", "release_notes", "userguide/debugging_guidelines", "userguide/index", "userguide/on_target_inference", "userguide/quantization_tools", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/tensorflow/adaround.rst", "apiref/tensorflow/amp.rst", "apiref/tensorflow/autoquant.rst", "apiref/tensorflow/bn.rst", "apiref/tensorflow/bnf.rst", "apiref/tensorflow/cle.rst", "apiref/tensorflow/compress.rst", "apiref/tensorflow/index.rst", "apiref/tensorflow/layer_output_generation.rst", "apiref/tensorflow/model_preparer.rst", "apiref/tensorflow/quant_analyzer.rst", "apiref/tensorflow/quantsim.rst", "apiref/torch/adaround.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "examples/index.rst", "examples/onnx/quantization/AMP.ipynb", "examples/onnx/quantization/adaround.ipynb", "examples/onnx/quantization/cle.ipynb", "examples/onnx/quantization/quantsim.ipynb", "examples/tensorflow/quantization/keras/KerasAMP.ipynb", "examples/tensorflow/quantization/keras/adaround.ipynb", "examples/tensorflow/quantization/keras/autoquant.ipynb", "examples/tensorflow/quantization/keras/bn_reestimation.ipynb", "examples/tensorflow/quantization/keras/keras_transformer_qat.ipynb", "examples/tensorflow/quantization/keras/model_preparer.ipynb", "examples/tensorflow/quantization/keras/qat.ipynb", "examples/tensorflow/quantization/keras/qat_range_learning.ipynb", "examples/tensorflow/quantization/keras/quant_analyzer.ipynb", "examples/tensorflow/quantization/keras/quantsim_adaround_pcq.ipynb", "examples/tensorflow/quantization/keras/quantsim_cle.ipynb", "examples/torch/compression/channel_pruning.ipynb", "examples/torch/compression/spatial_svd.ipynb", "examples/torch/compression/spatial_svd_channel_pruning.ipynb", "examples/torch/quantization/AMP.ipynb", "examples/torch/quantization/adaround.ipynb", "examples/torch/quantization/autoquant.ipynb", "examples/torch/quantization/bn_reestimation.ipynb", "examples/torch/quantization/cle_bc.ipynb", "examples/torch/quantization/qat.ipynb", "examples/torch/quantization/qat_range_learning.ipynb", "examples/torch/quantization/quant_analyzer.ipynb", "featureguide/adaround.rst", "featureguide/analysis tools/index.rst", "featureguide/analysis tools/interactive_visualization.rst", "featureguide/analysis tools/layer_output_generation.rst", "featureguide/analysis tools/quant_analyzer.rst", "featureguide/autoquant.rst", "featureguide/bn.rst", "featureguide/bnf.rst", "featureguide/cle.rst", "featureguide/compression/channel_pruning.rst", "featureguide/compression/feature_guidebook.rst", "featureguide/compression/greedy_compression_ratio_selection.rst", "featureguide/compression/index.rst", "featureguide/compression/spatial_svd.rst", "featureguide/compression/visualization_compression.rst", "featureguide/compression/weight_svd.rst", "featureguide/compression/winnowing.rst", "featureguide/index.rst", "featureguide/mixed precision/amp.rst", "featureguide/mixed precision/index.rst", "featureguide/mixed precision/mmp.rst", "featureguide/quantized LoRa/index.rst", "featureguide/quantized LoRa/qw_lora.rst", "featureguide/quantized LoRa/qwa_lora.rst", "featureguide/seq_mse.rst", "glossary.rst", "index.rst", "install/index.rst", "install/install_docker.rst", "install/install_host.rst", "install/quick-start.rst", "quantsim/blockwise.rst", "quantsim/calibration.rst", "quantsim/encoding_spec.rst", "quantsim/index.rst", "quantsim/qat.rst", "quantsim/runtime_config.rst", "quantsim/tensorflow/model_guidelines.rst", "quantsim/torch/model_guidelines.rst", "release_notes.rst", "userguide/debugging_guidelines.rst", "userguide/index.rst", "userguide/on_target_inference.rst", "userguide/quantization_tools.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[238, "term-Accelerator", true]], "accuracy": [[238, "term-Accuracy", true]], "activation": [[238, "term-Activation", true]], "activation quantization": [[238, "term-Activation-Quantization", true]], "adaptermetadata (class in aimet_torch.peft)": [[175, "aimet_torch.peft.AdapterMetaData", false]], "adaround": [[238, "term-AdaRound", true]], "adaroundparameters (class in aimet_onnx.adaround.adaround_weight)": [[1, "aimet_onnx.adaround.adaround_weight.AdaroundParameters", false], [213, "aimet_onnx.adaround.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_tensorflow.keras.adaround_weight)": [[11, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false], [213, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.adaround.adaround_weight)": [[23, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false], [213, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[180, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[172, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[238, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[238, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [217, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[173, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [233, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_onnx.adaround.adaround_weight.adaround)": [[1, "aimet_onnx.adaround.adaround_weight.Adaround.apply_adaround", false], [213, "aimet_onnx.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_tensorflow.keras.adaround_weight.adaround)": [[11, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false], [213, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.adaround.adaround_weight.adaround)": [[23, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false], [213, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[180, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_seq_mse() (in module aimet_onnx.sequential_mse.seq_mse.sequentialmse)": [[10, "aimet_onnx.sequential_mse.seq_mse.SequentialMse.apply_seq_mse", false], [237, "aimet_onnx.sequential_mse.seq_mse.SequentialMse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[179, "aimet_torch.seq_mse.apply_seq_mse", false], [237, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[185, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "auto (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false]], "autoquant": [[238, "term-AutoQuant", true]], "autoquantwithautomixedprecision (class in aimet_tensorflow.keras.auto_quant_v2)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false]], "batch normalization": [[238, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[238, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[238, "term-BN", true]], "callbackfunc (class in aimet_common.defs)": [[2, "aimet_common.defs.CallbackFunc", false], [12, "aimet_common.defs.CallbackFunc", false], [173, "aimet_common.defs.CallbackFunc", false], [181, "aimet_common.defs.CallbackFunc", false], [231, "aimet_common.defs.CallbackFunc", false], [231, "id0", false], [231, "id1", false]], "callbackfunc (class in aimet_common.utils)": [[176, "aimet_common.utils.CallbackFunc", false], [183, "aimet_common.utils.CallbackFunc", false], [217, "aimet_common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_fast_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[12, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false], [231, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [231, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[12, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false], [231, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[173, "aimet_torch.mixed_precision.choose_mixed_precision", false], [231, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[181, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[238, "term-CNN", true]], "compress_model() (aimet_tensorflow.keras.compress.modelcompressor static method)": [[17, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false], [226, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false]], "compression": [[238, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantizationsimmodel method)": [[9, "aimet_onnx.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[29, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantizationsimmodel method)": [[178, "aimet_torch.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[184, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "convolutional layer": [[238, "term-Convolutional-Layer", true]], "convolutional neural network": [[238, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[157, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[158, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[162, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[157, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[238, "term-Device", true]], "disable_lora_adapters() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.disable_lora_adapters", false]], "dlf": [[238, "term-DLF", true]], "dynamic layer fusion": [[238, "term-Dynamic-Layer-Fusion", true]], "edge device": [[238, "term-Edge-device", true]], "enable_adapter_and_load_weights() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.enable_adapter_and_load_weights", false]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [217, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[238, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[4, "aimet_onnx.cross_layer_equalization.equalize_model", false], [221, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_tensorflow.keras.cross_layer_equalization)": [[16, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false], [221, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[27, "aimet_torch.cross_layer_equalization.equalize_model", false], [221, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [231, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[173, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [181, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [231, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "export() (aimet_onnx.quantizationsimmodel method)": [[9, "aimet_onnx.QuantizationSimModel.export", false]], "export() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.export", false]], "export() (aimet_torch.quantizationsimmodel method)": [[178, "aimet_torch.QuantizationSimModel.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[184, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export_adapter_weights() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.export_adapter_weights", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_tensorflow.keras.batch_norm_fold)": [[15, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false], [220, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[26, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [220, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_scale() (in module aimet_tensorflow.keras.batch_norm_fold)": [[14, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false], [219, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[3, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [220, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[29, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[99, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[160, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[161, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[179, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [237, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[185, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[238, "term-FP32", true]], "freeze_base_model() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.freeze_base_model", false]], "freeze_base_model_activation_quantizers() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.freeze_base_model_activation_quantizers", false]], "freeze_base_model_param_quantizers() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.freeze_base_model_param_quantizers", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[29, "aimet_torch.nn.QuantizationMixin.from_module", false]], "from_str() (aimet_common.defs.quantscheme class method)": [[9, "aimet_common.defs.QuantScheme.from_str", false], [22, "aimet_common.defs.QuantScheme.from_str", false], [178, "aimet_common.defs.QuantScheme.from_str", false], [184, "aimet_common.defs.QuantScheme.from_str", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [216, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_tensorflow.keras.layer_output_utils.layeroutpututil method)": [[19, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [216, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[168, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [216, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_param_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[29, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_fp_lora_layer() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.get_fp_lora_layer", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[29, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_loss_fn() (aimet_torch.seq_mse.seqmseparams method)": [[179, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false], [237, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false]], "get_loss_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[185, "aimet_torch.v1.seq_mse.SeqMseParams.get_loss_fn", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "get_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false]], "get_quantized_lora_layer() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.get_quantized_lora_layer", false]], "greedyselectionparameters (class in aimet_common.defs)": [[17, "aimet_common.defs.GreedySelectionParameters", false], [226, "aimet_common.defs.GreedySelectionParameters", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[29, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[238, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[29, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[238, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[238, "term-KL-Divergence", true]], "layer": [[238, "term-Layer", true]], "layer-wise quantization": [[238, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [216, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_tensorflow.keras.layer_output_utils)": [[19, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false], [216, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[168, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [216, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[184, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lookup_quantizer() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup static method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false]], "lora mobilenet": [[238, "term-LoRA-MobileNet", true]], "manual (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[173, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [233, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[238, "term-Model", true]], "modelcompressor (class in aimet_tensorflow.keras.compress)": [[17, "aimet_tensorflow.keras.compress.ModelCompressor", false], [226, "aimet_tensorflow.keras.compress.ModelCompressor", false]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[172, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[168, "aimet_torch.layer_output_utils.NamingScheme", false], [216, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[238, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[238, "term-NNCF", true]], "node": [[238, "term-Node", true]], "normalization": [[238, "term-Normalization", true]], "onnx": [[238, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[168, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [216, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "open neural network exchange": [[238, "term-Open-Neural-Network-Exchange", true]], "optimize() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[29, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[29, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "peftquantutils (class in aimet_torch.peft)": [[175, "aimet_torch.peft.PeftQuantUtils", false]], "per-channel quantization": [[238, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[238, "term-Post-Training-Quantization", true]], "prepare_model() (in module aimet_tensorflow.keras.model_preparer)": [[20, "aimet_tensorflow.keras.model_preparer.prepare_model", false]], "prepare_model() (in module aimet_torch.model_preparer)": [[171, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[238, "term-Pruning", true]], "ptq": [[238, "term-PTQ", true]], "pytorch": [[238, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[168, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [216, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[238, "term-QAT", true]], "qdo": [[238, "term-QDO", true]], "qualcomm innovation center": [[238, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[8, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [217, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[176, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [217, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[183, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[238, "term-Quantization", true]], "quantization simulation": [[238, "term-Quantization-Simulation", true]], "quantization-aware training": [[238, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[29, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx)": [[9, "aimet_onnx.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_tensorflow.keras.quantsim)": [[22, "aimet_tensorflow.keras.quantsim.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch)": [[178, "aimet_torch.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[184, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantize (class in aimet_torch.quantization.affine)": [[160, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[157, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[158, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[163, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[164, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantize_lora_scale_with_fixed_range() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.quantize_lora_scale_with_fixed_range", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[157, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[158, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[159, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[161, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[158, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[159, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[147, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[148, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[149, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[150, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[151, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[152, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[153, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[154, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[155, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[156, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_tensorflow.keras.amp.quantizer_groups)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantscheme (class in aimet_common.defs)": [[9, "aimet_common.defs.QuantScheme", false], [22, "aimet_common.defs.QuantScheme", false], [178, "aimet_common.defs.QuantScheme", false], [184, "aimet_common.defs.QuantScheme", false]], "quantsim": [[238, "term-QuantSim", true]], "quic": [[238, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_tensorflow.keras.bn_reestimation)": [[14, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false], [219, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[25, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [219, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "replace_lora_layers_with_quantizable_layers() (aimet_torch.peft method)": [[175, "aimet_torch.peft.replace_lora_layers_with_quantizable_layers", false]], "run_inference() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[184, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_onnx.sequential_mse.seq_mse)": [[10, "aimet_onnx.sequential_mse.seq_mse.SeqMseParams", false], [237, "aimet_onnx.sequential_mse.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[179, "aimet_torch.seq_mse.SeqMseParams", false], [237, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[185, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[169, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_adaround_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false]], "set_bitwidth_for_lora_adapters() (aimet_torch.peft.peftquantutils method)": [[175, "aimet_torch.peft.PeftQuantUtils.set_bitwidth_for_lora_adapters", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[169, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[29, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[165, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_onnx.quantsim)": [[7, "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[169, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[29, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_mixed_precision_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[173, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [233, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[173, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [233, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[173, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [233, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[13, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false], [218, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "spatialsvdparameters (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false]], "spatialsvdparameters.automodeparams (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false]], "spatialsvdparameters.manualmodeparams (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false]], "spatialsvdparameters.mode (class in aimet_tensorflow.keras.defs)": [[17, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false], [226, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [231, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[173, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [181, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [231, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[238, "term-Target-Hardware-Accelerator", true]], "target runtime": [[238, "term-Target-Runtime", true]], "tensorflow": [[238, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [231, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[12, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false], [231, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[173, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [181, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [231, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "torchscript": [[238, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[168, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [216, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "track_lora_meta_data() (aimet_torch.peft method)": [[175, "aimet_torch.peft.track_lora_meta_data", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[172, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[238, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[167, "aimet_torch.v2.visualization_tools.visualize_stats", false], [215, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[238, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[29, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_common.defs": [[231, 0, 1, "id1", "CallbackFunc"], [226, 0, 1, "", "GreedySelectionParameters"], [184, 0, 1, "", "QuantScheme"]], "aimet_common.defs.QuantScheme": [[184, 1, 1, "", "from_str"]], "aimet_common.utils": [[217, 0, 1, "", "CallbackFunc"]], "aimet_onnx": [[9, 0, 1, "", "QuantizationSimModel"]], "aimet_onnx.QuantizationSimModel": [[9, 1, 1, "", "compute_encodings"], [9, 1, 1, "", "export"]], "aimet_onnx.adaround.adaround_weight": [[213, 0, 1, "", "AdaroundParameters"]], "aimet_onnx.adaround.adaround_weight.Adaround": [[213, 2, 1, "", "apply_adaround"]], "aimet_onnx.amp.mixed_precision_algo": [[231, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[231, 1, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[231, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[231, 1, 1, "", "get_activation_quantizers"], [231, 1, 1, "", "get_active_quantizers"], [231, 1, 1, "", "get_candidate"], [231, 1, 1, "", "get_param_quantizers"], [231, 1, 1, "", "set_quantizers_to_candidate"], [231, 1, 1, "", "to_list"]], "aimet_onnx.batch_norm_fold": [[220, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.cross_layer_equalization": [[221, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[216, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[216, 1, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[231, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[217, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[217, 1, 1, "", "analyze"], [217, 1, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[7, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_onnx.sequential_mse.seq_mse": [[237, 0, 1, "", "SeqMseParams"]], "aimet_onnx.sequential_mse.seq_mse.SequentialMse": [[237, 2, 1, "", "apply_seq_mse"]], "aimet_tensorflow.keras.adaround_weight": [[213, 0, 1, "", "AdaroundParameters"]], "aimet_tensorflow.keras.adaround_weight.Adaround": [[213, 2, 1, "", "apply_adaround"]], "aimet_tensorflow.keras.amp.quantizer_groups": [[231, 0, 1, "", "QuantizerGroup"]], "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup": [[231, 1, 1, "", "get_active_param_quantizers"], [231, 1, 1, "", "get_active_quantizers"], [231, 1, 1, "", "get_candidate"], [231, 1, 1, "", "lookup_quantizer"], [231, 1, 1, "", "set_quantizers_to_candidate"], [231, 1, 1, "", "to_list"]], "aimet_tensorflow.keras.auto_quant_v2": [[218, 0, 1, "", "AutoQuantWithAutoMixedPrecision"]], "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision": [[218, 1, 1, "", "get_quant_scheme_candidates"], [218, 1, 1, "", "optimize"], [218, 1, 1, "", "run_inference"], [218, 1, 1, "", "set_adaround_params"], [218, 1, 1, "", "set_mixed_precision_params"], [218, 1, 1, "", "set_quant_scheme_candidates"]], "aimet_tensorflow.keras.batch_norm_fold": [[220, 2, 1, "", "fold_all_batch_norms"], [219, 2, 1, "", "fold_all_batch_norms_to_scale"]], "aimet_tensorflow.keras.bn_reestimation": [[219, 2, 1, "", "reestimate_bn_stats"]], "aimet_tensorflow.keras.compress": [[226, 0, 1, "", "ModelCompressor"]], "aimet_tensorflow.keras.compress.ModelCompressor": [[226, 1, 1, "", "compress_model"]], "aimet_tensorflow.keras.cross_layer_equalization": [[221, 2, 1, "", "equalize_model"]], "aimet_tensorflow.keras.defs": [[226, 0, 1, "", "SpatialSvdParameters"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters": [[226, 0, 1, "", "AutoModeParams"], [226, 0, 1, "", "ManualModeParams"], [226, 0, 1, "", "Mode"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode": [[226, 3, 1, "", "auto"], [226, 3, 1, "", "manual"]], "aimet_tensorflow.keras.layer_output_utils": [[216, 0, 1, "", "LayerOutputUtil"]], "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil": [[216, 1, 1, "", "generate_layer_outputs"]], "aimet_tensorflow.keras.mixed_precision": [[231, 2, 1, "", "choose_fast_mixed_precision"], [231, 2, 1, "", "choose_mixed_precision"]], "aimet_tensorflow.keras.model_preparer": [[20, 2, 1, "", "prepare_model"]], "aimet_tensorflow.keras.quantsim": [[22, 0, 1, "", "QuantizationSimModel"]], "aimet_tensorflow.keras.quantsim.QuantizationSimModel": [[22, 1, 1, "", "compute_encodings"], [22, 1, 1, "", "export"]], "aimet_torch": [[178, 0, 1, "", "QuantizationSimModel"]], "aimet_torch.QuantizationSimModel": [[178, 1, 1, "", "compute_encodings"], [178, 1, 1, "", "export"]], "aimet_torch.adaround.adaround_weight": [[213, 0, 1, "", "AdaroundParameters"]], "aimet_torch.adaround.adaround_weight.Adaround": [[213, 2, 1, "", "apply_adaround"]], "aimet_torch.amp.mixed_precision_algo": [[231, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[231, 1, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[231, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[231, 1, 1, "", "get_active_quantizers"], [231, 1, 1, "", "get_candidate"], [231, 1, 1, "", "get_input_quantizer_modules"], [231, 1, 1, "", "set_quantizers_to_candidate"], [231, 1, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[220, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[219, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.cross_layer_equalization": [[221, 2, 1, "", "equalize_model"]], "aimet_torch.layer_output_utils": [[216, 0, 1, "", "LayerOutputUtil"], [216, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[216, 1, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[216, 3, 1, "", "ONNX"], [216, 3, 1, "", "PYTORCH"], [216, 3, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[231, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[171, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[172, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[172, 1, 1, "", "add_check"], [172, 1, 1, "", "validate_model"]], "aimet_torch.nn": [[29, 0, 1, "", "QuantizationMixin"], [30, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [31, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [32, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [33, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [34, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [35, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [36, 0, 1, "", "QuantizedAlphaDropout"], [37, 0, 1, "", "QuantizedAvgPool1d"], [38, 0, 1, "", "QuantizedAvgPool2d"], [39, 0, 1, "", "QuantizedAvgPool3d"], [40, 0, 1, "", "QuantizedBCELoss"], [41, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [42, 0, 1, "", "QuantizedBatchNorm1d"], [43, 0, 1, "", "QuantizedBatchNorm2d"], [44, 0, 1, "", "QuantizedBatchNorm3d"], [45, 0, 1, "", "QuantizedBilinear"], [46, 0, 1, "", "QuantizedCELU"], [47, 0, 1, "", "QuantizedCTCLoss"], [48, 0, 1, "", "QuantizedChannelShuffle"], [49, 0, 1, "", "QuantizedCircularPad1d"], [50, 0, 1, "", "QuantizedCircularPad2d"], [51, 0, 1, "", "QuantizedCircularPad3d"], [52, 0, 1, "", "QuantizedConstantPad1d"], [53, 0, 1, "", "QuantizedConstantPad2d"], [54, 0, 1, "", "QuantizedConstantPad3d"], [55, 0, 1, "", "QuantizedConv1d"], [56, 0, 1, "", "QuantizedConv2d"], [57, 0, 1, "", "QuantizedConv3d"], [58, 0, 1, "", "QuantizedConvTranspose1d"], [59, 0, 1, "", "QuantizedConvTranspose2d"], [60, 0, 1, "", "QuantizedConvTranspose3d"], [61, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [62, 0, 1, "", "QuantizedCosineSimilarity"], [63, 0, 1, "", "QuantizedCrossEntropyLoss"], [64, 0, 1, "", "QuantizedDropout"], [65, 0, 1, "", "QuantizedDropout1d"], [66, 0, 1, "", "QuantizedDropout2d"], [67, 0, 1, "", "QuantizedDropout3d"], [68, 0, 1, "", "QuantizedELU"], [69, 0, 1, "", "QuantizedEmbedding"], [70, 0, 1, "", "QuantizedEmbeddingBag"], [71, 0, 1, "", "QuantizedFeatureAlphaDropout"], [72, 0, 1, "", "QuantizedFlatten"], [73, 0, 1, "", "QuantizedFold"], [74, 0, 1, "", "QuantizedFractionalMaxPool2d"], [75, 0, 1, "", "QuantizedFractionalMaxPool3d"], [76, 0, 1, "", "QuantizedGELU"], [77, 0, 1, "", "QuantizedGLU"], [78, 0, 1, "", "QuantizedGRU"], [79, 0, 1, "", "QuantizedGRUCell"], [80, 0, 1, "", "QuantizedGaussianNLLLoss"], [81, 0, 1, "", "QuantizedGroupNorm"], [82, 0, 1, "", "QuantizedHardshrink"], [83, 0, 1, "", "QuantizedHardsigmoid"], [84, 0, 1, "", "QuantizedHardswish"], [85, 0, 1, "", "QuantizedHardtanh"], [86, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [87, 0, 1, "", "QuantizedHuberLoss"], [88, 0, 1, "", "QuantizedInstanceNorm1d"], [89, 0, 1, "", "QuantizedInstanceNorm2d"], [90, 0, 1, "", "QuantizedInstanceNorm3d"], [91, 0, 1, "", "QuantizedKLDivLoss"], [92, 0, 1, "", "QuantizedL1Loss"], [93, 0, 1, "", "QuantizedLPPool1d"], [94, 0, 1, "", "QuantizedLPPool2d"], [95, 0, 1, "", "QuantizedLSTM"], [96, 0, 1, "", "QuantizedLSTMCell"], [97, 0, 1, "", "QuantizedLayerNorm"], [98, 0, 1, "", "QuantizedLeakyReLU"], [99, 0, 1, "", "QuantizedLinear"], [100, 0, 1, "", "QuantizedLocalResponseNorm"], [101, 0, 1, "", "QuantizedLogSigmoid"], [102, 0, 1, "", "QuantizedLogSoftmax"], [103, 0, 1, "", "QuantizedMSELoss"], [104, 0, 1, "", "QuantizedMarginRankingLoss"], [105, 0, 1, "", "QuantizedMaxPool1d"], [106, 0, 1, "", "QuantizedMaxPool2d"], [107, 0, 1, "", "QuantizedMaxPool3d"], [108, 0, 1, "", "QuantizedMaxUnpool1d"], [109, 0, 1, "", "QuantizedMaxUnpool2d"], [110, 0, 1, "", "QuantizedMaxUnpool3d"], [111, 0, 1, "", "QuantizedMish"], [112, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [113, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [114, 0, 1, "", "QuantizedMultiMarginLoss"], [115, 0, 1, "", "QuantizedNLLLoss"], [116, 0, 1, "", "QuantizedNLLLoss2d"], [117, 0, 1, "", "QuantizedPReLU"], [118, 0, 1, "", "QuantizedPairwiseDistance"], [119, 0, 1, "", "QuantizedPixelShuffle"], [120, 0, 1, "", "QuantizedPixelUnshuffle"], [121, 0, 1, "", "QuantizedPoissonNLLLoss"], [122, 0, 1, "", "QuantizedRNN"], [123, 0, 1, "", "QuantizedRNNCell"], [124, 0, 1, "", "QuantizedRReLU"], [125, 0, 1, "", "QuantizedReLU"], [126, 0, 1, "", "QuantizedReLU6"], [127, 0, 1, "", "QuantizedReflectionPad1d"], [128, 0, 1, "", "QuantizedReflectionPad2d"], [129, 0, 1, "", "QuantizedReflectionPad3d"], [130, 0, 1, "", "QuantizedReplicationPad1d"], [131, 0, 1, "", "QuantizedReplicationPad2d"], [132, 0, 1, "", "QuantizedReplicationPad3d"], [133, 0, 1, "", "QuantizedSELU"], [134, 0, 1, "", "QuantizedSiLU"], [135, 0, 1, "", "QuantizedSigmoid"], [136, 0, 1, "", "QuantizedSmoothL1Loss"], [137, 0, 1, "", "QuantizedSoftMarginLoss"], [138, 0, 1, "", "QuantizedSoftmax"], [139, 0, 1, "", "QuantizedSoftmax2d"], [140, 0, 1, "", "QuantizedSoftmin"], [141, 0, 1, "", "QuantizedSoftplus"], [142, 0, 1, "", "QuantizedSoftshrink"], [143, 0, 1, "", "QuantizedSoftsign"], [144, 0, 1, "", "QuantizedTanh"], [145, 0, 1, "", "QuantizedTanhshrink"], [146, 0, 1, "", "QuantizedThreshold"], [147, 0, 1, "", "QuantizedTripletMarginLoss"], [148, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [149, 0, 1, "", "QuantizedUnflatten"], [150, 0, 1, "", "QuantizedUnfold"], [151, 0, 1, "", "QuantizedUpsample"], [152, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [153, 0, 1, "", "QuantizedUpsamplingNearest2d"], [154, 0, 1, "", "QuantizedZeroPad1d"], [155, 0, 1, "", "QuantizedZeroPad2d"], [156, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[29, 1, 1, "", "compute_encodings"], [29, 1, 1, "", "forward"], [29, 1, 1, "", "from_module"], [29, 1, 1, "", "get_default_kernel"], [29, 1, 1, "", "get_kernel"], [29, 1, 1, "", "implements"], [29, 3, 1, "", "input_quantizers"], [29, 3, 1, "", "output_quantizers"], [29, 3, 1, "", "param_quantizers"], [29, 1, 1, "", "set_default_kernel"], [29, 1, 1, "", "set_kernel"], [29, 1, 1, "", "wrap"]], "aimet_torch.nn.QuantizedLinear": [[99, 1, 1, "", "forward"]], "aimet_torch.peft": [[175, 0, 1, "", "AdapterMetaData"], [175, 0, 1, "", "PeftQuantUtils"], [175, 1, 1, "", "replace_lora_layers_with_quantizable_layers"], [175, 1, 1, "", "track_lora_meta_data"]], "aimet_torch.peft.PeftQuantUtils": [[175, 1, 1, "", "disable_lora_adapters"], [175, 1, 1, "", "enable_adapter_and_load_weights"], [175, 1, 1, "", "export_adapter_weights"], [175, 1, 1, "", "freeze_base_model"], [175, 1, 1, "", "freeze_base_model_activation_quantizers"], [175, 1, 1, "", "freeze_base_model_param_quantizers"], [175, 1, 1, "", "get_fp_lora_layer"], [175, 1, 1, "", "get_quantized_lora_layer"], [175, 1, 1, "", "quantize_lora_scale_with_fixed_range"], [175, 1, 1, "", "set_bitwidth_for_lora_adapters"]], "aimet_torch.quant_analyzer": [[217, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[217, 1, 1, "", "analyze"], [217, 1, 1, "", "check_model_sensitivity_to_quantization"], [217, 1, 1, "", "export_per_layer_encoding_min_max_range"], [217, 1, 1, "", "export_per_layer_mse_loss"], [217, 1, 1, "", "export_per_layer_stats_histogram"], [217, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [217, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[157, 0, 1, "", "DequantizedTensor"], [158, 0, 1, "", "QuantizedTensor"], [159, 0, 1, "", "QuantizedTensorBase"]], "aimet_torch.quantization.DequantizedTensor": [[157, 1, 1, "", "dequantize"], [157, 1, 1, "", "quantize"], [157, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[158, 1, 1, "", "dequantize"], [158, 1, 1, "", "quantize"], [158, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[159, 1, 1, "", "clone"], [159, 1, 1, "", "dequantize"], [159, 1, 1, "", "detach"], [159, 1, 1, "", "new_empty"], [159, 1, 1, "", "quantize"], [159, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[160, 0, 1, "", "Quantize"], [161, 0, 1, "", "QuantizeDequantize"], [162, 2, 1, "", "dequantize"], [163, 2, 1, "", "quantize"], [164, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[160, 1, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[161, 1, 1, "", "forward"]], "aimet_torch.quantization.float": [[165, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[165, 4, 1, "", "bitwidth"], [165, 1, 1, "", "compute_encodings"], [165, 1, 1, "", "forward"], [165, 1, 1, "", "get_encodings"], [165, 1, 1, "", "get_extra_state"], [165, 1, 1, "", "is_bfloat16"], [165, 1, 1, "", "is_float16"], [165, 1, 1, "", "load_state_dict"], [165, 1, 1, "", "set_extra_state"]], "aimet_torch.seq_mse": [[237, 0, 1, "", "SeqMseParams"], [237, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[237, 1, 1, "", "forward_fn"], [237, 1, 1, "", "get_loss_fn"]], "aimet_torch.v1.adaround.adaround_weight": [[180, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[180, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[181, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[183, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[183, 1, 1, "", "analyze"], [183, 1, 1, "", "check_model_sensitivity_to_quantization"], [183, 1, 1, "", "export_per_layer_encoding_min_max_range"], [183, 1, 1, "", "export_per_layer_mse_loss"], [183, 1, 1, "", "export_per_layer_stats_histogram"], [183, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [183, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[184, 0, 1, "", "QuantizationSimModel"], [184, 1, 1, "", "load_checkpoint"], [184, 1, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[184, 1, 1, "", "compute_encodings"], [184, 1, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[185, 0, 1, "", "SeqMseParams"], [185, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[185, 1, 1, "", "forward_fn"], [185, 1, 1, "", "get_loss_fn"]], "aimet_torch.v2.mixed_precision": [[233, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[233, 1, 1, "", "apply"], [233, 1, 1, "", "set_model_input_precision"], [233, 1, 1, "", "set_model_output_precision"], [233, 1, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[169, 2, 1, "", "set_activation_quantizers_to_float"], [169, 2, 1, "", "set_blockwise_quantization_for_weights"], [169, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[215, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "property", "Python property"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:attribute", "4": "py:property"}, "terms": {"": [1, 2, 7, 8, 12, 20, 21, 22, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 165, 169, 171, 173, 174, 176, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 233, 237, 238, 242, 244, 245, 246, 247, 248, 250, 252, 253, 256, 257], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258], "00": [163, 164, 202, 203, 204, 220, 221], "000": [213, 247], "0000": [157, 164, 177], "0000e": [163, 164], "001": [187, 205, 219], "0014807": 240, "00183112e": 220, "00215936e": 220, "0030": 177, "0032": 177, "0034": 177, "00347728e": 221, "0035": 177, "0036": [161, 177], "0037": 177, "0038": 177, "0039": [161, 177], "0059": 177, "0063": 177, "0064": 177, "0068": 177, "0069": 177, "0073": 177, "0074": 177, "0078": 177, "00_224": [213, 220, 221], "01": [1, 11, 23, 163, 164, 180, 191, 193, 207, 213, 218, 220, 221], "0115": 161, "0117": 177, "0142": 159, "01457286e": [220, 221], "0156": 177, "0158": 161, "0176": 161, "0195": 177, "02": [163, 164, 220, 221], "02078857e": [220, 221], "0234": 177, "0235": 218, "026354755942277083": 240, "02635476": 240, "0273": 177, "0278355": 240, "02887694e": 221, "0293162": 240, "0295": 161, "03": [220, 221], "0312": 177, "0352": 177, "03798249e": 220, "0386": 161, "0391": 177, "04": [220, 221, 240, 243], "04025269e": 220, "0406616e": 221, "0424": 161, "0428": 219, "0430": 177, "0449": 243, "0469": 177, "0471": 161, "04721": 201, "05": [163, 164, 175, 220, 221], "0500e": [163, 164], "0508": 177, "05270951": 240, "0541903": 240, "0549": 161, "05549544e": [220, 221], "0564": 161, "0639": 161, "0667": 164, "0680": 161, "0769": 243, "0784": 161, "07906426": 240, "08": [163, 164], "080545": 240, "0819": 161, "0820258": 240, "0859": 165, "0861": 243, "08742931e": [220, 221], "0882": 159, "0889": 165, "0891": 165, "09": 231, "09111059e": 220, "0947": 165, "09685047e": [220, 221], "0x7f127685a598": 172, "0x7f9dd9bd90d0": 172, "0x7ff5703eff28": 172, "0x7ff577373598": 172, "1": [1, 2, 10, 12, 13, 17, 20, 21, 23, 24, 28, 29, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 195, 221, 222, 224, 225, 226, 228, 229, 233, 234, 235, 236, 240, 241, 242, 243, 244, 247, 258], "10": [17, 20, 28, 29, 157, 160, 161, 163, 171, 172, 174, 175, 177, 178, 184, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 208, 210, 211, 213, 219, 222, 224, 225, 226, 228, 231, 240, 241, 242, 243, 244, 245, 248, 250, 252], "100": [2, 12, 14, 25, 170, 181, 194, 208, 219, 231, 238, 240, 243], "1000": [8, 21, 159, 176, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 222, 226, 228, 245], "10000": [11, 187, 188, 192, 200, 205, 206, 213], "1000e": [163, 164], "102": [177, 243], "1024": [195, 213, 231, 245], "103": [160, 177], "104": 243, "105": 243, "1050": 243, "10541902": 240, "10569119e": [220, 221], "106": 160, "1060": 161, "1068997": 240, "107": 177, "10708203e": [220, 221], "108": 177, "109": 160, "10k": [1, 23, 180, 213], "11": [158, 159, 163, 175, 177, 204, 240, 241, 242, 252], "110": [160, 177], "111": [160, 177], "11176670e": 221, "112": [213, 220, 221], "1128": 161, "1155": 243, "116": 243, "1176": 161, "118": 177, "119": [177, 243], "12": [163, 177, 240, 241, 242, 243, 244, 252], "12039044e": 221, "121": 243, "122": 160, "1232": 161, "127": [158, 159, 177, 178, 243, 245, 248], "128": [2, 13, 24, 158, 159, 169, 171, 173, 177, 178, 181, 182, 195, 197, 198, 213, 218, 220, 221, 231, 240, 243, 244, 245, 248], "129": 160, "13": [163, 177, 243, 258], "1307": 161, "131": 160, "13177378": 240, "1333": 164, "1398": 225, "14": [163, 177, 243, 252, 258], "1406": 225, "141": 160, "143": 160, "144": 160, "145": 160, "1458": 243, "146": 160, "15": [163, 164, 177, 194, 195, 197, 198, 208, 210, 211, 219, 225, 248, 252], "150": 160, "1500e": [163, 164], "152": 160, "15259957e": 221, "153": 158, "155": 160, "15717569e": 220, "15812853": 240, "15e": [202, 204], "15k": [1, 23, 180, 213], "16": [2, 9, 12, 22, 29, 160, 165, 171, 173, 174, 175, 177, 178, 181, 184, 187, 191, 194, 205, 209, 213, 218, 226, 231, 235, 236, 240, 243, 244, 245, 248, 257, 258], "1619": 243, "162": 160, "16245179e": [220, 221], "1647": 161, "16839484e": 221, "16966406e": 221, "17": [160, 177, 221, 258], "1709": 161, "172": 160, "1727": 165, "1729": 165, "1741": 161, "178": 160, "17871511e": 221, "179": 160, "18": [177, 187, 188, 189, 190, 243, 258], "181": 160, "18136823e": 220, "18448329": 240, "186": 160, "18673885e": 220, "187": 160, "188": 160, "1889": 161, "19": [160, 177, 220, 221, 258], "1906": 201, "19186290e": [220, 221], "192": 160, "1921e": [163, 164], "194": 160, "1943": 225, "1945": 159, "1955": 225, "1977": 161, "19778645e": 220, "1_all": 242, "1e": [20, 175, 195, 196, 219, 220, 221, 245, 248], "1k": [213, 218, 219, 231, 237, 245], "1m": [188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 195, 214, 215, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 247, 250, 251, 254, 255, 256, 258], "20": [1, 10, 11, 20, 23, 159, 179, 180, 185, 193, 194, 195, 196, 197, 198, 208, 210, 211, 213, 219, 220, 221, 226, 235, 236, 237, 248, 258], "200": [20, 170, 195, 196, 207], "2000": [1, 23, 158, 164, 180, 188, 192, 193, 200, 206, 207, 209, 213], "20000": [20, 195, 196], "2000e": [163, 164], "2012": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "2014": 225, "2016": 225, "2017": 225, "2019": [189, 201, 209], "203": 160, "20433941e": [220, 221], "2048": [213, 218, 219, 248], "205": 158, "2068": 243, "207": 160, "20k": 195, "21": [159, 220, 221, 258], "21066449e": 220, "21083805": 240, "2118": 161, "2123188": 240, "21250950e": 220, "2137995": 240, "216": 160, "218": 160, "2196": 161, "22": [177, 220, 221, 240, 243, 258], "2205": 159, "2212": 161, "2217": 243, "22219264e": [220, 221], "224": [178, 187, 188, 189, 190, 191, 194, 197, 198, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 226, 231, 233, 237, 243, 245, 248], "225": [207, 213, 218, 231, 245], "22583652e": 220, "226": 160, "2260": 159, "2265": 161, "229": [207, 213, 218, 231, 245], "23": [158, 159, 258], "23011987e": [220, 221], "2315": 243, "23156421e": [220, 221], "2353": 161, "2355": 161, "2363": [161, 243], "237": 160, "23719281": 240, "238": 160, "24": 258, "240": 177, "2401543": 240, "241": 177, "242": 177, "24257803e": 220, "243": 177, "2431": 243, "244": 177, "245": 177, "2458": 161, "2459": 243, "246": 177, "247": 177, "248": 177, "249": 177, "25": [177, 258], "250": 177, "2500e": [163, 164], "251": 177, "252": 177, "253": 177, "254": 177, "2546": 161, "255": [157, 160, 177, 178, 245, 248], "256": [8, 177, 191, 205, 213, 217, 218, 231, 245], "2568": 161, "2592": 172, "26": [158, 177, 243, 258], "26179108e": 220, "26354757": 240, "2650282": 240, "2667": 164, "27": [243, 258], "27415752e": 221, "2771": 161, "28": [158, 159, 200, 222, 226, 228, 243, 258], "28238320e": 221, "288": [213, 221], "28990233": 240, "29": [177, 258], "291383": 240, "2921": 159, "2930528e": 221, "29590677e": [220, 221], "2b": 223, "2d": [200, 222, 225, 229, 244], "3": [2, 12, 13, 24, 28, 157, 159, 163, 164, 168, 170, 171, 172, 173, 175, 177, 178, 179, 181, 182, 185, 195, 221, 222, 223, 226, 228, 229, 233, 235, 240, 241, 242, 243, 244, 258], "30": [160, 177, 243, 258], "300": 205, "3000": [158, 195], "30122258e": [220, 221], "3038": 161, "30402938e": 221, "31": [1, 8, 11, 21, 23, 176, 178, 180, 183, 184, 213, 217, 243, 245, 248, 258], "31080866e": 220, "312": 158, "3137": 161, "31625706": 240, "32": [20, 160, 171, 172, 177, 178, 184, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 213, 217, 218, 219, 220, 221, 231, 237, 238, 243, 245, 246, 248, 250, 253, 258], "32141271e": 220, "3216": 161, "33": [160, 202, 203, 204, 243, 258], "3333": 164, "33731920e": [220, 221], "34": 258, "34215236e": 221, "34261182": 240, "3451": 161, "3467390e": 221, "34694423e": 221, "347054": 240, "3470540046691895": 240, "35": [178, 243, 245, 248, 258], "350m": [235, 236], "35139937e": 221, "36": 160, "3610": 243, "36896658": 240, "37": [159, 160, 177], "3734087606114667": 240, "37757687e": 221, "38": [158, 159, 225], "38100997e": [220, 221], "3861": 243, "39": 157, "39502389e": [220, 221], "3992": 161, "3d": 244, "4": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 190, 195, 199, 202, 203, 204, 207, 212, 214, 215, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 246, 247, 250, 251, 254, 255, 256, 258], "40": [158, 159], "4000": [157, 164], "406": [207, 213, 218, 231, 245], "41": 157, "41059163e": 221, "4157": 161, "41715762e": [220, 221], "42": [177, 243], "42083430e": 220, "4216761": 240, "4231569": 240, "4246376": 240, "42477691e": 220, "43": [177, 243], "4392": 161, "44408584e": [220, 221], "44632760e": 221, "4475": 161, "44803086": 240, "4495116": 240, "4499": 243, "44993666e": 221, "4549": 161, "455": [240, 243], "456": [207, 213, 218, 231, 245], "4585028e": 221, "46642041e": 221, "4667": 164, "4694": 161, "4706": 161, "47438562": 240, "4758663": 240, "4784": 161, "48": [177, 243], "48045555e": 221, "48399768e": [220, 221], "485": [207, 213, 218, 231, 245], "4863": 161, "49": [160, 177, 243], "49024737e": [220, 221], "4d": 244, "4f": [218, 219, 245, 248], "5": [159, 160, 161, 163, 164, 165, 170, 171, 174, 175, 177, 188, 189, 190, 191, 192, 195, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211, 213, 219, 220, 221, 222, 223, 226, 228, 231, 240, 241, 242, 243, 244], "50": [187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 223, 226, 231, 243], "500": [188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 206, 207, 209, 210, 211, 212, 213, 217, 222, 231, 245, 247], "5000": [193, 207], "50000": [191, 226, 231, 245], "5000e": [163, 164], "5006": 227, "50074035": 240, "5022211": 240, "51": 157, "512": [235, 236], "51446089e": [220, 221], "51547194e": [220, 221], "5157": 243, "51876003e": 220, "52": [157, 243], "521": 157, "5220": 161, "5255": 161, "52709514": 240, "52912678e": [220, 221], "52974629e": 220, "5301": 243, "5307": 243, "5333": [161, 164], "53403008e": [220, 221], "53976866e": [220, 221], "54": 177, "54620111e": [220, 221], "55": [160, 177, 243], "55344987": 240, "5540": 161, "5549306": 240, "55578518e": 221, "56": 243, "56632766e": 220, "5695": 161, "57": [158, 159, 177], "57021021e": 221, "57980466": 240, "57984132e": [220, 221], "58": 243, "5856506e": 221, "5876": 219, "5897": 161, "59": [160, 177, 243], "5962": 243, "59643137e": 220, "5e": [203, 204, 208, 210, 211], "6": [20, 159, 163, 164, 171, 175, 177, 194, 195, 196, 197, 198, 204, 206, 209, 210, 211, 220, 221, 231, 241, 242, 244, 245, 248, 252], "60": 177, "6000": [157, 158, 159, 164], "6039": 161, "6054": 161, "6061594": 240, "60713138e": [220, 221], "6079": 161, "6091": 243, "61": 177, "61087702e": 220, "6177": 161, "6196": 161, "62": 177, "6213797e": 221, "6247": 161, "62498877e": [220, 221], "63": [160, 243], "63172388e": 220, "6325141": 240, "6327": 243, "6340": 243, "64": [7, 8, 20, 160, 169, 177, 178, 200, 213, 217, 218, 231, 240, 243, 244, 245, 248, 250], "6431": 161, "64579314e": [220, 221], "65": [223, 243], "65535": 243, "6583": 248, "6588689": 240, "66": [160, 202, 203, 204, 223, 243], "6603496": 240, "6667": 164, "6695": 161, "67278278e": [220, 221], "67416465e": 221, "67510852e": [220, 221], "67677957e": [220, 221], "68": [157, 243], "68016": 248, "68522364": 240, "6867044": 240, "69": 243, "6910": 248, "69716495e": 221, "7": [159, 163, 164, 165, 175, 177, 178, 203, 204, 206, 208, 209, 210, 211, 220, 221, 229, 241, 242, 245, 248, 252], "70": 177, "7013": 245, "70130579e": 221, "70838": 248, "71": 159, "7115784": 240, "7164": 218, "71659231e": [220, 221], "7173": 245, "72": 243, "72468403e": 221, "73242160e": 220, "7333": 164, "73793316": 240, "74": 160, "74478185e": 220, "7466": 243, "75": [223, 243], "75162792e": 221, "75700430e": [220, 221], "76": 177, "76428795": 240, "77": [177, 243], "77213307e": 220, "7741": 161, "7765": 161, "7788": 243, "7793": 243, "78": 243, "7894": 161, "79": 243, "7932": 161, "8": [1, 2, 7, 8, 9, 12, 13, 21, 22, 23, 24, 29, 157, 158, 159, 160, 161, 163, 164, 165, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 226, 228, 229, 231, 233, 237, 238, 240, 241, 242, 243, 244, 245, 248], "80": [160, 226, 243], "8000": [161, 164], "80053532e": 220, "8078": 161, "81": 243, "81699747": 240, "81760912e": 220, "8188": 243, "81884710e": 220, "8229": 161, "83": [160, 177, 243], "83640555e": 221, "8366": 243, "83861901e": [220, 221], "84": 177, "8405": 243, "8433522": 240, "8442": 243, "85": 243, "86": [160, 243], "8606": 243, "864": [213, 220, 221], "8667": 164, "86945379e": 220, "8706": 161, "8711877": 240, "87471542e": [220, 221], "87630311e": [220, 221], "87656835e": 220, "8796": 161, "88": 243, "88260233e": [220, 221], "8836": 161, "88373709e": 221, "89": [160, 243], "89074164e": 221, "89249423e": [220, 221], "89348674e": 220, "8984": 165, "8994": 165, "8998": 165, "8bit": 252, "9": [157, 163, 164, 171, 175, 177, 202, 203, 204, 205, 206, 209, 210, 211, 212, 219, 220, 221, 226, 231, 241, 242, 252], "90": [231, 243], "90229788e": [220, 221], "9054": 243, "91": [177, 243], "9150": 243, "9157": 161, "9176": 161, "92": 177, "9216": 171, "92511864e": [220, 221], "93": [177, 243], "93232973e": [220, 221], "9333": 164, "93787616e": 220, "94": [160, 177], "9487": 161, "94877124": 240, "9490": 161, "95": 177, "95088911e": [220, 221], "95260113e": 220, "9570": 177, "95997976e": [220, 221], "96": [177, 243], "9609": 177, "96155685e": 220, "9648": 177, "9688": 177, "97": 157, "9700": 161, "9714": 243, "9727": 177, "97294299e": [220, 221], "9766": 177, "9805": 177, "9826": 243, "9844": 177, "9883": 177, "99": 243, "9904": 243, "9922": 177, "9961": [161, 177], "A": [1, 2, 3, 8, 9, 12, 13, 14, 15, 17, 21, 22, 23, 24, 26, 27, 28, 169, 173, 175, 176, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 224, 225, 226, 228, 231, 233, 235, 238, 244, 245, 246, 247, 248, 256], "And": [187, 200, 205, 256], "As": [29, 187, 189, 191, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 208, 209, 210, 211, 213, 217, 218, 223, 224, 231, 241, 244, 251], "At": [223, 227], "But": [171, 187, 194, 195, 199, 200, 205, 208, 212, 225, 233], "By": [9, 13, 17, 20, 28, 174, 175, 178, 194, 205, 206, 208, 209, 210, 211, 213, 218, 222, 225, 226, 228, 245, 247, 248], "For": [8, 20, 21, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 170, 171, 172, 174, 175, 176, 177, 183, 186, 187, 191, 194, 195, 196, 199, 200, 205, 208, 212, 213, 216, 217, 218, 221, 222, 223, 224, 225, 231, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 256, 257], "If": [1, 2, 8, 9, 12, 13, 17, 20, 21, 22, 23, 24, 28, 29, 99, 160, 161, 162, 163, 164, 165, 166, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 222, 223, 224, 226, 227, 228, 229, 231, 233, 235, 237, 242, 244, 245, 246, 248, 249, 250, 251, 252, 253, 257], "In": [20, 29, 159, 170, 171, 172, 174, 178, 181, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 223, 224, 225, 227, 229, 231, 232, 235, 236, 242, 243, 244, 245, 248, 249, 250, 251, 255, 256], "It": [1, 2, 6, 8, 9, 10, 20, 170, 171, 178, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 213, 214, 216, 217, 218, 222, 224, 229, 231, 237, 242, 245, 246, 248, 252], "Its": 229, "NOT": [13, 24, 182, 200, 218], "No": [172, 220, 256], "Not": [8, 21, 175, 176, 183, 186, 187, 194, 199, 200, 208, 212, 217, 219, 224, 237, 244, 248], "ONE": 240, "OR": 170, "Of": [197, 198, 200, 202, 203, 204, 210, 211], "On": [187, 200, 205], "One": [21, 22, 188, 189, 190, 191, 199, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 217, 245, 247, 248], "Or": [1, 22, 171, 184, 213, 216, 217, 225, 245, 248], "Such": 171, "That": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 229], "The": [1, 2, 4, 6, 7, 8, 9, 12, 14, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 163, 164, 165, 167, 168, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 255, 256, 257], "Then": [8, 176, 183, 210, 211, 217, 225], "There": [168, 170, 172, 184, 187, 188, 191, 192, 194, 195, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 216, 217, 218, 248, 256, 257], "These": [1, 13, 22, 24, 170, 172, 174, 182, 184, 187, 189, 193, 195, 200, 205, 207, 209, 213, 217, 218, 222, 223, 231, 235, 236, 242, 245, 247, 248, 249, 253], "To": [2, 12, 20, 29, 170, 173, 174, 175, 181, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 219, 220, 221, 224, 226, 227, 231, 233, 237, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 253, 256, 257], "With": [157, 196, 199, 212, 240, 248], "_": [8, 9, 20, 21, 29, 160, 161, 164, 167, 174, 176, 177, 178, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 231, 235, 237, 239, 240, 244, 245, 246, 247, 248, 252, 255, 256, 257], "__________________________________________________________________________________________________": [213, 220, 221], "__getitem__": 207, "__init__": [13, 20, 24, 171, 172, 174, 182, 188, 196, 207, 213, 218, 231, 245, 250, 251, 252], "__iter__": [188, 213, 218, 231, 245], "__len__": [8, 188, 207, 217], "__next__": [188, 213, 218, 231, 245], "__quant_init__": [29, 174], "__torch_function__": 171, "__version__": 242, "_batch_index": 218, "_create_sampled_data_load": 207, "_current_iter": [213, 231, 245], "_data": [213, 218, 231, 245], "_dataset": 207, "_default_forward_fn": [13, 24, 182, 218], "_encodingmismatchinfo": 245, "_get_unlabled_data_load": 217, "_is_encoding_frozen": 170, "_iter": 188, "_max": 165, "_module_to_wrap": 170, "_not_specifi": [9, 178, 245, 248], "_quantizationsimmodelinterfac": [176, 181, 183, 184, 217], "_quantschemepair": [13, 24, 182, 218], "_remove_input_quant": 170, "_remove_output_quant": 170, "_remove_param_quant": 170, "_step": [163, 164], "_tie_quant": 252, "_torch_data_load": 188, "_unlabel": [213, 231, 245], "ab": 201, "abil": [252, 257], "abl": [8, 20, 24, 157, 158, 159, 171, 172, 182, 188, 189, 190, 192, 193, 194, 196, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 217, 218], "about": [157, 188, 189, 190, 191, 192, 193, 196, 197, 198, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 213, 221, 239, 243, 244, 246, 257], "abov": [171, 173, 187, 191, 199, 200, 202, 203, 204, 205, 212, 217, 222, 225, 226, 227, 228, 229, 233, 241, 242, 244, 247, 248], "absolut": [160, 161, 222, 226, 228, 231, 241], "absolute_path_to_workspac": [186, 241], "absorpt": [189, 201, 209], "abstract": [29, 159, 174], "acc": [193, 194, 195, 199, 217, 218], "acc_top1": 218, "acccuraci": 191, "acceler": [190, 195, 197, 198, 210, 211, 222, 225, 226, 228, 238, 252], "accept": [224, 231, 252, 253, 256, 257], "access": [170, 195, 205, 206, 209, 210, 211], "accord": [9, 168, 178, 216, 245, 248], "accordingli": 252, "account": [224, 252, 253], "accumul": [223, 225, 226, 228, 230], "accur": [195, 213, 220], "accuraci": [1, 2, 12, 13, 17, 22, 23, 24, 28, 175, 180, 181, 182, 194, 195, 199, 207, 208, 212, 213, 214, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 237, 238, 239, 244, 245, 247, 248, 252, 253], "achiev": [177, 188, 192, 200, 206, 223, 232, 233, 238, 256, 257], "acronym": 239, "across": [171, 175, 217, 221, 230, 238, 240, 247], "act": [2, 8, 12, 21, 173, 176, 181, 183, 187, 217, 231], "act_bw": 209, "action": [217, 229], "activ": [2, 8, 9, 12, 15, 20, 21, 22, 167, 169, 171, 173, 174, 175, 176, 179, 181, 183, 185, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 214, 215, 220, 221, 230, 231, 232, 233, 234, 235, 236, 237, 238, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256, 257], "activation_bitwidth": [213, 245, 246, 248], "activation_encod": 246, "activation_quant": [2, 231], "activations_pdf": [199, 212, 217], "actual": [187, 191, 194, 196, 205, 208, 214, 217], "acuraci": [191, 231], "ad": [172, 175, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 234, 245, 246, 249, 252], "ada_model": [188, 192, 200, 206], "ada_round_data": [192, 200], "ada_rounded_model": 213, "adam": [193, 194, 195, 197, 198, 199, 217], "adamw": [235, 236], "adapater_name_to_meta_data": 175, "adapt": [23, 25, 175, 179, 180, 185, 186, 194, 200, 208, 217, 218, 219, 234, 235, 236, 237, 238, 252], "adapter1": 175, "adapter1_weight": 175, "adapter_weights_path": 175, "adaptermetadata": 175, "adaptiveavgpool1d": 30, "adaptiveavgpool2d": 31, "adaptiveavgpool3d": 32, "adaptivemaxpool1d": 33, "adaptivemaxpool2d": 34, "adaptivemaxpool3d": 35, "adaptiveround": 252, "adaround": [5, 13, 18, 24, 166, 170, 182, 186, 193, 207, 213, 217, 218, 238, 252, 253], "adaround_data_load": [207, 218], "adaround_dataset": 193, "adaround_dataset_s": [193, 207, 218], "adaround_param": [13, 24, 182, 193, 207, 218], "adaround_weight": [1, 11, 23, 170, 180, 188, 192, 193, 200, 206, 207, 213, 218], "adarounded_model": 213, "adaroundparamet": [1, 11, 13, 23, 24, 180, 182, 188, 192, 193, 200, 206, 207, 213, 218], "add": [20, 22, 171, 172, 174, 175, 184, 190, 194, 197, 198, 202, 203, 204, 210, 211, 229, 245, 246, 247, 248, 249, 250, 251, 252, 256, 257], "add_check": 172, "add_lora_to_r": 236, "addit": [29, 177, 178, 184, 187, 194, 195, 196, 199, 200, 205, 208, 212, 219, 244, 245, 246, 248, 249, 252, 255], "address": [227, 253], "adequ": 256, "adher": 257, "adjac": [187, 188, 189, 190, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211, 220, 221, 230, 238, 249], "adjust": [167, 201, 209, 213, 215, 222, 223, 230, 235, 236, 244, 253], "admin": 186, "advanc": [177, 238, 252], "advantag": 237, "affect": [1, 23, 180, 213, 238, 244, 249, 257], "affin": [7, 29, 157, 158, 159, 160, 161, 162, 163, 164, 169, 174, 175, 220, 221, 235, 243, 244], "affine_q": 170, "affine_qdq": 170, "affine_quant": 170, "affinequant": [165, 170], "after": [8, 21, 165, 171, 172, 174, 176, 177, 183, 184, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 217, 218, 219, 220, 221, 223, 225, 227, 238, 248, 252, 253], "again": [189, 200, 201, 202, 203, 204, 209, 231, 248, 257], "against": [8, 13, 21, 176, 183, 197, 198, 200, 208, 210, 211, 217, 218, 222], "aggress": 238, "agre": 257, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258], "aim": [194, 202, 203, 204, 208, 257], "aimet": [5, 17, 18, 20, 28, 29, 170, 171, 172, 174, 175, 177, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 230, 231, 232, 233, 237, 238, 240, 244, 245, 246, 248, 250, 251, 252, 253, 255, 257], "aimet_cl": 201, "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "aimet_common_def": 226, "aimet_export_artifact": 216, "aimet_exported_model": 255, "aimet_exported_model_path": 255, "aimet_onnx": [187, 188, 189, 190, 213, 216, 217, 220, 221, 231, 237, 240, 242, 245, 246], "aimet_spatial_svd": 226, "aimet_tensorflow": [191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 213, 216, 217, 218, 219, 220, 221, 226, 231, 240, 242, 245, 246, 248], "aimet_tensorflow_def": 226, "aimet_torch": [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 187, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 240, 241, 242, 243, 244, 245, 246, 248, 252], "aimet_vari": 241, "algo": [181, 231], "algorithm": [2, 12, 17, 28, 181, 222, 223, 224, 225, 226, 228, 229, 246, 253], "alia": [9, 22, 178, 184, 245, 248], "aliasbackward0": [157, 158, 159, 160, 161, 177, 243], "align": [245, 249, 252], "all": [1, 2, 3, 7, 8, 9, 12, 14, 15, 20, 21, 22, 26, 29, 99, 166, 167, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 220, 221, 222, 224, 225, 226, 228, 230, 231, 233, 234, 235, 236, 237, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 253, 256], "all_q_modul": 170, "all_quant_wrapp": 170, "allclos": 171, "allow": [2, 12, 13, 17, 20, 22, 24, 28, 157, 158, 159, 169, 171, 181, 182, 187, 191, 193, 195, 205, 207, 214, 218, 222, 225, 226, 227, 228, 230, 231, 232, 244, 245, 246, 248, 251, 252, 253], "allow_custom_downsample_op": [28, 202, 204, 222], "allow_overwrit": [170, 213, 236, 245, 248], "allowed_accuracy_drop": [2, 12, 13, 24, 181, 182, 187, 191, 193, 205, 207, 218, 231], "alon": [175, 247], "along": [158, 175, 177, 231, 247], "alpha": [175, 231], "alphadropout": 36, "alreadi": [27, 29, 177, 178, 184, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 221, 235, 241, 245, 248, 253], "also": [1, 11, 13, 22, 23, 159, 171, 177, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 220, 222, 225, 229, 231, 233, 240, 242, 245, 246, 248, 249, 251, 252, 255, 256], "alter": [188, 192, 200, 206], "altern": [8, 176, 183, 187, 191, 202, 203, 204, 205, 217, 242, 244], "alwai": [224, 235, 236], "am": [8, 21, 176, 183, 217], "among": [1, 13, 23, 24, 180, 182, 213, 218, 256], "amount": [199, 212, 231, 237], "amp": [2, 12, 13, 24, 166, 173, 181, 182, 186, 218, 231, 232, 252], "amp_search_algo": [2, 12, 181, 187, 205, 231], "ampsearchalgo": [2, 12, 181, 187, 205, 231], "an": [1, 8, 12, 13, 17, 20, 22, 24, 28, 29, 157, 158, 159, 165, 167, 169, 170, 171, 172, 174, 177, 178, 182, 184, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 222, 223, 224, 225, 226, 228, 229, 231, 238, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257], "analys": [199, 212, 217], "analysi": [2, 8, 12, 17, 21, 28, 176, 181, 183, 187, 191, 195, 202, 203, 204, 205, 222, 225, 226, 228, 252], "analyt": 227, "analyz": [8, 17, 21, 28, 160, 161, 174, 176, 183, 186, 193, 207, 218, 222, 226, 227, 228, 230, 233, 240, 252], "anchor": [147, 148], "andrea": 225, "andrei": 225, "andrew": 225, "ani": [2, 8, 9, 12, 17, 20, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 171, 172, 173, 176, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 222, 226, 228, 231, 233, 235, 237, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 252, 256], "anneal": [1, 11, 23, 180, 213], "anoth": [175, 178, 184, 195, 228, 229, 238, 245, 246, 248, 249], "any_nam": 241, "any_tag": 241, "anyth": [2, 12, 181, 187, 191, 199, 205, 212, 231], "api": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 167, 168, 169, 170, 173, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 214, 227, 235, 236, 238, 241, 246, 250, 251, 252, 255], "appear": [17, 28, 171, 172, 222, 226, 228], "append": 241, "appli": [2, 7, 8, 13, 16, 24, 29, 99, 160, 161, 162, 163, 164, 169, 172, 173, 174, 179, 182, 185, 186, 187, 194, 195, 202, 203, 204, 205, 207, 208, 213, 217, 218, 220, 221, 225, 227, 230, 231, 235, 237, 238, 239, 240, 244, 245, 247, 248, 249, 252, 253, 256, 257], "applic": [2, 12, 181, 187, 188, 191, 192, 193, 197, 198, 199, 200, 201, 205, 206, 209, 213, 217, 218, 219, 220, 221, 226, 231, 245, 248, 255, 257], "apply_adaround": [1, 11, 23, 180, 188, 192, 200, 206, 213], "apply_seq_algo": [10, 237], "apply_seq_ms": [10, 170, 179, 185, 237], "approach": [173, 233, 235, 247], "appropri": [23, 174, 178, 180, 181, 184, 187, 191, 194, 195, 199, 200, 205, 208, 212, 213, 224, 231, 235, 236, 240, 245, 247, 248, 253], "approx": 247, "approxim": [8, 217, 223, 237, 247, 257], "apt": [240, 242], "ar": [1, 2, 8, 9, 10, 11, 12, 17, 20, 21, 22, 23, 24, 28, 29, 99, 160, 161, 165, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257], "arang": [163, 164, 171, 177], "arbitrari": 244, "architectur": [196, 238, 240, 242, 252], "archiv": 242, "area": [217, 230], "arg": [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 169, 173, 174, 178, 196, 231, 233, 244, 245, 248, 252], "argmax": [213, 218, 231, 245, 248], "argument": [1, 2, 9, 12, 13, 22, 23, 24, 29, 165, 169, 171, 172, 173, 176, 178, 179, 180, 181, 182, 183, 184, 185, 193, 194, 199, 212, 213, 216, 217, 218, 231, 237, 244, 245, 246, 248, 255], "around": [195, 200, 217, 252], "arrai": [169, 188, 244, 246], "arrang": 227, "art": [187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212], "arthmet": [20, 196], "artifact": [9, 173, 195, 232, 233, 241, 245, 252, 255, 256], "arxiv": 201, "asic": 238, "ask": [29, 187, 191, 194, 195, 199, 205, 208, 212], "assert": [170, 171, 177, 196, 205, 245], "assert_array_equ": 196, "assess": 224, "assign": [29, 160, 161, 165, 174, 175, 197, 198, 231, 241, 244], "associ": [2, 12, 13, 15, 24, 29, 159, 172, 173, 174, 177, 181, 182, 199, 212, 218, 220, 231, 255], "assum": [7, 13, 24, 169, 182, 218, 224, 242, 244], "astyp": 217, "asym": [10, 179, 185, 237], "asymmetr": [160, 161, 177, 247, 249], "att": [20, 196], "attempt": [9, 221, 245], "attent": [20, 195, 196], "attention_mask": [235, 236], "attn_output": [20, 196], "attribut": [12, 20, 29, 170, 171, 173, 174, 175, 177, 181, 200, 217, 231], "augment": 227, "auto": [9, 17, 28, 187, 191, 202, 203, 204, 205, 222, 226, 228, 231, 232, 245, 246], "auto_param": [202, 203, 204, 222, 226, 228], "auto_qu": [193, 207, 218], "auto_quant_v2": [18, 218], "autoconfig": [235, 236], "autograd": [157, 158, 159], "autom": [171, 205, 206, 209, 210, 211, 212, 238, 251], "automat": [13, 17, 24, 28, 173, 182, 186, 193, 207, 217, 222, 223, 225, 226, 228, 238, 241, 242, 244, 252], "automodelforcausallm": [235, 236], "automodeparam": [17, 28, 202, 203, 204, 222, 226, 228], "autoqu": [5, 13, 24, 166, 182, 186, 218, 238, 252], "autoquantwithautomixedprecis": [13, 24, 182, 218], "autotoken": [235, 236], "avail": [10, 171, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 194, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 217, 237, 240, 249, 252, 253, 254, 255], "avgpool1d": 37, "avgpool2d": [38, 233], "avgpool3d": 39, "avoid": [8, 22, 176, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 213, 217, 223, 238, 245, 248, 250, 251, 252], "awai": 200, "awar": [29, 186, 213, 219, 225, 230, 237, 238, 247, 253, 257], "axi": [177, 199, 212, 213, 217, 218, 231, 245, 246, 247], "b": [29, 160, 161, 162, 163, 164, 175, 177, 191, 226, 231, 244], "b_": [160, 161, 162, 163, 164], "b_0": [160, 161, 162, 163, 164], "b_1": [160, 161, 162, 163, 164, 244], "b_2": 244, "b_d": [160, 161, 162, 163, 164], "b_n": 244, "back": [2, 12, 157, 158, 173, 176, 181, 183, 217, 231, 247, 248, 249], "backend": [9, 178, 184, 194, 232, 245, 248, 255, 256], "backpropag": [157, 158, 159], "backslash": 252, "backward": [166, 219, 220, 235, 236, 245, 248, 252], "bad": 20, "balanc": [225, 238, 257], "bandwidth": 223, "base": [2, 8, 21, 29, 159, 160, 161, 163, 164, 165, 170, 174, 175, 176, 177, 181, 183, 187, 188, 191, 192, 193, 194, 200, 205, 206, 207, 208, 217, 222, 223, 224, 227, 231, 232, 234, 236, 237, 240, 242, 244, 245, 248, 252, 255, 256], "base_encod": 175, "base_model": 175, "baselin": [2, 12, 181, 194, 195, 207, 224, 225, 231, 248], "bash": 241, "basi": [200, 252], "basic": [187, 191, 194, 195, 205, 208, 231, 244, 248, 252, 255], "batch": [1, 2, 8, 10, 11, 13, 14, 15, 21, 23, 24, 25, 165, 173, 179, 180, 181, 182, 185, 191, 193, 194, 202, 203, 204, 208, 213, 217, 218, 221, 226, 231, 235, 236, 237, 238, 245, 248, 252], "batch_cntr": [187, 188, 189, 190, 192, 197, 198, 200, 201, 206, 208, 209, 210, 211, 212, 218], "batch_data": [213, 231, 245], "batch_id": [235, 236], "batch_norm": [3, 14, 15, 26, 219, 220], "batch_norm_fold": [5, 14, 18, 166, 187, 188, 189, 190, 191, 192, 194, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 219, 220, 231, 248], "batch_siz": [1, 23, 180, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 226, 231, 235, 236, 237, 245, 248], "batchnorm": [13, 15, 20, 24, 25, 182, 186, 187, 188, 189, 190, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211, 213, 218, 219, 220, 221, 229, 252], "batchnorm1d": [26, 42, 220], "batchnorm2d": [26, 43, 172, 220, 221], "batchnorm3d": 44, "batchnrom": 208, "bc": 189, "bc_param": 209, "bceloss": 40, "bcewithlogitsloss": 41, "becaus": [20, 171, 196, 208, 244, 256], "becom": 252, "becuas": [20, 196, 216], "been": [20, 29, 157, 167, 172, 193, 194, 195, 200, 207, 215, 229, 235, 238, 242, 243, 246, 247, 252, 255], "befor": [1, 2, 8, 9, 20, 29, 167, 170, 174, 177, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 215, 216, 217, 218, 219, 220, 221, 225, 231, 234, 235, 236, 245, 247, 248, 250, 252, 256], "begin": [20, 160, 161, 162, 163, 164, 171, 172, 196, 239], "behav": [29, 99, 174, 252, 253], "behavior": [170, 171, 174, 187, 200, 205, 206, 209, 210, 211, 220, 239, 247, 249, 252, 256], "behind": 256, "being": [12, 28, 170, 171, 172, 176, 183, 217, 219, 222, 226, 228, 231, 244, 246, 250, 251, 252], "below": [20, 29, 160, 161, 163, 164, 170, 173, 174, 175, 177, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 216, 222, 224, 225, 227, 231, 233, 235, 236, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 253], "benefici": [187, 191, 194, 195, 199, 200, 205, 212], "benefit": [232, 244, 246, 247], "bert": 253, "besid": 244, "bespok": 235, "best": [13, 24, 173, 182, 193, 200, 207, 218, 223, 225, 230, 233, 240, 247], "beta": [1, 11, 23, 180, 213, 252], "better": [188, 192, 197, 198, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 218, 225, 244, 248, 256], "between": [1, 2, 8, 11, 17, 21, 23, 28, 168, 170, 173, 174, 176, 180, 181, 183, 187, 200, 205, 213, 214, 216, 217, 222, 226, 228, 230, 231, 237, 238, 244, 245, 247, 249, 257], "bfloat16": 165, "bia": [29, 165, 170, 171, 172, 174, 175, 178, 189, 194, 201, 220, 221, 222, 243, 245, 248, 249, 252], "bias": [187, 189, 201, 205, 209, 220], "bias_correct": 209, "bilinear": 45, "billion": [256, 257], "bin": [175, 241, 242, 255], "binari": [2, 12, 181, 187, 205, 231, 255], "binary_classifi": 20, "binary_fil": 255, "binary_file_nam": 255, "bit": [1, 2, 8, 12, 21, 23, 165, 169, 176, 177, 180, 181, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 217, 230, 232, 235, 238, 240, 243, 244, 246, 247, 252, 253, 256, 257], "bitop": [2, 12, 181, 231], "bitwidth": [1, 2, 7, 8, 9, 11, 12, 13, 21, 22, 23, 24, 157, 158, 159, 160, 161, 163, 164, 165, 169, 170, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 187, 188, 191, 192, 200, 205, 206, 213, 217, 218, 221, 231, 232, 235, 238, 240, 244, 245, 246, 247, 248, 256], "biwidth": [188, 192, 200, 206], "blankevoort": 225, "block": [7, 24, 160, 161, 162, 163, 164, 169, 170, 182, 195, 218, 235, 236, 241, 246, 247, 252], "block_group": [169, 244], "block_siz": [7, 160, 161, 162, 163, 164, 169, 177, 244, 246], "blockwis": [7, 169, 177, 246, 247, 252], "bn": [3, 14, 15, 25, 26, 187, 188, 189, 190, 192, 194, 197, 198, 200, 201, 205, 206, 209, 210, 211, 217, 219, 220, 230, 238, 252], "bn1": [20, 172], "bn2": 20, "bn_conv1": [213, 220, 221], "bn_num_batch": [14, 219], "bn_re_estimation_dataset": [14, 219], "bn_reestim": [166, 194, 208, 219], "bnf": [213, 219, 220, 221], "bokeh": 231, "bool": [1, 2, 9, 12, 13, 17, 22, 24, 28, 160, 161, 163, 164, 165, 169, 171, 172, 173, 175, 178, 181, 182, 184, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 218, 222, 226, 228, 231, 233, 244, 245, 246, 248], "boolean": [29, 173, 233], "both": [20, 21, 163, 164, 170, 171, 174, 176, 183, 187, 191, 198, 204, 205, 210, 211, 217, 220, 225, 226, 227, 229, 231, 234, 237, 240, 243, 245, 247, 248, 249, 253, 255, 257], "bottleneck": 253, "box": 252, "bq": [169, 244, 252], "branch": [171, 186, 249], "break": [187, 188, 189, 190, 191, 192, 197, 198, 200, 201, 206, 208, 209, 210, 211, 212, 218, 226, 231, 235, 236, 245, 248, 252], "bridg": 232, "british": 225, "broken": [234, 246], "browser": 186, "bruteforc": [12, 187, 205, 231], "buffer": 165, "bug": [246, 252], "bugfix": 252, "build": [20, 170, 196, 242], "built": [2, 29, 173, 174, 181, 194, 195, 231, 240], "bw": [2, 7, 12, 24, 169, 173, 175, 178, 181, 182, 184, 218, 231, 244, 245, 246, 248, 251], "bw_output": 251, "c": [177, 223, 242], "c_": 177, "cach": [2, 12, 13, 24, 181, 182, 187, 191, 205, 218, 231, 242, 252], "cache_id": [13, 24, 182, 218], "calcul": [2, 12, 21, 22, 24, 174, 181, 182, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 212, 217, 218, 224, 231, 234, 235, 245, 247, 248, 252], "calculate_and_fuse_encodings_into_weight": 235, "calibr": [8, 21, 160, 161, 167, 174, 176, 178, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 214, 215, 231, 232, 235, 237, 238, 243, 244, 247, 252, 253, 256], "calibration_batch": [231, 248], "calibration_callback": [235, 236], "calibration_data_load": [174, 245], "calibration_data_s": 245, "calibration_dataset": [213, 219, 245, 248], "calibration_dataset_s": [193, 207, 218], "call": [2, 8, 10, 12, 13, 14, 20, 24, 29, 99, 157, 158, 159, 165, 171, 173, 174, 176, 177, 181, 182, 183, 188, 189, 190, 192, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 209, 210, 211, 212, 217, 218, 219, 222, 226, 228, 231, 233, 237, 244, 245, 250, 252, 255, 256, 257], "call_funct": 171, "callabl": [1, 2, 8, 9, 12, 13, 17, 23, 24, 25, 28, 29, 169, 171, 172, 173, 176, 178, 179, 180, 181, 182, 183, 185, 213, 217, 218, 219, 222, 226, 228, 231, 237, 244, 245, 248], "callal": [2, 181, 231], "callback": [2, 8, 12, 13, 17, 21, 22, 28, 173, 176, 181, 183, 184, 191, 193, 195, 197, 198, 199, 202, 203, 204, 212, 218, 222, 226, 228, 231, 235, 237, 247, 248], "callbackfunc": [2, 12, 21, 173, 176, 181, 183, 187, 191, 199, 205, 212, 217, 231], "callbak": [187, 205], "can": [1, 2, 6, 8, 9, 11, 12, 19, 20, 22, 23, 26, 27, 157, 159, 160, 161, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 237, 240, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257], "candid": [2, 10, 12, 13, 17, 24, 28, 173, 179, 181, 182, 185, 187, 191, 205, 218, 222, 224, 225, 226, 228, 231, 237, 252], "cannot": [160, 161, 171, 172, 187, 188, 189, 190, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211], "capabl": [174, 227, 240, 243], "captur": [6, 19, 168, 171, 178, 188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212, 214, 216, 245, 248], "card": [240, 243], "care": 225, "carefulli": 238, "carri": [157, 158, 159], "case": [163, 164, 169, 170, 171, 172, 174, 175, 195, 196, 199, 212, 213, 217, 224, 233, 234, 243, 244, 249, 251, 252, 257], "cast": 165, "cat": [177, 242], "categor": [191, 192, 197, 198, 200, 213, 218, 219, 226, 231, 245, 248], "categorical_crossentropi": [197, 198], "categoricalaccuraci": [193, 194, 199, 213, 217, 218, 219, 245, 248], "categoricalcrossentropi": [193, 194, 199, 213, 217, 218, 219, 245, 248], "caus": [20, 202, 203, 204, 225, 249, 253], "caution": 213, "cd": 186, "cdot": [160, 161, 162, 163, 164], "ceil": [1, 23, 180, 213, 218, 231, 245], "cell": [187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "celu": 46, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "center_crop": [191, 231], "centercrop": [205, 207, 213, 218, 231, 245], "certain": [171, 176, 183, 205, 206, 209, 210, 211, 212, 217, 225, 231, 233, 246, 249, 251, 252], "chain": [235, 236], "challeng": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "chang": [2, 12, 159, 167, 170, 171, 175, 177, 178, 181, 187, 188, 189, 190, 191, 194, 195, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 215, 217, 219, 220, 221, 225, 229, 231, 232, 233, 237, 245, 246, 248, 249, 251, 252, 256], "channel": [7, 8, 17, 21, 28, 169, 174, 175, 176, 183, 186, 187, 188, 189, 190, 201, 203, 205, 206, 208, 209, 210, 211, 212, 217, 219, 220, 221, 223, 224, 226, 228, 229, 230, 238, 244, 246, 247, 249, 252, 253], "channel_index": [8, 21, 176, 183, 217], "channel_index_0": [199, 212], "channel_index_1": [199, 212], "channel_index_n": [199, 212], "channel_prun": [28, 202, 204, 222, 226, 228], "channel_pruning_auto_mod": 222, "channel_pruning_manual_mod": 222, "channelpruningparamet": [28, 202, 204, 222, 226, 228], "channelshuffl": 48, "characterist": [187, 205, 208, 212], "chart": [221, 253], "check": [20, 24, 171, 172, 177, 182, 187, 191, 194, 195, 199, 205, 208, 212, 214, 216, 218, 241, 251], "check_model_sensitivity_to_quant": [8, 21, 176, 183, 217], "checker": [172, 252], "checkpoint": [179, 184, 185, 237], "checkpoints_config": [179, 185, 237], "chipset": 256, "choic": [199, 200, 212, 217, 225, 240, 241, 246, 247], "choos": [184, 187, 191, 202, 203, 204, 205, 213, 216, 222, 223, 225, 231, 238, 242, 249, 252], "choose_fast_mixed_precis": [12, 191, 231], "choose_mixed_precis": [2, 12, 173, 181, 187, 191, 205, 231], "chosen": [186, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 241], "chunk": 247, "cin": 177, "circularpad1d": 49, "circularpad2d": 50, "circularpad3d": 51, "cl": [29, 189, 201, 209, 252], "clamp": [160, 161, 163, 164, 165, 247], "class": [1, 2, 6, 8, 9, 10, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 165, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 244, 245, 248, 249, 250, 251], "class_nam": [191, 196, 226, 231], "classif": [20, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 225, 237, 238], "classifi": 20, "classmethod": [9, 22, 29, 178, 184, 245, 248], "cle": [4, 16, 27, 186, 213, 217, 218, 221, 252, 253], "cle_applied_model": [201, 221], "clean": 231, "clean_start": [2, 12, 181, 187, 191, 205, 231], "clear_sess": 194, "clearli": 252, "clip": [194, 220, 247, 249], "clone": [159, 186, 195], "clone_lay": 195, "close": [223, 247], "closer": [188, 192, 200, 206], "cloud": [238, 241, 256], "cmp_re": 231, "cnn": 238, "cnt": [191, 226, 231], "code": [187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 221, 240, 241, 243, 244], "codelinaro": 241, "collate_fn": [235, 236], "collect": [8, 21, 24, 167, 176, 182, 183, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 209, 210, 211, 214, 215, 217, 218, 224, 238], "color": 229, "column": 224, "com": [186, 240, 241, 242], "combin": [2, 12, 171, 181, 187, 191, 205, 218, 223, 225, 231, 238, 241, 244, 252], "come": [225, 232, 248], "command": [186, 227, 240, 241, 242, 243, 255], "common": [170, 175, 177, 187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 241, 242, 244, 252, 253], "commonli": [1, 23, 177, 180, 213, 238], "comp": [17, 28, 222, 226, 228], "comp_stat": [202, 203], "compar": [2, 12, 171, 181, 187, 191, 194, 195, 196, 199, 205, 208, 212, 217, 231, 232, 238, 248, 256], "comparison": [188, 189, 190, 206, 209, 210, 211, 214, 216], "compat": [166, 169, 191, 194, 195, 231, 240, 243, 244, 245, 251, 252], "compil": [1, 9, 193, 194, 195, 197, 198, 199, 213, 217, 218, 219, 245, 248, 252, 257], "complet": [2, 12, 181, 187, 191, 202, 203, 204, 205, 226, 231, 242, 253, 255], "complex": [1, 22, 184, 195, 213, 217, 233, 245, 248], "compli": [205, 206, 209, 210, 211, 212], "complic": 252, "compon": [170, 177, 245, 252], "compos": [173, 205, 207, 213, 218, 231, 233, 243, 245], "compress": [18, 166, 186, 229, 238, 239, 246, 252], "compress_model": [17, 28, 202, 203, 204, 222, 226, 227, 228], "compress_schem": [17, 28, 202, 203, 204, 222, 226, 228], "compressed_bw": 246, "compressed_model": [202, 203, 222, 226, 228], "compressionschem": [17, 28, 202, 203, 204, 222, 226, 228], "compressionstat": [17, 28, 222, 226, 228], "compressor": [17, 28, 222, 226, 228], "compris": [187, 191, 205, 224], "compromis": [202, 203, 204], "compuat": [13, 24, 182, 218], "comput": [1, 2, 8, 9, 12, 13, 21, 22, 24, 28, 29, 99, 165, 175, 176, 178, 179, 181, 182, 183, 184, 185, 188, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 206, 209, 210, 211, 212, 213, 216, 217, 218, 220, 222, 225, 226, 227, 228, 229, 231, 235, 236, 237, 238, 240, 242, 243, 244, 246, 247, 252, 256, 257], "computation": 227, "compute_encod": [9, 22, 29, 158, 159, 160, 161, 165, 167, 170, 173, 174, 177, 178, 179, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 213, 215, 219, 231, 233, 235, 236, 237, 243, 245, 248], "computeencod": 240, "concat": 252, "concatenated_exampl": [235, 236], "concept": 170, "concis": 246, "concret": 171, "concrete_arg": [24, 171, 182, 218], "condit": [171, 172, 219], "confer": 225, "config": [2, 8, 21, 22, 173, 176, 178, 179, 181, 183, 184, 185, 187, 200, 205, 217, 219, 231, 233, 235, 236, 237, 245, 248, 249, 252], "config_fil": [8, 9, 11, 13, 21, 22, 24, 176, 178, 182, 183, 184, 194, 199, 200, 212, 213, 217, 218, 219, 233, 237, 243, 245, 248], "config_util": [166, 244], "configr": 175, "configur": [1, 8, 9, 11, 13, 17, 21, 23, 24, 28, 165, 173, 175, 176, 177, 178, 180, 182, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 201, 205, 206, 208, 209, 210, 211, 213, 217, 218, 222, 226, 228, 233, 235, 236, 243, 244, 245, 246, 248, 252], "confirm": 243, "conflict": [173, 233], "conform": [245, 257], "conjunct": 247, "connect": [213, 220, 221, 222, 225, 228, 238], "connectedgraph": [172, 252], "consecut": [175, 189, 201, 209, 221], "consid": [195, 202, 204, 226, 228, 253, 257], "consist": [168, 169, 170, 187, 205, 216, 224, 231, 244, 247], "consol": 214, "constant": [171, 190, 197, 210, 211, 224], "constantpad2d": [52, 53], "constantpad3d": 54, "constrain": [238, 244], "constraint": 246, "construct": [216, 241, 242, 252], "constructor": [6, 19, 168, 171, 216, 251], "consum": [193, 207, 218, 225, 232, 246], "contain": [2, 8, 12, 21, 28, 29, 157, 165, 167, 171, 172, 173, 174, 176, 181, 183, 188, 189, 190, 192, 193, 197, 198, 199, 201, 202, 203, 204, 206, 207, 209, 210, 211, 212, 215, 217, 221, 222, 226, 228, 231, 238, 239, 240, 243, 245, 246, 247, 248, 249, 255, 256], "container_nam": 241, "content": 238, "context": [29, 174, 255, 257], "contigu": [235, 236, 252], "continu": [2, 20, 172, 181, 184, 187, 205, 231, 235, 241, 242, 252, 253], "contrast": [20, 170, 200], "contribut": [217, 231, 240, 253], "control": [24, 171, 174, 177, 182, 218, 247], "conv": [1, 3, 7, 11, 14, 15, 23, 26, 169, 171, 180, 189, 194, 201, 209, 213, 219, 220, 221, 226, 228, 229, 233, 244, 249, 252], "conv1": [20, 171, 172, 178, 194, 202, 203, 204, 213, 220, 221, 222, 226, 228, 245, 248, 251], "conv1_relu": [213, 220, 221], "conv1d": [26, 55, 220, 252], "conv2": [20, 169, 171, 194, 222, 226, 228, 244, 251], "conv2d": [15, 20, 26, 29, 56, 169, 170, 171, 172, 174, 178, 194, 195, 200, 213, 220, 221, 222, 225, 229, 244, 245, 248, 252], "conv2d_lay": 200, "conv2dnormactiv": [220, 221, 243], "conv2dtranspos": [15, 220], "conv3d": 57, "conv_1": [20, 196, 244], "conv_2": [20, 196], "conv_weight": 220, "conv_weight_arrai": 220, "conv_weight_nam": 220, "conveni": [241, 256], "convent": [249, 252], "converg": [238, 248], "convers": [251, 252], "convert": [2, 12, 20, 157, 170, 171, 173, 178, 181, 191, 205, 213, 218, 220, 221, 238, 245, 248, 250, 255, 257], "convert_to_pb": [22, 195, 245, 248], "convinplacelinear": 175, "convolut": [20, 169, 187, 188, 189, 190, 192, 196, 197, 198, 200, 201, 202, 204, 205, 206, 209, 210, 211, 219, 220, 221, 222, 223, 225, 226, 228, 229, 230, 238, 244, 253], "convtranspose1d": [58, 252], "convtranspose2d": [26, 59, 220], "convtranspose3d": 60, "copi": [20, 22, 159, 165, 184, 186, 188, 205, 206, 209, 210, 211, 235, 236, 241, 245, 247, 248], "copy_": 170, "corp": 231, "correct": [13, 24, 182, 189, 200, 201, 207, 212, 218, 231, 233, 238, 245, 248, 257], "correct_bia": 209, "correct_predict": [213, 231, 245], "correctli": [243, 252], "correl": [187, 205, 231], "correspond": [1, 2, 3, 6, 8, 11, 12, 14, 15, 19, 21, 23, 26, 165, 168, 172, 173, 174, 175, 176, 178, 179, 180, 181, 183, 184, 185, 193, 200, 207, 213, 216, 217, 219, 220, 222, 229, 230, 231, 237, 240, 241, 242, 244, 245, 247, 248], "cosineembeddingloss": 61, "cosinesimilar": 62, "cost": [17, 28, 202, 204, 222, 224, 225, 226, 228, 230, 232, 247, 248], "cost_metr": [17, 28, 202, 203, 204, 222, 226, 228], "costmetr": [17, 28, 202, 203, 204, 222, 226, 228], "could": [1, 22, 170, 184, 187, 188, 189, 190, 192, 194, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 229, 245, 246, 248, 251], "count_param": 196, "counter": [22, 194, 208, 245, 248], "counterpart": 174, "cours": [197, 198, 200, 202, 203, 204, 210, 211], "cout": 177, "cover": [187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 247], "cp": [222, 223, 224, 229], "cp310": [240, 242], "cp_comp_stat": 204, "cpu": [27, 171, 178, 184, 187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 218, 219, 220, 221, 231, 233, 237, 238, 240, 241, 243, 245, 248, 252], "cpuexecutionprovid": [187, 188, 189, 190], "creat": [2, 8, 9, 10, 12, 13, 17, 20, 22, 24, 28, 29, 165, 167, 171, 173, 174, 175, 177, 181, 182, 184, 186, 193, 195, 199, 202, 203, 204, 207, 212, 213, 215, 216, 218, 219, 222, 225, 226, 228, 232, 233, 235, 237, 238, 243, 247, 248, 252, 255, 256, 257], "create_quantsim_and_encod": [8, 217], "critic": 191, "crop_length": [191, 231], "cropped_imag": [191, 231], "cross": [4, 13, 16, 24, 27, 182, 186, 213, 217, 218, 253], "cross_layer_equ": [5, 18, 166, 189, 201, 209, 221], "crossentropyloss": [63, 219, 235, 236, 248], "crosslayerequ": 252, "ctcloss": 47, "ctivations_pdf": [8, 21, 176, 183, 217], "cu118": [240, 242], "cu121": [240, 241, 242], "cubla": 252, "cuda": [1, 6, 9, 187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 231, 233, 235, 236, 237, 240, 242, 243, 245, 248, 252], "cudaexecutionprovid": [187, 188, 189, 190], "cudnn": [240, 243], "cudnn_conv_algo_search": [187, 188, 189, 190], "cumul": [193, 207], "current": [12, 17, 20, 22, 28, 29, 159, 165, 172, 173, 181, 194, 195, 196, 205, 212, 222, 226, 228, 231, 232, 245, 248, 252, 258], "curv": [2, 12, 181, 187, 191, 205, 224, 231], "custom": [1, 9, 20, 22, 29, 171, 174, 187, 193, 195, 207, 213, 238, 241, 245, 246, 247, 248, 252, 253], "custom_function_not_to_be_trac": 171, "custom_object": [22, 245, 248], "customdataload": [213, 218, 231, 245], "custommodel": 171, "custommodul": 171, "cycl": 256, "d": [160, 161, 162, 163, 164, 246], "dangl": [245, 248], "dark": [187, 188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212], "data": [1, 2, 8, 9, 10, 11, 12, 17, 21, 22, 23, 24, 25, 28, 157, 158, 159, 167, 170, 171, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 222, 226, 228, 230, 231, 235, 236, 237, 238, 244, 245, 247, 248, 252, 253], "data_load": [1, 2, 10, 23, 24, 28, 167, 173, 179, 180, 181, 182, 185, 187, 188, 189, 190, 192, 193, 194, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 218, 219, 222, 231, 237, 245, 248], "data_loader_wrapp": [12, 191, 231], "data_set": [11, 192, 200, 213], "data_typ": [24, 170, 178, 182, 184, 218, 245, 248], "datacent": 241, "dataload": [2, 12, 23, 24, 25, 173, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 231, 235, 236, 237, 245, 248], "dataloader_wrapp": [191, 231], "dataloadermnist": 222, "dataparallel": 248, "dataset": [2, 8, 9, 12, 13, 14, 21, 24, 25, 176, 178, 181, 182, 183, 186, 195, 213, 217, 218, 219, 226, 231, 235, 236, 237, 243, 245, 247, 248, 256], "dataset_dir": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 226, 231], "dataset_train": [197, 198], "dataset_valid": [197, 198], "datasetv2": [11, 13, 14, 21, 213, 217, 218, 219], "datatyp": [13, 24, 182, 218, 246], "de": [177, 247], "deb": 242, "debian": [240, 242], "debug": [170, 191, 214, 216, 246, 252], "decai": 225, "decid": [179, 185, 187, 188, 189, 190, 191, 192, 200, 202, 203, 204, 205, 206, 209, 210, 211, 212, 227, 231, 237, 241], "decim": [202, 203, 204, 222, 226, 228], "decis": 257, "declar": 29, "decode_predict": [191, 226, 231], "decompos": [225, 226, 228], "decomposit": [203, 204, 225, 226, 228], "decompress": [7, 169, 244], "decompressed_bw": [7, 169, 244], "decor": 29, "decreas": 238, "dedic": 238, "deem": 256, "deep": [220, 225, 232, 238], "deepcopi": 188, "def": [2, 9, 12, 17, 20, 22, 28, 29, 170, 171, 172, 173, 178, 181, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 222, 226, 228, 231, 235, 236, 237, 243, 245, 248, 250, 251], "default": [1, 2, 8, 9, 10, 11, 12, 13, 17, 20, 21, 22, 23, 24, 28, 29, 159, 160, 161, 163, 164, 165, 167, 171, 173, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 217, 218, 220, 221, 222, 224, 225, 226, 227, 228, 231, 233, 237, 238, 242, 243, 245, 246, 247, 248, 252], "default_activation_bw": [8, 9, 187, 188, 189, 190, 213, 217, 231, 245], "default_beta_rang": [1, 11, 23, 180, 213], "default_bitwidth": 231, "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "default_config_fil": [1, 23, 180, 213], "default_config_per_channel": [194, 249], "default_data_col": [235, 236], "default_data_typ": [9, 22, 178, 184, 245, 248], "default_forward_fn": [179, 185, 237], "default_num_iter": [1, 11, 23, 180, 188, 192, 200, 206, 207, 213], "default_output_bw": [9, 21, 22, 176, 178, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 219, 231, 233, 235, 236, 237, 243, 245, 248], "default_param_bw": [1, 8, 9, 11, 21, 22, 23, 176, 178, 180, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 219, 231, 233, 235, 236, 237, 243, 245, 248], "default_quant_schem": [1, 11, 23, 180, 188, 192, 200, 206, 213], "default_reg_param": [1, 11, 23, 180, 213], "default_warm_start": [1, 11, 23, 180, 213], "defin": [2, 12, 20, 29, 171, 172, 174, 177, 181, 191, 196, 199, 200, 202, 203, 204, 206, 209, 210, 211, 212, 217, 222, 226, 228, 231, 241, 245, 246, 247, 249, 250, 251], "definit": [2, 12, 28, 29, 168, 170, 171, 173, 175, 181, 184, 187, 191, 194, 195, 199, 200, 205, 206, 209, 210, 211, 212, 216, 222, 226, 228, 231, 245, 247, 250, 251], "degrad": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 256, 257], "degre": [226, 228], "deleg": 177, "delet": [2, 12, 181, 187, 191, 205, 217, 231], "deliber": [187, 191, 199, 200, 205, 212], "delta": [8, 20, 21, 176, 183, 195, 196, 217, 240, 247], "demand": 238, "demonstr": [186, 199, 212], "denot": [226, 229, 246], "dens": [15, 20, 194, 195, 196, 220, 250, 252], "depend": [157, 158, 159, 171, 186, 200, 223, 240, 241, 242, 249, 252], "deploi": [247, 254, 256], "deploy": [238, 239, 254, 255, 257], "deprec": [178, 245, 248, 252], "depth": [213, 219, 221, 252, 253], "depthwis": 252, "depthwise_conv2d": 221, "depthwiseconv2d": [15, 220, 221], "dequant": [157, 158, 159, 161, 164, 165, 177, 238, 240, 252, 256, 257], "dequantizedtensor": [158, 159, 161, 177, 243], "deriv": [160, 161, 163, 164, 177, 188, 191, 205, 213, 231, 247], "descend": 165, "describ": [165, 170, 186, 200, 225, 227, 231, 240, 241, 242, 243, 244, 246, 247, 249, 253, 254, 256, 257], "descript": [174, 187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 246, 247], "design": [172, 187, 191, 196, 199, 200, 205, 212, 213, 238, 244], "desir": [8, 22, 159, 176, 177, 178, 183, 184, 187, 191, 202, 203, 204, 205, 217, 223, 225, 226, 231, 232, 245, 248, 253, 255, 257], "detach": 159, "detail": [24, 171, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 218, 222, 224, 227, 233, 239, 241, 242, 244, 246, 247, 253, 255, 257], "detect": 225, "determin": [1, 8, 21, 22, 165, 169, 174, 176, 183, 184, 187, 195, 205, 207, 213, 217, 218, 230, 235, 236, 237, 238, 244, 245, 246, 248], "determinist": 171, "dev": [241, 242], "develop": [167, 215, 238, 241, 242, 256], "devic": [1, 6, 9, 13, 23, 24, 159, 171, 175, 178, 180, 181, 182, 184, 187, 188, 189, 190, 194, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 221, 231, 233, 235, 236, 237, 238, 239, 243, 245, 247, 248, 254, 255, 256, 257], "diagnost": 253, "diagram": [195, 235, 236], "dict": [2, 8, 9, 12, 17, 21, 22, 24, 28, 165, 168, 171, 173, 174, 175, 176, 181, 182, 183, 184, 216, 217, 218, 222, 226, 228, 231, 233, 245, 246, 248, 251], "dictat": 256, "dictionari": [8, 12, 17, 21, 28, 165, 176, 178, 183, 184, 193, 194, 196, 217, 222, 224, 225, 226, 228, 231, 245, 248, 249], "dicuss": 196, "did": [200, 241], "didn": 208, "diff": 252, "differ": [17, 28, 170, 171, 175, 187, 191, 194, 195, 200, 202, 203, 204, 205, 210, 211, 221, 222, 224, 225, 226, 228, 230, 231, 232, 233, 234, 237, 238, 242, 244, 246, 247, 253, 255, 257], "dim": [169, 177, 205, 244], "dimens": [7, 169, 174, 226, 228, 244, 246, 247, 253], "dir": [9, 194, 199, 200, 201, 242, 245], "dir_path": [6, 168, 216], "direct": [216, 220, 231, 239, 244, 246, 247, 252, 256, 257], "directli": [8, 14, 194, 208, 217, 219, 233, 235, 247, 256], "directori": [2, 6, 8, 9, 12, 13, 17, 19, 21, 24, 28, 168, 175, 176, 181, 182, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 241, 242, 245, 248, 255], "directrori": 231, "disabl": [2, 8, 21, 170, 174, 175, 176, 178, 179, 183, 184, 185, 191, 202, 204, 217, 220, 224, 225, 231, 237, 245, 247, 248, 249], "disable_lora_adapt": 175, "discard": 217, "disciplin": 213, "discuss": [186, 191, 202, 203, 204, 213], "disk": [6, 19, 168, 216], "displai": [186, 191, 195, 215, 227], "display_comp_ratio_plot": 227, "display_eval_scor": 227, "dist": [241, 242], "distict": 252, "distinct": 172, "distort": 247, "distribut": [17, 28, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 221, 222, 226, 228, 238, 247, 252, 253], "divbackward0": 177, "diverg": 238, "divid": [223, 244, 248], "divis": [177, 238, 244], "dlc": 255, "dlc_path": 255, "dlf": 238, "do": [1, 9, 12, 20, 23, 171, 172, 175, 178, 180, 187, 188, 189, 190, 191, 192, 193, 194, 196, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 217, 220, 225, 231, 233, 235, 241, 242, 243, 245, 247, 248, 256, 257], "do_constant_fold": [187, 188, 189, 190, 220], "do_not_trace_m": 171, "doc": [171, 175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 241, 252], "docker": 239, "docker_container_nam": 241, "docker_image_nam": 241, "docker_run_command": 241, "dockerfil": 241, "docstr": 244, "document": [175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 205, 206, 207, 208, 209, 210, 211, 212, 216, 220, 227, 231, 240, 244, 246, 247, 252, 255, 256, 257], "doe": [20, 29, 171, 181, 187, 188, 189, 190, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 215, 217, 222, 224, 226, 228, 237, 247, 248, 251, 252, 253, 256, 257], "doesn": [29, 187, 188, 189, 190, 192, 193, 201, 202, 203, 204, 205, 206, 209, 210, 211, 245, 248, 256], "doesnt": [173, 233], "don": [29, 171, 175, 191, 194, 195, 199, 200, 212, 217, 235, 236], "done": [160, 161, 177, 191, 196, 200, 208, 225, 249, 250, 252], "down": [200, 234, 246], "download": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 239, 240, 242], "download_url": [241, 242], "downsampl": [202, 204], "downstream": [229, 246, 252], "dpkg": 242, "dq_output": 29, "drastic": [224, 257], "drawback": 247, "driver": [226, 240, 243], "drop": [2, 12, 13, 24, 174, 181, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 217, 218, 223, 225, 230, 231, 247, 248, 253], "dropout": [20, 64, 195, 196], "dropout1": [20, 196], "dropout1d": 65, "dropout2": [20, 196], "dropout2d": 66, "dropout3d": 67, "dtype": [157, 158, 159, 165, 169, 170, 171, 173, 175, 233, 235, 236, 244, 246, 252], "due": [20, 172, 252], "dummi": [8, 9, 23, 24, 26, 27, 168, 172, 176, 178, 180, 181, 182, 183, 184, 187, 199, 205, 212, 213, 216, 217, 218, 220, 221, 235, 236, 245, 248], "dummy_attention_mask": [235, 236], "dummy_data": 217, "dummy_input": [8, 9, 23, 24, 26, 27, 167, 168, 175, 176, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 231, 233, 235, 236, 237, 243, 244, 245, 248, 251], "dummy_input_dict": 216, "dummy_input_id": [235, 236], "dump": [194, 252], "duplic": [159, 171], "dure": [1, 2, 11, 12, 13, 17, 20, 23, 24, 28, 29, 165, 167, 174, 180, 181, 182, 188, 190, 192, 194, 197, 198, 199, 200, 201, 202, 203, 204, 206, 208, 210, 211, 212, 213, 214, 215, 218, 222, 225, 226, 227, 228, 231, 238, 239, 245, 247, 248, 249, 252, 257], "dynam": [165, 171, 238, 247, 252], "dynamic_ax": [187, 188, 189, 190, 213, 220, 231, 245], "e": [1, 8, 9, 22, 178, 184, 187, 191, 194, 199, 200, 205, 208, 212, 213, 217, 230, 231, 237, 245, 246, 248], "each": [1, 2, 8, 11, 12, 20, 21, 23, 29, 167, 172, 174, 176, 177, 180, 181, 183, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 214, 215, 217, 220, 221, 222, 223, 224, 225, 226, 231, 233, 238, 244, 245, 246, 247, 249, 253, 255], "earli": [2, 12, 181, 231], "easi": [233, 252, 256], "easier": 170, "easili": [177, 222, 226, 228], "ed": 213, "edg": [238, 239, 256], "edit": [187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 246], "effect": [1, 11, 14, 22, 23, 25, 174, 175, 180, 190, 195, 197, 198, 210, 211, 213, 217, 219, 230, 238, 245, 248, 256, 257], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 194, 208, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "efficientnetb4": 252, "effort": [13, 24, 173, 182, 193, 207, 218, 233, 256, 257], "eights_pdf": [8, 21, 176, 183, 217], "either": [17, 28, 169, 173, 175, 177, 187, 192, 200, 202, 203, 204, 205, 222, 226, 228, 229, 231, 233, 241, 244, 251], "element": 246, "elementari": 195, "elementwis": [171, 174, 252], "elimin": [217, 220, 238, 247], "els": [2, 12, 171, 172, 181, 187, 188, 189, 190, 191, 192, 194, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 218, 219, 220, 221, 226, 231, 233, 237, 243, 245, 248], "elu": 68, "embed": [20, 69, 171, 178, 184, 195, 196, 225, 245, 248, 252, 253], "embed_dim": [20, 195, 196], "embedding_lay": [20, 196], "embeddingbag": 70, "embodi": 238, "empir": [193, 207], "emploi": [238, 239], "empti": 249, "emul": [239, 247], "enabl": [2, 7, 8, 12, 13, 17, 21, 24, 28, 165, 170, 173, 175, 176, 179, 181, 182, 183, 185, 191, 198, 200, 202, 204, 205, 211, 216, 218, 219, 220, 222, 226, 227, 228, 230, 231, 237, 238, 244, 247, 249, 252, 255, 256], "enable_adapter_and_load_weight": 175, "enable_convert_op_reduct": [12, 173, 181, 191, 205, 231], "enable_onnx_check": [178, 184, 245, 248], "enable_per_layer_mse_loss": [8, 21, 199, 212, 217], "enbl": 244, "enc": 29, "enc_typ": 246, "encapsul": [2, 12, 173, 176, 181, 183, 217, 231], "encod": [1, 2, 8, 9, 11, 12, 13, 21, 22, 23, 24, 29, 157, 158, 159, 160, 161, 165, 170, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 188, 189, 190, 192, 193, 194, 195, 197, 198, 200, 201, 206, 207, 209, 210, 211, 213, 214, 216, 218, 230, 231, 233, 234, 235, 236, 237, 238, 240, 244, 248, 252, 255, 256], "encoding_analyz": [160, 161, 165], "encoding_file_path": [245, 248], "encoding_path": [188, 192, 193, 200, 206, 207, 213, 218], "encoding_vers": [244, 246], "encodinganalyz": [160, 161, 165], "encodinganalyzerforpython": 240, "encodingbas": [157, 158, 165], "encodingmismatchinfo": 245, "encodingtyp": 246, "encount": [29, 195], "encourag": [20, 171, 250, 251], "end": [14, 160, 161, 162, 163, 164, 171, 172, 186, 194, 195, 197, 198, 202, 203, 204, 208, 210, 211, 213, 219, 225, 231, 237, 239, 245, 256], "end_beta": [1, 11, 23, 180, 213], "end_idx": 218, "enforc": 165, "engin": [216, 220, 231, 239, 241, 244, 246, 247, 252, 256, 257], "enhanc": [8, 21, 175, 176, 183, 199, 212, 217, 245, 247, 252], "enough": [191, 202, 203, 204, 213, 257], "ensur": [174, 200, 216, 219, 224, 231, 235, 236, 242, 243, 252, 253, 257], "enter": [29, 174, 218], "entir": [8, 21, 175, 176, 177, 183, 187, 188, 189, 190, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 222, 225, 226, 228, 247], "entireti": [20, 200], "entri": [24, 178, 182, 184, 218, 245, 246, 248, 249], "entrypoint": 241, "enum": [1, 2, 9, 12, 17, 23, 28, 168, 178, 180, 181, 184, 187, 191, 194, 195, 205, 208, 213, 216, 222, 226, 228, 231, 245, 246, 248], "enumer": [17, 28, 168, 196, 216, 222, 226, 228, 231, 235, 236, 245, 248], "environ": [186, 187, 191, 193, 194, 197, 198, 205, 212, 231, 238, 240, 242], "envsetup": [241, 242], "ep": [220, 221], "epoch": [188, 189, 190, 192, 193, 194, 195, 197, 198, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 219, 222, 225, 226, 228, 235, 236, 248], "epsilon": [20, 195, 196, 252], "equal": [4, 13, 16, 24, 27, 158, 159, 169, 177, 182, 186, 192, 196, 213, 217, 218, 223, 224, 244, 247, 253], "equalize_model": [4, 16, 27, 189, 201, 209, 221], "equat": [160, 161, 162, 163, 164, 244, 247], "equival": [24, 29, 163, 164, 165, 169, 173, 178, 181, 182, 184, 187, 200, 205, 206, 209, 210, 211, 212, 218, 231, 244, 245, 248, 251, 255], "erorr": 191, "error": [13, 20, 24, 29, 171, 177, 178, 182, 191, 214, 218, 231, 233, 238, 245, 247, 248, 253], "especi": [187, 200, 205, 235, 238, 248, 253, 256], "essenti": [187, 191, 194, 195, 205, 208], "estim": [186, 247, 252, 256], "esults_dir": [8, 21, 176, 183, 217], "etc": [187, 191, 194, 195, 200, 205, 223, 238, 241, 246, 252], "eval": [2, 8, 12, 13, 17, 21, 24, 28, 171, 173, 176, 179, 181, 182, 183, 185, 187, 188, 189, 190, 191, 194, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 231, 233, 237, 243, 245, 248], "eval_callback": [8, 13, 17, 21, 24, 28, 176, 182, 183, 193, 194, 199, 202, 203, 204, 207, 212, 217, 218, 222, 226, 228, 231], "eval_callback_factori": [187, 205], "eval_callback_fn": 217, "eval_callback_for_phase1": [2, 12, 181, 187, 205, 231], "eval_callback_for_phase2": [2, 12, 181, 187, 205, 231], "eval_callback_for_phase_1": 231, "eval_callback_for_phase_2": 231, "eval_callback_phase1": 191, "eval_callback_phase2": 191, "eval_data_load": [213, 231, 245], "eval_data_s": 245, "eval_dataset": [193, 194, 213, 217, 218, 219, 245, 248], "eval_dataset_s": [193, 194, 207, 218], "eval_func": [191, 199, 226, 231], "eval_iter": [17, 28, 202, 203, 204, 222, 226, 228], "eval_scor": [8, 17, 21, 28, 176, 183, 217, 222, 226, 228], "evalcallbackfactori": [2, 173, 181, 187, 205, 231], "evalfunct": 226, "evalu": [2, 8, 12, 13, 17, 21, 24, 28, 173, 176, 181, 182, 183, 186, 193, 195, 207, 213, 218, 219, 222, 224, 225, 226, 227, 228, 231, 237, 243, 247, 257], "evaluate_accuraci": 205, "evaluate_model": [222, 226, 228], "even": [29, 177, 187, 191, 194, 195, 200, 205, 231, 232, 233], "evenli": 244, "eventu": 233, "everi": [1, 6, 8, 11, 19, 21, 23, 168, 176, 177, 180, 183, 187, 188, 189, 190, 191, 194, 195, 197, 198, 199, 200, 202, 203, 204, 205, 208, 210, 211, 212, 213, 214, 216, 217, 224, 225, 247, 248], "evlauat": 191, "exactli": [13, 29, 99, 165, 174, 199, 212, 218, 247], "examin": 171, "exampl": [7, 9, 17, 28, 29, 157, 158, 159, 160, 161, 163, 164, 165, 167, 169, 172, 173, 174, 175, 177, 178, 179, 185, 195, 196, 207, 213, 215, 216, 217, 218, 220, 221, 223, 229, 233, 235, 236, 237, 238, 241, 242, 244, 245, 247, 248, 249, 250, 251, 252, 253, 257], "exce": [167, 215], "exceed": [167, 215], "except": [20, 179, 185, 187, 188, 189, 190, 196, 199, 212, 213, 220, 221, 231, 233, 237, 244, 245, 248], "exchang": 238, "exclud": [24, 171, 172, 176, 179, 182, 183, 185, 217, 218, 237, 246, 252], "excluded_lay": 246, "exclus": [165, 169, 244], "execut": [2, 9, 12, 13, 24, 171, 178, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 218, 220, 227, 231, 242, 243, 245, 248, 252], "exercis": [187, 194, 199, 200, 205, 208, 212], "exist": [29, 99, 165, 174, 178, 184, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 231, 244, 245, 248, 256], "exist_ok": [187, 188, 191, 192, 194, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211], "exit": [2, 12, 13, 24, 29, 174, 181, 182, 218, 231], "expand": [244, 252], "expand_dim": 218, "expanded_conv_depthwis": [213, 221], "expans": [225, 244], "expect": [2, 8, 13, 17, 21, 22, 23, 24, 28, 167, 171, 172, 173, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 194, 202, 203, 204, 205, 208, 213, 215, 217, 218, 222, 225, 226, 228, 231, 233, 237, 245, 248, 250], "experi": [17, 28, 169, 188, 192, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211, 222, 225, 226, 228, 240, 244, 257], "experienc": 240, "experiment": [225, 244, 252], "experss": [20, 196], "expert": [193, 207], "explain": [187, 191, 194, 195, 199, 205, 208, 212, 222, 225, 252], "explan": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 239], "explicitli": [13, 24, 182, 218, 229], "expon": [165, 169, 244], "exponent_bit": [165, 169, 244], "export": [8, 9, 21, 22, 24, 168, 170, 173, 175, 176, 178, 179, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 195, 197, 198, 200, 201, 205, 206, 207, 209, 210, 211, 213, 214, 216, 217, 218, 219, 220, 221, 225, 231, 232, 233, 237, 241, 242, 246, 251, 252, 255], "export_adapter_weight": 175, "export_model": [9, 175, 178, 184, 245, 248], "export_param": [187, 188, 189, 190], "export_per_layer_encoding_min_max_rang": [8, 21, 176, 183, 217], "export_per_layer_mse_loss": [8, 21, 176, 183, 217], "export_per_layer_stats_histogram": [8, 21, 176, 183, 217], "export_to_torchscript": [178, 184, 245, 248], "exported_model": 244, "expos": [171, 213, 217], "express": [17, 28, 222, 226, 228, 235, 236], "extend": [170, 252], "extens": [166, 174, 186, 241, 242, 255], "extern": 252, "extra": [165, 174, 252], "extract": [188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212, 238, 245], "extrem": [187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 247], "ey": 217, "f": [171, 172, 177, 194, 207, 213, 218, 219, 220, 221, 231, 240, 241, 242, 245, 248], "f0": 223, "facebook": [235, 236, 238], "facilit": 239, "fact": 235, "factor": [194, 195, 197, 198, 202, 203, 204, 208, 210, 211, 223, 225, 244], "factori": [2, 12, 173, 181, 231], "fail": [171, 172, 173, 178, 187, 188, 189, 190, 213, 218, 220, 221, 231, 233, 245, 248, 252], "fair": 238, "fairli": [191, 194, 195, 199, 200, 212], "fake": [161, 164, 165, 173, 181, 187, 188, 189, 190, 192, 194, 195, 197, 198, 200, 201, 205, 206, 207, 208, 209, 210, 211, 231, 245, 252], "fakequ": [178, 184, 245, 248], "fall": [202, 203, 204, 224, 249, 257], "fallback": 255, "fals": [2, 7, 9, 12, 13, 17, 20, 22, 24, 28, 29, 157, 159, 160, 161, 163, 164, 165, 169, 170, 171, 172, 173, 174, 175, 177, 178, 181, 182, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 218, 220, 221, 222, 226, 228, 231, 233, 235, 236, 240, 243, 244, 245, 246, 248, 249], "famili": [233, 238], "familiar": 186, "far": [194, 208], "farther": [188, 192, 206], "fashion": 200, "fast": [12, 231], "faster": [187, 193, 194, 200, 205, 207, 231, 252], "fc": [225, 228], "fc1": 171, "fc2": 171, "featur": [7, 20, 167, 170, 171, 172, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 218, 220, 221, 225, 227, 231, 233, 238, 243, 247, 250, 251, 252, 253], "featurealphadropout": 71, "feed": [20, 195, 196, 247], "feel": [194, 195, 208, 233], "feez": [179, 185, 237], "few": [187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 219, 223], "fewer": [225, 256], "ff_dim": [20, 195, 196], "ffn": [20, 196], "ffn_output": [20, 196], "field": [2, 165, 181, 187, 205, 231, 246], "figur": [213, 218, 221, 222, 224, 226, 227, 228, 229, 247, 253], "file": [1, 2, 8, 9, 11, 13, 17, 21, 22, 23, 24, 28, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 193, 195, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 222, 226, 228, 231, 233, 237, 240, 241, 242, 244, 245, 246, 247, 248, 252, 255, 256], "file_path": [184, 213, 220, 221, 231, 245], "filenam": [1, 9, 11, 22, 23, 175, 178, 180, 184, 187, 188, 189, 190, 213, 219, 237, 245, 248], "filename_prefix": [1, 9, 11, 22, 23, 175, 178, 180, 184, 187, 188, 191, 192, 194, 200, 201, 205, 206, 207, 208, 209, 210, 211, 213, 219, 231, 237, 245, 248], "filename_prefix_encod": [175, 178, 184, 245, 248], "fill": [24, 178, 182, 184, 218, 245, 248], "filter": [20, 200, 238], "final": [17, 20, 28, 172, 179, 185, 187, 191, 194, 195, 199, 205, 208, 212, 217, 222, 223, 224, 226, 228, 231, 235, 236, 237, 242, 244, 248, 253, 256], "find": [2, 12, 15, 172, 173, 179, 181, 184, 185, 187, 188, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 217, 220, 224, 232, 233, 237, 247, 248, 257], "find_pkg_url_str": [241, 242], "fine": [17, 22, 28, 175, 184, 188, 189, 190, 192, 193, 195, 197, 198, 201, 206, 207, 209, 210, 211, 222, 223, 226, 228, 239, 245, 248, 256, 257], "finer": [165, 177, 244], "finess": 257, "finetun": [188, 189, 192, 193, 194, 200, 201, 202, 203, 204, 206, 208, 210, 211], "finetuned_accuraci": [208, 210, 211], "finetuned_accuracy_bn_reestim": 208, "finetuned_model": [202, 203], "finish": [197, 198, 210, 211], "first": [20, 23, 171, 174, 177, 179, 180, 185, 194, 195, 196, 199, 200, 202, 203, 204, 208, 212, 213, 221, 225, 226, 237, 241, 252, 253, 256], "fit": [17, 28, 194, 195, 197, 198, 208, 219, 222, 224, 226, 228, 248, 256], "five": [189, 209], "fix": [172, 187, 188, 189, 190, 231, 246, 247, 252], "flag": [2, 12, 13, 24, 167, 170, 171, 172, 173, 178, 181, 182, 184, 187, 191, 205, 215, 218, 231, 233, 245, 248], "flatten": [72, 171, 194, 235, 236, 246], "flexibl": [166, 187, 205], "float": [1, 2, 8, 9, 10, 11, 12, 13, 17, 21, 22, 23, 24, 28, 29, 157, 158, 165, 169, 173, 176, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 220, 221, 222, 226, 228, 231, 232, 237, 238, 239, 244, 245, 246, 247, 248, 252, 253, 256, 257], "float16": [165, 169, 170, 244], "float32": 217, "float_fallback": 255, "floatencod": 165, "floatquant": [165, 170], "floatquantizedequant": 170, "flow": [24, 171, 182, 218, 247, 253], "fo": 200, "focu": 208, "fold": [3, 13, 14, 15, 24, 26, 73, 182, 191, 213, 217, 218, 219, 221, 231, 238, 252], "fold_all_batch_norm": [15, 26, 191, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211, 220, 231, 248], "fold_all_batch_norms_to_scal": [14, 194, 208, 219], "fold_all_batch_norms_to_weight": [3, 187, 188, 189, 190, 220], "folded_model": 220, "folder": [199, 212, 217], "follow": [5, 6, 8, 9, 10, 18, 19, 20, 21, 22, 29, 99, 168, 170, 171, 172, 173, 174, 175, 176, 178, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 232, 233, 237, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257], "footprint": 238, "forall_": [160, 161, 162, 163, 164], "forc": [187, 188, 189, 190, 202, 203, 204, 205, 206, 209, 210, 211, 212], "form": [20, 188, 199, 212], "formal": 244, "format": [1, 8, 11, 21, 22, 23, 159, 175, 176, 178, 180, 183, 184, 213, 217, 218, 231, 237, 238, 239, 244, 245, 247, 248, 250, 252, 255, 256], "former": 251, "forward": [2, 8, 9, 12, 13, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 160, 161, 165, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 190, 191, 194, 195, 196, 197, 198, 199, 205, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 231, 235, 236, 237, 241, 245, 248, 251, 252, 253, 257], "forward_fn": [1, 2, 13, 23, 24, 25, 173, 179, 180, 181, 182, 185, 187, 188, 194, 205, 208, 213, 218, 219, 231, 237], "forward_one_batch": [187, 205], "forward_pass": [213, 231, 235, 236, 237, 243], "forward_pass_arg": 245, "forward_pass_call_back": [191, 231], "forward_pass_callback": [1, 2, 8, 9, 12, 21, 22, 176, 178, 181, 183, 184, 187, 188, 189, 190, 192, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 231, 245, 248], "forward_pass_callback_2": [12, 231], "forward_pass_callback_arg": [1, 9, 22, 178, 184, 187, 188, 189, 190, 191, 192, 194, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 213, 231, 245, 248], "forward_pass_callback_fn": 217, "found": [170, 196, 247, 248], "four": [218, 247], "fp16": [173, 233, 252, 256], "fp32": [2, 6, 8, 12, 19, 21, 168, 173, 175, 176, 179, 181, 183, 185, 194, 195, 212, 216, 217, 231, 237, 238, 246, 247, 248, 252, 255, 256, 257], "fp32_acccuraci": 191, "fp32_layer_output": 216, "fp32_layer_output_util": 216, "fp32_output": [2, 231], "fp_qdq": 170, "fp_quantiz": 170, "frac": [160, 161, 162, 163, 164, 165, 177, 247], "fraction": [231, 257], "fractionalmaxpool2d": 74, "fractionalmaxpool3d": 75, "framework": [0, 186, 216, 220, 238, 239, 240, 241, 242, 243, 245, 247, 249, 255, 257], "free": [189, 194, 195, 201, 208, 209, 233], "freez": [170, 175, 188, 192, 200, 206, 237], "freeze_base_model": 175, "freeze_base_model_activation_quant": 175, "freeze_base_model_param_quant": 175, "freeze_encod": 170, "fresh": 240, "friendli": [166, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 218, 256], "from": [2, 9, 12, 13, 20, 22, 23, 24, 25, 29, 158, 159, 160, 161, 165, 166, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 231, 233, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 252, 253, 255, 257], "from_modul": 29, "from_pretrain": [235, 236], "from_str": [9, 22, 178, 184, 245, 248], "from_tensor_slic": 217, "front": [2, 12, 13, 24, 181, 182, 187, 191, 205, 218], "frozen": [200, 235, 236], "full": [187, 188, 189, 190, 191, 192, 194, 197, 198, 201, 205, 206, 208, 209, 210, 211, 226, 231, 232, 238, 250, 251], "fulli": [166, 225, 228, 245, 246, 252], "func": [2, 12, 173, 176, 181, 183, 217, 231], "func_callback_arg": [2, 12, 173, 176, 181, 183, 187, 205, 217, 231], "func_wrapp": [191, 226, 231], "function": [1, 2, 5, 8, 9, 10, 12, 13, 17, 18, 20, 21, 22, 23, 24, 25, 28, 29, 157, 158, 159, 163, 164, 165, 167, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 188, 189, 191, 192, 195, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 221, 222, 224, 225, 226, 227, 228, 231, 235, 236, 237, 238, 244, 245, 247, 248, 250, 251, 252, 255, 257], "function_nam": [202, 203, 204], "functional_callback": 195, "functional_model": [194, 195, 196], "functional_model_weight_ord": 196, "functional_op": 172, "fundament": 170, "furhter": [193, 207], "furo": 252, "further": [157, 160, 161, 162, 163, 164, 171, 177, 204, 222, 225, 247, 257], "furthermor": 20, "fuse": [194, 235, 247, 249], "fuse_bn_into_conv": 220, "fusion": 238, "futur": [167, 215], "fx": [24, 171, 182, 218], "g": [1, 9, 22, 184, 187, 191, 194, 199, 200, 205, 208, 212, 213, 217, 231, 241, 245, 246, 248], "gain": [188, 192, 197, 198, 200, 201, 202, 203, 204, 206, 209, 210, 211, 213, 222, 223, 225, 257], "gap": 232, "gather": 245, "gaussiannllloss": 80, "gave": 218, "gelu": 76, "gemm": [7, 194, 249], "gener": [6, 9, 19, 29, 160, 161, 162, 163, 164, 168, 175, 177, 181, 186, 187, 191, 195, 205, 207, 213, 217, 219, 220, 221, 225, 231, 233, 235, 236, 237, 238, 243, 244, 245, 246, 247, 248, 249, 252, 253, 255, 256], "generate_calibration_callback": [235, 236], "generate_layer_output": [6, 19, 168, 216], "get": [2, 12, 17, 28, 165, 171, 173, 175, 181, 187, 188, 189, 190, 191, 194, 195, 196, 199, 216, 225, 226, 228, 231, 232, 233, 236, 237, 240, 242], "get_activation_quant": [2, 231], "get_active_param_quant": [12, 231], "get_active_quant": [2, 12, 173, 181, 231], "get_available_provid": [187, 188, 189, 190], "get_calibration_and_eval_data_load": 245, "get_candid": [2, 12, 173, 181, 231], "get_data_loader_wrapp": [191, 231], "get_default_kernel": 29, "get_devic": 207, "get_encod": 165, "get_eval_func": [191, 226, 231], "get_extra_st": 165, "get_fp_lora_lay": 175, "get_input": [187, 188, 189, 190, 213, 231, 245], "get_input_quantizer_modul": [173, 181, 231], "get_kernel": 29, "get_loss_fn": [179, 185, 237], "get_model": [191, 250], "get_offset": 177, "get_original_models_weights_in_functional_model_ord": 196, "get_param_quant": [2, 231], "get_path_for_per_channel_config": [213, 218, 219, 233, 237, 243, 245], "get_peft_model": 175, "get_pre_processed_input": 216, "get_quant_scheme_candid": [13, 24, 182, 218], "get_quantized_lora_lay": 175, "get_scal": [159, 177], "get_subclass_model_with_functional_lay": 20, "get_text_classificaiton_model": 20, "get_top5_acc": 191, "get_val_dataload": [187, 188, 189, 190, 202, 204, 205, 206, 208, 209, 210, 211, 212], "get_val_dataset": [192, 193, 194, 199, 200, 201], "get_weight": [196, 220, 221], "git": [186, 241], "github": [186, 240, 241, 242], "give": [191, 195, 199, 200, 202, 203, 204, 212, 213, 217, 221, 225, 231, 252, 254], "given": [2, 4, 12, 13, 16, 17, 22, 23, 24, 25, 27, 28, 29, 167, 173, 174, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 215, 218, 219, 221, 222, 224, 225, 226, 228, 231, 237, 245, 248, 257], "global": [237, 238, 253], "globalaveragepooling1d": [20, 195, 196], "glu": 77, "gnu": 242, "gnupg2": 242, "go": [20, 187, 191, 194, 195, 199, 200, 205, 208, 212, 252], "goal": [13, 24, 182, 187, 205, 217, 218, 256], "good": [20, 175, 194, 195, 197, 198, 202, 203, 204, 208, 210, 211, 225, 233, 257], "googl": 238, "got": [3, 14, 26, 171, 219, 220], "gpu": [202, 203, 204, 206, 208, 209, 210, 211, 212, 216, 222, 226, 228, 238, 240, 241, 242, 243, 252], "grad_fn": [157, 158, 159, 160, 161, 177, 243], "gradient": [157, 158, 159, 235, 236, 245, 248, 252], "grant": 186, "granular": [17, 28, 195, 202, 203, 204, 222, 225, 226, 228, 244, 253], "granularli": 244, "graph": [20, 23, 24, 159, 171, 178, 180, 182, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 202, 204, 205, 206, 208, 209, 210, 211, 212, 213, 218, 220, 221, 226, 227, 231, 233, 245, 246, 247, 248, 252, 255, 256], "graphmodul": 171, "greater": [17, 28, 222, 223, 224, 226, 228, 244], "greedi": [17, 28, 222, 225, 226, 227, 228], "greedili": [187, 191, 205], "greedy_param": [202, 203, 204, 222, 226, 228], "greedy_select_param": [17, 28, 202, 203, 204, 222, 226], "greedymixedprecisionalgo": [12, 173, 181, 191, 205, 231], "greedyselectionparamet": [17, 28, 202, 203, 204, 222, 226, 228], "green": 229, "grep": 241, "grid": [10, 177, 179, 185, 237, 244, 247], "group": [2, 7, 12, 169, 173, 181, 187, 205, 241, 244, 247, 249, 252], "groupedblockquantizedequant": 244, "groupnorm": 81, "gru": [78, 252], "grucel": 79, "guarante": 20, "guid": [186, 187, 191, 193, 205, 206, 207, 209, 210, 211, 212, 213, 231, 234, 241, 245, 252, 256], "guidebook": 225, "guidelin": [171, 205, 206, 209, 210, 211, 212, 213, 245, 248], "h": [186, 228, 229, 242], "h5": [216, 255], "ha": [1, 8, 11, 20, 23, 24, 29, 157, 170, 171, 172, 175, 178, 180, 181, 182, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 223, 224, 227, 229, 233, 234, 237, 238, 243, 244, 245, 247, 248, 252, 256], "had": [250, 251], "half": 223, "hand": [187, 205], "handl": [13, 14, 24, 25, 182, 218, 219, 240, 243, 244, 247, 252], "hard": 171, "hardshrink": 82, "hardsigmoid": 83, "hardswish": 84, "hardtanh": 85, "hardwar": [9, 178, 184, 238, 239, 245, 247, 248, 256], "hat": 247, "have": [8, 20, 21, 29, 167, 168, 169, 170, 171, 172, 176, 177, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 216, 217, 225, 231, 233, 235, 238, 242, 244, 245, 246, 248, 252, 253, 255, 257], "hba": [189, 201, 209], "he": 225, "head": [20, 195, 196], "heavi": [167, 215, 227], "height": [187, 188, 189, 190, 195, 205, 206, 208, 209, 210, 211, 212, 220, 221, 226, 228, 229], "held": [29, 174, 210, 211], "help": [170, 172, 173, 175, 178, 184, 189, 199, 201, 209, 212, 217, 224, 225, 231, 232, 233, 244, 245, 248, 249, 253, 256, 257], "helper": [173, 177, 181, 194, 208, 231, 235], "hen": [13, 24, 182, 218], "here": [1, 23, 170, 175, 177, 180, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 208, 209, 210, 211, 212, 213, 221, 226, 235, 236, 248, 249, 256, 257], "heterogen": 256, "heurist": [193, 207], "hidden": [20, 195, 196], "hide": 233, "high": [2, 4, 12, 16, 27, 170, 181, 188, 189, 190, 192, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211, 221, 223, 224, 230, 231, 234, 238, 239, 252], "higher": [1, 23, 180, 181, 187, 191, 202, 203, 204, 205, 213, 222, 224, 226, 228, 230, 231, 232, 244, 247, 248, 253, 256], "highest": [13, 24, 182, 218, 224], "highlight": 227, "hingeembeddingloss": 86, "histogram": [8, 21, 176, 183, 199, 212, 214, 247, 252], "histogram_freq": 195, "historgram": [8, 217], "histori": [194, 195, 197, 198], "hold": [157, 158, 159, 174, 219, 233, 249, 253], "home": 241, "honor": [222, 226, 228], "hood": 170, "hook": 247, "hope": [187, 191, 194, 195, 200, 205, 208], "hopefulli": 196, "host": [227, 241, 243, 252], "hostnam": 241, "hotspot": [8, 21, 176, 183, 217, 230], "how": [2, 12, 172, 174, 175, 177, 178, 181, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 225, 226, 228, 231, 233, 234, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 252, 253, 254, 257], "howev": [20, 170, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 222, 226, 228, 234, 240, 244, 247, 250, 252, 256, 257], "html": [8, 21, 167, 171, 176, 183, 186, 199, 212, 214, 215, 217, 240, 241, 242, 252], "htp": 255, "htp_v66": [9, 178, 184, 245, 248], "htp_v68": [9, 178, 184, 245, 248], "htp_v69": [9, 178, 184, 245, 248], "htp_v73": [9, 178, 184, 245, 248], "htp_v75": [9, 178, 184, 245, 248], "htp_v79": [9, 178, 184, 245, 248], "htp_v81": [9, 178, 184, 245, 248], "http": [171, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 227, 240, 241, 242], "hub": [216, 220, 231, 239, 244, 246, 247, 252, 256, 257], "huberloss": 87, "huggingfac": [175, 235, 236], "hx": [78, 79, 95, 96, 122, 123], "hxwx5": 229, "hxwx8": 229, "hyper": [188, 192, 194, 195, 197, 198, 201, 202, 203, 204, 206, 208, 209, 210, 211, 219, 248, 256], "hyperparamet": 248, "i": [1, 2, 4, 7, 8, 9, 12, 13, 17, 20, 21, 22, 23, 24, 27, 28, 29, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257], "i_": [160, 161, 162, 163, 164], "i_0": [160, 161, 162, 163, 164], "i_d": [160, 161, 162, 163, 164], "iccv": [189, 201, 209, 225], "id": [1, 6, 13, 24, 182, 213, 216, 218, 227, 235, 236, 241], "ideal": [191, 199, 200, 212, 252], "idempot": 159, "ident": [196, 220, 250, 251], "identifi": [169, 172, 186, 214, 217, 222, 229, 230, 231, 241, 242, 244, 247, 252, 253, 256], "ie": 233, "ieee": [165, 225], "ignor": [2, 17, 28, 171, 181, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 212, 222, 226, 228, 231, 245, 257], "ignore_quant_ops_list": [1, 23, 180, 213], "illustr": [200, 202, 203, 204, 213, 222, 224, 226, 227, 228, 229, 247, 256], "ilsvrc": [213, 218, 231, 245], "ilsvrc2012": [187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "imag": [8, 21, 176, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 231, 237, 238, 240, 242, 245, 248], "image_bw": 251, "image_dataset": 217, "image_dataset_from_directori": [191, 192, 197, 198, 200, 213, 218, 219, 226, 231, 245, 248], "image_height": [192, 200], "image_net_config": [187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212], "image_net_data_load": [187, 188, 189, 190, 202, 204, 206, 208, 209, 210, 211, 212], "image_net_dataset": [192, 193, 194, 199, 200, 201], "image_net_evalu": [187, 188, 189, 190, 192, 193, 194, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "image_net_train": [202, 203, 204, 208, 209, 210, 211], "image_rgb": 251, "image_s": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 218, 219, 226, 231, 245, 248], "image_width": [192, 200], "imageclassificationevalu": 237, "imagefold": [205, 207, 245], "imagenet": [186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 219, 220, 221, 226, 231, 237, 245, 248], "imagenet_dataset": [207, 213, 218, 219, 245, 248], "imagenet_dir": [197, 198], "imagenetdataload": [187, 188, 189, 190, 199, 202, 204, 206, 208, 209, 210, 211, 212], "imagenetdatapipelin": [187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "imagenetdataset": [192, 193, 194, 199, 200, 201], "imagenetevalu": [187, 188, 189, 190, 192, 193, 194, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "imagenettrain": [202, 203, 204, 208, 209, 210, 211], "images_dir": 208, "images_mean": 205, "images_std": 205, "imagin": 200, "imdb": 195, "img": [191, 226, 231], "img_height": [191, 231], "img_width": [191, 231], "immedi": [189, 193, 201, 207, 209], "impact": [187, 200, 205, 224, 238, 253], "implement": [6, 19, 22, 29, 168, 177, 186, 187, 205, 216, 217, 218, 235, 236, 245, 248, 252, 253, 256, 257], "impli": [187, 205, 231], "import": [1, 11, 20, 22, 23, 29, 157, 158, 159, 160, 161, 163, 164, 165, 166, 169, 170, 171, 172, 174, 175, 177, 178, 180, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 238, 240, 242, 243, 244, 245, 246, 247, 248, 250], "impos": 244, "improv": [188, 189, 190, 192, 193, 194, 195, 197, 198, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 213, 220, 223, 225, 230, 232, 242, 244, 248, 252, 253, 254, 257], "in1": 233, "in2": 233, "in_channel": [169, 244], "in_eval_mod": 207, "in_featur": [29, 170, 174], "in_plac": [22, 178, 184, 235, 236, 245, 248], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "includ": [2, 13, 24, 167, 181, 182, 215, 217, 218, 220, 221, 224, 227, 231, 238, 239, 240, 242, 246, 247, 248, 249, 252, 257], "include_top": [191, 192, 200, 201], "incompat": [194, 195], "incorrect": [173, 233, 252], "incorrectli": [252, 257], "increas": [17, 28, 187, 200, 205, 222, 224, 226, 228, 230, 247], "increment": 257, "incur": 217, "independ": [171, 238, 253], "index": [8, 21, 174, 176, 183, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 252], "index_0": [199, 212], "index_1": [199, 212], "index_n": [199, 212], "indic": [17, 28, 108, 109, 110, 173, 174, 177, 178, 193, 200, 205, 207, 222, 223, 226, 228, 229, 233, 245, 246, 248], "indirect": [187, 205, 231], "individu": [8, 21, 176, 183, 217, 224, 239, 249], "induc": 247, "infer": [1, 11, 13, 22, 23, 24, 180, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 220, 223, 226, 231, 233, 237, 238, 244, 245, 247, 248, 252, 257], "inferencesess": [2, 8, 9, 187, 188, 189, 190, 216, 217, 231, 245], "influenc": 247, "info": [15, 172, 191, 220, 252], "inform": [2, 12, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 170, 172, 181, 187, 191, 205, 208, 231, 239, 243, 244, 246, 252], "inherit": [20, 29, 99, 174], "init": [175, 220, 221], "initi": [6, 9, 160, 161, 165, 174, 177, 178, 184, 187, 188, 192, 198, 199, 200, 205, 206, 210, 211, 212, 213, 216, 220, 221, 245], "initial_accuraci": [207, 218], "initializd": 174, "inner": 253, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "inp_data": 226, "inp_symmetri": [10, 179, 185, 237], "inplac": [165, 172, 220, 221], "input": [2, 6, 7, 8, 9, 10, 12, 17, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 208, 212, 213, 215, 217, 218, 219, 220, 221, 222, 224, 225, 226, 228, 229, 231, 235, 236, 237, 238, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255], "input1": [45, 61, 104], "input2": [45, 61, 104], "input_": [160, 161, 162, 163, 164], "input_1": [213, 220, 221], "input_batch": 216, "input_channel": [7, 169, 244], "input_data": [187, 188, 189, 190, 205, 206, 208, 209, 210, 211, 212, 218, 245], "input_dim": [20, 195, 196], "input_dlc": 255, "input_id": [235, 236], "input_inst": [6, 19, 168, 216], "input_lay": [20, 196], "input_length": 47, "input_list": 255, "input_nam": [178, 184, 187, 188, 189, 190, 213, 220, 231, 245, 248], "input_network": 255, "input_op_nam": [17, 226], "input_q": 177, "input_qdq": 177, "input_qtzr": 29, "input_quant": [12, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 170, 173, 174, 178, 181, 231, 243, 245, 248], "input_shap": [12, 26, 27, 28, 171, 187, 188, 189, 190, 191, 192, 196, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 213, 217, 220, 221, 222, 226, 228, 231, 233, 245, 250], "input_tensor": [20, 171, 191, 192, 200, 201], "inputlay": [20, 213, 220, 221], "inputs_batch": [187, 188, 189, 190, 205, 206, 208, 209, 210, 211, 212, 245], "insert": [171, 187, 188, 189, 190, 192, 194, 195, 197, 198, 200, 201, 202, 204, 205, 206, 208, 209, 210, 211, 245, 247, 256], "insid": [20, 29, 171, 174, 195, 196, 226, 233], "insight": [227, 253], "inspect": [195, 225], "instabl": [194, 208], "instal": [186, 197, 198, 201, 207, 209, 210, 211, 227, 238, 252, 256], "instanc": [6, 19, 29, 168, 171, 172, 184, 216, 227, 257], "instancenorm1d": 88, "instancenorm2d": 89, "instancenorm3d": 90, "instanti": [175, 177, 187, 199, 200, 205, 208, 212, 227, 231, 235, 236, 243, 244, 246, 249, 251], "instead": [165, 171, 172, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 226, 229, 244, 246, 250, 251, 252], "instruct": [186, 239, 240, 241, 242, 244, 248, 252, 255, 257], "int": [1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 21, 22, 23, 24, 25, 28, 58, 59, 60, 108, 109, 110, 160, 161, 162, 163, 164, 165, 169, 170, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 191, 193, 194, 199, 202, 203, 204, 205, 207, 213, 216, 217, 218, 219, 222, 226, 228, 231, 235, 236, 237, 244, 245, 246, 247, 248, 252], "int16": [173, 187, 191, 205, 218, 231, 233, 247, 256, 257], "int32": [235, 236, 246], "int4": [173, 218, 233, 252, 256], "int8": [158, 159, 173, 187, 188, 189, 190, 191, 200, 205, 206, 209, 210, 211, 218, 231, 233, 238, 247, 248, 256, 257], "int_multipli": 29, "intact": 231, "integ": [1, 22, 163, 164, 169, 177, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 231, 235, 238, 244, 245, 246, 248, 256, 257], "integr": [13, 24, 175, 182, 218], "intel": [240, 243], "intellig": 225, "intend": [186, 217, 222, 226, 228, 233, 238, 246], "inter": 214, "interact": [167, 170, 238], "intercept": 247, "interchang": 244, "interest": [8, 176, 183, 217], "interfac": [166, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 240, 252], "intermedi": [2, 6, 12, 19, 24, 168, 178, 181, 182, 184, 187, 191, 205, 214, 216, 218, 231, 238, 245, 248, 252], "intern": [13, 20, 24, 170, 182, 184, 188, 192, 195, 196, 200, 206, 218, 225, 249], "interpol": [12, 187, 205, 224, 231], "interpret": [244, 245, 248], "introduc": [174, 221, 231, 247, 249, 252], "invalid": [171, 244], "invoc": [202, 203, 204], "invok": [1, 8, 9, 21, 174, 176, 178, 183, 184, 200, 213, 217, 225, 227, 245, 248], "involv": [2, 12, 170, 181, 187, 191, 205, 219, 231, 234, 235, 236, 253, 257], "io": [173, 233], "ip": 186, "ipynb": 186, "is_avail": [202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 219, 220, 221, 231, 233, 237, 243, 248], "is_bfloat16": 165, "is_float16": 165, "is_initi": [29, 160, 161, 165, 174, 177], "is_input_quant": [194, 249], "is_leaf_modul": 171, "is_output_quant": [194, 249], "is_quant": [194, 249], "is_sym": 246, "is_symmetr": [194, 240, 246, 249], "is_train": [187, 188, 189, 190, 191, 202, 204, 206, 208, 209, 210, 211, 212, 231], "is_unsigned_symmetr": 170, "isinst": [169, 235, 236, 244], "isol": 247, "issu": [20, 172, 214, 216, 219, 227, 233, 252, 253, 257], "item": [165, 196, 199, 212, 231, 235, 236, 245, 248, 255], "iter": [1, 8, 11, 17, 23, 24, 28, 180, 182, 187, 188, 189, 190, 191, 192, 193, 194, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 217, 218, 222, 226, 228, 231, 245], "itertool": [235, 236], "its": [8, 12, 20, 29, 99, 157, 165, 173, 174, 186, 187, 188, 189, 190, 192, 193, 194, 196, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 217, 222, 226, 229, 231, 233, 235, 236, 238, 239, 244, 247, 249, 257], "itself": [199, 208, 212, 216, 225, 226], "j_": [160, 161, 162, 163, 164], "j_0": [160, 161, 162, 163, 164], "j_d": [160, 161, 162, 163, 164], "jaderberg": 225, "jan": 225, "jenkin": 241, "jian": 225, "jianhua": 225, "jit": [235, 236, 251], "job": [194, 195, 197, 198, 202, 203, 204, 208, 210, 211, 255], "join": [188, 192, 197, 198, 200, 205, 206, 207, 213, 220, 221, 222, 226, 228, 231, 245], "jointli": [198, 210, 211], "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 194, 199, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "jupyt": [186, 239, 252], "just": [187, 191, 194, 195, 199, 200, 205, 210, 211, 212, 229, 247, 252], "k": [177, 207, 228, 235, 236], "kaim": 225, "kd": 252, "keep": [171, 190, 197, 231, 232, 249, 257], "kei": [165, 173, 188, 189, 190, 192, 196, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211, 233, 235, 236, 242, 246], "kept": [12, 231, 245, 248, 253], "kera": [11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 186, 191, 192, 193, 197, 198, 199, 200, 201, 213, 216, 217, 218, 219, 220, 221, 226, 231, 239, 245, 248, 250, 252], "keraslayeroutput": [19, 216], "kernel": [29, 200, 222, 226, 228, 244, 252], "kernel_s": [20, 171, 172, 178, 200, 220, 221, 243, 245, 248], "key_dim": [20, 195, 196], "keyring_1": 242, "keyword": [29, 172], "kill": 241, "kl": 238, "kldivloss": 91, "know": [29, 187, 194, 195, 200, 205, 208], "knowledg": 246, "known": [172, 247], "kullback": 238, "kuzmin": 225, "kwarg": [20, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 172, 173, 174, 178, 196, 231, 245, 248, 252], "l1": [10, 179, 185, 233, 237], "l1loss": 92, "l2": 233, "lab": [238, 256], "label": [187, 191, 192, 193, 194, 197, 198, 200, 205, 207, 213, 217, 218, 219, 226, 231, 235, 236, 245, 248], "label_dataset": 217, "label_mod": [191, 192, 197, 198, 200, 213, 218, 219, 226, 231, 245, 248], "labeled_data": 218, "labeled_data_load": 218, "lambda": [24, 169, 173, 181, 182, 191, 192, 193, 194, 195, 196, 200, 213, 217, 218, 219, 231, 244, 245, 248, 252], "laptop": [238, 239], "larg": [175, 191, 223, 225, 226, 228, 230, 234, 248, 257], "larger": [226, 228, 244], "last": [12, 231], "lastli": 245, "latenc": [223, 232, 252, 257], "later": [184, 187, 194, 195, 199, 200, 208, 212, 240, 243, 252], "latest": [173, 233, 240, 241, 242, 243, 252], "launch": 186, "layer": [1, 3, 4, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 99, 167, 168, 169, 172, 174, 175, 176, 178, 179, 180, 182, 183, 184, 185, 186, 191, 195, 202, 203, 204, 213, 215, 218, 219, 220, 222, 223, 226, 227, 228, 229, 232, 235, 236, 237, 238, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256, 257], "layer1": [199, 212], "layer2": [199, 212], "layer_nam": [8, 21, 176, 183, 217], "layer_output_util": [5, 18, 166, 216], "layern": [199, 212], "layernorm": [20, 97, 195, 196, 252], "layernorm1": [20, 196], "layernorm2": [20, 196], "layeroutpututil": [6, 19, 168, 216], "layers_to_exclud": 172, "layout": [159, 171], "lceil": [160, 161, 163, 164, 165, 247], "lead": [8, 201, 209, 213, 217, 253], "leaf": [171, 173, 252], "leakyrelu": 98, "learn": [170, 184, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 220, 221, 225, 232, 234, 235, 236, 237, 238, 242, 248, 252, 257], "learnabl": [160, 161], "learnedgrid": 252, "learnedgridquant": 170, "learning_r": [202, 203, 204, 208, 210, 211, 219, 248], "learning_rate_schedul": [202, 203, 204, 208, 210, 211], "least": [181, 188, 192, 206], "leav": [213, 236], "left": [160, 161, 162, 163, 164, 165, 177, 187, 188, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 224, 229, 231, 234, 244, 247], "legaci": [240, 252], "leibler": 238, "len": [1, 23, 171, 180, 188, 191, 195, 205, 207, 213, 218, 226, 231, 235, 236, 245, 248], "length": [169, 174, 244, 246], "leq": [160, 161, 162, 163, 164], "less": [191, 205, 222, 224, 231, 232, 238, 244, 247, 249, 256], "lesser": [187, 205], "let": [171, 187, 191, 194, 195, 200, 205, 256], "level": [1, 2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27, 28, 169, 170, 171, 173, 176, 177, 179, 180, 181, 182, 183, 184, 185, 188, 190, 191, 192, 195, 197, 198, 202, 203, 204, 206, 210, 211, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 231, 232, 233, 234, 237, 239, 244, 245, 247, 248, 252, 253], "lfloor": [160, 161, 162, 163, 164, 165], "lib": [241, 242], "libjpeg": 242, "liblapack": 240, "libpymo": [170, 240, 252], "libqnnhtp": 255, "libqnnmodeldlc": 255, "librari": [1, 9, 175, 213, 238, 241, 242, 245, 255], "lie": 244, "light": [187, 188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212], "lightweight": 234, "like": [20, 170, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 216, 217, 225, 230, 231, 232, 239, 246, 247, 249, 255, 256], "limit": [187, 188, 189, 190, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 252], "line": [241, 242, 245, 255], "linear": [1, 3, 11, 14, 15, 23, 26, 29, 99, 169, 170, 171, 172, 174, 175, 180, 213, 219, 220, 221, 222, 230, 244, 252], "linear1": [169, 244], "linear_1": 244, "link": [186, 242], "linux": [240, 242], "list": [1, 2, 3, 6, 9, 12, 13, 14, 15, 17, 19, 20, 23, 24, 26, 27, 28, 58, 59, 60, 108, 109, 110, 163, 164, 165, 168, 169, 171, 172, 173, 174, 176, 179, 180, 181, 182, 183, 185, 187, 191, 194, 196, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 224, 226, 228, 233, 235, 236, 237, 241, 242, 244, 245, 246, 249, 257], "list_of_module_comp_ratio_pair": [17, 28, 222, 226, 228], "listen": 227, "lite": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "liter": [173, 233], "littl": [170, 188, 192, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211, 225, 257], "llm": [252, 256], "ln": 242, "load": [22, 171, 172, 175, 184, 187, 188, 189, 190, 195, 213, 218, 219, 220, 221, 222, 225, 226, 228, 231, 235, 236, 237, 238, 245, 248, 252, 255, 256], "load_adapt": [235, 236], "load_checkpoint": 184, "load_data": 195, "load_dataset": [213, 218, 219, 231, 235, 236, 237, 245], "load_encod": [213, 245, 248], "load_encodings_to_sim": [216, 245, 248], "load_model": [187, 188, 189, 190, 213, 216, 220, 221, 231, 245], "load_state_dict": 165, "loader": [1, 2, 8, 10, 23, 24, 25, 173, 179, 180, 181, 182, 185, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 204, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 231, 237, 245, 248], "local": [186, 227, 237, 240, 241, 242], "localresponsenorm": 100, "locat": [187, 191, 194, 199, 200, 205, 208, 212, 241, 257], "log": [172, 173, 191, 194, 195, 197, 198, 217, 231, 233], "log_2": 165, "log_dir": [194, 195, 197, 198], "log_fil": [173, 233], "log_input": 121, "log_prob": 47, "logdir": 195, "logger": 172, "logic": [2, 12, 29, 99, 174, 181, 187, 188, 189, 190, 202, 203, 204, 205, 206, 209, 210, 211, 212, 231, 252], "logit": [205, 207, 231, 235, 236, 248], "logsigmoid": 101, "logsoftmax": 102, "long": 235, "longer": [170, 178, 202, 203, 204, 240, 244, 245, 246, 248, 256], "look": [20, 187, 191, 194, 195, 196, 199, 200, 205, 212, 239, 246, 255], "lookup": 196, "lookup_quant": [12, 231], "loop": [171, 187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 248, 253], "lora": [175, 238, 252], "lora_a": [175, 235, 236], "lora_a_lay": 236, "lora_add_lay": 236, "lora_alpha": 175, "lora_b": [175, 235, 236], "lora_b_lay": 236, "lora_config": 175, "lora_dropout": 175, "lora_modul": 175, "lora_mul_lay": 236, "lora_weights_after_adaptation_for_adapter1": 175, "loraconfig": 175, "loralay": [235, 236], "lose": 229, "loss": [1, 8, 10, 11, 21, 23, 157, 176, 179, 180, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 201, 206, 209, 210, 211, 213, 218, 219, 225, 232, 235, 236, 237, 239, 245, 247, 248, 252], "loss_fn": [10, 179, 185, 194, 219, 235, 236, 237, 248], "lost": [247, 257], "low": [170, 177, 200, 202, 204, 209, 225, 230, 234, 238, 245, 246, 252], "lower": [1, 23, 180, 187, 191, 200, 205, 213, 221, 224, 231, 232, 238, 244, 253, 256], "lowest": 222, "lpbq": [169, 244, 246, 252], "lppool1d": 93, "lppool2d": 94, "lr": [219, 248], "lstm": [95, 252], "lstmcell": 96, "lsvrc": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "lt": 240, "m": [186, 195, 228, 240, 241, 242, 243], "mac": [17, 28, 187, 202, 203, 204, 205, 222, 223, 225, 226, 228, 230, 231], "machin": [225, 234, 238, 240, 241, 242, 252, 257], "made": [171, 173, 205, 206, 209, 210, 211, 233, 238, 249, 252], "magnitud": 222, "mai": [2, 13, 20, 24, 29, 157, 158, 159, 167, 170, 171, 174, 175, 181, 182, 187, 189, 191, 194, 195, 199, 200, 201, 205, 208, 209, 212, 215, 217, 218, 221, 225, 231, 244, 246, 252, 257], "main": [219, 249], "maintain": [188, 192, 206, 218, 224, 225, 238], "major": [225, 246], "make": [173, 174, 194, 195, 196, 200, 208, 233, 238, 249, 250, 251, 252, 256, 257], "makedir": [187, 188, 191, 192, 194, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211], "man": 242, "manag": [240, 243, 256], "mandatori": 216, "manditori": 20, "mani": [171, 187, 191, 194, 195, 199, 200, 202, 203, 204, 205, 212, 213, 223, 252], "manner": [13, 24, 182, 218, 221], "mantissa": [165, 169, 244], "mantissa_bit": [165, 169, 244], "manual": [17, 28, 160, 161, 170, 173, 202, 203, 204, 218, 222, 225, 226, 228, 240, 246, 252], "manual_param": [222, 226, 228], "manual_se": 205, "manualmodeparam": [17, 28, 222, 226, 228], "manylinux_2_34_x86_64": [240, 242], "map": [12, 13, 22, 29, 158, 163, 164, 172, 174, 175, 191, 192, 193, 194, 196, 200, 213, 217, 218, 219, 224, 231, 235, 236, 238, 245, 246, 247, 248, 249, 252], "map_loc": 216, "marginrankingloss": 104, "marku": 225, "mask": 29, "maskedadd": 29, "match": [165, 178, 184, 196, 217, 222, 225, 226, 228, 229, 244, 245, 248, 249, 252, 253], "math": [213, 218, 231, 245, 257], "mathemat": [177, 187, 200, 205, 238, 251], "matmul": [7, 252], "matmul_8": 172, "matric": 235, "matrix": 200, "matter": [29, 212], "max": [8, 21, 160, 161, 165, 167, 170, 175, 176, 183, 190, 197, 198, 200, 214, 215, 225, 240, 245, 246, 252, 255, 256], "max_epoch": [202, 203, 204, 208, 210, 211], "max_iter": [235, 236], "maximum": [2, 12, 13, 21, 24, 163, 164, 165, 181, 182, 187, 191, 192, 193, 194, 199, 200, 201, 205, 212, 217, 218, 231, 247], "maxlen": [20, 195, 196], "maxpool1d": 105, "maxpool2d": 106, "maxpool3d": 107, "maxpooling2d": 194, "maxunpool1d": 108, "maxunpool2d": 109, "maxunpool3d": 110, "mdoel": 219, "mean": [25, 174, 187, 191, 194, 195, 196, 199, 200, 202, 203, 204, 205, 207, 208, 209, 212, 213, 214, 218, 219, 231, 245, 247, 249], "meaning": 191, "measur": [2, 8, 12, 21, 28, 176, 181, 183, 187, 202, 203, 204, 205, 217, 222, 226, 228, 231, 238, 257], "mechan": [22, 171, 177, 213, 233, 245, 248], "meet": [13, 24, 181, 182, 187, 191, 205, 218, 223, 224, 231, 257], "member": [10, 237, 249], "memori": [17, 28, 159, 175, 202, 203, 204, 222, 223, 225, 226, 228, 229, 230, 232, 238, 252, 257], "memory_format": 159, "merg": 238, "messag": 191, "met": [2, 12, 181, 187, 191, 205, 231], "meta": 175, "meta_data": 175, "metapackag": 186, "method": [6, 13, 19, 22, 24, 29, 99, 159, 168, 170, 171, 173, 174, 178, 181, 182, 184, 188, 189, 190, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 216, 218, 224, 225, 230, 231, 233, 235, 237, 238, 245, 247, 248, 253], "methodologi": 257, "metric": [17, 28, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 212, 213, 217, 218, 219, 222, 226, 228, 231, 237, 245, 247, 248, 257], "mha": [195, 252], "middl": 257, "might": [2, 12, 181, 187, 188, 191, 192, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 212, 213, 217, 231, 242, 246, 252, 253, 257], "migrat": [166, 245, 252], "mimic": 256, "mimick": 170, "min": [8, 21, 160, 161, 167, 170, 175, 176, 183, 190, 197, 198, 200, 214, 215, 240, 246, 252, 255, 256], "min_max": [9, 245, 252], "min_max_rang": [199, 212, 217], "minim": [179, 185, 187, 190, 191, 197, 198, 205, 210, 211, 221, 230, 231, 232, 237, 238, 239, 247], "minima": 237, "minimum": [163, 164, 171, 199, 212, 237, 247, 252], "minmaxencodinganalyz": 165, "minor": [246, 252], "miou": [187, 205], "mish": 111, "mismatch": [177, 200, 214, 216, 245], "miss": [20, 165, 172, 255], "missing_kei": 165, "mix": [2, 12, 13, 24, 173, 181, 182, 186, 218, 250, 252, 257], "mixed_precis": [5, 18, 166, 187, 191, 205, 231, 233], "mixed_precision_algo": [2, 173, 181, 187, 191, 205, 231], "mixed_preision_quant_model": 231, "mixedprecisionconfigur": [173, 233], "mixin": [29, 174], "mkdir": 186, "ml": [190, 195, 197, 198, 210, 211, 225], "mmp": 232, "mmp_log": [173, 233], "mnist": [222, 226, 228], "mnist_after_bn_re_estimation_qat_range_learn": 194, "mnist_torch_model": 222, "mnist_trained_on_gpu": [222, 226, 228], "mnt": 241, "mobil": [238, 239], "mobilenet": [213, 219, 237, 238], "mobilenet_v2": [213, 218, 219, 220, 221, 231, 233, 237, 243, 245, 248], "mobilenet_v2_weight": [213, 220, 221, 231, 237, 245], "mobilenetv2": [213, 218, 219, 220, 221, 243, 245, 248], "mobilenetv2_1": [213, 220, 221], "mode": [9, 13, 17, 22, 24, 28, 171, 178, 182, 184, 202, 203, 204, 218, 222, 226, 228, 245, 249, 252], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 234, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 249, 252, 254, 255, 256, 258], "model_after_qat": [195, 197, 198], "model_id": [235, 236], "model_input": [172, 194], "model_or_pipelin": [213, 237], "model_output": 194, "model_prepar": [18, 166, 196, 205, 206, 208, 209, 210, 211, 212, 217, 220, 221, 252], "model_prepare_requir": [24, 182, 218], "model_preparer_elementwise_add_exampl": 171, "model_preparer_functional_exampl": 171, "model_preparer_reused_exampl": 171, "model_preparer_subclassed_model_with_functional_lay": 20, "model_preparer_two_subclassed_lay": 20, "model_torch": 216, "model_transform": 171, "model_valid": 166, "model_weights_in_correct_ord": 196, "modelcompressor": [17, 28, 202, 203, 204, 222, 226, 228], "modeling_opt": [235, 236], "modelprepar": [20, 171, 196, 205, 206, 209, 210, 211, 212], "modelproto": [1, 3, 4, 6, 8, 9, 213, 216, 217, 220, 221, 245], "modelvalid": 172, "modelwithelementwiseaddop": 171, "modelwithfunctionallinear": 172, "modelwithfunctionalrelu": 171, "modelwithnontorchfunct": 171, "modelwithoutfunctionallinear": 172, "modelwithoutreusednod": 172, "modelwithreusednod": 172, "modelwithreusedrelu": 171, "modif": [205, 206, 209, 210, 211], "modifi": [22, 171, 175, 178, 184, 187, 188, 189, 190, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 220, 221, 229, 231, 235, 236, 242, 245, 247, 248, 252, 256, 257], "modul": [1, 2, 7, 12, 17, 23, 24, 25, 26, 27, 28, 29, 99, 165, 167, 168, 169, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 229, 231, 233, 235, 236, 237, 243, 244, 245, 248, 250, 251, 252, 257], "module_cl": 29, "module_classes_to_exclud": 171, "module_nam": [235, 236], "module_to_exclud": 171, "modulecompratiopair": [17, 28, 222, 226, 228], "moduledict": [29, 170, 174, 178, 243, 245, 248], "modulelist": [29, 170, 174, 178, 243, 245, 248], "modules_to_exclud": [24, 171, 179, 182, 185, 218, 237], "modules_to_ignor": [17, 28, 176, 183, 202, 203, 204, 217, 222, 226, 228], "momentum": [219, 220, 221], "monitor": 217, "monoton": [17, 28, 222, 224, 226, 228], "more": [1, 17, 22, 23, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 166, 170, 172, 174, 180, 181, 184, 187, 191, 194, 195, 196, 199, 200, 205, 208, 212, 213, 217, 221, 222, 224, 225, 226, 227, 228, 231, 232, 233, 238, 245, 247, 248, 249, 252, 253, 255, 256, 257], "most": [177, 195, 218, 238, 240, 241, 247, 249], "move": [186, 248], "movi": 195, "mp": [173, 233], "mp_configur": 233, "mse": [8, 10, 21, 170, 176, 179, 183, 185, 217, 247, 252], "mseloss": 103, "much": [202, 203, 204, 229, 257], "mul": 175, "mul_scal": 236, "multi": [195, 252], "multigpu": 252, "multiheadattent": [20, 195, 196, 252], "multilabelmarginloss": 112, "multilabelsoftmarginloss": 113, "multimarginloss": 114, "multipl": [17, 24, 26, 27, 28, 159, 168, 171, 172, 173, 174, 178, 182, 184, 199, 212, 216, 218, 220, 221, 222, 225, 226, 228, 233, 242, 244, 245, 247, 248, 252], "multipli": [174, 187, 205, 223, 225, 226, 228, 230], "must": [20, 159, 165, 169, 172, 174, 188, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 206, 209, 210, 211, 212, 213, 217, 218, 219, 223, 229, 237, 244, 245, 249, 256], "mutual": [165, 169, 244], "my_functional_model": 20, "myfunctionalmodel": 20, "n": [8, 13, 21, 176, 177, 183, 217, 218, 228, 244], "nagel": 225, "name": [1, 2, 8, 12, 17, 20, 21, 29, 167, 168, 173, 174, 175, 176, 178, 181, 183, 184, 187, 188, 189, 190, 194, 196, 199, 212, 213, 215, 216, 217, 220, 221, 226, 227, 231, 233, 235, 236, 241, 242, 245, 246, 247, 248, 252, 255], "name_": [8, 21, 176, 183, 199, 212, 217], "name_to_module_dict": 175, "name_to_quantizer_dict": [2, 12, 173, 181, 231], "named_modul": [235, 236], "named_paramet": 235, "namedtupl": 165, "namespac": [166, 170, 180, 181, 182, 183, 184, 185], "naming_schem": [168, 216], "namingschem": [168, 216], "nativ": [174, 241], "navig": 186, "na\u00efv": 230, "nconv": 220, "ndarrai": [2, 6, 8, 9, 12, 196, 213, 216, 217, 218, 231, 245], "nearest": [9, 13, 21, 22, 24, 182, 184, 188, 191, 192, 195, 197, 198, 200, 201, 206, 209, 213, 217, 218, 245, 248], "necessari": [157, 158, 159, 178, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 208, 209, 210, 211, 212, 217, 220, 222, 226, 227, 228, 245, 248, 257], "necessarili": [187, 191, 205, 231], "necessit": 240, "need": [1, 8, 17, 19, 20, 21, 22, 23, 28, 168, 173, 175, 176, 180, 183, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 217, 219, 220, 221, 222, 226, 228, 231, 232, 233, 237, 238, 240, 242, 244, 245, 246, 247, 248, 249, 250, 252, 255, 256, 257], "neg": [147, 148, 163, 164, 187, 194, 195, 200, 205], "negat": [188, 192, 206], "nest": 252, "net": [186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 255], "network": [20, 174, 193, 195, 196, 200, 207, 223, 224, 225, 227, 238, 243, 247, 248, 252, 253], "neural": [187, 193, 200, 205, 207, 223, 225, 238, 247, 248, 253], "neuron": 238, "new": [15, 20, 159, 160, 161, 170, 171, 175, 184, 192, 196, 197, 198, 200, 201, 205, 212, 220, 231, 233, 234, 235, 239, 240, 244, 250, 251, 252], "new_empti": 159, "next": [175, 187, 189, 191, 194, 195, 200, 205, 208, 213, 221, 242, 243, 245, 247, 252, 253], "next_conv_weight": 221, "nfolded_model": 220, "night": [188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212], "nllloss": 115, "nllloss2d": 116, "nmodel": [220, 221], "nn": [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 160, 161, 166, 167, 169, 170, 171, 172, 173, 178, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 222, 226, 228, 231, 233, 235, 236, 237, 243, 244, 245, 248, 251, 252, 257], "nncf": 238, "nnext": 221, "no_grad": [172, 178, 205, 206, 207, 209, 210, 211, 212, 213, 231, 235, 236, 237, 243, 245, 248], "node": [22, 24, 171, 178, 181, 182, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 209, 210, 211, 218, 220, 221, 237, 238, 245, 248, 251, 252, 256, 257], "noffset": 177, "nois": [217, 221, 248, 249, 256], "noisi": [194, 208], "non": [171, 173, 179, 185, 193, 207, 227, 237, 247, 252, 255, 256], "none": [1, 2, 6, 8, 9, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 58, 59, 60, 70, 78, 79, 95, 96, 99, 108, 109, 110, 122, 123, 159, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 227, 228, 231, 233, 237, 240, 241, 242, 243, 244, 245, 248], "norm": [15, 191, 201, 213, 217, 221, 231, 252], "normal": [15, 207, 213, 217, 218, 220, 231, 238, 245, 252], "notabl": 246, "note": [2, 8, 9, 12, 13, 20, 21, 22, 24, 170, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 216, 217, 218, 222, 226, 227, 228, 231, 233, 237, 240, 241, 242, 244, 245, 248], "note1": [187, 205, 208, 212], "note2": [187, 205, 208, 212], "notebook": [188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 252], "noth": [17, 28, 222, 226, 228, 257], "notic": [167, 215, 252], "notimplementederror": 29, "now": [20, 171, 172, 177, 178, 187, 188, 189, 190, 191, 194, 195, 196, 197, 198, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 220, 221, 231, 245, 246, 248, 252], "np": [2, 20, 191, 196, 213, 217, 218, 226, 231, 240, 245], "nprepared_model": [220, 221], "nprev": 221, "nscale": 177, "num": [163, 164], "num_batch": [1, 8, 10, 11, 21, 23, 25, 179, 180, 185, 188, 189, 192, 193, 194, 199, 200, 206, 207, 208, 209, 212, 213, 217, 218, 219, 237, 245], "num_bias_correct_sampl": 209, "num_calibration_sampl": [213, 218, 219, 231, 245, 248], "num_candid": [10, 179, 185, 237], "num_channel": 246, "num_class": 217, "num_comp_ratio_candid": [17, 28, 202, 203, 204, 222, 226, 228], "num_epoch": 219, "num_eval_sampl": [231, 245], "num_head": [20, 195, 196], "num_iter": [191, 226, 231], "num_of_sampl": 218, "num_quant_sampl": 209, "num_reconstruction_sampl": [28, 202, 204, 222], "num_sampl": [2, 173, 181, 193, 194, 205, 207, 213, 217, 231], "num_samples_for_phase_1": [13, 24, 182, 218], "num_samples_for_phase_2": [13, 24, 182, 218], "num_step": [163, 164], "num_word": 195, "num_work": [187, 188, 189, 190, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 218, 219, 237], "number": [1, 2, 8, 10, 11, 13, 14, 17, 20, 21, 22, 23, 24, 25, 28, 29, 163, 164, 165, 169, 171, 173, 174, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 217, 218, 219, 222, 224, 225, 226, 227, 228, 229, 231, 234, 237, 238, 241, 244, 245, 246, 247, 248, 252, 256, 257], "numer": [168, 216], "numpi": [187, 188, 189, 190, 196, 213, 217, 218, 226, 231, 240, 245], "numpy_help": [220, 221], "nupi": [12, 231], "nvidia": [238, 240, 241, 242, 243], "o": [187, 188, 191, 192, 193, 194, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 220, 221, 222, 226, 228, 231, 240, 245], "object": [1, 2, 7, 8, 9, 10, 12, 13, 22, 24, 29, 157, 158, 159, 165, 167, 169, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 187, 193, 194, 199, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 222, 226, 228, 231, 232, 233, 237, 239, 244, 245, 247, 248, 256, 257], "observ": [9, 29, 165, 174, 178, 200, 208, 217, 225, 245, 247, 248], "obtain": [6, 168, 217, 223, 246, 255], "obvious": [222, 226, 228], "occur": [167, 215, 246], "occurr": [8, 21, 176, 183, 217, 222], "oct": 225, "off": [1, 11, 22, 23, 180, 187, 191, 205, 213, 219, 231, 245, 248, 257], "offer": [8, 176, 183, 193, 207, 214, 217, 218, 240], "offset": [8, 21, 29, 70, 160, 161, 162, 163, 164, 176, 177, 183, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 217, 240, 243, 245, 246, 247, 248, 252, 255, 256], "offset_": [160, 161, 162, 163, 164], "often": [218, 220, 221, 225], "older": [233, 240], "omit": [224, 241, 249], "onc": [29, 159, 172, 175, 177, 190, 195, 197, 198, 202, 203, 204, 209, 210, 211, 222, 225, 233, 248, 250, 251, 252, 255, 256, 257], "one": [9, 23, 24, 26, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 218, 220, 222, 225, 226, 227, 228, 233, 235, 236, 240, 241, 242, 243, 244, 245, 246, 248, 252, 256], "ones": [243, 247], "ones_lik": [160, 161], "onli": [2, 8, 9, 12, 22, 157, 158, 159, 163, 164, 167, 169, 170, 171, 173, 174, 176, 177, 178, 181, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 215, 216, 217, 220, 225, 226, 227, 229, 231, 233, 234, 235, 236, 237, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 257], "onnx": [0, 1, 2, 3, 5, 6, 8, 9, 24, 168, 173, 178, 182, 184, 186, 213, 215, 216, 217, 218, 219, 220, 221, 231, 233, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 251, 252, 255, 256, 257], "onnx_encoding_path": 245, "onnx_export_arg": [24, 168, 178, 182, 184, 216, 218, 245, 248], "onnx_file_nam": 251, "onnx_model": 217, "onnx_util": 216, "onnxexportapiarg": [24, 168, 178, 182, 184, 216, 218, 245, 248], "onnxmodel": [8, 9, 217, 245], "onnxruntim": [187, 188, 189, 190, 216, 217, 242, 245, 252], "onnxruntime_v": 242, "onnxsim": [187, 188, 189, 190, 213, 216, 217, 220, 221, 231, 245], "onto": 256, "op": [1, 2, 9, 11, 12, 15, 17, 22, 23, 24, 172, 178, 180, 181, 182, 184, 187, 188, 189, 190, 192, 194, 195, 197, 198, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 213, 218, 220, 226, 245, 247, 248, 249, 252, 255], "op_list": [194, 249], "op_typ": [2, 7, 181, 187, 194, 205, 231, 249], "op_type_map": 172, "open": [194, 238], "oper": [7, 17, 20, 29, 170, 171, 172, 174, 187, 190, 191, 196, 197, 198, 205, 210, 211, 216, 221, 226, 230, 231, 237, 238, 243, 245, 247, 248, 249, 251, 252, 253, 256, 257], "oppos": [189, 201, 209], "opset": 252, "opset_vers": [178, 184, 245, 248], "opt": [191, 235, 236], "optim": [1, 2, 11, 12, 13, 23, 24, 28, 165, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 225, 227, 231, 235, 236, 237, 238, 239, 242, 245, 247, 248, 252, 256, 257], "optimized_accuraci": [207, 218], "option": [1, 2, 8, 9, 10, 11, 12, 13, 17, 21, 22, 23, 24, 25, 28, 29, 160, 161, 162, 163, 164, 165, 169, 171, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 222, 226, 228, 231, 237, 239, 240, 241, 242, 244, 245, 247, 248, 249, 252, 255], "optlearnedpositionalembed": [235, 236], "optmiz": [187, 205], "orang": 229, "order": [29, 99, 172, 174, 188, 194, 195, 196, 197, 198, 199, 202, 203, 204, 206, 208, 210, 211, 212, 214, 222, 233, 244, 248, 249, 250, 251, 255, 256], "org": [171, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 240, 241, 242], "org_top1": 231, "organ": [238, 254], "origin": [17, 20, 28, 29, 166, 170, 171, 174, 179, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 216, 217, 222, 223, 224, 226, 227, 228, 230, 231, 237, 238, 247, 252, 257], "original_model": [20, 196], "original_model_weight": 196, "ort": [9, 187, 188, 189, 190, 245], "oscil": 219, "other": [159, 169, 170, 171, 173, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 224, 233, 234, 235, 236, 239, 240, 243, 244, 247, 251, 252, 253, 254], "otherwis": [9, 13, 160, 161, 163, 164, 169, 172, 175, 178, 184, 188, 189, 190, 192, 193, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 218, 221, 244, 245, 246, 248, 253, 257], "our": [175, 194, 195, 208], "out": [8, 20, 21, 22, 160, 161, 162, 163, 164, 165, 171, 175, 176, 177, 178, 183, 184, 196, 217, 240, 243, 245, 248, 252], "out1": [20, 196, 233], "out2": 233, "out3": 233, "out_": [160, 161, 162, 163, 164], "out_channel": [169, 244], "out_featur": [29, 170, 174], "outlier": [217, 247], "outlin": [223, 234], "output": [2, 6, 7, 8, 9, 12, 13, 17, 19, 20, 21, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 160, 161, 163, 165, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 225, 226, 228, 229, 230, 231, 237, 238, 240, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255], "output_bw": [13, 24, 175, 182, 218], "output_dim": [20, 195, 196], "output_dir": 255, "output_dir_path": 255, "output_dlc": 255, "output_encod": 29, "output_nam": [178, 184, 187, 188, 189, 190, 213, 220, 231, 245, 248], "output_op_nam": [12, 17, 226, 231], "output_path": 255, "output_qtzr": 29, "output_quant": [12, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 170, 173, 174, 178, 181, 231, 243, 245, 248], "output_s": [58, 59, 60, 108, 109, 110], "output_shap": 196, "outsid": 249, "over": [8, 20, 24, 163, 164, 174, 177, 182, 217, 218, 222, 224, 225, 226, 228, 230, 231, 247], "overal": [223, 224, 253], "overfit": 237, "overhead": [191, 202, 204, 205, 247], "overlin": [161, 164], "overload": [9, 163, 164, 178, 245, 248], "overrid": [24, 159, 171, 178, 182, 184, 205, 206, 209, 210, 211, 218, 245, 248], "overridden": [29, 174, 249], "overtax": 257, "overview": [170, 254], "overwri": [245, 248], "overwriiten": [245, 248], "overwritten": [170, 236], "own": [217, 218, 256], "p": [187, 205, 241], "p1": 233, "p2": 233, "packag": [166, 186, 242, 243, 252], "pad": [20, 171, 172, 178, 194, 220, 221, 243, 245, 248], "pad_sequ": 195, "page": [223, 239, 240, 241, 242, 243, 244, 247, 257], "pair": [3, 14, 17, 26, 28, 219, 220, 222, 226, 228], "pairwisedist": 118, "parallel": 248, "param": [1, 2, 10, 11, 12, 17, 21, 23, 28, 29, 99, 172, 173, 174, 175, 176, 179, 180, 181, 183, 185, 187, 188, 189, 190, 192, 193, 194, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 217, 220, 221, 222, 226, 228, 231, 232, 233, 235, 237, 244, 246, 256], "param_bitwidth": [213, 245, 246, 248], "param_bw": [13, 24, 175, 182, 218], "param_bw_override_list": [1, 23, 180, 213], "param_encod": [170, 246], "param_nam": [8, 21, 176, 183, 217], "param_name_": [8, 21, 176, 183, 199, 212, 217], "param_quant": [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 170, 174, 175, 178, 243, 244, 245, 248], "paramet": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 215, 216, 217, 218, 219, 220, 221, 222, 225, 226, 228, 230, 231, 233, 235, 236, 237, 238, 244, 245, 246, 249, 252, 255, 256, 257], "parameter_quant": [2, 12, 173, 181, 231], "parent": [29, 99, 174, 231, 252], "pareto": [2, 12, 13, 24, 181, 182, 187, 191, 205, 218], "pareto_front": 218, "pareto_front_list": [187, 205, 231], "pars": [23, 178, 180, 184, 213, 245, 248], "part": [8, 20, 21, 176, 183, 217, 225, 244, 245, 256], "partial": [24, 171, 182, 218, 245, 248], "particular": [169, 172, 187, 191, 205, 231, 244, 249], "pass": [1, 2, 8, 9, 12, 13, 20, 21, 22, 23, 24, 25, 29, 99, 165, 168, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 226, 227, 231, 233, 235, 236, 237, 238, 244, 245, 247, 248, 251, 252, 253, 255, 257], "pass_calibration_data": [187, 188, 189, 190, 192, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 219, 245, 248], "passwd": 241, "past": [186, 252], "patch": 246, "path": [1, 2, 8, 9, 11, 12, 13, 17, 21, 22, 23, 24, 28, 167, 171, 173, 175, 176, 178, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 241, 242, 245, 248, 255], "path_to_imagenet": [231, 245, 248], "pathlik": [245, 248], "pattern": [225, 252], "pb": 255, "pcq": [8, 21, 176, 183, 217], "pcq_quantsim_config": 200, "pdf": [8, 21, 176, 183, 217, 252], "peft": [166, 235, 236, 252], "peft_model_id": [235, 236], "peft_util": 175, "peftquantutil": 175, "pendyam": 225, "per": [7, 8, 17, 21, 28, 168, 169, 174, 175, 176, 183, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 216, 219, 220, 221, 222, 226, 227, 228, 230, 231, 237, 238, 246, 247, 248, 249, 252], "per_block": 246, "per_block_int_scal": 246, "per_channel": 246, "per_channel_quant": [174, 194, 200, 246, 249], "per_layer_mse_loss": [199, 212, 217], "per_layer_quant_dis": [199, 212, 217], "per_layer_quant_en": [199, 212, 217], "per_sample_weight": 70, "per_tensor": 246, "percentag": [188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212, 238], "perform": [2, 3, 4, 8, 10, 12, 13, 16, 21, 23, 24, 25, 27, 29, 99, 160, 161, 165, 174, 175, 176, 179, 180, 181, 182, 183, 185, 187, 188, 189, 190, 191, 192, 193, 195, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 230, 234, 235, 236, 237, 238, 240, 242, 243, 244, 245, 246, 247, 248, 252, 254, 256, 257], "perform_per_layer_analysis_by_disabling_quant": [8, 217], "perform_per_layer_analysis_by_disabling_quant_wrapp": [21, 176, 183, 217], "perform_per_layer_analysis_by_enabling_quant": [8, 217], "perform_per_layer_analysis_by_enabling_quant_wrapp": [21, 176, 183, 217], "perhap": [187, 194, 199, 200, 205, 208, 212], "period": [1, 11, 23, 180, 213], "persist": 165, "person": 238, "perspect": [187, 200, 205], "phase": [2, 12, 13, 24, 173, 181, 182, 187, 191, 205, 218, 225], "phase1": [2, 12, 181, 187, 205, 231], "phase1_optim": [2, 12, 181, 187, 205, 231], "phase2": 181, "phase2_revers": 181, "phone": [238, 239], "php": [186, 187, 191, 194, 199, 200, 205, 208, 212], "pick": [2, 12, 20, 181, 187, 191, 205, 223, 224, 231], "pickl": [17, 28, 222, 226, 228], "pictur": [188, 189, 190, 191, 192, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212], "piec": [20, 171], "pillow": 242, "pin": 252, "pin_memori": [159, 171, 205], "pink": 229, "pinpoint": 217, "pip": [186, 227, 240, 241, 242, 243, 252], "pip3": 242, "pipelin": [24, 182, 195, 197, 198, 216, 218, 247, 252, 253, 256], "pitr": 225, "pixelshuffl": 119, "pixelunshuffl": 120, "place": [2, 4, 12, 22, 23, 27, 178, 180, 181, 184, 187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 221, 231, 245, 248, 249, 252], "place_model": [235, 236], "placehold": [235, 236], "placement": [187, 188, 189, 190, 202, 203, 204, 205, 206, 209, 210, 211, 212, 252], "plai": 200, "plan": [241, 256], "platform": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 217, 238, 239, 240], "pleas": [166, 170, 172, 175, 178, 184, 187, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 208, 212, 217, 222, 226, 228, 240, 245, 248, 252, 255], "plot": [2, 12, 167, 181, 187, 191, 199, 205, 212, 215, 217, 231], "pmatrix": [160, 161, 162, 163, 164], "point": [2, 8, 12, 20, 21, 29, 157, 158, 169, 170, 175, 176, 181, 183, 184, 187, 188, 189, 191, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 217, 220, 221, 231, 232, 238, 239, 244, 246, 247, 252, 253, 256, 257], "pointer": [191, 194, 195, 199, 200, 212], "poissonnllloss": 121, "pool": [191, 192, 194, 200, 201], "pop": 196, "popul": 246, "popular": 234, "port": [227, 241, 252], "port_id": 241, "portabl": 238, "portion": 20, "pos_emb": [20, 196], "posit": [20, 147, 148, 163, 164, 194, 195, 196], "possibl": [2, 3, 9, 12, 13, 20, 22, 24, 172, 178, 181, 182, 184, 187, 191, 193, 196, 205, 207, 217, 218, 220, 224, 231, 244, 245, 248, 249, 250, 251, 253, 257], "post": [13, 24, 182, 188, 189, 190, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 217, 218, 221, 225, 230, 238, 239, 245, 246, 247, 248, 252, 257], "post1": 242, "post_training_tf": [1, 8, 9, 11, 21, 22, 23, 167, 176, 180, 183, 187, 191, 192, 194, 195, 197, 200, 201, 205, 208, 209, 213, 215, 217, 218, 245, 246, 248, 252], "post_training_tf_enhanc": [1, 8, 11, 13, 21, 22, 23, 24, 176, 178, 180, 182, 183, 184, 187, 188, 189, 190, 191, 194, 195, 199, 205, 206, 208, 209, 210, 212, 213, 217, 218, 245, 246, 248], "potenti": [194, 208, 227], "power": [232, 238, 246, 252, 253], "pp": 225, "practic": [187, 191, 193, 194, 195, 199, 200, 202, 203, 204, 205, 207, 212, 225], "pre": [186, 214, 216, 217, 220, 222, 225, 230, 237, 240, 248, 252], "preced": [219, 229, 230], "precis": [2, 12, 13, 24, 160, 161, 162, 163, 164, 165, 173, 181, 182, 186, 188, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 206, 208, 209, 210, 211, 212, 218, 238, 246, 252, 253], "precomput": [235, 245], "precursor": 220, "pred": [191, 205, 226, 231], "pred_label": [213, 218, 231, 245], "pred_prob": [213, 218, 231, 245], "predefin": [193, 207], "predict": [12, 191, 199, 216, 217, 226, 231, 238, 245], "prefac": 242, "prefer": [166, 170, 240, 244, 256], "prefix": [1, 11, 22, 23, 174, 175, 178, 180, 184, 213, 245, 248], "prelu": [117, 252], "prepar": [20, 24, 171, 175, 182, 193, 218, 220, 221, 231, 240, 250, 251, 252, 254, 257], "prepare_model": [20, 171, 196, 205, 206, 208, 209, 210, 211, 212, 217, 220, 221], "prepared_model": [171, 217, 220, 221], "prepend": [174, 186], "preprocess": [191, 192, 195, 197, 198, 200, 213, 218, 219, 226, 231, 245, 248], "preprocess_input": [191, 192, 197, 198, 200, 201, 213, 218, 219, 226, 231, 245, 248], "prerequisit": 227, "presenc": 248, "present": [168, 170, 172, 175, 196, 208, 216, 244, 246], "preserv": [165, 171, 213], "preserve_format": 159, "preset": 231, "pretrain": [187, 188, 189, 190, 191, 192, 197, 198, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 231, 233, 245, 247, 248], "pretti": [222, 226, 228], "prev": 221, "prev_conv_weight": 221, "prevent": [170, 171, 222, 236, 252], "previou": [2, 12, 17, 28, 177, 181, 187, 191, 205, 209, 222, 223, 226, 228, 231, 238, 240, 241, 253, 255], "primari": 239, "print": [29, 163, 164, 170, 171, 172, 174, 177, 178, 187, 188, 189, 190, 194, 195, 196, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 217, 218, 219, 220, 221, 222, 226, 228, 231, 240, 242, 243, 245, 248], "prior": [2, 12, 181, 187, 191, 205, 206, 209, 210, 211, 231, 246, 252], "privileg": [186, 242], "probabl": 238, "problem": [171, 240, 252, 253, 257], "problemat": [171, 253], "proce": [13, 24, 182, 218, 220, 242, 257], "procedur": [189, 201, 202, 204, 209, 224, 227, 240, 242, 245, 252], "proceed": [187, 188, 189, 190, 213, 220, 221, 231, 245], "process": [187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 209, 210, 211, 216, 218, 220, 222, 224, 225, 226, 233, 238, 242, 244, 245, 247, 256], "processor": [238, 240, 243], "produc": [13, 24, 157, 158, 167, 171, 182, 187, 199, 200, 205, 212, 214, 215, 217, 218, 224, 231, 238, 246, 247], "product": [223, 238, 239], "profil": [173, 231, 255], "progbar": [192, 197, 198, 200, 201], "progbar_stat_upd": [192, 197, 198, 200, 201], "progress": 227, "project": 238, "prone": [218, 233], "pronounc": 232, "propag": [229, 233, 252], "propagate_encod": [24, 178, 182, 184, 218, 245, 248], "proper": 252, "properli": [160, 161, 216, 252], "properti": [165, 170], "provid": [1, 2, 5, 8, 12, 18, 20, 21, 23, 24, 29, 165, 170, 172, 175, 176, 178, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 220, 223, 224, 225, 226, 227, 231, 233, 235, 237, 238, 244, 245, 246, 247, 248, 253, 254, 255, 256, 257], "proxi": 171, "prune": [28, 186, 203, 223, 224, 226, 228, 229, 230, 238, 252], "pt": 175, "pt_model": [187, 188, 189, 190, 213, 220, 221, 231, 245], "pth": [22, 175, 178, 184, 216, 222, 226, 228, 245, 248], "ptq": [13, 24, 182, 193, 207, 217, 218, 221, 225, 230, 238, 247, 248, 252, 256, 257], "public": [166, 245, 252], "publish": 252, "pure": [20, 251], "purpos": [187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 238], "put": [181, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "py38": [240, 241, 242], "pypi": [241, 242], "python": [20, 238, 240, 241, 242, 243, 252], "python3": [186, 240, 241, 242, 243, 252], "pythonpath": [186, 216], "pytorch": [0, 24, 166, 168, 171, 172, 174, 177, 178, 179, 182, 184, 185, 186, 192, 193, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 238, 239, 240, 241, 242, 244, 245, 247, 248, 249, 252, 255, 257], "q": [29, 157, 158, 159, 160, 161, 163, 164, 165, 170, 174, 177, 235, 236, 243, 247], "q_": 247, "q_modul": 170, "q_output": 29, "qadd": 174, "qairt": 255, "qat": [1, 11, 23, 180, 186, 187, 190, 195, 200, 205, 213, 219, 235, 236, 238, 247, 252, 253, 256, 257], "qat2": 252, "qc": 252, "qc_quantize_op": 252, "qcquantizeop": [2, 231], "qcquantizewrapp": [14, 170, 219, 252], "qdo": 238, "qdq": [161, 165, 235, 256, 257], "qlinear": [29, 170, 174], "qmax": [160, 161, 163, 164, 177, 178, 243, 245, 248], "qmin": [160, 161, 163, 164, 177, 178, 243, 245, 248], "qmodul": 170, "qmul": 174, "qmult": 29, "qnn": [200, 201, 232, 252, 255, 256], "qol": 252, "qsim": 194, "qtzr": [177, 235], "quad": [160, 161, 162, 163, 164], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258], "qualiti": [187, 205], "quant": [1, 2, 8, 9, 13, 21, 22, 23, 24, 176, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 207, 208, 209, 210, 211, 213, 217, 218, 231, 237, 245, 248, 252], "quant_analyz": [5, 18, 166, 170, 199, 212, 217], "quant_analyzer_result": 217, "quant_dequ": 157, "quant_schem": [8, 9, 13, 21, 22, 24, 167, 176, 178, 182, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 237, 240, 243, 245, 246, 248], "quant_sim": 233, "quant_sim_model": [184, 245], "quant_stats_visu": [167, 215], "quant_wrapp": 170, "quantanalyz": [8, 21, 170, 176, 183, 214, 252], "quantiz": [1, 2, 5, 7, 8, 9, 11, 12, 13, 14, 18, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 165, 166, 167, 169, 171, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 193, 196, 202, 203, 204, 207, 213, 215, 216, 219, 220, 221, 223, 225, 227, 231, 232, 233, 236, 237, 238, 240, 243, 245, 249, 251, 252], "quantizablemultiheadattent": 252, "quantizaion": 230, "quantizaiton": 7, "quantization_overrid": 255, "quantization_tf": 240, "quantizationdatatyp": [2, 9, 12, 13, 22, 24, 170, 173, 178, 181, 182, 184, 187, 191, 205, 218, 231, 245, 248], "quantizationmixin": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 174, 235, 236], "quantizationmod": 240, "quantizationsim": [187, 188, 189, 190, 192, 193, 195, 197, 198, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "quantizationsimmodel": [2, 7, 8, 9, 10, 12, 13, 14, 21, 22, 24, 29, 167, 169, 170, 173, 175, 176, 177, 178, 179, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 231, 233, 235, 237, 243, 244, 245, 246, 247, 248, 249, 252, 255], "quantizationsimmodelv1": 170, "quantizationsimmodelv2": 170, "quantize_lora_scale_with_fixed_rang": 175, "quantized_": [213, 237], "quantized_callback": [194, 195, 197, 198], "quantized_dlc": 255, "quantized_linear": 29, "quantized_mobilenet_v2": [213, 219, 245, 248], "quantized_mobilenetv2": 248, "quantized_model": [178, 245, 248], "quantized_repr": [29, 157, 158, 159], "quantizedadd": 174, "quantizedconv2d": [170, 174, 178, 243, 244, 245, 248], "quantizedequant": [157, 170, 174, 175, 177, 178, 235, 240, 243, 244, 245, 248], "quantizedlinear": [29, 170, 174, 244], "quantizedmaskedadd": 29, "quantizedmultipli": [29, 174], "quantizedoptlearnedpositionalembed": [235, 236], "quantizedrelu": 170, "quantizedsoftmax": 174, "quantizedtensor": [29, 157, 159, 160, 177], "quantizer_arg": 246, "quantizer_config": [12, 231], "quantizer_group": [2, 12, 173, 181, 231], "quantizer_info": [2, 231], "quantizer_nam": [12, 231], "quantizerbas": [29, 174, 177, 236], "quantizergroup": [2, 12, 13, 24, 173, 181, 182, 218, 231], "quantparam": 209, "quantschem": [1, 8, 9, 11, 13, 21, 22, 23, 24, 167, 176, 178, 180, 182, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 231, 237, 243, 245, 248, 252], "quantsim": [1, 5, 6, 8, 11, 18, 19, 21, 23, 166, 167, 168, 170, 173, 175, 176, 180, 183, 186, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 205, 206, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 231, 232, 233, 235, 236, 237, 238, 239, 243, 244, 246, 248, 252, 256, 257], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "quantsim_layer_output": 216, "quantsim_layer_output_util": 216, "quantsimmodel": 237, "quatiz": 256, "quic": [186, 238, 240, 241, 242], "quick": [177, 196, 200, 202, 204, 240, 245], "quickli": [187, 191, 199, 200, 205, 212, 239, 243], "qwa": 252, "r": [2, 8, 12, 21, 175, 176, 181, 183, 200, 217, 231], "radic": 257, "rais": [20, 29, 165, 196, 213, 218, 231, 245], "rand": [172, 205, 206, 208, 209, 210, 211, 212, 216, 217], "randn": [29, 158, 159, 160, 161, 171, 172, 174, 178, 187, 188, 189, 190, 207, 213, 217, 218, 219, 220, 221, 231, 233, 237, 240, 243, 245, 248], "random": [20, 196, 207, 217, 222, 231, 240, 243], "random_input": [20, 196], "random_split": 245, "randperm": 205, "rang": [8, 20, 21, 163, 164, 167, 171, 176, 177, 183, 184, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 200, 201, 205, 206, 207, 208, 209, 210, 213, 214, 215, 219, 221, 224, 230, 231, 235, 236, 238, 245, 247, 248, 252, 253], "rank": [17, 28, 175, 226, 228, 230, 234], "rank_select": 228, "rank_select_schem": [28, 228], "rankselectschem": [28, 222, 226, 228], "rare": 252, "rate": [20, 194, 195, 196, 197, 198, 202, 203, 204, 208, 210, 211, 225, 248], "rather": [171, 194, 208, 237, 249], "ratio": [17, 28, 202, 203, 204, 222, 223, 226, 228], "raw": [235, 236], "rceil": [163, 164], "re": [2, 12, 177, 179, 181, 185, 186, 187, 196, 205, 231, 237, 252, 257], "re_estimation_dataset": 194, "re_estimation_dataset_s": 194, "reach": [193, 207, 218], "read": 217, "reader": [187, 194, 199, 200, 205, 208, 212], "readi": [22, 177, 178, 184, 187, 188, 189, 190, 191, 194, 195, 197, 198, 200, 202, 203, 204, 205, 206, 209, 210, 211, 212, 245, 248, 253, 256], "readili": [187, 191, 194, 199, 200, 205, 208, 212], "real": [157, 158, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 227, 231, 243], "realiz": [173, 233], "realli": [187, 194, 199, 200, 208, 212], "reason": [172, 188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 257], "recalcul": [194, 208], "receiv": 171, "recent": [241, 256], "recip": 252, "recogn": [172, 247], "recommend": [1, 2, 8, 9, 20, 23, 177, 180, 187, 188, 189, 190, 191, 192, 193, 196, 200, 206, 207, 213, 217, 219, 220, 221, 223, 231, 240, 241, 243, 244, 245, 253, 256, 257], "recomput": [175, 252], "reconstruct": [1, 11, 23, 180, 202, 204, 213], "record": [8, 21, 176, 183, 217], "recov": [189, 201, 202, 203, 204, 209, 252, 253, 256, 257], "recoveri": 257, "recurr": 252, "recurs": 252, "redefin": 172, "redesign": 252, "reduc": [174, 175, 187, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 219, 221, 225, 229, 230, 232, 238, 244, 247, 252, 253], "reduct": [202, 203, 204, 223, 257], "redund": [220, 225], "reestim": [14, 25, 194, 219], "reestimate_bn_stat": [14, 25, 194, 208, 219], "ref": 242, "refer": [168, 170, 175, 179, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 216, 231, 237, 244, 245, 246, 248, 255], "reflect": [197, 198, 210, 211, 247], "reflectionpad1d": 127, "reflectionpad2d": 128, "reflectionpad3d": 129, "regard": [2, 12, 181, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212, 231], "regist": [29, 165, 187, 235, 236, 252], "regress": 222, "regular": [1, 11, 12, 22, 23, 29, 177, 178, 180, 184, 187, 195, 213, 231, 245, 248], "rel": [12, 17, 28, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 222, 223, 226, 228, 231, 253], "relat": [170, 222, 226, 228, 239, 247], "relationship": 244, "releas": [186, 240, 241, 242], "release_tag": [186, 241, 242], "relev": [202, 203, 204, 232, 256], "reli": [170, 187, 188, 189, 190, 191, 194, 195, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "reload": 209, "relu": [20, 125, 170, 171, 172, 194, 195, 196, 213, 220, 221, 229, 249, 250, 251, 252], "relu1": [20, 172, 251], "relu2": [20, 172, 250, 251], "relu6": [126, 220, 221, 252], "remain": [170, 179, 185, 213, 237, 252], "remov": [14, 24, 25, 171, 182, 196, 218, 219, 220, 222, 229, 236, 238, 241, 245, 246, 247, 248, 252], "remove_all_quant": 236, "remove_column": [235, 236], "reorder": 196, "reorgan": 252, "repeat": [175, 200, 222, 257], "repeatedli": 256, "replac": [170, 171, 174, 175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 235, 242, 244, 247, 248, 252], "replace_lora_layers_with_quantizable_lay": [175, 235, 236], "replaced_module_typ": 175, "replicationpad1d": 130, "replicationpad2d": 131, "replicationpad3d": 132, "repo": [241, 242], "report": [233, 252, 257], "repositori": [186, 241, 242], "repres": [1, 2, 8, 9, 13, 17, 20, 21, 22, 24, 157, 158, 159, 165, 173, 174, 175, 176, 178, 181, 182, 183, 184, 199, 212, 213, 217, 218, 222, 224, 226, 228, 231, 233, 238, 243, 245, 246, 247, 248, 251, 252, 255], "represent": [157, 158, 159, 165, 177, 196, 238, 247, 252, 255], "reqs_deb_common": 242, "reqs_deb_onnx_common": 242, "reqs_deb_onnx_gpu": 242, "reqs_deb_tf_gpu": 242, "reqs_deb_torch_common": 242, "reqs_deb_torch_gpu": 242, "requant": 220, "request": 233, "requir": [2, 17, 20, 28, 29, 168, 170, 171, 175, 177, 178, 184, 187, 188, 191, 196, 199, 202, 203, 204, 205, 206, 209, 210, 211, 212, 213, 216, 217, 220, 221, 222, 223, 225, 226, 227, 228, 231, 233, 235, 237, 238, 240, 241, 242, 244, 245, 246, 247, 248, 249, 251, 252, 256, 257], "requires_grad": [158, 159, 171, 245, 248], "requires_grad_": [170, 235, 236], "rerun": 172, "resblock": 20, "research": 238, "resembl": 20, "resid": [241, 252], "residu": 222, "resiz": [205, 213, 218, 231, 245], "resnet": [178, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 223, 226, 231, 245, 248], "resnet18": [178, 187, 188, 189, 190, 192, 193, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 245, 248], "resnet18_after_adaround": 188, "resnet18_after_cle_bc": [206, 207, 209], "resnet18_after_qat": [208, 210, 211], "resnet18_mixed_precis": [187, 205], "resnet50": [191, 192, 193, 197, 198, 199, 200, 201, 217, 226, 231], "resnet50_after_adaround": 192, "resnet50_after_amp": 191, "resnet50_after_cl": 201, "resnet50_pcq_adaround": 200, "resolv": [172, 252], "resort": 253, "resourc": [187, 194, 195, 196, 200, 205, 208, 238, 257], "respecit": [187, 205], "respect": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 174, 177, 179, 185, 197, 198, 217, 231, 237, 250, 252], "respond": [199, 212], "respons": [2, 12, 181, 187, 191, 205, 225, 231, 238, 257], "ressembl": 196, "rest": [192, 197, 198, 200, 201, 252, 253], "restor": [184, 202, 203, 204, 247, 252, 253, 256], "restrict": 244, "resu": 20, "result": [2, 8, 12, 13, 20, 21, 24, 29, 157, 158, 159, 176, 177, 178, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 221, 222, 223, 225, 226, 228, 231, 235, 236, 237, 244, 245, 247, 248, 249], "results_dir": [2, 8, 12, 13, 21, 24, 176, 181, 182, 183, 187, 191, 199, 205, 212, 217, 218, 231], "retain": [231, 238], "retest": 257, "retrain": [238, 251, 256], "retriev": 29, "retrieve_context": 255, "retuern": 231, "return": [1, 2, 3, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 157, 158, 159, 160, 161, 165, 167, 168, 169, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 224, 226, 228, 231, 235, 236, 237, 244, 245, 247, 248, 250, 251], "return_dict": [235, 236], "reus": [171, 172, 187, 205, 250, 251], "reveal": 253, "revert": 253, "review": 195, "revis": 246, "revisit": 223, "rework": 246, "rewrit": 172, "rfloor": [160, 161, 162, 163, 164, 165, 247], "rgb": [213, 218, 231, 245, 251], "rgb_output": 251, "right": [160, 161, 162, 163, 164, 165, 174, 177, 187, 191, 193, 195, 205, 207, 229, 231, 244, 247], "rm": 241, "rmsnorm": 252, "rmsnormal": 252, "rnn": [122, 252], "rnncell": 123, "ro": 241, "robust": [237, 252, 256], "root": [205, 207, 241, 242, 252], "rough": [187, 205], "roughli": [24, 173, 181, 182, 218, 231], "round": [1, 9, 11, 13, 17, 21, 22, 23, 24, 28, 180, 182, 184, 186, 200, 217, 218, 226, 228, 238, 245, 247, 248, 252], "round_mod": 209, "round_nearest": 240, "rounding_mod": [9, 13, 21, 22, 24, 178, 182, 184, 191, 192, 195, 197, 198, 200, 201, 217, 218, 245, 248], "roundingmod": 240, "routin": [187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 245], "rrelu": 124, "rtype": [12, 20, 157, 158, 159, 173, 181, 231], "rule": [22, 231, 233, 244, 245, 248, 249, 256], "run": [2, 8, 9, 12, 13, 17, 20, 21, 22, 24, 25, 28, 160, 161, 170, 171, 172, 173, 174, 176, 177, 178, 181, 182, 183, 184, 188, 189, 190, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 216, 218, 219, 222, 225, 226, 227, 228, 231, 232, 233, 238, 239, 241, 242, 244, 245, 246, 247, 248, 252, 255, 256, 257], "run_forward_pass": [9, 178, 245, 248], "run_infer": [13, 24, 182, 207, 218], "runnabl": 256, "runtim": [22, 29, 177, 178, 184, 187, 188, 189, 190, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211, 216, 217, 220, 222, 223, 225, 226, 228, 231, 238, 239, 241, 244, 245, 246, 248, 252, 254, 255, 256, 257], "runtimeerror": [165, 178, 245, 248], "s_1": 244, "s_2": 244, "s_n": 244, "safe": 159, "safetensor": 175, "sai": [171, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 223, 231, 233], "sake": [189, 209, 216], "same": [8, 20, 21, 24, 29, 99, 157, 158, 168, 170, 171, 172, 173, 174, 175, 176, 178, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 216, 217, 218, 221, 230, 233, 234, 244, 245, 248, 249], "sampl": [1, 2, 8, 13, 21, 22, 24, 167, 173, 174, 176, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 217, 218, 220, 221, 222, 226, 231, 235, 236, 237, 243, 245, 247, 248, 252], "sampled_dataset": [193, 194, 199], "sandeep": 225, "saniti": 177, "satisfactori": [213, 219, 237, 253, 257], "satisfi": [171, 173, 188, 193, 207, 218, 233, 244], "saurabh": 225, "save": [1, 2, 6, 8, 9, 11, 12, 13, 17, 19, 21, 22, 23, 24, 28, 167, 168, 175, 176, 178, 180, 181, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 222, 226, 228, 231, 237, 245, 247, 248, 255, 256], "save_checkpoint": 184, "save_dir": [19, 216], "save_path": [167, 215], "saved_eval_scores_dict": [17, 28, 222, 226, 228], "saw": 200, "scalar": [8, 13, 21, 176, 183, 217, 218, 252], "scale": [14, 29, 158, 159, 160, 161, 162, 163, 164, 165, 175, 177, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 217, 219, 221, 230, 238, 240, 243, 244, 245, 246, 247, 248, 252, 255, 256], "scale_": [160, 161, 162, 163, 164, 177], "scale_max": 175, "scale_min": 175, "scenario": [188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 206, 209, 210, 211, 212, 229], "scene": 256, "schedul": [202, 203, 204, 208, 210, 211, 248], "schema": 252, "scheme": [1, 8, 9, 11, 13, 17, 21, 22, 23, 24, 28, 168, 176, 178, 180, 182, 183, 184, 187, 191, 194, 195, 199, 202, 203, 204, 205, 208, 212, 213, 216, 217, 218, 222, 225, 226, 228, 238, 245, 248, 252], "scope": 171, "score": [2, 8, 12, 13, 17, 21, 24, 28, 176, 181, 182, 183, 187, 188, 189, 190, 191, 193, 194, 207, 208, 217, 218, 222, 224, 225, 226, 227, 228, 231], "script": [208, 241, 242], "sdk": [187, 200, 205, 239, 256], "search": [2, 10, 12, 13, 24, 179, 181, 182, 185, 187, 194, 195, 197, 198, 202, 203, 204, 205, 208, 210, 211, 218, 230, 231, 237, 248, 249, 252, 256], "searcher": 225, "sec": [187, 191, 200, 205, 231], "second": [9, 13, 20, 23, 174, 178, 179, 180, 185, 199, 212, 213, 218, 220, 221, 231, 237, 244, 245, 248], "section": [2, 172, 181, 187, 194, 205, 207, 231, 235, 236, 241, 242, 244, 247, 249, 254, 257], "see": [17, 20, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 160, 161, 166, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 221, 222, 225, 226, 227, 228, 239, 240, 243, 244, 245, 247, 249, 252, 253, 257], "seed": 222, "seem": 225, "seen": [199, 212, 217], "select": [13, 17, 24, 28, 182, 187, 191, 205, 217, 218, 223, 226, 227, 228, 229, 235, 236, 238, 240, 241, 242, 247, 249, 255], "select_param": [28, 228], "self": [13, 20, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 171, 172, 188, 196, 207, 213, 218, 231, 245, 250, 251], "selu": 133, "sens": 174, "sensit": [2, 8, 12, 21, 176, 181, 183, 187, 189, 191, 201, 205, 209, 214, 219, 224, 230, 232, 247, 252, 256, 257], "sentiment": 195, "separ": [1, 11, 20, 22, 23, 170, 171, 172, 178, 180, 184, 191, 213, 217, 219, 245, 248, 252, 253], "separableconv2d": 252, "seq": 237, "seq_length": [235, 236], "seq_ms": [5, 166, 170, 237], "seqms": [230, 237, 252], "seqmseparam": [10, 179, 185, 237], "sequanti": 196, "sequenc": [178, 193, 195, 207, 213, 218, 230, 245, 248, 249], "sequenti": [10, 20, 170, 179, 185, 194, 195, 196, 220, 221, 243, 249, 250, 252, 253], "sequential_ms": [10, 237], "sequentialms": [10, 237], "seri": [24, 173, 178, 182, 184, 193, 207, 218, 233, 245, 248, 255], "serial": 255, "serializetostr": [187, 188, 189, 190, 216], "serv": [174, 213, 217, 218, 227], "servic": 233, "sess": [9, 187, 188, 189, 190, 226, 245], "session": [9, 17, 187, 188, 189, 190, 213, 216, 217, 226, 231, 245], "set": [1, 2, 7, 8, 11, 12, 13, 17, 20, 21, 24, 28, 29, 165, 167, 169, 170, 171, 172, 173, 174, 175, 176, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 217, 218, 219, 222, 223, 224, 225, 226, 228, 231, 232, 234, 235, 236, 238, 241, 242, 244, 245, 246, 247, 248, 249, 253, 256, 257], "set_activation_quantizers_to_float": [169, 244], "set_adaround_param": [13, 24, 182, 193, 207, 218], "set_and_freeze_param_encod": [188, 192, 200, 206, 213], "set_bitwidth_for_lora_adapt": 175, "set_blockwise_quantization_for_weight": [169, 244], "set_default_kernel": 29, "set_export_param": [24, 182, 218], "set_extra_st": 165, "set_grouped_blockwise_quantization_for_weight": [5, 169, 244], "set_kernel": 29, "set_mixed_precision_param": [13, 24, 182, 218], "set_model_input_precis": [173, 233], "set_model_output_precis": [173, 233], "set_model_preparer_param": [24, 182, 218], "set_precis": [173, 233], "set_quant_scheme_candid": [13, 24, 182, 218], "set_quantizers_to_candid": [2, 12, 173, 181, 231], "set_rang": [157, 158], "set_transform": [213, 218, 231, 245], "set_verbos": [191, 231], "settabl": 247, "setup": [170, 220, 231, 240, 243], "sever": [172, 174, 217, 223, 237, 250, 251, 252], "sgd": [219, 245, 248], "sh": [241, 242], "shall": 246, "shape": [12, 20, 26, 27, 28, 157, 158, 159, 160, 161, 165, 169, 170, 171, 172, 174, 175, 177, 178, 187, 188, 189, 190, 194, 195, 196, 200, 205, 206, 208, 209, 210, 211, 212, 213, 217, 220, 221, 222, 226, 228, 231, 235, 243, 244, 245, 248, 250], "share": [172, 174], "sharp": 225, "sharpli": [202, 203, 204, 257], "shell": 241, "shift": [201, 209], "shift_label": [235, 236], "shift_logit": [235, 236], "should": [1, 8, 12, 17, 20, 21, 22, 28, 29, 99, 159, 166, 168, 170, 171, 174, 175, 176, 178, 179, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 217, 218, 222, 223, 226, 228, 231, 237, 240, 243, 244, 245, 246, 248, 250, 251, 255, 257], "shouldn": 177, "show": [172, 175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 221, 224, 244, 253], "showcas": [199, 212], "shown": [170, 175, 199, 212, 217, 218, 229, 240, 241, 243, 251, 253], "shuffl": [191, 192, 197, 198, 200, 213, 218, 219, 222, 226, 231, 245, 248], "side": 229, "sigmoid": [20, 135, 171], "sign": [163, 164, 170, 177, 247], "signatur": [9, 17, 28, 29, 163, 164, 178, 188, 202, 203, 204, 217, 218, 222, 226, 228, 245, 248], "signific": [213, 253], "significantli": 252, "silu": 134, "sim": [1, 2, 7, 8, 9, 10, 12, 14, 21, 22, 167, 169, 170, 173, 175, 176, 178, 179, 181, 183, 184, 185, 191, 194, 195, 200, 207, 213, 215, 217, 218, 219, 231, 233, 236, 237, 243, 244, 245, 246, 248], "sim1": 170, "sim2": 170, "sim_model": [192, 197, 198, 199, 200, 201, 206, 208, 209, 210, 211, 212], "simd": 242, "similar": [175, 177, 221, 238, 240, 243, 244, 247], "similarli": [187, 191, 199, 200, 205, 208, 212, 253, 257], "simpl": [171, 191, 194, 195, 199, 200, 212, 217, 222, 226, 228], "simpler": 170, "simpli": [1, 8, 21, 22, 171, 176, 183, 184, 187, 191, 194, 195, 200, 205, 208, 213, 217, 245, 248], "simplic": 216, "simplif": [221, 245], "simplifi": [1, 2, 8, 9, 170, 213, 216, 217, 220, 221, 231, 245, 255], "simuat": [188, 189, 192, 197, 198, 200, 201, 206, 209, 210, 211], "simul": [9, 22, 165, 169, 174, 175, 178, 184, 186, 194, 195, 202, 203, 204, 212, 213, 214, 215, 216, 219, 220, 231, 232, 237, 238, 244, 245, 246, 248, 249, 251, 252, 257], "sinc": [175, 178, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 223, 231, 237, 244, 245, 247, 248], "singl": [2, 6, 13, 19, 24, 168, 171, 173, 181, 182, 199, 200, 212, 216, 217, 218, 222, 224, 225, 226, 228, 231, 244, 246, 247, 252], "singular": [203, 204, 225, 226, 228], "situat": 217, "six": 249, "size": [7, 8, 20, 159, 160, 161, 162, 163, 164, 169, 172, 174, 177, 188, 192, 193, 194, 195, 196, 200, 206, 213, 217, 226, 228, 231, 232, 238, 244, 245, 246, 256, 257], "skew": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211], "skip": [1, 23, 24, 179, 180, 182, 185, 189, 201, 209, 213, 218, 219, 222, 235, 237, 241, 245, 248], "skipped_optim": 220, "slight": 200, "slightli": [187, 205, 234], "slim": 252, "slow": 225, "small": [9, 178, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 219, 234, 237, 245, 248], "smaller": [1, 2, 12, 23, 180, 181, 187, 200, 205, 213, 225, 226, 228, 231, 253, 256], "smoothl1loss": 136, "snapdragon": [187, 200, 205], "snippet": [171, 187, 205, 243, 244], "snpe": [200, 201, 252], "so": [1, 8, 9, 22, 23, 170, 171, 172, 174, 178, 180, 184, 187, 188, 191, 192, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 210, 211, 212, 213, 217, 220, 221, 222, 224, 226, 228, 235, 236, 238, 242, 245, 248, 255, 257], "softmarginloss": 137, "softmax": [20, 138, 174, 195, 196, 205], "softmax2d": 139, "softmin": 140, "softplu": [141, 171], "softshrink": 142, "softsign": 143, "softwar": [238, 239, 252], "sole": 217, "solut": [191, 205, 224, 231, 248, 253], "some": [17, 20, 28, 170, 171, 174, 177, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 221, 222, 224, 226, 228, 229, 231, 235, 236, 247, 253, 256, 257], "someth": [1, 22, 184, 213, 217, 225, 245, 248], "sometim": [187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 205, 206, 209, 210, 211, 217, 222, 225, 257], "somewher": 257, "soon": 218, "sort": 196, "sourc": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 237, 238, 241, 242, 244, 245, 248, 253], "space": [28, 222, 226, 228, 231, 247], "spars": 252, "sparse_categorical_crossentropi": 195, "spatial": [17, 28, 186, 222, 223, 224, 228, 230, 252], "spatial_svd": [28, 203, 204, 222, 226, 228], "spatial_svd_auto_mod": 226, "spatial_svd_manual_mod": 226, "spatialsvdparamet": [17, 28, 203, 204, 222, 226, 228], "special": [24, 171, 182, 218, 238], "specif": [8, 17, 20, 22, 24, 28, 29, 169, 172, 174, 176, 178, 182, 183, 184, 187, 188, 191, 192, 194, 196, 200, 205, 206, 208, 217, 218, 222, 226, 228, 234, 244, 245, 247, 248, 249, 252, 255, 257], "specifi": [1, 2, 8, 13, 17, 21, 22, 23, 28, 160, 161, 162, 163, 164, 165, 173, 176, 178, 180, 181, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 222, 223, 224, 225, 226, 227, 228, 231, 233, 241, 244, 245, 246, 248, 249], "speed": [17, 28, 175, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 220, 222, 225, 226, 228, 248, 252], "speedup": [179, 185, 187, 200, 205, 237], "split": [179, 185, 213, 218, 219, 231, 235, 236, 237, 244, 245, 247, 248], "sqnr": [2, 10, 13, 24, 173, 179, 181, 182, 185, 187, 191, 205, 218, 231, 237, 247], "sqrt": 171, "squar": [214, 247], "squeez": [194, 205], "ssvd": 223, "ssvd_comp_stat": 204, "ssvd_compressed_model": 204, "ssvd_cp_compressed_model": 204, "ssvd_cp_finetuned_model": 204, "ssvd_finetuned_model": 204, "stabl": [171, 194, 208, 213], "stack": [213, 231, 245], "stand": 252, "standalon": [213, 252], "standard": [12, 165, 171, 174, 190, 195, 197, 198, 210, 211, 231, 238], "start": [1, 2, 11, 12, 20, 23, 163, 164, 171, 172, 180, 181, 186, 187, 191, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 208, 210, 211, 212, 213, 223, 231, 240, 245, 247, 249, 257], "start_beta": [1, 11, 23, 180, 213], "start_i": [191, 231], "start_x": [191, 231], "starting_op_nam": [12, 231], "stat": [8, 25, 167, 215, 217, 219, 222, 226, 228], "statatist": 208, "state": [165, 179, 185, 187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 231, 237], "state_dict": 165, "stateless": 251, "statement": 171, "static": [12, 17, 20, 28, 171, 172, 222, 226, 228, 231], "static_patch_count": 20, "staticgridperchannelquant": 170, "staticgridquant": 170, "staticgridquantwrapp": 170, "staticmethod": [171, 187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212], "statist": [8, 9, 17, 21, 25, 28, 160, 161, 165, 167, 174, 176, 178, 183, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 214, 215, 219, 222, 226, 228, 230, 245, 248], "std": [205, 207, 213, 218, 231, 245], "step": [8, 10, 12, 21, 163, 164, 171, 173, 175, 176, 179, 183, 185, 186, 187, 189, 190, 191, 193, 194, 195, 199, 200, 205, 208, 212, 222, 223, 224, 225, 232, 233, 234, 235, 236, 240, 241, 242, 243, 247, 253, 254, 255, 256], "still": [166, 170, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 240, 244, 250, 253, 257], "stochast": [21, 22, 184, 217, 245, 248], "stop": [1, 11, 23, 180, 213, 218], "stopiter": [213, 218, 231, 245], "storag": [244, 247], "store": [1, 11, 22, 23, 158, 173, 175, 178, 180, 184, 187, 191, 205, 213, 232, 233, 244, 245, 248], "str": [1, 2, 6, 7, 8, 9, 10, 11, 12, 13, 17, 19, 21, 22, 23, 24, 165, 167, 168, 171, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 191, 196, 213, 215, 216, 217, 218, 226, 231, 233, 237, 245, 248, 251], "str_idx": 218, "straightforward": 257, "strategi": 257, "stream": [213, 218, 219, 237], "strict": [7, 165, 173, 233, 245, 247, 248, 249], "strict_symmetr": [194, 240, 249], "strict_valid": [13, 24, 182, 218], "strictli": 165, "stride": [159, 171, 172, 178, 220, 221, 243, 245, 248], "strike": 225, "string": [9, 22, 173, 178, 184, 233, 241, 245, 246, 248, 249], "strongli": [20, 171, 177, 188, 192, 200, 206], "structur": [24, 171, 174, 182, 199, 204, 212, 218, 225, 233, 238], "style": 200, "sub": [199, 212], "subbackward0": 177, "subclass": [20, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 159, 177, 194, 195, 245, 250, 252], "subdirectori": [187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "subfold": [187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "sublay": 172, "submit": 255, "submodul": [20, 251], "subpackag": [29, 252], "subsequ": [157, 187, 189, 200, 201, 205, 213, 220, 221, 238, 249, 252], "subset": [8, 9, 21, 176, 178, 183, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 219, 229, 233, 244, 245, 248], "subsetrandomsampl": 207, "subsidiari": 239, "substanti": 246, "substep": 241, "substitut": 245, "success": [193, 207], "successfulli": [178, 216, 245, 248], "sudo": [186, 242], "suffic": [188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211], "suffici": [217, 245, 247, 248, 256], "suffix": 241, "suggest": [22, 184, 202, 204, 224, 225, 245, 248, 257], "suit": [193, 207, 218, 235, 238], "suitabl": 238, "sum": [191, 205, 207, 213, 218, 226, 231, 245, 248], "summari": [195, 213, 220, 221, 225, 252], "sun": 225, "super": [20, 29, 171, 172, 196, 213, 218, 231, 245, 252], "supergroup": [194, 247, 252], "suppli": 244, "support": [1, 2, 8, 9, 11, 12, 20, 21, 22, 23, 28, 29, 171, 172, 173, 176, 178, 179, 180, 181, 183, 184, 185, 187, 191, 194, 195, 196, 205, 208, 210, 211, 213, 215, 217, 218, 219, 220, 222, 223, 225, 226, 228, 230, 231, 237, 239, 240, 241, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 257], "supported_kernel": [2, 181, 187, 205, 231], "supported_kernel_op": [173, 181, 231], "suppos": [2, 12, 181, 187, 191, 205, 231], "svd": [17, 28, 186, 222, 223, 224, 230, 252], "switch": 244, "symbol": [20, 171], "symbolic_trac": [24, 171, 182, 218], "symfp": [10, 179, 185, 237], "symmetr": [9, 29, 157, 158, 159, 160, 161, 169, 170, 174, 175, 177, 178, 196, 235, 243, 244, 245, 246, 247, 248, 249, 252], "symmetri": [10, 170, 179, 185, 237], "symqt": [10, 179, 185, 237], "sync": 248, "syntax": 246, "systemat": 238, "t": [2, 9, 12, 24, 29, 171, 175, 177, 178, 179, 181, 182, 185, 187, 191, 194, 195, 199, 200, 205, 208, 212, 217, 218, 231, 235, 236, 237, 241, 245, 248, 249, 256], "tabl": [167, 186, 215, 227, 241, 246], "tag": [186, 241, 242], "take": [2, 9, 12, 13, 24, 28, 29, 169, 172, 173, 177, 178, 181, 182, 187, 191, 193, 194, 196, 199, 200, 201, 202, 203, 204, 205, 208, 212, 213, 218, 219, 222, 224, 225, 226, 228, 229, 231, 233, 244, 245, 248, 252, 253, 256], "taken": [20, 229], "tanh": 144, "tanhshrink": 145, "tap": [21, 176, 183, 217], "tar": [28, 228], "target": [9, 17, 20, 22, 28, 40, 41, 47, 61, 63, 80, 86, 87, 91, 92, 103, 104, 112, 113, 114, 115, 116, 121, 136, 137, 175, 178, 181, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 216, 218, 220, 222, 223, 224, 225, 226, 228, 231, 237, 238, 245, 246, 247, 248, 252, 253, 257], "target_comp_ratio": [17, 28, 202, 203, 204, 222, 226, 228], "target_data": [187, 188, 189, 190, 205, 206, 208, 209, 210, 211, 212], "target_length": 47, "target_modul": 175, "task": [187, 188, 189, 190, 191, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 227, 238, 246], "taxonomi": 225, "tbd": [215, 237], "teach": 186, "techiqu": 209, "techniqu": [13, 24, 182, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 217, 218, 220, 221, 222, 223, 226, 228, 231, 232, 234, 235, 238, 239, 244, 247, 248, 252, 253, 256, 257], "technologi": [238, 239], "tell": 200, "temporari": [213, 220, 221, 231, 245], "temporarili": [170, 236], "tend": 213, "tensor": [2, 12, 13, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 168, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 187, 189, 196, 199, 200, 201, 205, 209, 210, 211, 212, 213, 216, 217, 218, 220, 221, 222, 226, 228, 231, 233, 235, 236, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253], "tensor_nam": 246, "tensor_quant": 170, "tensorboard": [194, 195, 197, 198], "tensorflow": [0, 18, 186, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 212, 213, 215, 216, 217, 218, 219, 220, 221, 226, 231, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 248, 249, 252, 255, 256, 257], "tensorquant": [12, 231], "tensorquantizationsimforpython": 240, "term": [178, 226, 228, 238, 239, 244, 245, 248], "termin": 186, "test": [8, 21, 176, 183, 194, 195, 196, 217, 235, 236, 242, 256, 257], "test_dataload": [235, 236], "test_dataset": [235, 236], "text": [20, 160, 161, 162, 163, 164, 196, 235, 236], "textclassif": 196, "tf": [2, 8, 11, 14, 16, 17, 20, 21, 176, 183, 187, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 205, 208, 212, 213, 216, 217, 218, 219, 221, 226, 231, 241, 245, 250, 252], "tf_cpp_min_log_level": [191, 193, 194, 197, 198, 231], "tf_dataset": [192, 200, 201], "tf_enhanc": [22, 187, 191, 194, 195, 205, 208, 245, 248], "tfencod": 170, "tflite": [187, 200, 205], "tfoplambda": [20, 196, 252], "than": [1, 17, 23, 28, 170, 171, 172, 180, 181, 184, 187, 188, 192, 194, 195, 205, 206, 208, 213, 222, 223, 226, 228, 231, 233, 237, 244, 248, 249, 252, 256], "thei": [171, 172, 187, 188, 189, 190, 192, 195, 196, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 227, 231, 234, 235, 236, 244, 247, 250, 253, 256], "them": [13, 20, 22, 24, 165, 170, 171, 172, 174, 175, 182, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 218, 225, 233, 238, 245, 248], "theme": 252, "thereaft": 231, "therebi": [187, 205, 232], "therefor": [2, 12, 175, 181, 187, 191, 194, 195, 196, 205, 220, 223, 231], "theta_": [160, 161], "thi": [1, 2, 6, 8, 9, 12, 13, 17, 19, 20, 21, 22, 23, 24, 28, 29, 99, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 233, 234, 235, 236, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257], "thing": [188, 189, 190, 192, 193, 200, 201, 202, 203, 204, 206, 209, 210, 211, 255], "those": [8, 176, 183, 196, 217, 233, 235, 249, 256], "though": [29, 177, 187, 191, 194, 195, 200, 205, 232, 244, 249, 257], "three": [171, 187, 202, 203, 204, 205, 223, 256], "threshold": [146, 167, 215, 218], "through": [2, 20, 21, 159, 170, 171, 173, 174, 176, 181, 183, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 213, 217, 221, 231, 233, 235, 236, 245, 247, 248, 257], "throughout": [238, 249], "throw": [29, 177, 178, 245, 248], "thrown": [245, 248], "thu": [157, 158, 159], "tijmen": 225, "till": [2, 12, 181, 187, 191, 205, 231], "time": [17, 28, 159, 171, 172, 191, 193, 199, 207, 209, 212, 213, 217, 218, 222, 225, 226, 227, 228, 231, 233, 244, 248, 252, 256], "tmp": [8, 13, 21, 24, 176, 182, 183, 194, 199, 212, 213, 217, 218, 219, 220, 221, 231, 245, 248], "tmp_dir": 175, "tmpdir": 175, "to_arrai": [220, 221], "to_list": [2, 12, 173, 181, 231], "todo": [173, 233], "togeth": [200, 231], "toi": 217, "token": [20, 195, 196, 235, 236, 253], "token_and_position_embed": 196, "token_emb": [20, 196], "tokenandpositionembed": [20, 196], "tokenized_dummy_text": [235, 236], "toler": [193, 207, 218, 223], "tolist": 205, "too": [202, 204, 244], "tool": [21, 176, 183, 196, 217, 221, 229, 232, 238, 244, 252, 254, 255, 257], "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258], "top": [1, 2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27, 28, 169, 173, 176, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 226, 227, 228, 231, 233, 237, 244, 245, 248], "top1": [191, 207, 226, 231], "top1_accuraci": 205, "top5": 191, "topk": [205, 207], "torch": [13, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 169, 170, 171, 172, 173, 174, 175, 177, 178, 181, 182, 184, 186, 187, 188, 189, 190, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 241, 242, 243, 244, 245, 248, 251, 252, 257], "torch_stabl": [240, 241, 242], "torchscript": [24, 168, 178, 182, 184, 216, 218, 238, 245, 248], "torchvis": [178, 187, 188, 189, 190, 192, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 231, 233, 237, 243, 245, 248], "total": [191, 196, 224, 226, 231, 235, 236, 245, 247], "total_length": [235, 236], "total_sampl": [213, 231, 245], "totensor": [205, 207, 213, 218, 231, 245], "touch": 20, "toward": 253, "tpu": 238, "tqdm": [205, 207, 231, 235, 236, 245, 248], "trace": [20, 24, 167, 171, 182, 215, 218, 251], "traceabl": [171, 235, 236, 251], "traceback": 171, "traceerror": 171, "tracer": 171, "track": [175, 199, 212, 217], "track_lora_meta_data": 175, "track_meta_data": 175, "track_running_stat": [220, 221], "trade": [1, 11, 23, 180, 187, 191, 205, 213, 231], "tradeoff": [187, 191, 205, 231, 238, 257], "train": [8, 9, 13, 14, 17, 20, 21, 24, 25, 28, 176, 178, 182, 183, 186, 187, 191, 196, 205, 207, 213, 214, 216, 217, 218, 219, 221, 222, 225, 226, 228, 230, 231, 234, 237, 238, 239, 245, 247, 252, 253, 257], "train_dataload": [178, 235, 236, 245, 248], "train_dataset": [194, 235, 236], "train_dataset_s": 194, "train_flag": [28, 222, 226, 228], "train_load": [208, 222], "train_model": [28, 222, 226, 228], "train_one_epoch": [235, 236], "trainabl": [198, 211, 234, 235, 236], "trainer": [17, 28, 186, 202, 203, 204, 208, 210, 211, 222, 226, 228], "training_range_learning_with_tf_init": [178, 194, 198, 208, 211, 219, 237, 243, 245, 248], "trainingmod": [187, 190], "transact": 225, "transform": [20, 171, 196, 205, 206, 207, 209, 210, 211, 212, 213, 218, 231, 235, 236, 244, 245, 252], "transformer_block": [20, 196], "transformerblock": [20, 196], "transit": 231, "translat": 200, "transpos": 177, "trap": 237, "travers": 233, "tri": [202, 203, 204, 225], "tripletmarginloss": 147, "tripletmarginwithdistanceloss": 148, "triumph": 233, "true": [1, 2, 9, 12, 13, 17, 22, 24, 28, 29, 158, 159, 160, 161, 165, 169, 170, 171, 172, 173, 174, 175, 177, 178, 181, 182, 184, 187, 188, 189, 190, 191, 192, 194, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 240, 243, 244, 245, 246, 248, 249], "true_quant": [235, 236], "truli": 171, "try": [17, 28, 187, 188, 189, 190, 191, 200, 202, 203, 204, 205, 213, 218, 220, 221, 222, 223, 225, 226, 228, 231, 245, 253, 256], "tune": [17, 22, 28, 175, 184, 188, 189, 190, 192, 193, 195, 197, 198, 201, 206, 207, 209, 210, 211, 222, 223, 226, 228, 239, 245, 248, 256, 257], "tuner": 235, "tupl": [1, 2, 6, 7, 8, 11, 12, 13, 14, 15, 17, 19, 21, 22, 23, 24, 26, 27, 28, 96, 160, 161, 162, 163, 164, 168, 169, 172, 173, 176, 177, 178, 180, 181, 182, 183, 184, 187, 191, 199, 205, 212, 213, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 244, 245, 248, 251], "turn": [249, 252], "tweak": 175, "twice": 20, "two": [12, 20, 28, 170, 171, 172, 173, 181, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 209, 210, 211, 212, 217, 224, 225, 226, 228, 231, 233, 238, 244, 247, 248, 255, 256], "txt": [173, 233, 242, 255], "type": [1, 2, 3, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25, 26, 28, 29, 157, 158, 159, 160, 161, 165, 167, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 191, 193, 194, 196, 199, 202, 203, 204, 205, 207, 210, 211, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 227, 228, 231, 237, 238, 241, 243, 244, 245, 248, 249, 252, 255, 256], "typeerror": 171, "typic": [20, 29, 174, 187, 188, 192, 193, 194, 195, 197, 198, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 223, 238, 244, 247, 256, 257], "u": [195, 200, 241], "ubuntu": [240, 241, 243], "ubuntu22": 252, "ubuntu2204": 242, "uint16": 246, "uint32": 246, "uint8": [157, 246], "unaccept": 257, "unchang": [179, 185, 237, 245, 248], "uncompress": 223, "under": [2, 12, 167, 170, 181, 187, 205, 215, 217, 219, 231, 249, 253], "undergo": 238, "underli": [29, 177, 253], "understand": [170, 177, 187, 191, 194, 195, 196, 199, 200, 205, 208, 212], "undo": [14, 25, 219], "uneven": 253, "unexpect": 165, "unexpected_kei": 165, "unflatten": 149, "unfold": 150, "unid": [13, 24, 182, 218], "uniniti": [213, 237], "uninstal": 242, "unintuit": [13, 24, 182, 218], "union": [6, 7, 8, 9, 15, 17, 19, 20, 22, 23, 24, 26, 27, 28, 168, 169, 172, 173, 176, 178, 180, 181, 182, 183, 184, 213, 216, 217, 218, 220, 221, 222, 226, 228, 233, 244, 245, 248], "uniqu": [200, 241], "unit": 238, "unknown": 223, "unlabel": [8, 13, 24, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 217, 218, 231, 245], "unlabeled_data": 218, "unlabeled_data_load": [213, 217, 218, 231], "unlabeled_dataset": [21, 193, 194, 199, 213, 217, 218, 219], "unlabeled_dataset_iter": [8, 217], "unlabeled_imagenet_data_load": 207, "unlabeled_imagenet_dataset": 207, "unlabeleddatasetwrapp": [193, 207], "unlabelled_data_load": 218, "unless": [13, 24, 29, 178, 182, 184, 218, 229, 241, 245, 248, 257], "unlik": [165, 208, 220], "unmodifi": [224, 234], "unnecessari": [187, 200, 205, 220, 229, 252], "unrol": [171, 252], "unsign": [247, 249], "unsigned_symmetr": [194, 240, 249], "unsimplifi": [187, 188, 189, 190, 213, 220, 221, 231, 245], "unsqueez": [235, 236], "until": [13, 24, 160, 161, 178, 182, 193, 207, 218, 219, 245, 248], "untouch": [245, 248], "unus": 241, "unwrap": 196, "up": [1, 11, 17, 20, 22, 23, 28, 173, 175, 180, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 219, 222, 225, 226, 228, 229, 231, 233, 238, 245, 248, 249, 256], "updat": [165, 186, 188, 190, 192, 196, 197, 198, 200, 201, 206, 207, 209, 210, 211, 219, 227, 236, 242, 245, 246, 247, 248, 252], "updatestat": 240, "upgrad": [166, 242, 252], "upon": [14, 25, 29, 174, 219, 255], "upsampl": 151, "upsamplingbilinear2d": 152, "upsamplingnearest2d": 153, "upstream": [222, 229], "upto": [187, 194, 199, 200, 205, 208, 212, 231], "url": [17, 28, 186, 222, 226, 227, 228, 241, 242], "us": [1, 2, 6, 7, 8, 9, 11, 12, 13, 14, 17, 19, 20, 21, 22, 23, 24, 25, 28, 29, 157, 158, 160, 161, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221, 223, 227, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257], "usabl": 246, "usag": [172, 175, 177, 238, 246, 252, 255], "use_all_amp_candid": [2, 181, 187, 205, 231], "use_cach": [235, 236], "use_cuda": [1, 9, 17, 28, 187, 188, 189, 190, 191, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 216, 222, 226, 228, 240, 245], "use_embedded_encod": [178, 184, 245, 248], "use_monotonic_fit": [17, 28, 222, 226, 228], "use_safetensor": 175, "use_strict_symmetr": 170, "use_symmetric_encod": [9, 170, 245], "user": [1, 2, 9, 12, 13, 14, 17, 20, 22, 23, 24, 28, 29, 166, 170, 171, 173, 177, 178, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 222, 225, 226, 228, 231, 233, 240, 241, 242, 245, 246, 248, 250, 251, 252], "user_onnx_lib": [1, 9, 213, 245], "userflow": [179, 185, 237], "usr": [241, 242], "usual": [219, 225, 247, 248, 257], "util": [6, 8, 20, 170, 172, 175, 176, 183, 187, 188, 189, 190, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 231, 233, 235, 236, 237, 243, 244, 245, 248, 252], "v": [1, 2, 11, 12, 23, 180, 181, 187, 191, 199, 205, 212, 213, 231, 241, 244, 247, 257], "v1": [170, 191, 205, 206, 207, 208, 209, 210, 211, 212, 216, 231, 235, 236, 240, 252], "v2": [29, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 169, 173, 178, 215, 233, 235, 236, 244, 245, 248, 252], "val": [187, 188, 189, 190, 197, 198, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "val_dataset": 199, "val_transform": 207, "valid": [2, 8, 12, 21, 172, 176, 181, 183, 187, 191, 194, 195, 197, 198, 199, 200, 205, 207, 208, 212, 213, 217, 218, 222, 226, 228, 231, 237, 244, 245, 246, 252], "validate_example_model": 172, "validate_for_missing_modul": 172, "validate_for_reused_modul": 172, "validate_model": 172, "validation_check": 172, "validation_d": [191, 226, 231], "validation_data": [194, 195, 197, 198], "valu": [1, 2, 8, 9, 12, 13, 17, 21, 22, 23, 28, 29, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169, 171, 173, 175, 176, 178, 180, 181, 183, 184, 187, 188, 189, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 212, 213, 215, 216, 217, 218, 221, 222, 223, 224, 225, 226, 228, 231, 233, 238, 240, 243, 244, 245, 246, 247, 248, 249, 252], "value_qtzr": 29, "vanilla": 195, "var": [25, 80, 219], "vari": [2, 12, 181, 223, 231], "variabl": [17, 20, 28, 160, 161, 171, 186, 222, 224, 226, 228, 241, 242], "varianc": [194, 208, 221], "variant": [238, 240, 242, 243, 252], "variant_str": 241, "varieti": 221, "varint": 241, "variou": [2, 17, 28, 173, 181, 193, 199, 207, 212, 222, 225, 226, 228, 230, 231, 241, 247, 252, 253], "vector": [188, 192, 200, 206, 252], "vedaldi": 225, "venic": 225, "ver": 252, "verbos": 191, "veri": [188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 206, 207, 209, 210, 211, 212, 217, 223, 225, 231], "verifi": [20, 171, 206, 209, 210, 211], "versa": [175, 187, 191, 205, 224, 231, 247, 253], "version": [29, 99, 167, 170, 171, 174, 178, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 216, 235, 236, 238, 241, 242, 243, 245, 247, 248, 252, 257], "via": [20, 24, 182, 218, 223, 238, 240, 247, 252, 255], "vice": [175, 187, 191, 205, 224, 231, 247, 253], "view": [167, 172, 177, 214, 215, 227, 231, 235, 236, 239, 240], "view_a": 207, "viewabl": 186, "vision": 225, "visit": 242, "visual": [17, 28, 195, 199, 212, 217, 222, 225, 226, 228, 231, 241, 252], "visualization_tool": [166, 215], "visualization_url": [17, 28, 222, 226, 228], "visualize_stat": [167, 215], "visualizecompress": 227, "vocab_s": [20, 195, 196, 235, 236], "vol": 225, "volum": 229, "w": [2, 8, 12, 21, 176, 181, 183, 194, 217, 218, 228, 229, 231, 241, 245, 252], "w16a16": 252, "w4a16": 252, "w4a8": [218, 252, 256, 257], "w4fp16": 252, "w8a16": [218, 245, 256, 257], "w8a8": [218, 248, 256, 257], "wa": [17, 28, 157, 175, 177, 187, 188, 191, 192, 194, 195, 196, 200, 202, 204, 205, 206, 208, 216, 222, 225, 226, 228, 231, 233, 246, 252], "wai": [170, 177, 179, 184, 185, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 205, 206, 209, 210, 211, 212, 216, 237, 245, 256], "want": [2, 6, 12, 22, 29, 168, 171, 181, 184, 187, 191, 194, 195, 199, 200, 205, 212, 216, 231, 235, 236, 240, 241, 242, 245, 248, 257], "warm": [1, 11, 23, 180, 213], "warn": 172, "we": [1, 2, 6, 12, 20, 168, 170, 171, 172, 174, 175, 177, 181, 187, 188, 191, 192, 193, 194, 195, 196, 199, 200, 202, 204, 205, 206, 207, 208, 212, 213, 216, 220, 221, 223, 225, 226, 231, 232, 235, 236, 241, 244, 245, 253, 256, 257], "websit": [223, 239], "websocket": 227, "weight": [1, 2, 3, 7, 8, 11, 12, 20, 21, 23, 26, 28, 29, 167, 169, 170, 172, 173, 174, 175, 176, 177, 178, 180, 181, 183, 187, 188, 189, 190, 191, 192, 193, 196, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 219, 220, 221, 223, 226, 230, 231, 233, 234, 236, 237, 238, 243, 244, 245, 247, 248, 249, 252, 256, 257], "weight_bw": 209, "weight_decai": 219, "weight_info": 196, "weight_nam": 196, "weight_q": 177, "weight_qdq": 177, "weight_svd": [28, 222, 226, 228], "weight_svd_auto_mod": 228, "weight_svd_manual_mod": 228, "weights_in_correct_ord": 196, "weights_pdf": [199, 212, 217], "weightsvdparamet": [28, 222, 226, 228], "well": [157, 172, 175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 205, 206, 207, 209, 210, 211, 212, 213, 217, 225, 231], "were": [175, 193, 200, 207, 216, 223, 229, 245, 246, 249, 252], "weren": 171, "wget": 242, "what": [196, 227, 256], "whatev": 241, "wheel": [241, 242], "wheel_file_nam": [241, 242], "when": [2, 8, 9, 12, 17, 20, 21, 22, 24, 28, 29, 165, 171, 174, 176, 178, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 218, 219, 222, 225, 226, 227, 228, 229, 231, 232, 233, 237, 241, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253], "whenev": 251, "where": [1, 2, 11, 12, 13, 17, 22, 23, 28, 160, 161, 162, 163, 164, 165, 171, 175, 177, 178, 180, 181, 184, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 222, 224, 226, 227, 228, 229, 231, 238, 241, 242, 245, 246, 247, 248, 251, 252, 255], "wherea": [177, 181, 187, 205], "wherein": [6, 168, 216], "whether": [24, 165, 171, 172, 173, 182, 187, 188, 189, 190, 192, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 216, 218, 224, 233, 245, 248], "which": [1, 2, 6, 7, 11, 12, 13, 17, 19, 20, 23, 24, 28, 29, 157, 158, 159, 160, 161, 163, 164, 165, 168, 169, 171, 172, 173, 174, 175, 177, 180, 181, 182, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 237, 238, 240, 244, 245, 246, 247, 249, 252, 255, 256], "while": [7, 17, 28, 165, 174, 175, 187, 188, 189, 190, 191, 192, 195, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 222, 224, 226, 228, 234, 246, 247, 253, 256, 257], "whl": [240, 241, 242], "who": 246, "whole": [13, 193, 194, 218, 244, 247], "whose": [167, 168, 169, 171, 215, 216, 244, 249, 255], "why": [187, 200, 205, 253], "wide": [220, 221, 238], "width": [8, 21, 176, 183, 187, 188, 189, 190, 200, 205, 206, 208, 209, 210, 211, 212, 217, 220, 221, 226, 228, 229, 230, 232, 244, 246, 247, 253, 256, 257], "wikitext": [235, 236], "wildcard": 177, "wise": [8, 21, 176, 179, 183, 185, 217, 219, 237, 238, 252, 253], "wiseconv2d": [213, 221], "wish": [213, 220, 221, 231, 233, 245], "within": [29, 157, 158, 159, 174, 195, 217, 223, 238, 241, 247, 252, 256], "without": [13, 22, 24, 157, 159, 165, 167, 175, 178, 182, 184, 188, 192, 193, 195, 197, 198, 200, 201, 206, 207, 209, 210, 211, 215, 218, 229, 237, 241, 245, 248, 256, 257], "won": [24, 171, 179, 182, 185, 218, 237, 249], "word": [159, 195], "work": [12, 22, 170, 172, 184, 186, 187, 188, 190, 191, 192, 193, 194, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 221, 225, 231, 234, 244, 245, 248, 252, 257], "workaround": 171, "workflow": [223, 230, 234, 255, 256], "workspac": [186, 241], "world": 243, "wors": 225, "worth": 177, "would": [20, 170, 173, 187, 191, 194, 199, 200, 205, 208, 212, 226, 233, 235, 249, 252], "wq": 252, "wrap": [29, 170, 171, 195, 199, 212], "wrap_linear": 170, "wrapped_module_nam": [21, 176, 183, 217], "wrapper": [21, 176, 183, 191, 195, 205, 206, 209, 210, 211, 217, 222, 226, 228, 231, 252], "write": [173, 188, 189, 190, 192, 197, 198, 201, 206, 209, 210, 211, 233, 244, 245, 248], "written": [9, 178, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 245, 248], "wrt": [12, 231], "wsl2": 252, "www": 186, "x": [20, 157, 158, 159, 165, 166, 171, 172, 174, 187, 188, 189, 190, 191, 192, 195, 196, 199, 200, 205, 206, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 223, 226, 231, 240, 243, 245, 246, 247, 248, 250, 251, 252], "x1": [62, 118, 171], "x2": [62, 118, 171, 250, 251], "x86": [240, 242, 243], "x86_64": [240, 242], "x_": 247, "x_c": 165, "x_dq": 158, "x_q": [158, 159], "x_qdq": 157, "x_train": 195, "x_val": 195, "xarg": 242, "xiangyu": 225, "xx": 246, "y": [171, 191, 192, 199, 200, 212, 213, 217, 218, 219, 231, 242, 245, 248], "y_train": 195, "y_val": 195, "y_zero_point": 252, "ybelkada": [235, 236], "ye": [225, 242], "yet": [187, 191, 194, 195, 200, 205], "yield": [2, 8, 13, 23, 24, 25, 173, 179, 180, 181, 182, 185, 194, 208, 213, 217, 218, 219, 223, 231, 237, 247, 248, 253], "yihui": 225, "you": [8, 17, 24, 28, 29, 166, 170, 171, 176, 177, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 231, 233, 235, 237, 240, 241, 242, 244, 245, 247, 248, 249, 252, 253, 255, 256, 257], "your": [8, 24, 29, 166, 170, 171, 172, 176, 182, 183, 186, 187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 214, 216, 217, 218, 219, 223, 225, 227, 230, 235, 240, 242, 245, 247, 248, 256, 257], "your_imagenet_validation_data_path": [213, 218, 219, 245, 248], "yy": 246, "zero": [1, 11, 23, 175, 180, 213, 243, 247, 252], "zero_grad": [219, 235, 236, 248], "zeropad1d": 154, "zeropad2d": 155, "zeropad3d": 156, "zhang": 225, "zip": [191, 217, 226, 231], "zisserman": 225, "zou": 225, "zz": 246, "\u00aa": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00b2": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00b3": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00b5": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00b9": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00ba": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00bc": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00bd": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u00be": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u03c9": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u210e": 226, "\u215b": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u215c": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u215d": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\u215e": [216, 220, 231, 239, 244, 246, 247, 252, 255, 256, 257], "\ud835\udc58": [226, 228], "\ud835\udc5a": 226, "\ud835\udc5b": 226, "\ud835\udc64": 226}, "titles": ["AIMET API", "aimet_onnx.adaround", "aimet_onnx.mixed_precision", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.seq_mse", "aimet_tensorflow.adaround", "aimet_tensorflow.mixed_precision", "aimet_tensorflow.auto_quant_v2", "aimet_tensorflow.keras.bn_reestimation", "aimet_tensorflow.batch_norm_fold", "aimet_tensorflow.cross_layer_equalization", "aimet_tensorflow.compress", "aimet_tensorflow API", "aimet_tensorflow.layer_output_utils", "aimet_tensorflow.model_preparer", "aimet_tensorflow.quant_analyzer", "aimet_tensorflow.quantsim", "aimet_torch.adaround", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "Examples", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quantization simulation", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Quantization-Aware Training with a Keras Transformer Model", "Keras Model Preparer", "Quantization-aware training", "Quantization-Aware training with range learning", "Quant Analyzer", "Quantsim and Adaround - Per Channel Quantization (PCQ)", "Cross-Layer Equalization with QuantSim", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization and Bias Correction", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "Adaptive rounding", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Optimization techniques", "Automatic mixed precision", "Mixed precision", "Manual mixed precision", "Quantized LoRa", "QW-LoRa", "QWA-LoRa", "Sequential MSE", "Glossary", "AIMET Documentation", "Installation", "AIMET installation in Docker", "AIMET manual installation and setup", "Quick Start (PyTorch)", "Per-block quantization", "Calibration", "Encoding Format Specification", "Quantization simulation guide", "Quantization-aware training", "Runtime configuration", "TensorFlow model guidelines", "PyTorch model guidelines", "Release notes", "Quantization debugging guidelines", "Quantization user guide", "On-target inference", "AIMET features", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [191, 246, 252], "1": [170, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 231, 237, 245, 246, 248, 249, 252, 253, 257], "13": 252, "16": 252, "17": 252, "18": 252, "19": 252, "2": [170, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 231, 237, 245, 246, 248, 249, 252, 253, 257], "20": 252, "21": 252, "22": 252, "23": 252, "24": 252, "25": 252, "26": 252, "27": 252, "28": 252, "29": 252, "3": [186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 231, 237, 245, 246, 248, 249, 252, 253, 257], "30": 252, "31": 252, "32": 252, "33": 252, "34": 252, "35": 252, "4": [187, 188, 189, 191, 192, 193, 194, 196, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 213, 216, 217, 218, 219, 231, 237, 245, 248, 249, 252, 253, 257], "5": [193, 194, 208, 217, 218, 237, 245, 248, 249, 252, 253], "6": [217, 218, 246, 253], "7": [218, 253], "8": 253, "For": [188, 189, 190, 192, 193, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211], "On": [254, 255], "accuraci": [187, 188, 189, 190, 191, 192, 193, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 256, 257], "activ": [217, 253], "adapt": [188, 192, 206, 213, 230], "adaround": [1, 11, 23, 180, 188, 192, 200, 206], "affin": [170, 177], "ai": 255, "aimet": [0, 227, 239, 241, 242, 243, 247, 254, 256, 258], "aimet_onnx": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "aimet_tensorflow": [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "aimet_torch": [0, 23, 24, 25, 26, 27, 28, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185], "algorithm": [187, 191, 205, 231], "also": 247, "altern": 240, "amp": [187, 191, 205], "an": [187, 188, 189, 190, 241], "analysi": [199, 212, 214, 217, 230, 231, 253], "analyz": [199, 212, 214, 217], "api": [0, 5, 18, 20, 166, 171, 172, 174, 175, 177, 187, 191, 205, 213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 237, 239, 244, 245, 248, 256], "appli": [188, 189, 192, 193, 199, 200, 201, 206, 209, 212, 233], "arg": 246, "auto_qu": [24, 182], "auto_quant_v2": 13, "automat": [187, 191, 205, 218, 230, 231, 232], "autoqu": [193, 207], "awar": [194, 195, 197, 198, 208, 210, 211, 248, 256], "base": [233, 235], "baselin": [187, 188, 189, 190, 191, 192, 193, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211], "batch": [187, 188, 189, 190, 192, 197, 198, 200, 201, 205, 206, 209, 210, 211, 219, 220, 230], "batch_norm_fold": [3, 15, 26], "batchnorm": [194, 208], "bc": 209, "between": 196, "bia": 209, "bit": 231, "block": [177, 244], "blockwis": 244, "bn_reestim": [14, 25], "bokeh": 227, "brows": 186, "build": [240, 241], "calibr": [217, 236, 245, 248], "call": [187, 191, 205, 247], "callback": [187, 194, 205, 217, 236, 245], "case": [225, 231], "channel": [177, 200, 202, 204, 222, 225], "check": 253, "choos": [240, 241], "cle": [189, 201, 209], "code": [20, 170, 171, 186, 222, 226, 228, 231], "compil": 255, "complementari": 213, "compress": [17, 28, 202, 203, 204, 222, 223, 224, 225, 226, 227, 228, 230], "comput": [174, 187, 191, 205, 245, 248], "confid": 253, "config_util": 169, "configur": [174, 247, 249], "constant": [193, 194, 207], "contain": 241, "context": [213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237], "convers": 255, "convert": [187, 188, 189, 190, 196, 231], "correct": 209, "cp": 225, "creat": [187, 188, 189, 190, 191, 192, 194, 196, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 217, 231, 236, 245], "cross": [189, 201, 209, 221, 230], "cross_layer_equ": [4, 16, 27], "data": 246, "dataset": [187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "debug": [253, 254], "default": 249, "defin": [187, 193, 194, 205, 207], "definit": 22, "deploi": 257, "deploy": 256, "dequant": 162, "dequantizedtensor": 157, "descript": 217, "design": 227, "determin": [188, 189, 190, 192, 193, 197, 198, 200, 201, 206, 209, 210, 211, 247], "dictionari": 246, "differ": 196, "direct": 255, "disabl": [199, 212], "discuss": 196, "docker": [240, 241], "document": [239, 258], "download": [186, 241], "enabl": [199, 212, 217], "encod": [174, 187, 191, 199, 205, 212, 217, 245, 246, 247], "engin": 255, "enum": 22, "environ": 241, "equal": [189, 201, 209, 221, 230], "error": 217, "estim": [194, 208, 219, 230], "evalu": [187, 188, 189, 190, 191, 192, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 217, 245, 248], "exampl": [20, 170, 171, 186, 187, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212, 222, 226, 228, 231, 239, 243], "execut": [219, 221, 244, 255], "explor": 224, "export": [194, 208, 244, 245, 247, 248, 256], "fake": 191, "faq": 225, "fast": 191, "featur": [223, 239, 254, 256], "file": 249, "find": [191, 231], "fine": [202, 203, 204, 225], "fix": 253, "float": 170, "floatquantizedequant": 165, "flow": [175, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "fold": [187, 188, 189, 190, 192, 194, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 220, 230], "format": 246, "fp16": 257, "fp32": [187, 188, 189, 190, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 253], "from": [170, 240], "front": 231, "function": [187, 193, 194, 196, 205, 207], "gener": [214, 216, 257], "get": [192, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211], "glossari": [238, 239], "gpu": 248, "granular": 247, "greedi": 224, "group": 231, "guid": [170, 239, 247, 254], "guidebook": 223, "guidelin": [250, 251, 253, 254, 257], "helper": [193, 207], "histogram": 217, "host": 240, "how": [170, 224, 229, 247, 249], "hub": 255, "hyper": 213, "i": [187, 191, 199, 200, 205, 208, 212], "imag": 241, "import": [216, 217], "improv": 256, "individu": 253, "infer": [254, 255, 256], "inform": [188, 189, 190, 192, 193, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211], "initi": 248, "input": [216, 233], "insert": 191, "instal": [239, 240, 241, 242, 243], "instanti": [188, 189, 190, 191, 192, 193, 194, 197, 198, 201, 202, 203, 204, 206, 209, 210, 211], "interact": [214, 215], "kera": [14, 194, 195, 196], "layer": [187, 188, 189, 190, 192, 194, 196, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 214, 216, 217, 221, 224, 225, 230, 231, 233, 253], "layer_output_util": [6, 19, 168], "leaf": 233, "learn": [198, 211], "learnedgrid": 170, "level": 246, "librari": 217, "limit": [20, 171, 196], "list": 231, "load": [191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 216], "lora": [230, 234, 235, 236], "loss": [199, 212, 217], "low": 244, "manual": [232, 233, 242], "max": [199, 212, 217, 247], "mean": 217, "method": 191, "migrat": 170, "min": [199, 212, 217, 247], "mix": [187, 191, 205, 230, 231, 232, 233, 256], "mixed_precis": [2, 12, 173, 181], "mmp": 233, "mode": 248, "model": [187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 216, 217, 225, 233, 235, 244, 245, 248, 250, 251, 253, 257], "model_input": 249, "model_output": 249, "model_prepar": [20, 171], "model_valid": 172, "modifi": 249, "modul": [170, 174], "more": [188, 189, 190, 192, 193, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211], "move": 170, "mse": [199, 212, 230, 237], "multi": 248, "next": [188, 192, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 257], "nn": 174, "nois": 247, "non": 233, "norm": [189, 209, 219, 220, 230], "normal": [187, 188, 190, 192, 197, 198, 200, 201, 205, 206, 210, 211], "note": [225, 239, 252], "notebook": [186, 187, 191, 199, 200, 205, 208, 212, 239], "obtain": 216, "old": 240, "onnx": [187, 188, 189, 190], "op": [191, 231], "optim": 230, "option": [225, 233], "origin": 196, "output": [214, 216, 233], "overal": [187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "overhead": 231, "overview": [224, 225, 227, 229, 247, 249], "packag": [240, 241], "param": 249, "paramet": [187, 191, 205, 213, 247, 248], "pareto": 231, "path": 256, "pcq": 200, "pdf": [199, 212], "peft": 175, "per": [177, 199, 200, 212, 217, 224, 225, 244, 253], "perform": [194, 197, 198, 208, 210, 211, 231, 253], "phase": 231, "pipelin": [187, 188, 189, 190, 192, 193, 194, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212], "platform": 243, "post": 256, "power": 244, "prebuilt": 241, "precis": [187, 191, 205, 230, 231, 232, 233, 256, 257], "prepar": [194, 196, 217], "prerequisit": [213, 217, 218, 219, 237, 240, 241, 242, 245, 248], "pretrain": [193, 199, 207], "procedur": [218, 220, 222, 237, 257], "process": 170, "profil": 233, "prune": [202, 204, 222, 225], "ptq": 235, "pypi": 240, "pytorch": [187, 188, 189, 190, 243, 251], "qat": [194, 197, 198, 208, 210, 211, 248], "qualcomm": 255, "quant": [199, 212], "quant_analyz": [8, 21, 176, 183], "quantanalyz": [199, 212, 217], "quantiz": [160, 163, 170, 174, 177, 187, 188, 189, 190, 191, 192, 194, 195, 197, 198, 199, 200, 201, 205, 206, 208, 209, 210, 211, 212, 214, 217, 218, 230, 234, 235, 239, 244, 246, 247, 248, 253, 254, 255, 256, 257], "quantizationmixin": 29, "quantizationsim": 194, "quantizationsimmodel": 236, "quantize_dequant": 164, "quantizedadaptiveavgpool1d": 30, "quantizedadaptiveavgpool2d": 31, "quantizedadaptiveavgpool3d": 32, "quantizedadaptivemaxpool1d": 33, "quantizedadaptivemaxpool2d": 34, "quantizedadaptivemaxpool3d": 35, "quantizedalphadropout": 36, "quantizedavgpool1d": 37, "quantizedavgpool2d": 38, "quantizedavgpool3d": 39, "quantizedbatchnorm1d": 42, "quantizedbatchnorm2d": 43, "quantizedbatchnorm3d": 44, "quantizedbceloss": 40, "quantizedbcewithlogitsloss": 41, "quantizedbilinear": 45, "quantizedcelu": 46, "quantizedchannelshuffl": 48, "quantizedcircularpad1d": 49, "quantizedcircularpad2d": 50, "quantizedcircularpad3d": 51, "quantizedconstantpad1d": 52, "quantizedconstantpad2d": 53, "quantizedconstantpad3d": 54, "quantizedconv1d": 55, "quantizedconv2d": 56, "quantizedconv3d": 57, "quantizedconvtranspose1d": 58, "quantizedconvtranspose2d": 59, "quantizedconvtranspose3d": 60, "quantizedcosineembeddingloss": 61, "quantizedcosinesimilar": 62, "quantizedcrossentropyloss": 63, "quantizedctcloss": 47, "quantizeddropout": 64, "quantizeddropout1d": 65, "quantizeddropout2d": 66, "quantizeddropout3d": 67, "quantizedelu": 68, "quantizedembed": 69, "quantizedembeddingbag": 70, "quantizedequant": 161, "quantizedfeaturealphadropout": 71, "quantizedflatten": 72, "quantizedfold": 73, "quantizedfractionalmaxpool2d": 74, "quantizedfractionalmaxpool3d": 75, "quantizedgaussiannllloss": 80, "quantizedgelu": 76, "quantizedglu": 77, "quantizedgroupnorm": 81, "quantizedgru": 78, "quantizedgrucel": 79, "quantizedhardshrink": 82, "quantizedhardsigmoid": 83, "quantizedhardswish": 84, "quantizedhardtanh": 85, "quantizedhingeembeddingloss": 86, "quantizedhuberloss": 87, "quantizedinstancenorm1d": 88, "quantizedinstancenorm2d": 89, "quantizedinstancenorm3d": 90, "quantizedkldivloss": 91, "quantizedl1loss": 92, "quantizedlayernorm": 97, "quantizedleakyrelu": 98, "quantizedlinear": 99, "quantizedlocalresponsenorm": 100, "quantizedlogsigmoid": 101, "quantizedlogsoftmax": 102, "quantizedlppool1d": 93, "quantizedlppool2d": 94, "quantizedlstm": 95, "quantizedlstmcel": 96, "quantizedmarginrankingloss": 104, "quantizedmaxpool1d": 105, "quantizedmaxpool2d": 106, "quantizedmaxpool3d": 107, "quantizedmaxunpool1d": 108, "quantizedmaxunpool2d": 109, "quantizedmaxunpool3d": 110, "quantizedmish": 111, "quantizedmseloss": 103, "quantizedmultilabelmarginloss": 112, "quantizedmultilabelsoftmarginloss": 113, "quantizedmultimarginloss": 114, "quantizednllloss": 115, "quantizednllloss2d": 116, "quantizedpairwisedist": 118, "quantizedpixelshuffl": 119, "quantizedpixelunshuffl": 120, "quantizedpoissonnllloss": 121, "quantizedprelu": 117, "quantizedreflectionpad1d": 127, "quantizedreflectionpad2d": 128, "quantizedreflectionpad3d": 129, "quantizedrelu": 125, "quantizedrelu6": 126, "quantizedreplicationpad1d": 130, "quantizedreplicationpad2d": 131, "quantizedreplicationpad3d": 132, "quantizedrnn": 122, "quantizedrnncel": 123, "quantizedrrelu": 124, "quantizedselu": 133, "quantizedsigmoid": 135, "quantizedsilu": 134, "quantizedsmoothl1loss": 136, "quantizedsoftmarginloss": 137, "quantizedsoftmax": 138, "quantizedsoftmax2d": 139, "quantizedsoftmin": 140, "quantizedsoftplu": 141, "quantizedsoftshrink": 142, "quantizedsoftsign": 143, "quantizedtanh": 144, "quantizedtanhshrink": 145, "quantizedtensor": 158, "quantizedtensorbas": 159, "quantizedthreshold": 146, "quantizedtripletmarginloss": 147, "quantizedtripletmarginwithdistanceloss": 148, "quantizedunflatten": 149, "quantizedunfold": 150, "quantizedupsampl": 151, "quantizedupsamplingbilinear2d": 152, "quantizedupsamplingnearest2d": 153, "quantizedzeropad1d": 154, "quantizedzeropad2d": 155, "quantizedzeropad3d": 156, "quantsim": [7, 9, 22, 169, 178, 184, 200, 201, 245, 247], "quantwrapp": 170, "quick": [239, 243], "qw": [234, 235], "qwa": [234, 236], "rang": [198, 199, 211, 212, 217], "rank": 225, "ratio": [224, 225, 227], "re": [194, 208, 219, 230], "recommend": 248, "reconstruct": 222, "reduc": [231, 257], "reestim": 208, "refer": [174, 177, 225, 239], "regular": 191, "relat": 186, "releas": [239, 252], "restor": 257, "round": [188, 192, 206, 213, 225, 230], "run": [186, 187, 191, 205, 207, 217, 236, 243], "runtim": [247, 249], "scheme": 247, "score": [192, 197, 198, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211], "scratch": 240, "sdk": 255, "select": [222, 224, 225], "sensit": [217, 231, 253], "seq_ms": [10, 179, 185], "sequenti": [230, 237], "server": [186, 227], "session": 227, "set": 233, "set_grouped_blockwise_quantization_for_weight": 7, "setup": [213, 219, 221, 222, 226, 228, 233, 235, 236, 237, 241, 242, 248], "show": 196, "signal": 247, "sim": [187, 188, 189, 190, 192, 197, 198, 201, 205, 206, 208, 209, 210, 211], "similar": 196, "simplifi": [187, 188, 189, 190], "simul": [187, 188, 189, 190, 191, 192, 197, 198, 200, 201, 205, 206, 208, 209, 210, 211, 239, 247, 256], "sourc": 240, "spatial": [203, 204, 225, 226], "specif": 246, "squar": 217, "start": [227, 239, 241, 243], "staticgrid": 170, "statist": [199, 208, 212, 217], "step": [188, 192, 197, 198, 201, 202, 203, 204, 206, 207, 209, 210, 211, 213, 216, 217, 218, 219, 220, 231, 237, 245, 248, 257], "structur": [246, 249], "subclass": 196, "summari": [191, 194, 196, 200, 208], "supergroup": 249, "support": [248, 256], "svd": [203, 204, 225, 226, 228], "target": [254, 255, 256], "techniqu": [209, 213, 225, 230], "tensorflow": 250, "test": 243, "tf": 247, "thi": [187, 191, 199, 200, 205, 208, 212], "tool": [167, 214, 230, 256], "top": 246, "train": [188, 189, 190, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 206, 208, 209, 210, 211, 212, 235, 236, 248, 256], "transform": 195, "try": 257, "tune": [202, 203, 204, 225], "type": [233, 246], "updat": 235, "us": [170, 202, 203, 204, 222, 225, 226, 228, 231], "user": [175, 239, 254], "v": 170, "v1": [166, 180, 181, 182, 183, 184, 185], "valid": [188, 189, 190, 192, 193, 201, 202, 203, 204, 206, 209, 210, 211], "variant": 241, "verifi": [240, 243, 257], "version": [240, 246, 258], "visual": [167, 214, 215, 227, 253], "visualization_tool": 167, "w16a16": 257, "weight": [217, 222, 225, 228, 235, 253], "what": [187, 191, 199, 200, 205, 208, 212], "width": 231, "winnow": [222, 229], "work": [224, 229, 247], "workflow": [213, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 231, 233, 235, 236, 237, 245, 247, 248, 253, 254, 257], "wrapper": [199, 212], "x": 170, "your": 241}})