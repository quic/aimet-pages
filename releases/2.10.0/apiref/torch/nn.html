<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>aimet_torch.nn - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../../_static/aimet-furo.css?v=22b0637d" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.10.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../overview/index.html">Overview</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Overview</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../overview/install/quick-start.html">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../overview/install/index.html">Install</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Tutorials</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/quantization_workflow.html">Quantization Workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/quantsim.html">Quantization Simulation</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Quantization Simulation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/notebooks.html">Example Notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Example Notebooks</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/on_target_inference.html">Running Quantized Models on-device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/debugging_guidelines.html">Debugging Guide</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../techniques/index.html">Techniques</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Techniques</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../techniques/ptq.html">Post Training Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../techniques/qat.html">Quantization Aware Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../techniques/blockwise.html">Blockwise Quantization</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../techniques/mixed_precision/index.html">Mixed precision</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Mixed precision</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/mixed_precision/mmp.html">Manual mixed precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/mixed_precision/amp.html">Automatic mixed precision</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../techniques/analysis_tools/index.html">Analysis tools</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of Analysis tools</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/analysis_tools/interactive_visualization.html">Interactive visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/analysis_tools/quant_analyzer.html">Quantization analyzer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/analysis_tools/layer_output_generation.html">Layer output generation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../techniques/compression/index.html">Compression</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Compression</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/compression/feature_guidebook.html">Compression guidebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/compression/greedy_compression_ratio_selection.html">Greedy compression ratio selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/compression/visualization_compression.html">Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/compression/weight_svd.html">Weight SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../techniques/compression/spatial_svd.html">Spatial SVD</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../techniques/compression/channel_pruning.html">Channel pruning</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of Channel pruning</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../techniques/compression/winnowing.html">Winnowing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ptq_techniques/index.html">PTQ Techniques</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle navigation of PTQ Techniques</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/adascale.html">AdaScale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/omniquant.html">OmniQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ptq_techniques/autoquant.html">Automatic quantization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../external/index.html">External Resources</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle navigation of External Resources</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="http://www.qualcomm.com/developer/artificial-intelligence#overview">Qualcomm AI Stack</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-models/">Qualcomm Hub Models</a></li>
<li class="toctree-l2"><a class="reference external" href="https://github.com/quic/ai-hub-apps/">Qualcomm Hub Apps</a></li>
<li class="toctree-l2"><a class="reference external" href="https://aihub.qualcomm.com/">Qualcomm AI Hub</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="aimet-torch-nn">
<span id="apiref-torch-nn"></span><h1>aimet_torch.nn<a class="headerlink" href="#aimet-torch-nn" title="Link to this heading">¶</a></h1>
<section id="quantized-modules">
<h2>Quantized modules<a class="headerlink" href="#quantized-modules" title="Link to this heading">¶</a></h2>
<p>To simulate the effects of running networks at a reduced bitwidth, AIMET introduced <cite>quantized modules</cite>, the extension of
standard <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> with some extra capabilities for quantization.
These quantized modules serve as drop-in replacements for their PyTorch counterparts, but can
hold <a class="reference internal" href="quantization.html#api-beta-quantizers"><span class="std std-ref">input, output, and parameter quantizers</span></a> to perform quantization operations during the
module’s forward pass and compute quantization encodings.</p>
<p>More specifically, a quantized module inherits both from <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin" title="aimet_torch.nn.QuantizationMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationMixin</span></code></a> and a native <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> type,
typically with “Quantized-” prefix prepended to the original class name, such as <a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv2d.html#aimet_torch.nn.QuantizedConv2d" title="aimet_torch.nn.QuantizedConv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedConv2d</span></code></a>
for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> or <a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmax.html#aimet_torch.nn.QuantizedSoftmax" title="aimet_torch.nn.QuantizedSoftmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedSoftmax</span></code></a> for <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Softmax</span></code>.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_torch.nn.</span></span><span class="sig-name descname"><span class="pre">QuantizationMixin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Quantization mixin class for torch.nn.Module.</p>
<p>Specifically, a quantized module will quantize input, output, and parameter tensors with
its held <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects during the <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.forward" title="aimet_torch.nn.QuantizationMixin.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method and use the inherited <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>
forward method to compute the layer operation. If all input, output, and parameter quantizers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, a
quantized module will behave exactly the same as its parent <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">input_quantizers</span></span></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> containing <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects to be applied
to the layer’s input tensors</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">output_quantizers</span></span></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> containing <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects to be applied
to the layer’s output tensors</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">param_quantizers</span></span></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> mapping parameter names to associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code>
objects</p>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="p">)</span>
<span class="go">QuantizedLinear(</span>
<span class="go">  in_features=10, out_features=10, bias=True</span>
<span class="go">  (param_quantizers): ModuleDict(</span>
<span class="go">    (weight): None</span>
<span class="go">    (bias): None</span>
<span class="go">  )</span>
<span class="go">  (input_quantizers): ModuleList(</span>
<span class="go">    (0): None</span>
<span class="go">  )</span>
<span class="go">  (output_quantizers): ModuleList(</span>
<span class="go">    (0): None</span>
<span class="go">  )</span>
<span class="go">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__quant_init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Initializer for quantized module. This method will be invoked right after <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code>.</p>
<p>This method initializes the <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.input_quantizers" title="aimet_torch.nn.QuantizationMixin.input_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_quantizers</span></code></a>, <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.output_quantizers" title="aimet_torch.nn.QuantizationMixin.output_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_quantizers</span></code></a>, and <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.param_quantizers" title="aimet_torch.nn.QuantizationMixin.param_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">param_quantizers</span></code></a>
structures to the appropriate sizes based on the number of input tensors, output tensors, and parameters of the
base <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Module</span></code> class. All quantizers are initializd to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>For custom quantized classes, this method should be overridden to set the appropriate lengths of
<a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.input_quantizers" title="aimet_torch.nn.QuantizationMixin.input_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_quantizers</span></code></a> and <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.output_quantizers" title="aimet_torch.nn.QuantizationMixin.output_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_quantizers</span></code></a> for the given base class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Enters the <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.compute_encodings" title="aimet_torch.nn.QuantizationMixin.compute_encodings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_encodings()</span></code></a> context for all <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects in the layer.</p>
<p>Inside this context, each quantizer will observe all inputs passed to the quantizer and will compute
quantization encodings upon exiting the context.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Quantize</span><span class="p">((),</span> <span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">qlinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes a quantized version of the parent module’s forward method.</p>
<p>The <a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin.forward" title="aimet_torch.nn.QuantizationMixin.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method should perform the following logic in order:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Apply existing input quantizers to input tensors</p></li>
<li><p>Apply existing param quantizers to the layer’s parameters</p></li>
<li><p>Call the inherited <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> forward method with quantized inputs and parameters</p></li>
<li><p>Apply existing output quantizers to the outputs of the forward method</p></li>
</ol>
</div></blockquote>
<p>If all input, output, and parameter quantizers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this method will behave exactly the same as
its parent module’s forward pass.</p>
</dd></dl>

</dd></dl>

<section id="configuration">
<h3>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">¶</a></h3>
<p>The quantization behavior of a quantized module is controlled by the <a class="reference internal" href="quantization.html#api-beta-quantizers"><span class="std std-ref">quantizers</span></a> contained within the input, output,
and parameter quantizer attributes listed below.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">input_quantizers</span></code></p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for input tensors</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">param_quantizers</span></code></p></td>
<td><p>torch.nn.ModuleDict</p></td>
<td><p>Dict mapping parameter names to quantizers</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">output_quantizers</span></code></p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for output tensors</p></td>
</tr>
</tbody>
</table>
</div>
<p>By assigning and configuring quantizers to these structures, we define the type of quantization applied to the corresponding
input index, output index, or parameter name. By default, all the quantizers are set to <cite>None</cite>, meaning that no quantization
will be applied to the respective tensor.</p>
<dl>
<dt>Example: Create a linear layer which performs only per-channel weight quantization</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Per-channel weight quantization is performed over the `out_features` dimension, so encodings are shape (10, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_channel_quantizer</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">per_channel_quantizer</span>
</pre></div>
</div>
</dd>
<dt>Example: Create an elementwise multiply layer which quantizes only the output and the second input</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">custom</span><span class="o">.</span><span class="n">QuantizedMultiply</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>In some cases, it may make sense for multiple tensors to share the same quantizer. In this case, we can assign the same
quantizer to multiple indices.</p>
<dl>
<dt>Example: Create an elementwise add layer which shares the same quantizer between its inputs</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">custom</span><span class="o">.</span><span class="n">QuantizedAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantizer</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="computing-encodings">
<h3>Computing encodings<a class="headerlink" href="#computing-encodings" title="Link to this heading">¶</a></h3>
<p>Before a module can compute a quantized forward pass, all quantizers must first be calibrated inside a <cite>compute_encodings</cite>
context. When a quantized module enters the <cite>compute_encodings</cite> context, it first disables all input and output quantization
while the quantizers observe the statistics of the activation tensors passing through them. Upon exiting the context,
the quantizers calculate appropriate quantization encodings based on these statistics (exactly <em>how</em> the encodings are
computed is determined by each quantizer’s <span class="xref std std-ref">encoding analyzer</span>).</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">... </span>    <span class="c1"># Pass several samples through the layer to ensure representative statistics</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">calibration_data_loader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">qlinear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd>
</dl>
</section>
</section>
<section id="api-reference">
<h2>API reference<a class="headerlink" href="#api-reference" title="Link to this heading">¶</a></h2>
<p id="api-built-in-quantized-modules"><strong>Built-in quantized modules</strong></p>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizationMixin.html#aimet_torch.nn.QuantizationMixin" title="aimet_torch.nn.QuantizationMixin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizationMixin</span></code></a></p></td>
<td><p>Quantization mixin class for torch.nn.Module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.html#aimet_torch.nn.QuantizedAdaptiveAvgPool1d" title="aimet_torch.nn.QuantizedAdaptiveAvgPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveAvgPool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveAvgPool1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.html#aimet_torch.nn.QuantizedAdaptiveAvgPool2d" title="aimet_torch.nn.QuantizedAdaptiveAvgPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveAvgPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveAvgPool2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.html#aimet_torch.nn.QuantizedAdaptiveAvgPool3d" title="aimet_torch.nn.QuantizedAdaptiveAvgPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveAvgPool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveAvgPool3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.html#aimet_torch.nn.QuantizedAdaptiveMaxPool1d" title="aimet_torch.nn.QuantizedAdaptiveMaxPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveMaxPool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveMaxPool1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.html#aimet_torch.nn.QuantizedAdaptiveMaxPool2d" title="aimet_torch.nn.QuantizedAdaptiveMaxPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveMaxPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveMaxPool2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.html#aimet_torch.nn.QuantizedAdaptiveMaxPool3d" title="aimet_torch.nn.QuantizedAdaptiveMaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAdaptiveMaxPool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AdaptiveMaxPool3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAlphaDropout.html#aimet_torch.nn.QuantizedAlphaDropout" title="aimet_torch.nn.QuantizedAlphaDropout"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAlphaDropout</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AlphaDropout</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool1d.html#aimet_torch.nn.QuantizedAvgPool1d" title="aimet_torch.nn.QuantizedAvgPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAvgPool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AvgPool1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool2d.html#aimet_torch.nn.QuantizedAvgPool2d" title="aimet_torch.nn.QuantizedAvgPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAvgPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AvgPool2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedAvgPool3d.html#aimet_torch.nn.QuantizedAvgPool3d" title="aimet_torch.nn.QuantizedAvgPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedAvgPool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.AvgPool3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBCELoss.html#aimet_torch.nn.QuantizedBCELoss" title="aimet_torch.nn.QuantizedBCELoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBCELoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.BCELoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.html#aimet_torch.nn.QuantizedBCEWithLogitsLoss" title="aimet_torch.nn.QuantizedBCEWithLogitsLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBCEWithLogitsLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.BCEWithLogitsLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm1d.html#aimet_torch.nn.QuantizedBatchNorm1d" title="aimet_torch.nn.QuantizedBatchNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBatchNorm1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.BatchNorm1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm2d.html#aimet_torch.nn.QuantizedBatchNorm2d" title="aimet_torch.nn.QuantizedBatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBatchNorm2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.BatchNorm2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBatchNorm3d.html#aimet_torch.nn.QuantizedBatchNorm3d" title="aimet_torch.nn.QuantizedBatchNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBatchNorm3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.BatchNorm3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedBilinear.html#aimet_torch.nn.QuantizedBilinear" title="aimet_torch.nn.QuantizedBilinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedBilinear</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Bilinear</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCELU.html#aimet_torch.nn.QuantizedCELU" title="aimet_torch.nn.QuantizedCELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCELU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CELU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCTCLoss.html#aimet_torch.nn.QuantizedCTCLoss" title="aimet_torch.nn.QuantizedCTCLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCTCLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CTCLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedChannelShuffle.html#aimet_torch.nn.QuantizedChannelShuffle" title="aimet_torch.nn.QuantizedChannelShuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedChannelShuffle</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ChannelShuffle</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad1d.html#aimet_torch.nn.QuantizedCircularPad1d" title="aimet_torch.nn.QuantizedCircularPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCircularPad1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CircularPad1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad2d.html#aimet_torch.nn.QuantizedCircularPad2d" title="aimet_torch.nn.QuantizedCircularPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCircularPad2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CircularPad2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCircularPad3d.html#aimet_torch.nn.QuantizedCircularPad3d" title="aimet_torch.nn.QuantizedCircularPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCircularPad3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CircularPad3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad1d.html#aimet_torch.nn.QuantizedConstantPad1d" title="aimet_torch.nn.QuantizedConstantPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConstantPad1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConstantPad2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad2d.html#aimet_torch.nn.QuantizedConstantPad2d" title="aimet_torch.nn.QuantizedConstantPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConstantPad2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConstantPad2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConstantPad3d.html#aimet_torch.nn.QuantizedConstantPad3d" title="aimet_torch.nn.QuantizedConstantPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConstantPad3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConstantPad3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv1d.html#aimet_torch.nn.QuantizedConv1d" title="aimet_torch.nn.QuantizedConv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConv1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Conv1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv2d.html#aimet_torch.nn.QuantizedConv2d" title="aimet_torch.nn.QuantizedConv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConv2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Conv2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConv3d.html#aimet_torch.nn.QuantizedConv3d" title="aimet_torch.nn.QuantizedConv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConv3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Conv3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose1d.html#aimet_torch.nn.QuantizedConvTranspose1d" title="aimet_torch.nn.QuantizedConvTranspose1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConvTranspose1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConvTranspose1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose2d.html#aimet_torch.nn.QuantizedConvTranspose2d" title="aimet_torch.nn.QuantizedConvTranspose2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConvTranspose2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConvTranspose2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedConvTranspose3d.html#aimet_torch.nn.QuantizedConvTranspose3d" title="aimet_torch.nn.QuantizedConvTranspose3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedConvTranspose3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ConvTranspose3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.html#aimet_torch.nn.QuantizedCosineEmbeddingLoss" title="aimet_torch.nn.QuantizedCosineEmbeddingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCosineEmbeddingLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CosineEmbeddingLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCosineSimilarity.html#aimet_torch.nn.QuantizedCosineSimilarity" title="aimet_torch.nn.QuantizedCosineSimilarity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCosineSimilarity</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CosineSimilarity</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedCrossEntropyLoss.html#aimet_torch.nn.QuantizedCrossEntropyLoss" title="aimet_torch.nn.QuantizedCrossEntropyLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedCrossEntropyLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.CrossEntropyLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout.html#aimet_torch.nn.QuantizedDropout" title="aimet_torch.nn.QuantizedDropout"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedDropout</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Dropout</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout1d.html#aimet_torch.nn.QuantizedDropout1d" title="aimet_torch.nn.QuantizedDropout1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedDropout1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Dropout1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout2d.html#aimet_torch.nn.QuantizedDropout2d" title="aimet_torch.nn.QuantizedDropout2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedDropout2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Dropout2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedDropout3d.html#aimet_torch.nn.QuantizedDropout3d" title="aimet_torch.nn.QuantizedDropout3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedDropout3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Dropout3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedELU.html#aimet_torch.nn.QuantizedELU" title="aimet_torch.nn.QuantizedELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedELU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ELU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedEmbedding.html#aimet_torch.nn.QuantizedEmbedding" title="aimet_torch.nn.QuantizedEmbedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedEmbedding</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Embedding</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedEmbeddingBag.html#aimet_torch.nn.QuantizedEmbeddingBag" title="aimet_torch.nn.QuantizedEmbeddingBag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedEmbeddingBag</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.EmbeddingBag</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.html#aimet_torch.nn.QuantizedFeatureAlphaDropout" title="aimet_torch.nn.QuantizedFeatureAlphaDropout"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedFeatureAlphaDropout</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.FeatureAlphaDropout</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFlatten.html#aimet_torch.nn.QuantizedFlatten" title="aimet_torch.nn.QuantizedFlatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedFlatten</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Flatten</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFold.html#aimet_torch.nn.QuantizedFold" title="aimet_torch.nn.QuantizedFold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedFold</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Fold</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.html#aimet_torch.nn.QuantizedFractionalMaxPool2d" title="aimet_torch.nn.QuantizedFractionalMaxPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedFractionalMaxPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.FractionalMaxPool2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.html#aimet_torch.nn.QuantizedFractionalMaxPool3d" title="aimet_torch.nn.QuantizedFractionalMaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedFractionalMaxPool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.FractionalMaxPool3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGELU.html#aimet_torch.nn.QuantizedGELU" title="aimet_torch.nn.QuantizedGELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGELU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GELU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGLU.html#aimet_torch.nn.QuantizedGLU" title="aimet_torch.nn.QuantizedGLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GLU</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGRU.html#aimet_torch.nn.QuantizedGRU" title="aimet_torch.nn.QuantizedGRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGRU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GRU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGRUCell.html#aimet_torch.nn.QuantizedGRUCell" title="aimet_torch.nn.QuantizedGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGRUCell</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GRUCell</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGaussianNLLLoss.html#aimet_torch.nn.QuantizedGaussianNLLLoss" title="aimet_torch.nn.QuantizedGaussianNLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGaussianNLLLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GaussianNLLLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedGroupNorm.html#aimet_torch.nn.QuantizedGroupNorm" title="aimet_torch.nn.QuantizedGroupNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedGroupNorm</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.GroupNorm</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardshrink.html#aimet_torch.nn.QuantizedHardshrink" title="aimet_torch.nn.QuantizedHardshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHardshrink</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Hardshrink</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardsigmoid.html#aimet_torch.nn.QuantizedHardsigmoid" title="aimet_torch.nn.QuantizedHardsigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHardsigmoid</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Hardsigmoid</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardswish.html#aimet_torch.nn.QuantizedHardswish" title="aimet_torch.nn.QuantizedHardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHardswish</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Hardswish</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHardtanh.html#aimet_torch.nn.QuantizedHardtanh" title="aimet_torch.nn.QuantizedHardtanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHardtanh</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Hardtanh</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.html#aimet_torch.nn.QuantizedHingeEmbeddingLoss" title="aimet_torch.nn.QuantizedHingeEmbeddingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHingeEmbeddingLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.HingeEmbeddingLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedHuberLoss.html#aimet_torch.nn.QuantizedHuberLoss" title="aimet_torch.nn.QuantizedHuberLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedHuberLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.HuberLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm1d.html#aimet_torch.nn.QuantizedInstanceNorm1d" title="aimet_torch.nn.QuantizedInstanceNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedInstanceNorm1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.InstanceNorm1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm2d.html#aimet_torch.nn.QuantizedInstanceNorm2d" title="aimet_torch.nn.QuantizedInstanceNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedInstanceNorm2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.InstanceNorm2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedInstanceNorm3d.html#aimet_torch.nn.QuantizedInstanceNorm3d" title="aimet_torch.nn.QuantizedInstanceNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedInstanceNorm3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.InstanceNorm3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedKLDivLoss.html#aimet_torch.nn.QuantizedKLDivLoss" title="aimet_torch.nn.QuantizedKLDivLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedKLDivLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.KLDivLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedL1Loss.html#aimet_torch.nn.QuantizedL1Loss" title="aimet_torch.nn.QuantizedL1Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedL1Loss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.L1Loss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLPPool1d.html#aimet_torch.nn.QuantizedLPPool1d" title="aimet_torch.nn.QuantizedLPPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLPPool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LPPool1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLPPool2d.html#aimet_torch.nn.QuantizedLPPool2d" title="aimet_torch.nn.QuantizedLPPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLPPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LPPool2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLSTM.html#aimet_torch.nn.QuantizedLSTM" title="aimet_torch.nn.QuantizedLSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLSTM</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LSTM</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLSTMCell.html#aimet_torch.nn.QuantizedLSTMCell" title="aimet_torch.nn.QuantizedLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLSTMCell</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LSTMCell</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLayerNorm.html#aimet_torch.nn.QuantizedLayerNorm" title="aimet_torch.nn.QuantizedLayerNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLayerNorm</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LayerNorm</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLeakyReLU.html#aimet_torch.nn.QuantizedLeakyReLU" title="aimet_torch.nn.QuantizedLeakyReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLeakyReLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LeakyReLU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLinear.html#aimet_torch.nn.QuantizedLinear" title="aimet_torch.nn.QuantizedLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLinear</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Linear</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLocalResponseNorm.html#aimet_torch.nn.QuantizedLocalResponseNorm" title="aimet_torch.nn.QuantizedLocalResponseNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLocalResponseNorm</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LocalResponseNorm</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLogSigmoid.html#aimet_torch.nn.QuantizedLogSigmoid" title="aimet_torch.nn.QuantizedLogSigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLogSigmoid</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LogSigmoid</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedLogSoftmax.html#aimet_torch.nn.QuantizedLogSoftmax" title="aimet_torch.nn.QuantizedLogSoftmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedLogSoftmax</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.LogSoftmax</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMSELoss.html#aimet_torch.nn.QuantizedMSELoss" title="aimet_torch.nn.QuantizedMSELoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMSELoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MSELoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMarginRankingLoss.html#aimet_torch.nn.QuantizedMarginRankingLoss" title="aimet_torch.nn.QuantizedMarginRankingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMarginRankingLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MarginRankingLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool1d.html#aimet_torch.nn.QuantizedMaxPool1d" title="aimet_torch.nn.QuantizedMaxPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxPool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxPool1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool2d.html#aimet_torch.nn.QuantizedMaxPool2d" title="aimet_torch.nn.QuantizedMaxPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxPool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxPool2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxPool3d.html#aimet_torch.nn.QuantizedMaxPool3d" title="aimet_torch.nn.QuantizedMaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxPool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxPool3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool1d.html#aimet_torch.nn.QuantizedMaxUnpool1d" title="aimet_torch.nn.QuantizedMaxUnpool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxUnpool1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxUnpool1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool2d.html#aimet_torch.nn.QuantizedMaxUnpool2d" title="aimet_torch.nn.QuantizedMaxUnpool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxUnpool2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxUnpool2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMaxUnpool3d.html#aimet_torch.nn.QuantizedMaxUnpool3d" title="aimet_torch.nn.QuantizedMaxUnpool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMaxUnpool3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MaxUnpool3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMish.html#aimet_torch.nn.QuantizedMish" title="aimet_torch.nn.QuantizedMish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMish</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Mish</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.html#aimet_torch.nn.QuantizedMultiLabelMarginLoss" title="aimet_torch.nn.QuantizedMultiLabelMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMultiLabelMarginLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MultiLabelMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.html#aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss" title="aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMultiLabelSoftMarginLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MultiLabelSoftMarginLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedMultiMarginLoss.html#aimet_torch.nn.QuantizedMultiMarginLoss" title="aimet_torch.nn.QuantizedMultiMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedMultiMarginLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.MultiMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedNLLLoss.html#aimet_torch.nn.QuantizedNLLLoss" title="aimet_torch.nn.QuantizedNLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedNLLLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.NLLLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedNLLLoss2d.html#aimet_torch.nn.QuantizedNLLLoss2d" title="aimet_torch.nn.QuantizedNLLLoss2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedNLLLoss2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.NLLLoss2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPReLU.html#aimet_torch.nn.QuantizedPReLU" title="aimet_torch.nn.QuantizedPReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedPReLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.PReLU</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPairwiseDistance.html#aimet_torch.nn.QuantizedPairwiseDistance" title="aimet_torch.nn.QuantizedPairwiseDistance"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedPairwiseDistance</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.PairwiseDistance</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPixelShuffle.html#aimet_torch.nn.QuantizedPixelShuffle" title="aimet_torch.nn.QuantizedPixelShuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedPixelShuffle</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.PixelShuffle</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPixelUnshuffle.html#aimet_torch.nn.QuantizedPixelUnshuffle" title="aimet_torch.nn.QuantizedPixelUnshuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedPixelUnshuffle</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.PixelUnshuffle</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedPoissonNLLLoss.html#aimet_torch.nn.QuantizedPoissonNLLLoss" title="aimet_torch.nn.QuantizedPoissonNLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedPoissonNLLLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.PoissonNLLLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRNN.html#aimet_torch.nn.QuantizedRNN" title="aimet_torch.nn.QuantizedRNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedRNN</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.RNN</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRNNCell.html#aimet_torch.nn.QuantizedRNNCell" title="aimet_torch.nn.QuantizedRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedRNNCell</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.RNNCell</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedRReLU.html#aimet_torch.nn.QuantizedRReLU" title="aimet_torch.nn.QuantizedRReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedRReLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.RReLU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReLU.html#aimet_torch.nn.QuantizedReLU" title="aimet_torch.nn.QuantizedReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReLU</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReLU6.html#aimet_torch.nn.QuantizedReLU6" title="aimet_torch.nn.QuantizedReLU6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReLU6</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReLU6</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad1d.html#aimet_torch.nn.QuantizedReflectionPad1d" title="aimet_torch.nn.QuantizedReflectionPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReflectionPad1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReflectionPad1d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad2d.html#aimet_torch.nn.QuantizedReflectionPad2d" title="aimet_torch.nn.QuantizedReflectionPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReflectionPad2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReflectionPad2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReflectionPad3d.html#aimet_torch.nn.QuantizedReflectionPad3d" title="aimet_torch.nn.QuantizedReflectionPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReflectionPad3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReflectionPad3d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad1d.html#aimet_torch.nn.QuantizedReplicationPad1d" title="aimet_torch.nn.QuantizedReplicationPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReplicationPad1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReplicationPad1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad2d.html#aimet_torch.nn.QuantizedReplicationPad2d" title="aimet_torch.nn.QuantizedReplicationPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReplicationPad2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReplicationPad2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedReplicationPad3d.html#aimet_torch.nn.QuantizedReplicationPad3d" title="aimet_torch.nn.QuantizedReplicationPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedReplicationPad3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ReplicationPad3d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSELU.html#aimet_torch.nn.QuantizedSELU" title="aimet_torch.nn.QuantizedSELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSELU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.SELU</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSiLU.html#aimet_torch.nn.QuantizedSiLU" title="aimet_torch.nn.QuantizedSiLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSiLU</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.SiLU</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSigmoid.html#aimet_torch.nn.QuantizedSigmoid" title="aimet_torch.nn.QuantizedSigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSigmoid</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Sigmoid</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSmoothL1Loss.html#aimet_torch.nn.QuantizedSmoothL1Loss" title="aimet_torch.nn.QuantizedSmoothL1Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSmoothL1Loss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.SmoothL1Loss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftMarginLoss.html#aimet_torch.nn.QuantizedSoftMarginLoss" title="aimet_torch.nn.QuantizedSoftMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftMarginLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.SoftMarginLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmax.html#aimet_torch.nn.QuantizedSoftmax" title="aimet_torch.nn.QuantizedSoftmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftmax</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softmax</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmax2d.html#aimet_torch.nn.QuantizedSoftmax2d" title="aimet_torch.nn.QuantizedSoftmax2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftmax2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softmax2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftmin.html#aimet_torch.nn.QuantizedSoftmin" title="aimet_torch.nn.QuantizedSoftmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftmin</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softmin</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftplus.html#aimet_torch.nn.QuantizedSoftplus" title="aimet_torch.nn.QuantizedSoftplus"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftplus</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softplus</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftshrink.html#aimet_torch.nn.QuantizedSoftshrink" title="aimet_torch.nn.QuantizedSoftshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftshrink</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softshrink</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedSoftsign.html#aimet_torch.nn.QuantizedSoftsign" title="aimet_torch.nn.QuantizedSoftsign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedSoftsign</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Softsign</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTanh.html#aimet_torch.nn.QuantizedTanh" title="aimet_torch.nn.QuantizedTanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTanh</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Tanh</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTanhshrink.html#aimet_torch.nn.QuantizedTanhshrink" title="aimet_torch.nn.QuantizedTanhshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTanhshrink</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Tanhshrink</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedThreshold.html#aimet_torch.nn.QuantizedThreshold" title="aimet_torch.nn.QuantizedThreshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedThreshold</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Threshold</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTripletMarginLoss.html#aimet_torch.nn.QuantizedTripletMarginLoss" title="aimet_torch.nn.QuantizedTripletMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTripletMarginLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.TripletMarginLoss</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.html#aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss" title="aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedTripletMarginWithDistanceLoss</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.TripletMarginWithDistanceLoss</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUnflatten.html#aimet_torch.nn.QuantizedUnflatten" title="aimet_torch.nn.QuantizedUnflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedUnflatten</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Unflatten</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUnfold.html#aimet_torch.nn.QuantizedUnfold" title="aimet_torch.nn.QuantizedUnfold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedUnfold</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Unfold</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsample.html#aimet_torch.nn.QuantizedUpsample" title="aimet_torch.nn.QuantizedUpsample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedUpsample</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.Upsample</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.html#aimet_torch.nn.QuantizedUpsamplingBilinear2d" title="aimet_torch.nn.QuantizedUpsamplingBilinear2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedUpsamplingBilinear2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.UpsamplingBilinear2d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.html#aimet_torch.nn.QuantizedUpsamplingNearest2d" title="aimet_torch.nn.QuantizedUpsamplingNearest2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedUpsamplingNearest2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.UpsamplingNearest2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad1d.html#aimet_torch.nn.QuantizedZeroPad1d" title="aimet_torch.nn.QuantizedZeroPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedZeroPad1d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ZeroPad1d</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad2d.html#aimet_torch.nn.QuantizedZeroPad2d" title="aimet_torch.nn.QuantizedZeroPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedZeroPad2d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ZeroPad2d</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/aimet_torch.nn.QuantizedZeroPad3d.html#aimet_torch.nn.QuantizedZeroPad3d" title="aimet_torch.nn.QuantizedZeroPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizedZeroPad3d</span></code></a></p></td>
<td><p>Quantized subclass of torch.nn.ZeroPad3d</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">aimet_torch.nn</a><ul>
<li><a class="reference internal" href="#quantized-modules">Quantized modules</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#computing-encodings">Computing encodings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#api-reference">API reference</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>