<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="aimet_torch.quantsim" href="quantsim.html" /><link rel="prev" title="aimet_torch API" href="index.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Migration guide - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../../_static/aimet-furo.css?v=6d7e6c94" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.0.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/quick-start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../userguide/index.html">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/quantization_workflow.html">Quantization workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/debugging_guidelines.html">Debugging guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../userguide/on_target_inference.html">On-target inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../quantsim/index.html">Quantization Simulation Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/calibration.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/qat.html">QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quantsim/advanced.html">Advanced</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../featureguide/index.html">Feature Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/mixed%20precision/index.html">Mixed precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/autoquant.html">Automatic quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/analysis%20tools/index.html">Analysis tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../featureguide/compression/index.html">Compression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">aimet_torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tensorflow/index.html">aimet_tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/index.html">aimet_onnx</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="migration-guide">
<span id="torch-migration-guide"></span><h1>Migration guide<a class="headerlink" href="#migration-guide" title="Link to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can interact with <code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch</span></code> through the high-level APIs in the same way.
Methods like <code class="xref py py-func docutils literal notranslate"><span class="pre">QuantizationSimModel.compute_encodings()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">QuantizationSimModel.export()</span></code>
will remain the same.
It may require little work to convert your code from aimet_torch 1.x to aimet_torch 2 only if you are
using low-level internal components of <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code>.</p>
</div>
<section id="aimet-torch-1-x-vs-aimet-torch-2">
<h2>aimet_torch 1.x vs aimet_torch 2<a class="headerlink" href="#aimet-torch-1-x-vs-aimet-torch-2" title="Link to this heading">¶</a></h2>
<p>Migration to aimet_torch 2 enables access to new features, easier debugging, and simpler code that is
easier to extend. This guide provides an overview of the migration process and describes the fundamental
differences between the two versions.</p>
<p>Before migrating, it is important to understand the behavior and API differences between aimet_torch 1.x
and aimet_torch 2. Under the hood, aimet_torch 2 has a different set of building blocks and properties than
aimet_torch 1.x, as shown below:</p>
<a class="reference internal image-reference" href="../../_images/quantsim2.0.png"><img alt="../../_images/quantsim2.0.png" src="../../_images/quantsim2.0.png" style="width: 800px;" />
</a>
</section>
<section id="migration-process">
<h2>Migration Process<a class="headerlink" href="#migration-process" title="Link to this heading">¶</a></h2>
<p>Following are some code examples to help you understand how low-level internal components of
<code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code> have changed from aimet_torch 1.x to aimet_torch 2.</p>
<section id="moving-from-quantwrapper-to-quantized-modules">
<h3>Moving from QuantWrapper to Quantized Modules<a class="headerlink" href="#moving-from-quantwrapper-to-quantized-modules" title="Link to this heading">¶</a></h3>
<p>To enable quantization in aimet_torch 1.x, modules are wrapped with a <code class="xref py py-class docutils literal notranslate"><span class="pre">QcQuantizeWrapper</span></code>. These
wrapped modules can be accessed as follows:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.v1.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span> <span class="k">as</span> <span class="n">QuantizationSimModelV1</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModelV1</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">all_quant_wrappers</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">quant_wrappers</span><span class="p">()</span>
<span class="k">for</span> <span class="n">quant_wrapper</span> <span class="ow">in</span> <span class="n">sim</span><span class="o">.</span><span class="n">quant_wrappers</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">quant_wrapper</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>StaticGridQuantWrapper(
    (_module_to_wrap): Linear(in_features=100, out_features=200, bias=True)
)
StaticGridQuantWrapper(
    (_module_to_wrap): ReLU()
)
</pre></div>
</div>
<p>In contrast, aimet_torch 2 enables quantization through quantized <code class="xref py py-mod docutils literal notranslate"><span class="pre">nn.Modules</span></code> - modules are no longer
wrapped but replaced with a quantized version. For example, a <code class="xref py py-mod docutils literal notranslate"><span class="pre">nn.Linear</span></code> would be replaced with
<code class="xref py py-mod docutils literal notranslate"><span class="pre">QuantizedLinear</span></code>, <code class="xref py py-mod docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> would be replace by <code class="xref py py-mod docutils literal notranslate"><span class="pre">QuantizedConv2d</span></code>, and so on.
The quantized module definitions can be found under <code class="xref py py-mod docutils literal notranslate"><span class="pre">aimet_torch.nn</span></code>.</p>
<p>These quantized modules can be accessed as follows:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span> <span class="k">as</span> <span class="n">QuantizationSimModelV2</span>
<span class="n">sim2</span> <span class="o">=</span> <span class="n">QuantizationSimModelV2</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">all_q_modules</span> <span class="o">=</span> <span class="n">sim2</span><span class="o">.</span><span class="n">qmodules</span><span class="p">()</span>
<span class="k">for</span> <span class="n">q_module</span> <span class="ow">in</span> <span class="n">sim2</span><span class="o">.</span><span class="n">qmodules</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">q_module</span><span class="p">)</span>
</pre></div>
</div>
<div class="script-output highlight-none notranslate"><div class="highlight"><pre><span></span>QuantizedLinear(
        in_features=100, out_features=200, bias=True
        (param_quantizers): ModuleDict(
            (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)
            (bias): None
        )
        (input_quantizers): ModuleList(
            (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
        )
        (output_quantizers): ModuleList(
            (0): None
        )
)
QuantizedReLU(
    (param_quantizers): ModuleDict()
    (input_quantizers): ModuleList(
        (0): None
    )
    (output_quantizers): ModuleList(
        (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)
    )
)
</pre></div>
</div>
<p>For more information on Quantized modules, please refer to the API reference guide <span class="xref std std-ref">here</span>.</p>
</section>
<section id="moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer">
<h3>Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer<a class="headerlink" href="#moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer" title="Link to this heading">¶</a></h3>
<p>In aimet_torch 1.x, we relied on <code class="xref py py-class docutils literal notranslate"><span class="pre">StaticGridQuantizer</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">LearnedGridQuantizer</span></code>. For both,
floating point quantization could be enabled based on <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationDataType</span></code> passed in.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.v1.tensor_quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">StaticGridPerChannelQuantizers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_common.defs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationDataType</span>

<span class="n">fp_quantizer</span> <span class="o">=</span> <span class="n">StaticGridPerChannelQuantizer</span><span class="p">(</span><span class="n">data_type</span> <span class="o">=</span> <span class="n">QuantizationDataType</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="n">affine_quantizer</span> <span class="o">=</span> <span class="n">StaticGridPerChannelQuantizer</span><span class="p">(</span><span class="n">data_type</span> <span class="o">=</span> <span class="n">QuantizationDataType</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>However, in aimet_torch 2, this functionality is separated into an <code class="xref py py-class docutils literal notranslate"><span class="pre">AffineQuantizer</span></code> and a
<code class="xref py py-class docutils literal notranslate"><span class="pre">FloatQuantizer</span></code>. Users can access these quantizers and related operations under
<code class="xref py py-mod docutils literal notranslate"><span class="pre">aimet_torch.quantization</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>

<span class="n">affine_q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">Quantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">affine_qdq</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">fp_qdq</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">float</span><span class="o">.</span><span class="n">FloatQuantizeDequantize</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<p>From the wrapped module (aimet_torch 1.x) or quantized module (aimet_torch 2), the attributes to access
the quantizers remain consistent: <code class="docutils literal notranslate"><span class="pre">.input_quantizers</span></code> for input quantizers, <code class="docutils literal notranslate"><span class="pre">.output_quantizers</span></code>
for output quantizers, and <code class="docutils literal notranslate"><span class="pre">.param_quantizers</span></code> for parameter quantizers.</p>
<p>For more information on Quantizers, please refer to the API reference guide <span class="xref std std-ref">here</span>.</p>
</section>
<section id="code-examples">
<h3>Code Examples<a class="headerlink" href="#code-examples" title="Link to this heading">¶</a></h3>
<p><strong>Setup</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.v1.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span> <span class="k">as</span> <span class="n">QuantizationSimModelV1</span>

<span class="n">sim1</span> <span class="o">=</span> <span class="n">QuantizationSimModelV1</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">wrap_linear</span> <span class="o">=</span> <span class="n">sim1</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span>

<span class="c1"># aimet_torch 2</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span> <span class="k">as</span> <span class="n">QuantizationSimModelV2</span>

<span class="n">sim2</span> <span class="o">=</span> <span class="n">QuantizationSimModelV2</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">qlinear</span> <span class="o">=</span> <span class="n">sim2</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">linear</span>
</pre></div>
</div>
<p><strong>Case 1: Manually setting common attributes</strong></p>
<p><em>Bitwidth</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># aimet_torch 2</span>
<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
    <span class="n">module</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bitwidth</span> <span class="o">=</span> <span class="mi">4</span>
</pre></div>
</div>
<p><em>Symmetry</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">use_symmetric_encodings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">is_unsigned_symmetric</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">use_strict_symmetric</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">use_symmetric_encodings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_unsigned_symmetric</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">use_strict_symmetric</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">use_symmetric_encodings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_unsigned_symmetric</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">use_strict_symmetric</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># aimet_torch 2</span>
<span class="c1"># Notes: simplified into two flags</span>
<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">symmetric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">signed</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">symmetric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">signed</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">symmetric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">signed</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p><em>Encoding Data</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">libpymo</span>

<span class="k">if</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span><span class="p">:</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">libpymo</span><span class="o">.</span><span class="n">TfEncoding</span><span class="p">()</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>

<span class="k">if</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span><span class="p">:</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">libpymo</span><span class="o">.</span><span class="n">TfEncoding</span><span class="p">()</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">wrap_linear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>

<span class="k">if</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span><span class="p">:</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">libpymo</span><span class="o">.</span><span class="n">TfEncoding</span><span class="p">()</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">encoding</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">wrap_linear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>

<span class="c1"># aimet_torch 2</span>
<span class="c1"># Notes: TfEncoding() is no longer used, encoding min/max are of type torch.nn.Parameter</span>
<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Case 2: Enabling and Disabling Quantization</strong></p>
<p><em>Is quantization enabled?</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="k">if</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="c1"># aimet_torch 2</span>
<span class="c1"># Notes: Quantizers no longer have an &#39;enabled&#39; attribute. If a quantizer is present, it is enabled</span>
<span class="k">if</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p><em>Disabling Quantization</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># aimet_torch 2</span>
<span class="c1"># Notes: Quantizers can be disabled by setting them to None OR using the utility API (_remove_input_quantizers, _remove_output_quantizers, _remove_param_quantizers)</span>
<span class="n">qlinear</span><span class="o">.</span><span class="n">param_encodings</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">qlinear</span><span class="o">.</span><span class="n">_remove_param_quantizers</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Enabling Quantization</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># aimet_torch 2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">aimet_torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">Q</span>
<span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Temporarily disabling Quantization</em></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="k">assert</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># Run other code here</span>
<span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># aimet_torch 2</span>
<span class="k">assert</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span>
<span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">_remove_param_quantizers</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="c1"># Run other code here</span>

<span class="k">assert</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>Case 3: Freezing encodings</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># aimet_torch 1.x</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">_is_encoding_frozen</span><span class="p">:</span>
    <span class="n">wrap_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">freeze_encodings</span><span class="p">()</span>

<span class="c1"># aimet_torch 2</span>
<span class="c1"># Notes: There is no longer a concept of &quot;freezing&quot;. Mimicking v1 freezing behavior involves the allow_overwrite and requires_grad_ flag</span>
<span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">allow_overwrite</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Prevents encodings from being overwritten by AIMET APIs</span>
<span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>   <span class="c1"># Prevents encodings from being learned</span>
</pre></div>
</div>
</section>
</section>
<section id="how-to-use-aimet-torch-1-x">
<h2>How to use aimet_torch 1.x<a class="headerlink" href="#how-to-use-aimet-torch-1-x" title="Link to this heading">¶</a></h2>
<p>If you still prefer to use aimet_torch 1.x, your imports should originate from the <code class="xref py py-mod docutils literal notranslate"><span class="pre">aimet_torch.v1</span></code>
namespace and be replaced as shown below.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>AIMET Classes</p></th>
<th class="head"><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">aimet_torch</span></code></p></th>
<th class="head"><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">aimet_torch.v1</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>QuantSim</p></td>
<td><p><a class="reference internal" href="quantsim.html#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.quantsim.QuantizationSimModel</span></code></a></p></td>
<td><p><a class="reference internal" href="v1/quantsim.html#aimet_torch.v1.quantsim.QuantizationSimModel" title="aimet_torch.v1.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.v1.quantsim.QuantizationSimModel</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>AdaRound</p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.adaround.adaround_weight.AdaRound</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.v1.adaround.AdaRound</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sequential MSE</p></td>
<td><p><a class="reference internal" href="../../featureguide/seq_mse.html#aimet_torch.seq_mse.apply_seq_mse" title="aimet_torch.seq_mse.apply_seq_mse"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.seq_mse.apply_seq_mse</span></code></a></p></td>
<td><p><a class="reference internal" href="v1/seq_mse.html#aimet_torch.v1.seq_mse.apply_seq_mse" title="aimet_torch.v1.seq_mse.apply_seq_mse"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.v1.seq_mse.apply_seq_mse</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>QuantAnalyzer</p></td>
<td><p><a class="reference internal" href="../../featureguide/analysis%20tools/quant_analyzer.html#aimet_torch.quant_analyzer.QuantAnalyzer" title="aimet_torch.quant_analyzer.QuantAnalyzer"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.quant_analyzer.QuantAnalyzer</span></code></a></p></td>
<td><p><a class="reference internal" href="v1/quant_analyzer.html#aimet_torch.v1.quant_analyzer.QuantAnalyzer" title="aimet_torch.v1.quant_analyzer.QuantAnalyzer"><code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.v1.quant_analyzer.QuantAnalyzer</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="quantsim.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">aimet_torch.quantsim</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="index.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">aimet_torch API</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Migration guide</a><ul>
<li><a class="reference internal" href="#aimet-torch-1-x-vs-aimet-torch-2">aimet_torch 1.x vs aimet_torch 2</a></li>
<li><a class="reference internal" href="#migration-process">Migration Process</a><ul>
<li><a class="reference internal" href="#moving-from-quantwrapper-to-quantized-modules">Moving from QuantWrapper to Quantized Modules</a></li>
<li><a class="reference internal" href="#moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer">Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer</a></li>
<li><a class="reference internal" href="#code-examples">Code Examples</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-use-aimet-torch-1-x">How to use aimet_torch 1.x</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>