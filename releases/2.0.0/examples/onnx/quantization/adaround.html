<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Adaptive Rounding (AdaRound) - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/aimet-furo.css?v=6d7e6c94" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.0.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install/quick-start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../userguide/index.html">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../userguide/quantization_workflow.html">Quantization workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../userguide/debugging_guidelines.html">Debugging guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../userguide/on_target_inference.html">On-target inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantsim/index.html">Quantization Simulation Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../quantsim/calibration.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../quantsim/qat.html">QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../quantsim/advanced.html">Advanced</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../featureguide/index.html">Feature Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/mixed%20precision/index.html">Mixed precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/autoquant.html">Automatic quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/analysis%20tools/index.html">Analysis tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../featureguide/compression/index.html">Compression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../apiref/index.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../apiref/torch/index.html">aimet_torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../apiref/tensorflow/index.html">aimet_tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../apiref/onnx/index.html">aimet_onnx</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../release_notes.html">Release Notes</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="Adaptive-Rounding-(AdaRound)">
<h1>Adaptive Rounding (AdaRound)<a class="headerlink" href="#Adaptive-Rounding-(AdaRound)" title="Link to this heading">¶</a></h1>
<p>This notebook contains a working example of AIMET adaptive rounding (AdaRound).</p>
<p>AIMET quantization features typically use the “nearest rounding” technique for achieving quantization. When using the nearest rounding technique, the weight value is quantized to the nearest integer value.</p>
<p>AdaRound optimizes a loss function using unlabeled training data to decide whether to quantize a specific weight to the closer integer value or the farther one. Using AdaRound, quantized accuracy is closer to the FP32 model than with nearest rounding.</p>
<section id="Overall-flow">
<h2>Overall flow<a class="headerlink" href="#Overall-flow" title="Link to this heading">¶</a></h2>
<p>The example follows these high-level steps:</p>
<ol class="arabic simple">
<li><p>Instantiate the example evaluation and training pipeline</p></li>
<li><p>Load the FP32 model and evaluate the model to find the baseline FP32 accuracy</p></li>
<li><p>Create a quantization simulation model (with fake quantization ops) and evaluate the quantized simuation model</p></li>
<li><p>Apply AdaRound and evaluate the simulation model to get a post-finetuned quantized accuracy score</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This notebook does not show state-of-the-art results. For example, it uses a relatively quantization-friendly model (Resnet18). Also, some optimization parameters like number of fine-tuning epochs are chosen to improve execution speed in the notebook.</p>
</div>
</section>
<hr class="docutils" />
<section id="Dataset">
<h2>Dataset<a class="headerlink" href="#Dataset" title="Link to this heading">¶</a></h2>
<p>This example does image classification on the ImageNet dataset. If you already have a version of the data set, use that. Otherwise download the data set, for example from <a class="reference external" href="https://image-net.org/challenges/LSVRC/2012/index">https://image-net.org/challenges/LSVRC/2012/index</a> .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dataloader provided in this example relies on these features of the ImageNet data set:</p>
<ul class="simple">
<li><p>Subfolders <code class="docutils literal notranslate"><span class="pre">train</span></code> for the training samples and <code class="docutils literal notranslate"><span class="pre">val</span></code> for the validation samples. See the <a class="reference external" href="https://pytorch.org/vision/0.8/_modules/torchvision/datasets/imagenet.html">pytorch dataset description</a> for more details.</p></li>
<li><p>One subdirectory per class, and one file per image sample.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To speed up the execution of this notebook, you can use a reduced subset of the ImageNet dataset. For example: The entire ILSVRC2012 dataset has 1000 classes, 1000 training samples per class and 50 validation samples per class. However, for the purpose of running this notebook, you can reduce the dataset to, say, two samples per class.</p>
</div>
<p>Edit the cell below to specify the directory where the downloaded ImageNet dataset is saved.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DATASET_DIR</span> <span class="o">=</span> <span class="s1">&#39;/path/to/dataset/&#39;</span>         <span class="c1"># Replace this path with a real directory</span>
</pre></div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="1.-Instantiate-the-example-training-and-validation-pipeline">
<h2>1. Instantiate the example training and validation pipeline<a class="headerlink" href="#1.-Instantiate-the-example-training-and-validation-pipeline" title="Link to this heading">¶</a></h2>
<p><strong>Use the following training and validation loop for the image classification task.</strong></p>
<p>Things to note:</p>
<ul class="simple">
<li><p>AIMET does not put limitations on how the training and validation pipeline is written. AIMET modifies the user’s model to create a QuantizationSim model, which is still a PyTorch model. The QuantizationSim model can be used in place of the original model when doing inference or training.</p></li>
<li><p>AIMET doesn not put limitations on the interface of the <code class="docutils literal notranslate"><span class="pre">evaluate()</span></code> or <code class="docutils literal notranslate"><span class="pre">train()</span></code> methods. You should be able to use your existing evaluate and train routines as-is.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ort</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">Examples.common</span><span class="w"> </span><span class="kn">import</span> <span class="n">image_net_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">Examples.onnx.utils.image_net_evaluator</span><span class="w"> </span><span class="kn">import</span> <span class="n">ImageNetEvaluator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">Examples.torch.utils.image_net_data_loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">ImageNetDataLoader</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ImageNetDataPipeline</span><span class="p">:</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_val_dataloader</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiates a validation dataloader for ImageNet dataset and returns it</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_loader</span> <span class="o">=</span> <span class="n">ImageNetDataLoader</span><span class="p">(</span><span class="n">DATASET_DIR</span><span class="p">,</span>
                                         <span class="n">image_size</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;image_size&#39;</span><span class="p">],</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">],</span>
                                         <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                         <span class="n">num_workers</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">[</span><span class="s1">&#39;num_workers&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">data_loader</span>
        <span class="k">return</span> <span class="n">data_loader</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">sess</span><span class="p">:</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a torch model, evaluates its Top-1 accuracy on the dataset</span>
<span class="sd">        :param sess: the model to evaluate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">evaluator</span> <span class="o">=</span> <span class="n">ImageNetEvaluator</span><span class="p">(</span><span class="n">DATASET_DIR</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;image_size&#39;</span><span class="p">],</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">],</span>
                                      <span class="n">num_workers</span><span class="o">=</span><span class="n">image_net_config</span><span class="o">.</span><span class="n">evaluation</span><span class="p">[</span><span class="s1">&#39;num_workers&#39;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&amp;-then-evaluate-baseline-FP32-accuracy">
<h2>2. Convert an FP32 PyTorch model to ONNX, simplify &amp; then evaluate baseline FP32 accuracy<a class="headerlink" href="#2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy" title="Link to this heading">¶</a></h2>
<p><strong>2.1 Load a pretrained resnet18 model from torchvision.</strong></p>
<p>You can load any pretrained PyTorch model instead.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">resnet18</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">onnx</span>

<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>    <span class="c1"># Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;./resnet18.onnx&quot;</span>

<span class="c1"># Load a pretrained ResNet-18 model in torch</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Export the torch model to onnx</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">pt_model</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span>
                  <span class="n">dummy_input</span><span class="p">,</span>
                  <span class="n">filename</span><span class="p">,</span>
                  <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span>
                  <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">],</span>
                  <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span>
                      <span class="s1">&#39;input&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">},</span>
                      <span class="s1">&#39;output&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;batch_size&#39;</span><span class="p">},</span>
                  <span class="p">}</span>
                  <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p><strong>2.2 It is recommended to simplify the model before using AIMET</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">onnxsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">simplify</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">simplify</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ONNX Simplifier failed. Proceeding with unsimplified model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p><strong>2.3 Decide whether to place the model on a CPU or CUDA device.</strong></p>
<p>This example uses CUDA if it is available. You can change this logic and force a device placement if needed.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># cudnn_conv_algo_search is fixing it to default to avoid changing in accuracies/outputs at every inference</span>
<span class="k">if</span> <span class="s1">&#39;CUDAExecutionProvider&#39;</span> <span class="ow">in</span> <span class="n">ort</span><span class="o">.</span><span class="n">get_available_providers</span><span class="p">():</span>
    <span class="n">providers</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;cudnn_conv_algo_search&#39;</span><span class="p">:</span> <span class="s1">&#39;DEFAULT&#39;</span><span class="p">}),</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">]</span>
    <span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">providers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">]</span>
    <span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p><strong>2.4 Create an onnxruntime session and determine the FP32 accuracy of this model using the evaluate() routine.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">(),</span> <span class="n">providers</span><span class="o">=</span><span class="n">providers</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy">
<h2>3. Create a quantization simulation model and determine quantized accuracy<a class="headerlink" href="#3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy" title="Link to this heading">¶</a></h2>
<section id="Fold-Batch-Normalization-layers">
<h3>Fold Batch Normalization layers<a class="headerlink" href="#Fold-Batch-Normalization-layers" title="Link to this heading">¶</a></h3>
<p>Before calculating the simulated quantized accuracy using QuantizationSimModel, fold the BatchNormalization (BN) layers into adjacent Convolutional layers. The BN layers that cannot be folded are left as they are.</p>
<p>BN folding improves inference performance on quantized runtimes but can degrade accuracy on these platforms. This step simulates this on-target drop in accuracy.</p>
<p><strong>3.1 Use the following code to call AIMET to fold the BN layers in-place on the given model.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_onnx.batch_norm_fold</span><span class="w"> </span><span class="kn">import</span> <span class="n">fold_all_batch_norms_to_weight</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">fold_all_batch_norms_to_weight</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Create-the-Quantization-Sim-Model">
<h3>Create the Quantization Sim Model<a class="headerlink" href="#Create-the-Quantization-Sim-Model" title="Link to this heading">¶</a></h3>
<p><strong>3.2 Use AIMET to create a QuantizationSimModel.</strong></p>
<p>In this step, AIMET inserts fake quantization ops in the model graph and configures them.</p>
<p>Key parameters:</p>
<ul class="simple">
<li><p>Setting <strong>default_output_bw</strong> to 8 performs all activation quantizations in the model using integer 8-bit precision</p></li>
<li><p>Setting <strong>default_param_bw</strong> to 8 performs all parameter quantizations in the model using integer 8-bit precision</p></li>
</ul>
<p>See <a class="reference external" href="https://quic.github.io/aimet-pages/AimetDocs/api_docs/torch_quantsim.html#aimet_torch.quantsim.QuantizationSimModel.compute_encodings">QuantizationSimModel in the AIMET API documentation</a> for a full explanation of the parameters.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_common.defs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantScheme</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_onnx.quantsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizationSimModel</span>

<span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">),</span>
                           <span class="n">quant_scheme</span><span class="o">=</span><span class="n">QuantScheme</span><span class="o">.</span><span class="n">post_training_tf_enhanced</span><span class="p">,</span>
                           <span class="n">default_activation_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">default_param_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">use_cuda</span><span class="o">=</span><span class="n">use_cuda</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p>AIMET has added quantizer nodes to the model graph, but before the sim model can be used for inference or training, scale and offset quantization parameters must be calculated for each quantizer node by passing unlabeled data samples through the model to collect range statistics. This process is sometimes referred to as calibration. AIMET refers to it as “computing encodings”.</p>
<p><strong>3.3 Create a routine to pass unlabeled data samples through the model.</strong></p>
<p>The following code is one way to write a routine that passes unlabeled samples through the model to compute encodings. It uses the existing train or validation data loader to extract samples and pass them to the model. Since there is no need to compute loss metrics, it ignores the model output.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pass_calibration_data</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">get_val_dataloader</span><span class="p">()</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">data_loader</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="n">input_name</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>

    <span class="n">batch_cntr</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>

        <span class="n">inputs_batch</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="n">input_name</span> <span class="p">:</span> <span class="n">inputs_batch</span><span class="p">})</span>

        <span class="n">batch_cntr</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_cntr</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">samples</span><span class="p">:</span>
            <span class="k">break</span>
</pre></div>
</div>
</div>
<p>A few notes regarding the data samples:</p>
<ul class="simple">
<li><p>A very small percentage of the data samples are needed. For example, the training dataset for ImageNet has 1M samples; 500 or 1000 suffice to compute encodings.</p></li>
<li><p>The samples should be reasonably well distributed. While it’s not necessary to cover all classes, avoid extreme scenarios like using only dark or only light samples. That is, using only pictures captured at night, say, could skew the results.</p></li>
</ul>
<hr class="docutils" />
<p><strong>3.4 Call AIMET to use the routine to pass data through the model and compute the quantization encodings.</strong></p>
<p>Encodings here refer to scale and offset quantization parameters.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">forward_pass_callback</span><span class="o">=</span><span class="n">pass_calibration_data</span><span class="p">,</span>
                      <span class="n">forward_pass_callback_args</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p>The QuantizationSim model is now ready to be used for inference or training.</p>
<p><strong>3.5 Pass the model to the same evaluation routine as before to calculate a simulated quantized accuracy score for INT8 quantization for comparison with the FP32 score.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="4.-Apply-Adaround">
<h2>4. Apply Adaround<a class="headerlink" href="#4.-Apply-Adaround" title="Link to this heading">¶</a></h2>
<p><strong>4.1 Use the code below to apply Adaround to the original model.</strong></p>
<p>Some key parameters:</p>
<ul class="simple">
<li><p><strong>dataloader:</strong> is a training or validation dataloader. Adaround needs a dataloader in order to use data samples to learn the rounding vectors.</p></li>
<li><p><strong>num_batches:</strong> is the number of batches used while calculating the quantization encodings. A typical value for Adaround is 2000 samples. To speed up the execution this example uses a batch size of one.</p></li>
<li><p><strong>default_num_iterations:</strong> is the number of iterations to apply to each layer. Default value is 10000, and we strongly recommend using at least this number. This example uses 32 to speed up execution.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aimet_onnx.adaround.adaround_weight</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adaround</span><span class="p">,</span> <span class="n">AdaroundParameters</span>

<span class="c1"># Dataloader satisfying the class signature required by AdaRound</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DataLoader</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This dataloader derives unlabeled samples in the form of numpy arrays from a torch dataloader</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_torch_data_loader</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">get_val_dataloader</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_torch_data_loader</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_torch_data_loader</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">input_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_torch_data_loader</span><span class="p">)</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">()</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">AdaroundParameters</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default_num_iterations</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                            <span class="n">forward_fn</span><span class="o">=</span><span class="n">pass_calibration_data</span><span class="p">,</span> <span class="n">forward_pass_callback_args</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;./output/&#39;</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ada_model</span> <span class="o">=</span> <span class="n">Adaround</span><span class="o">.</span><span class="n">apply_adaround</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span>
                                    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
                                    <span class="n">filename_prefix</span><span class="o">=</span><span class="s1">&#39;adaround&#39;</span><span class="p">,</span>
                                    <span class="n">default_param_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                    <span class="n">default_quant_scheme</span><span class="o">=</span><span class="n">QuantScheme</span><span class="o">.</span><span class="n">post_training_tf_enhanced</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p><strong>4.2 Quantize the Adarounded model.</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Two important points about the following code:</p>
</div>
<ul class="simple">
<li><p><strong>Parameter Biwidth Precision</strong>: The QuantizationSimModel must be created with the same parameter bitwidth precision that was used in <code class="docutils literal notranslate"><span class="pre">apply_adaround()</span></code>.</p></li>
<li><p><strong>Freezing the parameter encodings</strong>: After creating the QuantizationSimModel, you must call <code class="docutils literal notranslate"><span class="pre">set_and_freeze_param_encodings()</span></code> before calling <code class="docutils literal notranslate"><span class="pre">compute_encodings()</span></code>. During AdaRound, the parameters are rounded based on these initial internally created encodings. To maintain accuracy, it is important to freeze these encodings so that the call to <code class="docutils literal notranslate"><span class="pre">compute_encodings()</span></code> does not alter the parameter encodings negate the AdaRounded accuracy.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">ada_model</span><span class="p">,</span>
                           <span class="n">quant_scheme</span><span class="o">=</span><span class="n">QuantScheme</span><span class="o">.</span><span class="n">post_training_tf_enhanced</span><span class="p">,</span>
                           <span class="n">default_activation_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">default_param_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">use_cuda</span><span class="o">=</span><span class="n">use_cuda</span><span class="p">)</span>

<span class="n">sim</span><span class="o">.</span><span class="n">set_and_freeze_param_encodings</span><span class="p">(</span><span class="n">encoding_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s1">&#39;adaround.encodings&#39;</span><span class="p">))</span>

<span class="n">sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">forward_pass_callback</span><span class="o">=</span><span class="n">pass_calibration_data</span><span class="p">,</span>
                      <span class="n">forward_pass_callback_args</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p><strong>4.3 Compute the accuracy of the Adarounded model.</strong></p>
<p>Evaluate the simulation model as before to determine simulated quantized accuracy.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<p>There might be little gain in accuracy after this limited application of Adaround. Experiment with the hyper-parameters to get better results.</p>
</section>
<section id="Next-steps">
<h2>Next steps<a class="headerlink" href="#Next-steps" title="Link to this heading">¶</a></h2>
<p><strong>Export the model and encodings.</strong></p>
<ul class="simple">
<li><p>Export the model with the updated weights but without the fake quant ops.</p></li>
<li><p>Export the encodings (scale and offset quantization parameters). AIMET QuantizationSimModel provides an export API for this purpose.</p></li>
</ul>
<p>The following code performs these exports.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;./output/&#39;</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="s1">&#39;resnet18_after_adaround&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="For-more-information">
<h2>For more information<a class="headerlink" href="#For-more-information" title="Link to this heading">¶</a></h2>
<p>See the <a class="reference external" href="https://quic.github.io/aimet-pages/AimetDocs/api_docs/index.html">AIMET API docs</a> for details about the AIMET APIs and optional parameters.</p>
<p>See the <a class="reference external" href="https://github.com/quic/aimet/tree/develop/Examples/torch/quantization">other example notebooks</a> to learn how to use other AIMET post-training quantization techniques.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Adaptive Rounding (AdaRound)</a><ul>
<li><a class="reference internal" href="#Overall-flow">Overall flow</a></li>
<li><a class="reference internal" href="#Dataset">Dataset</a></li>
<li><a class="reference internal" href="#1.-Instantiate-the-example-training-and-validation-pipeline">1. Instantiate the example training and validation pipeline</a></li>
<li><a class="reference internal" href="#2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&amp;-then-evaluate-baseline-FP32-accuracy">2. Convert an FP32 PyTorch model to ONNX, simplify &amp; then evaluate baseline FP32 accuracy</a></li>
<li><a class="reference internal" href="#3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy">3. Create a quantization simulation model and determine quantized accuracy</a><ul>
<li><a class="reference internal" href="#Fold-Batch-Normalization-layers">Fold Batch Normalization layers</a></li>
<li><a class="reference internal" href="#Create-the-Quantization-Sim-Model">Create the Quantization Sim Model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#4.-Apply-Adaround">4. Apply Adaround</a></li>
<li><a class="reference internal" href="#Next-steps">Next steps</a></li>
<li><a class="reference internal" href="#For-more-information">For more information</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>