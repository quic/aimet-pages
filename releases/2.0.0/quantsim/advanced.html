<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Optimization techniques" href="../featureguide/index.html" /><link rel="prev" title="Quantization-aware training" href="qat.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>Advanced - AIMET</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/aimet-furo.css?v=6d7e6c94" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">AIMET</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">AIMET</span>
  
</a><div class="doc-versions" data-toggle="doc-versions" role="note" aria-label="versions">

  <span class="doc-current-version" data-toggle="doc-current-version">
    Version: 2.0.0
  </span>
  <br>
  <span class="doc-other-versions" data-toggle="doc-other-versions">
        <a href="https://quic.github.io/aimet-pages/releases/latest/versions.html">Other versions</a>
  </span>

</div><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install/quick-start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../userguide/index.html">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../userguide/quantization_workflow.html">Quantization workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../userguide/debugging_guidelines.html">Debugging guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../userguide/on_target_inference.html">On-target inference</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Quantization Simulation Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="calibration.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="qat.html">QAT</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Advanced</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../featureguide/index.html">Feature Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/adaround.html">Adaptive rounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/seq_mse.html">Sequential MSE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/bnf.html">Batch norm folding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/cle.html">Cross-layer equalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/mixed%20precision/index.html">Mixed precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/autoquant.html">Automatic quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/bn.html">Batch norm re-estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/analysis%20tools/index.html">Analysis tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../featureguide/compression/index.html">Compression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apiref/index.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../apiref/torch/index.html">aimet_torch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../apiref/tensorflow/index.html">aimet_tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../apiref/onnx/index.html">aimet_onnx</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
</ul>

</div></div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="advanced">
<span id="quantsim-advanced"></span><h1>Advanced<a class="headerlink" href="#advanced" title="Link to this heading">¶</a></h1>
<section id="per-block-quantization">
<h2>Per-block quantization<a class="headerlink" href="#per-block-quantization" title="Link to this heading">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Per-block quantization and blockwise quantization are used interchangeably.</p>
</div>
<p>When performing integer quantization, it is necessary to determine quantization parameters (also known as encodings)
like scale and offset in order to define a quantization grid for mapping floating point values to their quantized integer
counterparts. This process of determining appropriate quantization parameters is known as calibration or computing encodings.</p>
<p>When performing calibration for a particular tensor, one can choose to come up with encodings to cover the whole tensor,
or to split the tensor into sections and compute encodings for each section.</p>
<p>In general, it is recommended to use Blockwise quantization in favor of Per-Channel quantization when possible, and similarly
Per-Channel quantization in favor of Per-Tensor quantization. The finer granularity provided by Blockwise quantization typically
leads to better quantized accuracy as a result. Note that Blockwise and Per-Channel quantization are supported only for
weights and not activations on Qualcomm runtime.</p>
<p>Blockwise quantization can be enabled on an individual quantizer basis by instantiating a new QuantizeDequantize object with
the desired settings and replacing an existing quantizer with the new quantizer.
The block_size argument can be used to specify particular block sizes for each dimension of the tensor.
Note that there exists a relationship between the QuantizeDequantize’s shape and block_size arguments, along with the shape
of the actual tensor being quantized.</p>
<p>The following rules must apply:</p>
<ul class="simple">
<li><p>If block_size is provided, the length of block_size must match the length of the QuantizeDequantize’s shape.</p></li>
<li><p>If block_size is provided, the length of block_size must be at most as long as the tensor to quantize’s number of
dimensions.</p></li>
<li><p>For block_size [b_1, b_2, …, b_n] and QuantizeDequantize shape [s_1, s_2, …, s_n], the tensor to quantize’s shape
must satisfy tensor.shape[:-n] == [b_1 * s_1, b_2 * s_2, …, b_n * s_n]. In other words, block sizes for each
dimension must evenly divide the size of the tensor in the corresponding dimension. For example, if a tensor’s
shape is (2, 2, 6, 10), a valid block_size would be (2, 1, 3, 5), since each block size is divisible by the tensor’s
corresponding dimension size.</p></li>
<li><p>For each dimension, a block size value of ‘-1’ is permitted. In such cases, the block size is automatically determined
based on the tensor’s shape in that dimension and the QuantizeDequantize object’s shape. This is essentially determining
the block size for a dimension given the tensor’s size along with the number of blocks for that dimension.</p></li>
</ul>
<p>Below are examples of valid and invalid combinations of tensor shape, QuantizeDequantize shape, and block_size:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-0">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Invalid combination: block_size is not the same length as QuantizeDequantize shape</span>
<span class="n">tensor</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">QuantizeDequantize</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
<span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Invalid combination: block_size * QuantizeDequantize shape != tensor shape:</span>
<span class="n">tensor</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">QuantizeDequantize</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Valid combination:</span>
<span class="n">tensor</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">QuantizeDequantize</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Valid combination (note that though tensor shape is 3d, only the final 2 dimensions correspond to block_size</span>
<span class="c1"># and QuantizeDequantize shape):</span>
<span class="n">tensor</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">QuantizeDequantize</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Valid combination:</span>
<span class="n">tensor</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">QuantizeDequantize</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">block_size</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># block_size will be inferred to be (2, 5)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While the QuantizeDequantize object supports arbitrary block sizes for experimental purposes, Qualcomm runtime restricts
Blockwise quantization to take place with the following constraints:</p>
<ul class="simple">
<li><p>Blockwise quantization must run on weight quantizers only.</p></li>
<li><p>Block sizes must be set to 1 for the output channel dimension, may take arbitrary values for the input channel
dimension (it must still be divisible by the input channel tensor shape), and must have block sizes equal to the
tensor sizes for all other dimensions.</p></li>
<li><p>Layers with weights running with Blockwise quantization must themselves be running with floating-point quantized
activations.</p></li>
</ul>
</div>
</div>
</div>
<p>The below code examples show how to configure Convolution and Linear layers to Blockwise quantization:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-1" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-1">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.quantization.affine</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizeDequantize</span>

<span class="c1"># Assume sim.model.conv_1 refers to a QuantizedConv2d layer with weight param shape of (16, 64, 2, 2)</span>
<span class="c1"># Below settings equate to a block size of 16 in the input channels dimension.</span>
<span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                                 <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                                                 <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                                 <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># (-1, -1, -1, -1) works too</span>

<span class="c1"># Assume sim.model.linear_1 refers to a QuantizedLinear layer with weight param shape of (12, 16)</span>
<span class="c1"># Below settings equate to a block size of 4 in the input channels dimension.</span>
<span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                                                                 <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                                                 <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                                 <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># (-1, -1) works too</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="low-power-blockwise-quantization">
<h2>Low power blockwise quantization<a class="headerlink" href="#low-power-blockwise-quantization" title="Link to this heading">¶</a></h2>
<p>Qualcomm® AI Engine Direct supports an alternative to Blockwise Quantization referred to as Low Power Blockwise Quantization (LPBQ).</p>
<p>In this scheme, blockwise encodings at a lower bitwidth are determined and then adjusted such that they lie on a common
higher bitwidth per channel grid. This allows models to achieve benefits of Blockwise quantization while allowing runtimes
to leverage existing per channel kernels in order to run the model. An additional benefit is that LPBQ encodings take less
storage space than Blockwise quantization encodings, due to the fact that floating point encoding scales are stored per
channel, and only low bitwidth integer scale expansion factors need to be stored in a per block fashion.</p>
<p>LPBQ quantization is supported as part of the <code class="xref py py-class docutils literal notranslate"><span class="pre">aimet_torch.quantization.affine.GroupedBlockQuantizeDequantize</span></code> class.</p>
<p>In addition to the block_size argument described in the Blockwise Quantization section, LPBQ introduces two new arguments:</p>
<ul class="simple">
<li><p><strong>decompressed_bw</strong>: The higher bitwidth value for the per channel grid which the lower bitwidth blockwise encodings will
expand to. Decompressed_bw must be greater than or equal to the bitwidth of the quantizer.</p></li>
<li><p><strong>block_grouping</strong>: The block_grouping argument defines the number of blocks for each dimension which will be grouped
together when expanding the lower bitwidth blockwise encodings to the higher bitwidth per channel encodings.
The block grouping for a particular dimension must be divisible by the number of blocks for that dimension.</p></li>
</ul>
<p>As with block size, a block grouping value of ‘-1’ is valid, and will correspond automatically to the number of blocks for
that dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While the GroupedBlockQuantizeDequantize quantizer supports arbitrary block groupings for experimental purposes,
Qualcomm runtime restricts LPBQ to take place with the following constraints:</p>
<ul class="simple">
<li><p>Blockwise quantization must run on weight quantizers only.</p></li>
<li><p>Block sizes must be set to 1 for the output channel dimension, may take arbitrary values for the input channel
dimension (it must still be divisible by the input channel tensor shape), and must have block sizes equal to the
tensor sizes for all other dimensions.</p></li>
<li><p>Block groupings must be set to ‘1’ for all dimensions, except for the input channels dimension, which should be
set to the number of blocks for that dimension.</p></li>
</ul>
</div>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-2">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_torch.quantization.affine</span><span class="w"> </span><span class="kn">import</span> <span class="n">GroupedBlockQuantizeDequantize</span>

<span class="c1"># Assume sim.model.conv_1 refers to a QuantizedConv2d layer with weight param shape of (16, 64, 2, 2)</span>
<span class="c1"># Below settings equate to a block size of 16 in the input channels dimension.</span>
<span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv_1</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">GroupedBlockQuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                                                             <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                                                             <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                                             <span class="n">block_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                                                             <span class="n">decompressed_bw</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                                                                             <span class="n">block_grouping</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>   <span class="c1"># (1, -1, 1, 1) works too</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="export">
<h2>Export<a class="headerlink" href="#export" title="Link to this heading">¶</a></h2>
<p>Using Blockwise quantization results in a larger number of encodings produced as compared to Per-Tensor or Per-Channel
quantization. As a result, a new method of exporting encodings to json has been developed to both reduce the exported
encodings file size as well as reduce the time needed to write exported encodings to the json file.</p>
<p>The following code snippet shows how to export encodings in the new 1.0.0 format:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="platform" data-sync-id="torch" for="sd-tab-item-3">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">aimet_common</span><span class="w"> </span><span class="kn">import</span> <span class="n">quantsim</span>

<span class="c1"># Assume &#39;sim&#39; is a QuantizationSimModel object imported from aimet_torch.quantsim</span>

<span class="c1"># Set encoding_version to 1.0.0</span>
<span class="n">quantsim</span><span class="o">.</span><span class="n">encoding_version</span> <span class="o">=</span> <span class="s1">&#39;1.0.0&#39;</span>
<span class="n">sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="s1">&#39;exported_model&#39;</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The 1.0.0 encodings format is supported by Qualcomm runtime and can be used to export Per-Tensor, Per-Channel, Blockwise,
and LPBQ quantizer encodings.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If Blockwise and/or LPBQ quantizers are present in the model, the 1.0.0 format must be
used when exporting encodings for Qualcomm runtime.</p>
</div>
<p>See the <a class="reference internal" href="encoding_spec.html#quantsim-encoding-spec"><span class="std std-ref">Encoding specifications</span></a> page, which describes various encodings
specifications  in detail.</p>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="Link to this heading">¶</a></h2>
<p><strong>Top-level API to configure BQ quantization</strong></p>
<p>As mentioned above, Qualcomm runtime is constrained to running floating point activations for layers which use Blockwise
quantization. As a result, the following utility function is provided to assist in transforming multiple layers’ quantizers
to float quantization:</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">aimet_torch.v2.quantsim.config_utils.</span></span><span class="sig-name descname"><span class="pre">set_activation_quantizers_to_float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exponent_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mantissa_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/config_utils.html#set_activation_quantizers_to_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Set activation quantizers of modules to float.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sim</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="../apiref/torch/quantsim.html#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.v2.quantsim.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code></a></span>) – Quantsim to set activation quantizers for</p></li>
<li><p><strong>arg</strong> – <p>Argument determining which modules to set. This can consist of either:</p>
<ol class="arabic simple">
<li><p>A list of torch.nn.Module types, in which case all modules whose type is in the list will be set</p></li>
<li><p>A list of torch.nn.Modules, in which case all modules in the list will be set</p></li>
<li><p>A callable function which takes a torch.nn.Module as input and returns True if the module is to be set, False
otherwise</p></li>
</ol>
</p></li>
<li><p><strong>exponent_bits</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Number of exponent bits to simulate</p></li>
<li><p><strong>mantissa_bits</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Number of mantissa bits to simulate</p></li>
<li><p><strong>dtype</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</span>) – torch.dtype to simulate. This argument is mutually exclusive with exponent_bits and mantissa_bits.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assume &#39;sim&#39; is a QuantizationSimModel object imported from aimet_torch.v2.quantsim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of all Linear and Conv output quantizers to floating point activation quantization:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_activation_quantizers_to_float</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                   <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">],</span>
<span class="gp">... </span>                                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of specific model layers&#39; output quantizers to floating point activation quantization:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_activation_quantizers_to_float</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                   <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="p">,</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">linear1</span><span class="p">],</span>
<span class="gp">... </span>                                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of only Convolution layers with input channels dim == 128 to floating point activation quantization:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_activation_quantizers_to_float</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                   <span class="n">arg</span><span class="o">=</span><span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">128</span><span class="p">,</span>
<span class="gp">... </span>                                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">aimet_torch.v2.quantsim.config_utils.</span></span><span class="sig-name descname"><span class="pre">set_blockwise_quantization_for_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bitwidth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/config_utils.html#set_blockwise_quantization_for_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Set weight parameter quantizers of modules to blockwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sim</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="../apiref/torch/quantsim.html#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.v2.quantsim.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code></a></span>) – Quantsim to set weight quantizers for</p></li>
<li><p><strong>arg</strong> – <p>Argument determining which modules to set. This can consist of either:</p>
<ol class="arabic simple">
<li><p>A list of torch.nn.Module types, in which case all modules whose type is in the list will be set</p></li>
<li><p>A list of torch.nn.Modules, in which case all modules in the list will be set</p></li>
<li><p>A callable function which takes a torch.nn.Module as input and returns True if the module is to be set, False
otherwise</p></li>
</ol>
</p></li>
<li><p><strong>bitwidth</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Bitwidth for affine quantization</p></li>
<li><p><strong>symmetric</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – True if affine quantization is symmetric, False otherwise</p></li>
<li><p><strong>block_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]]</span>) – <p>Block size for affine quantization. This can be an array in which case all layers identified
by arg must have weight shapes compatible with the array length, or can be an integer value, in which case the
block size will be applied to the weight’s in_channels dimension, and per channel will be used for the weight’s
out_channels dimension.</p>
<p>A block size value of -1 for a particular dimension is equivalent to a block size equal
to the size of that particular dimension.</p>
</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assume &#39;sim&#39; is a QuantizationSimModel object imported from aimet_torch.v2.quantsim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of all Linear and Conv weight quantizers to block_size 64 in the input_channels dimension:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">],</span>
<span class="gp">... </span>                                       <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of specific model layers&#39; weight quantizer block_size to 64 in the input_channels dimension:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="p">,</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">linear1</span><span class="p">],</span>
<span class="gp">... </span>                                       <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of only Convolution layers with input channels dim == 128 to block_size 64 in the input_channels dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">arg</span><span class="o">=</span><span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">128</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                       <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<p>This utility allows users to configure certain quantized layers in a model to use blockwise quantization with a specified
block_size.</p>
<p>Of significance is the second argument in the function, which allows users to specify a subset of layers to switch to
Blockwise quantization. Refer to the function docstring for valid types of inputs this argument supports.</p>
<p>For this API, the block_size argument can be a single integer value instead of an array. In this case, all
affected layers would be set to a block size of 1 for the output channels dimension, the specified value for the input
channels dimension, and block size equal to dimension size for all other dimensions.</p>
<p>Note that this allows layers with differing weight shapes (ex. Conv layers with 4d weights vs. Linear layers with 2d weights)
to be handled with a single API call. If an array for block_size is passed in instead, due to the requirement for the
length of the block_size array to match the number of dimensions for a particular layer’s weight, the API would need to be
called multiple times for each set of layers with different weight dimensions.</p>
<p><strong>Top-level API to configure LPBQ quantization</strong></p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">aimet_torch.v2.quantsim.config_utils.</span></span><span class="sig-name descname"><span class="pre">set_grouped_blockwise_quantization_for_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bitwidth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decompressed_bw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_grouping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/quantsim/config_utils.html#set_grouped_blockwise_quantization_for_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Set weight parameter quantizers of modules to grouped blockwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sim</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="../apiref/torch/quantsim.html#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.v2.quantsim.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code></a></span>) – Quantsim to set weight quantizers for</p></li>
<li><p><strong>arg</strong> – <p>Argument determining which modules to set. This can consist of either:</p>
<ol class="arabic simple">
<li><p>A list of torch.nn.Module types, in which case all modules whose type is in the list will be set</p></li>
<li><p>A list of torch.nn.Modules, in which case all modules in the list will be set</p></li>
<li><p>A callable function which takes a torch.nn.Module as input and returns True if the module is to be set, False
otherwise</p></li>
</ol>
</p></li>
<li><p><strong>bitwidth</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Bitwidth for affine quantization</p></li>
<li><p><strong>symmetric</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – True if affine quantization is symmetric, False otherwise</p></li>
<li><p><strong>decompressed_bw</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Decompressed bw for grouped block quantization</p></li>
<li><p><strong>block_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]]</span>) – <p>Block size for affine quantization. This can be an array in which case all layers identified
by arg must have weight shapes compatible with the array length, or can be an integer value, in which case the
block size will be applied to the weight’s in_channels dimension and per channel will be used for the weight’s
out_channels dimension.</p>
<p>A block size value of -1 for a particular dimension is equivalent to a block size equal
to the size of that particular dimension.</p>
</p></li>
<li><p><strong>block_grouping</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]]</span>) – <p>Block grouping for grouped block quantization. This can be an array in which case all layers
identified by arg must have weight shapes compatible with the array length, or can be an integer value, in which
case the block grouping will be applied to the weight’s in_channels dimension, and no other dimensions will
experience block grouping.</p>
<p>A block grouping value of -1 for a particular dimension is equivalent to a block
grouping equal to the number of blocks for that particular dimension.</p>
</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assume &#39;sim&#39; is a QuantizationSimModel object imported from aimet_torch.v2.quantsim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of all Linear and Conv weight quantizers to LPBQ with block_size 64 in the input_channels dimension:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_grouped_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">],</span>
<span class="gp">... </span>                                               <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">decompressed_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_grouping</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of specific model layers&#39; weight quantizer to LPBQ with block_size 64 in the input_channels dimension:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_grouped_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">arg</span><span class="o">=</span><span class="p">[</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="p">,</span> <span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">linear1</span><span class="p">],</span>
<span class="gp">... </span>                                               <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">decompressed_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_grouping</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Allows setting of only Convolution layers with input channels dim == 128 to LPBQ with block_size 64 in the input_channels dimension:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_grouped_blockwise_quantization_for_weights</span><span class="p">(</span><span class="n">sim</span><span class="o">=</span><span class="n">sim</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">arg</span><span class="o">=</span><span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">128</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">decompressed_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                                               <span class="n">block_grouping</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<p>This utility allows users to configure certain quantized layers in a model to use grouped blockwise quantization with a
specified decompressed_bw, block_size, and block_grouping. Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">set_blockwise_quantization_for_weights()</span></code>,
block_grouping can be a single value, in which case it will automatically be applied to the input_channel’s dimension,
with all other dimensions using a block_grouping value of 1.</p>
<p>Additionally, as different layers may have a different number of blocks for the input channels dimension given the same
block size, a single block_grouping value of ‘-1’ may be used, in which case the input channels dimension will automatically
use a block_grouping value equal to the number of blocks for any affected layer. This effectively allows users to configure
all affected layers to LPBQ quantization with a single API call.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../featureguide/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Optimization techniques</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="qat.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Quantization-aware training</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020, Qualcomm Innovation Center, Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/quic/aimet" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Advanced</a><ul>
<li><a class="reference internal" href="#per-block-quantization">Per-block quantization</a></li>
<li><a class="reference internal" href="#low-power-blockwise-quantization">Low power blockwise quantization</a></li>
<li><a class="reference internal" href="#export">Export</a></li>
<li><a class="reference internal" href="#api">API</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=8a448e45"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    </body>
</html>