Search.setIndex({"alltitles": {"1) Find layer groups": [[232, "find-layer-groups"]], "1. Creating a Keras model with subclass layers": [[197, "1.-Creating-a-Keras-model-with-subclass-layers"]], "1. Define Constants and Helper functions": [[208, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[200, "1.-Example-evaluation-and-training-pipeline"], [201, "1.-Example-evaluation-and-training-pipeline"], [209, "1.-Example-evaluation-and-training-pipeline"], [213, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[188, "1.-Example-evaluation-pipeline"], [206, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence check": [[252, "fp32-confidence-check"]], "1. Instantiate the example evaluation and training datasets": [[198, "1.-Instantiate-the-example-evaluation-and-training-datasets"], [199, "1.-Instantiate-the-example-evaluation-and-training-datasets"]], "1. Instantiate the example evaluation and training pipeline": [[195, "1.-Instantiate-the-example-evaluation-and-training-pipeline"]], "1. Instantiate the example evaluation method": [[192, "1.-Instantiate-the-example-evaluation-method"]], "1. Instantiate the example training and validation pipeline": [[189, "1.-Instantiate-the-example-training-and-validation-pipeline"], [190, "1.-Instantiate-the-example-training-and-validation-pipeline"], [191, "1.-Instantiate-the-example-training-and-validation-pipeline"], [193, "1.-Instantiate-the-example-training-and-validation-pipeline"], [194, "1.-Instantiate-the-example-training-and-validation-pipeline"], [202, "1.-Instantiate-the-example-training-and-validation-pipeline"], [203, "1.-Instantiate-the-example-training-and-validation-pipeline"], [204, "1.-Instantiate-the-example-training-and-validation-pipeline"], [205, "1.-Instantiate-the-example-training-and-validation-pipeline"], [207, "1.-Instantiate-the-example-training-and-validation-pipeline"], [210, "1.-Instantiate-the-example-training-and-validation-pipeline"], [211, "1.-Instantiate-the-example-training-and-validation-pipeline"], [212, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Quantization simulation": [[255, "quantization-simulation"]], "1. Run the notebook server": [[187, "run-the-notebook-server"]], "1. Sensitivity analysis to weight and activation quantization": [[218, "sensitivity-analysis-to-weight-and-activation-quantization"]], "1. Versioning": [[245, "versioning"]], "1. defaults": [[248, "defaults"]], "1.13.0": [[251, "id28"]], "1.16.0": [[251, "id27"]], "1.16.1": [[251, "id26"]], "1.16.2": [[251, "id25"]], "1.17.0": [[251, "id24"]], "1.18.0": [[251, "id23"]], "1.19.1": [[251, "id22"]], "1.20.0": [[251, "id21"]], "1.21.0": [[251, "id20"]], "1.22.0": [[251, "id19"]], "1.22.1": [[251, "id18"]], "1.22.2": [[251, "id17"]], "1.23.0": [[251, "id16"]], "1.24.0": [[251, "id15"]], "1.25.0": [[251, "id14"]], "1.26.0": [[251, "id13"]], "1.27.0": [[251, "id12"]], "1.28.0": [[251, "id11"]], "1.29.0": [[251, "id10"]], "1.30.0": [[251, "id9"]], "1.31.0": [[251, "id8"]], "1.32.0": [[251, "id7"]], "1.33.0": [[251, "id6"]], "1.33.5": [[251, "id5"]], "1.34.0": [[251, "id4"]], "1.35.0": [[251, "id3"]], "1.35.1": [[251, "id2"]], "2) Perform sensitivity analysis (Phase 1)": [[232, "perform-sensitivity-analysis-phase-1"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[188, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [189, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [190, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [191, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Converting the Keras model with subclass layers to a Keras model with functional layers": [[197, "2.-Converting-the-Keras-model-with-subclass-layers-to-a-Keras-model-with-functional-layers"]], "2. Create the model in Keras": [[195, "2.-Create-the-model-in-Keras"]], "2. Define Constants and Datasets Prepare": [[195, "2.-Define-Constants-and-Datasets-Prepare"]], "2. Download the example notebooks and related code": [[187, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[209, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[194, "2.-Load-a-pretrained-FP32-model"], [200, "2.-Load-a-pretrained-FP32-model"], [208, "2.-Load-a-pretrained-FP32-model"]], "2. Load the FP32 model and evaluate the model to find the baseline FP32 accuracy": [[192, "2.-Load-the-FP32-model-and-evaluate-the-model-to-find-the-baseline-FP32-accuracy"]], "2. Load the model": [[213, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[193, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [198, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [199, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [201, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [202, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [203, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [204, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [205, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [206, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [207, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [210, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [211, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [212, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement analysis": [[218, "per-layer-quantizer-enablement-analysis"]], "2. Post-training quantization": [[255, "post-training-quantization"]], "2. Version 0.4.0 (up to)": [[245, "version-0-4-0-up-to"]], "2. Weights or activations quantization": [[252, "weights-or-activations-quantization"]], "2. params": [[248, "params"]], "2.0.0": [[251, "id1"]], "2.1. Encoding specification": [[245, "encoding-specification"]], "2.2. Encoding file example for PyTorch": [[245, "encoding-file-example-for-pytorch"]], "2.3. Encoding file example for TensorFlow": [[245, "encoding-file-example-for-tensorflow"]], "3) Create a Pareto-front list (Phase 2)": [[232, "create-a-pareto-front-list-phase-2"]], "3. Apply QuantAnalyzer to the model": [[200, "3.-Apply-QuantAnalyzer-to-the-model"], [213, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[203, "3.-Compress-the-model-and-fine-tune"], [204, "3.-Compress-the-model-and-fine-tune"], [205, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[188, "3.-Create-a-quantization-simulation-model"], [206, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[209, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[189, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [190, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [191, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [193, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [198, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [199, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [201, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [202, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [207, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [210, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [211, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [212, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Determine the baseline FP32 accuracy": [[194, "3.-Determine-the-baseline-FP32-accuracy"]], "3. Fixing weight quantization": [[252, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range analysis": [[218, "per-layer-encodings-min-max-range-analysis"]], "3. Quantization-aware training": [[255, "quantization-aware-training"]], "3. Run AutoQuant": [[208, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[187, "run-the-notebooks"]], "3. Showing similarities and differences between the original and converted models": [[197, "3.-Showing-similarities-and-differences-between-the-original-and-converted-models"]], "3. Train and evaluate the model": [[195, "3.-Train-and-evaluate-the-model"]], "3. Version 0.5.0": [[245, "version-0-5-0"]], "3. supergroups": [[248, "supergroups"]], "3.1. Encoding Specification": [[245, "id1"]], "3.2. Encoding file example for PyTorch": [[245, "id2"]], "3.3. Encoding file example for TensorFlow": [[245, "id3"]], "3.Create a quantization simulation model (with fake quantization ops inserted)": [[192, "3.Create-a-quantization-simulation-model-(with-fake-quantization-ops-inserted)"]], "4) Reduce Bit-width Convert Op Overhead (Phase 3)": [[232, "reduce-bit-width-convert-op-overhead-phase-3"]], "4. Apply Adaround": [[189, "4.-Apply-Adaround"], [193, "4.-Apply-Adaround"], [201, "4.-Apply-Adaround"], [207, "4.-Apply-Adaround"]], "4. Apply CLE": [[190, "4.-Apply-CLE"], [202, "4.-Apply-CLE"]], "4. Apply CLE and BC": [[210, "4.-Apply-CLE-and-BC"]], "4. Create a QuantizationSim Model": [[195, "4.-Create-a-QuantizationSim-Model"]], "4. Define constants and helper functions": [[194, "4.-Define-constants-and-helper-functions"]], "4. Discussing the limitations of the Keras Model Preparer": [[197, "4.-Discussing-the-limitations-of-the-Keras-Model-Preparer"]], "4. Fixing activation quantization": [[252, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[218, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[209, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[198, "4.-Perform-QAT"], [199, "4.-Perform-QAT"], [211, "4.-Perform-QAT"], [212, "4.-Perform-QAT"]], "4. Run AMP algorithm on the quantized model": [[188, "4.-Run-AMP-algorithm-on-the-quantized-model"], [192, "4.-Run-AMP-algorithm-on-the-quantized-model"], [206, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. Version 0.6.1": [[245, "version-0-6-1"]], "4. model_input": [[248, "model-input"]], "4.1. Encoding specification": [[245, "id4"]], "5. Apply AutoQuant": [[194, "5.-Apply-AutoQuant"]], "5. Export Model": [[195, "5.-Export-Model"], [209, "5.-Export-Model"]], "5. Per layer mean-square-error (MSE) loss": [[218, "per-layer-mean-square-error-mse-loss"]], "5. Perform QAT": [[195, "5.-Perform-QAT"]], "5. Performing per-layer analysis": [[252, "performing-per-layer-analysis"]], "5. model_output": [[248, "model-output"]], "6. Visualizing sensitive layers": [[252, "visualizing-sensitive-layers"]], "7. Fixing individual quantizers": [[252, "fixing-individual-quantizers"]], "8. Quantize the model": [[252, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[237, null]], "AIMET documentation versions": [[256, null]], "AIMET installation in Docker": [[240, null]], "AIMET manual installation and setup": [[241, null]], "AIMET visualization": [[228, null]], "API": [[21, "api"], [172, "api"], [173, "api"], [176, "api"], [214, "api"], [216, "api"], [217, "api"], [218, "api"], [219, "api"], [220, "api"], [221, "api"], [222, "api"], [223, "api"], [227, "api"], [229, "api"], [232, "api"], [234, "api"], [235, "api"], [243, "api"], [244, "api"], [247, "api"]], "API Call for Regular AMP": [[192, "API-Call-for-Regular-AMP"]], "API Reference": [[237, "api-reference"]], "API reference": [[175, "api-reference"], [178, "api-reference"]], "Adaptive Rounding (AdaRound)": [[189, null], [193, null], [207, null]], "Adaptive rounding": [[214, null], [231, "adaptive-rounding"]], "Advanced": [[243, null]], "Affine quantizers": [[178, "affine-quantizers"]], "Alternative packages": [[239, "alternative-packages"]], "Analysis tools": [[215, null], [231, "analysis-tools"]], "Apply PTQ or QAT at specified precision": [[255, "apply-ptq-or-qat-at-specified-precision"]], "Apply the profile": [[234, "apply-the-profile"]], "AutoQuant": [[194, null], [208, null]], "Automatic Mixed-Precision (AMP)": [[188, null], [192, null], [206, null]], "Automatic mixed precision": [[232, null], [233, "automatic-mixed-precision"]], "Automatic quantization": [[219, null], [231, "automatic-quantization"]], "Batch norm folding": [[221, null], [231, "batch-norm-folding"]], "Batch norm re-estimation": [[220, null], [231, "batch-norm-re-estimation"]], "Bias Correction": [[210, "Bias-Correction"]], "Browse the notebooks": [[187, "browse-the-notebooks"]], "Build a Docker image": [[240, "build-a-docker-image"]], "Building from source": [[239, "building-from-source"]], "CLE": [[210, "CLE"]], "Calibration": [[244, null]], "Calibration callback": [[244, "calibration-callback"]], "Call AMP API": [[188, "Call-AMP-API"], [206, "Call-AMP-API"]], "Channel pruning": [[223, null]], "Channel pruning (CP)": [[226, "channel-pruning-cp"]], "Channel selection": [[223, "channel-selection"]], "Choose and install a package": [[239, "choose-and-install-a-package"]], "Choose to download or build an image": [[240, "choose-to-download-or-build-an-image"]], "Choose your AIMET variant": [[240, "choose-your-aimet-variant"]], "Code Examples": [[21, "code-examples"], [171, "code-examples"], [172, "code-examples"]], "Code example": [[217, "code-example"], [218, "code-example"], [219, "code-example"], [221, "code-example"], [223, "code-example"], [227, "code-example"], [229, "code-example"], [232, "code-example"], [235, "code-example"]], "Compilation": [[254, "compilation"]], "Complementary techniques": [[214, "complementary-techniques"]], "Compressing using Spatial SVD": [[227, "compressing-using-spatial-svd"]], "Compression": [[226, null], [228, "compression"], [231, "compression"]], "Compression features Guidebook": [[224, null]], "Compression ratio selection": [[225, "compression-ratio-selection"], [226, "compression-ratio-selection"]], "Compression using Channel Pruning": [[223, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[229, "compression-using-weight-svd"]], "Compute Encodings": [[188, "Compute-Encodings"], [192, "Compute-Encodings"], [206, "Compute-Encodings"]], "Compute encodings": [[244, "compute-encodings"]], "Compute the initial quantization parameters": [[247, "compute-the-initial-quantization-parameters"]], "Computing encodings": [[175, "computing-encodings"]], "Configuration": [[175, "configuration"]], "Configuration file structure": [[248, "configuration-file-structure"]], "Context": [[214, "context"], [216, "context"], [217, "context"], [218, "context"], [219, "context"], [220, "context"], [221, "context"], [222, "context"], [223, "context"], [227, "context"], [229, "context"], [232, "context"], [234, "context"], [235, "context"]], "Conversion": [[254, "conversion"]], "Create Quantization Sim Model": [[188, "Create-Quantization-Sim-Model"], [206, "Create-Quantization-Sim-Model"], [209, "Create-Quantization-Sim-Model"]], "Create the Quantization Sim Model": [[189, "Create-the-Quantization-Sim-Model"], [190, "Create-the-Quantization-Sim-Model"], [191, "Create-the-Quantization-Sim-Model"], [193, "Create-the-Quantization-Sim-Model"], [198, "Create-the-Quantization-Sim-Model"], [199, "Create-the-Quantization-Sim-Model"], [202, "Create-the-Quantization-Sim-Model"], [207, "Create-the-Quantization-Sim-Model"], [210, "Create-the-Quantization-Sim-Model"], [211, "Create-the-Quantization-Sim-Model"], [212, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[190, null]], "Cross-Layer Equalization and Bias Correction": [[210, null]], "Cross-Layer Equalization with QuantSim": [[202, null]], "Cross-layer equalization": [[222, null], [231, "cross-layer-equalization"]], "Dataset": [[188, "Dataset"], [189, "Dataset"], [190, "Dataset"], [191, "Dataset"], [192, "Dataset"], [193, "Dataset"], [194, "Dataset"], [195, "Dataset"], [198, "Dataset"], [199, "Dataset"], [200, "Dataset"], [201, "Dataset"], [202, "Dataset"], [203, "Dataset"], [204, "Dataset"], [205, "Dataset"], [206, "Dataset"], [207, "Dataset"], [208, "Dataset"], [209, "Dataset"], [210, "Dataset"], [211, "Dataset"], [212, "Dataset"], [213, "Dataset"]], "Debugging guidelines": [[253, "debugging-guidelines"]], "Debugging workflow": [[252, "debugging-workflow"]], "Define callback functions for AMP": [[188, "Define-callback-functions-for-AMP"], [206, "Define-callback-functions-for-AMP"]], "Deploy": [[255, "deploy"]], "DequantizedTensor": [[158, null]], "Design": [[228, "design"]], "Detailed analysis descriptions": [[218, "detailed-analysis-descriptions"]], "Determine quantization parameters (encodings)": [[246, "determine-quantization-parameters-encodings"]], "Docker install": [[239, "docker-install"]], "Download a prebuilt Docker image": [[240, "download-a-prebuilt-docker-image"]], "Encoding Format Specification": [[245, null]], "Encoding min/max ranges": [[200, "Encoding-min/max-ranges"], [213, "Encoding-min/max-ranges"]], "Enum Definition": [[10, "enum-definition"], [23, "enum-definition"]], "Environment setup": [[240, "environment-setup"]], "Evaluation": [[244, "evaluation"], [247, "evaluation"]], "Example Notebooks": [[237, "example-notebooks"]], "Examples": [[187, null]], "Execution": [[254, "execution"]], "Export": [[243, "export"], [244, "export"], [247, "export"]], "FAQs": [[226, "faqs"]], "FP16 precision (No quantization)": [[255, "fp16-precision-no-quantization"]], "Fast AMP (AMP 2.0)": [[192, "Fast-AMP-(AMP-2.0)"]], "Feature Guide": [[237, "feature-guide"]], "Fine-tune quantized model": [[247, "fine-tune-quantized-model"]], "FloatQuantizeDequantize": [[166, null]], "Fold Batch Norm layers": [[190, "Fold-Batch-Norm-layers"], [210, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[188, "Fold-Batch-Normalization-layers"], [189, "Fold-Batch-Normalization-layers"], [191, "Fold-Batch-Normalization-layers"], [193, "Fold-Batch-Normalization-layers"], [198, "Fold-Batch-Normalization-layers"], [199, "Fold-Batch-Normalization-layers"], [201, "Fold-Batch-Normalization-layers"], [202, "Fold-Batch-Normalization-layers"], [206, "Fold-Batch-Normalization-layers"], [207, "Fold-Batch-Normalization-layers"], [211, "Fold-Batch-Normalization-layers"], [212, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[195, "Fold-BatchNorm-Layers"], [209, "Fold-BatchNorm-Layers"]], "For more information": [[189, "For-more-information"], [190, "For-more-information"], [191, "For-more-information"], [193, "For-more-information"], [194, "For-more-information"], [198, "For-more-information"], [199, "For-more-information"], [202, "For-more-information"], [203, "For-more-information"], [204, "For-more-information"], [205, "For-more-information"], [207, "For-more-information"], [208, "For-more-information"], [210, "For-more-information"], [211, "For-more-information"], [212, "For-more-information"]], "Glossary": [[236, null], [237, "glossary"]], "Greedy compression ratio selection": [[225, null]], "Host install from scratch": [[239, "host-install-from-scratch"]], "How it works": [[225, "how-it-works"]], "How to modify configuration file": [[248, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[171, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[230, "how-winnowing-works"]], "Hyper parameters": [[214, "hyper-parameters"]], "Install AIMET packages": [[240, "install-aimet-packages"]], "Installation": [[237, "installation"], [239, null]], "Installing AIMET": [[240, "installing-aimet"], [241, "installing-aimet"], [242, "installing-aimet"]], "Interactive visualization": [[215, "interactive-visualization"], [216, null]], "Keras Model Preparer": [[197, null]], "Layer output generation": [[215, "layer-output-generation"], [217, null]], "Limitations": [[21, "limitations"], [172, "limitations"]], "Low power blockwise quantization": [[243, "low-power-blockwise-quantization"]], "MMP API options": [[234, "mmp-api-options"]], "Manual mixed precision": [[233, "manual-mixed-precision"], [234, null]], "Migration Process": [[171, "migration-process"]], "Migration guide": [[171, null]], "Mixed Precision Algorithm": [[232, "mixed-precision-algorithm"]], "Mixed precision": [[231, "mixed-precision"], [233, null]], "Model compression": [[226, "model-compression"]], "Model compression using channel pruning": [[203, null]], "Model compression using spatial SVD": [[204, null]], "Model compression using spatial SVD and channel pruning": [[205, null]], "Model guidelines": [[250, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[171, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[171, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "Multi-GPU support": [[247, "multi-gpu-support"]], "NOTE": [[226, null]], "Next steps": [[189, "Next-steps"], [193, "Next-steps"], [198, "Next-steps"], [199, "Next-steps"], [202, "Next-steps"], [203, "Next-steps"], [204, "Next-steps"], [205, "Next-steps"], [207, "Next-steps"], [208, "Next-steps"], [210, "Next-steps"], [211, "Next-steps"], [212, "Next-steps"], [219, "next-steps"]], "Old versions": [[239, "old-versions"]], "On-target inference": [[253, "on-target-inference"], [254, null]], "Optimization techniques": [[231, null]], "Optional techniques": [[226, "optional-techniques"]], "Overall flow": [[188, "Overall-flow"], [189, "Overall-flow"], [190, "Overall-flow"], [191, "Overall-flow"], [192, "Overall-flow"], [193, "Overall-flow"], [194, "Overall-flow"], [195, "Overall-flow"], [196, "Overall-flow"], [197, "Overall-flow"], [198, "Overall-flow"], [199, "Overall-flow"], [200, "Overall-flow"], [201, "Overall-flow"], [202, "Overall-flow"], [203, "Overall-flow"], [204, "Overall-flow"], [205, "Overall-flow"], [206, "Overall-flow"], [207, "Overall-flow"], [208, "Overall-flow"], [209, "Overall-flow"], [210, "Overall-flow"], [211, "Overall-flow"], [212, "Overall-flow"], [213, "Overall-flow"]], "Overview": [[225, "overview"], [226, "overview"], [228, "overview"], [230, "overview"], [246, "overview"], [248, "overview"]], "PDF of statistics": [[200, "PDF-of-statistics"], [213, "PDF-of-statistics"]], "Parameters for AMP algorithm": [[188, "Parameters-for-AMP-algorithm"], [192, "Parameters-for-AMP-algorithm"], [206, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[178, "per-block-quantization"], [243, "per-block-quantization"]], "Per-channel quantization": [[178, "per-channel-quantization"]], "Per-layer MSE loss": [[200, "Per-layer-MSE-loss"], [213, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[200, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [213, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[225, "per-layer-exploration"]], "Per-layer fine-tuning": [[226, "per-layer-fine-tuning"]], "Perform QAT": [[209, "Perform-QAT"]], "Prepare the evaluation callback function": [[195, "Prepare-the-evaluation-callback-function"]], "Prerequisites": [[214, "prerequisites"], [218, "prerequisites"], [219, "prerequisites"], [220, "prerequisites"], [235, "prerequisites"], [239, "prerequisites"], [240, "prerequisites"], [241, "prerequisites"]], "Procedure": [[223, "procedure"]], "PyPI": [[239, "pypi"]], "PyTorch model guidelines": [[250, null]], "QAT modes": [[247, "qat-modes"]], "QAT recommendations": [[247, "qat-recommendations"]], "Qualcomm\u00ae AI Engine Direct SDK": [[254, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[254, "qualcommreg-ai-hub"]], "Quant Analyzer": [[200, null], [213, null]], "QuantSim creation": [[244, "quantsim-creation"]], "QuantSim workflow": [[246, "quantsim-workflow"]], "Quantization": [[254, "quantization"]], "Quantization Simulation Guide": [[237, "quantization-simulation-guide"]], "Quantization analyzer": [[215, "quantization-analyzer"], [218, null]], "Quantization debugging guidelines": [[252, null]], "Quantization features": [[255, "quantization-features"]], "Quantization granularity": [[246, "quantization-granularity"]], "Quantization schemes": [[246, "quantization-schemes"]], "Quantization simulation": [[191, null]], "Quantization simulation guide": [[246, null]], "Quantization user guide": [[253, null]], "Quantization workflow": [[253, "quantization-workflow"], [255, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[195, null], [209, null]], "Quantization-Aware Training with a Keras Transformer Model": [[196, null]], "Quantization-Aware training with range learning": [[199, null]], "Quantization-aware training": [[198, null], [211, null], [247, null]], "Quantization-aware training with range learning": [[212, null]], "QuantizationMixin": [[30, null]], "Quantize": [[161, null]], "QuantizeDequantize": [[162, null]], "Quantized modules": [[175, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[31, null]], "QuantizedAdaptiveAvgPool2d": [[32, null]], "QuantizedAdaptiveAvgPool3d": [[33, null]], "QuantizedAdaptiveMaxPool1d": [[34, null]], "QuantizedAdaptiveMaxPool2d": [[35, null]], "QuantizedAdaptiveMaxPool3d": [[36, null]], "QuantizedAlphaDropout": [[37, null]], "QuantizedAvgPool1d": [[38, null]], "QuantizedAvgPool2d": [[39, null]], "QuantizedAvgPool3d": [[40, null]], "QuantizedBCELoss": [[41, null]], "QuantizedBCEWithLogitsLoss": [[42, null]], "QuantizedBatchNorm1d": [[43, null]], "QuantizedBatchNorm2d": [[44, null]], "QuantizedBatchNorm3d": [[45, null]], "QuantizedBilinear": [[46, null]], "QuantizedCELU": [[47, null]], "QuantizedCTCLoss": [[48, null]], "QuantizedChannelShuffle": [[49, null]], "QuantizedCircularPad1d": [[50, null]], "QuantizedCircularPad2d": [[51, null]], "QuantizedCircularPad3d": [[52, null]], "QuantizedConstantPad1d": [[53, null]], "QuantizedConstantPad2d": [[54, null]], "QuantizedConstantPad3d": [[55, null]], "QuantizedConv1d": [[56, null]], "QuantizedConv2d": [[57, null]], "QuantizedConv3d": [[58, null]], "QuantizedConvTranspose1d": [[59, null]], "QuantizedConvTranspose2d": [[60, null]], "QuantizedConvTranspose3d": [[61, null]], "QuantizedCosineEmbeddingLoss": [[62, null]], "QuantizedCosineSimilarity": [[63, null]], "QuantizedCrossEntropyLoss": [[64, null]], "QuantizedDropout": [[65, null]], "QuantizedDropout1d": [[66, null]], "QuantizedDropout2d": [[67, null]], "QuantizedDropout3d": [[68, null]], "QuantizedELU": [[69, null]], "QuantizedEmbedding": [[70, null]], "QuantizedEmbeddingBag": [[71, null]], "QuantizedFeatureAlphaDropout": [[72, null]], "QuantizedFlatten": [[73, null]], "QuantizedFold": [[74, null]], "QuantizedFractionalMaxPool2d": [[75, null]], "QuantizedFractionalMaxPool3d": [[76, null]], "QuantizedGELU": [[77, null]], "QuantizedGLU": [[78, null]], "QuantizedGRU": [[79, null]], "QuantizedGRUCell": [[80, null]], "QuantizedGaussianNLLLoss": [[81, null]], "QuantizedGroupNorm": [[82, null]], "QuantizedHardshrink": [[83, null]], "QuantizedHardsigmoid": [[84, null]], "QuantizedHardswish": [[85, null]], "QuantizedHardtanh": [[86, null]], "QuantizedHingeEmbeddingLoss": [[87, null]], "QuantizedHuberLoss": [[88, null]], "QuantizedInstanceNorm1d": [[89, null]], "QuantizedInstanceNorm2d": [[90, null]], "QuantizedInstanceNorm3d": [[91, null]], "QuantizedKLDivLoss": [[92, null]], "QuantizedL1Loss": [[93, null]], "QuantizedLPPool1d": [[94, null]], "QuantizedLPPool2d": [[95, null]], "QuantizedLSTM": [[96, null]], "QuantizedLSTMCell": [[97, null]], "QuantizedLayerNorm": [[98, null]], "QuantizedLeakyReLU": [[99, null]], "QuantizedLinear": [[100, null]], "QuantizedLocalResponseNorm": [[101, null]], "QuantizedLogSigmoid": [[102, null]], "QuantizedLogSoftmax": [[103, null]], "QuantizedMSELoss": [[104, null]], "QuantizedMarginRankingLoss": [[105, null]], "QuantizedMaxPool1d": [[106, null]], "QuantizedMaxPool2d": [[107, null]], "QuantizedMaxPool3d": [[108, null]], "QuantizedMaxUnpool1d": [[109, null]], "QuantizedMaxUnpool2d": [[110, null]], "QuantizedMaxUnpool3d": [[111, null]], "QuantizedMish": [[112, null]], "QuantizedMultiLabelMarginLoss": [[113, null]], "QuantizedMultiLabelSoftMarginLoss": [[114, null]], "QuantizedMultiMarginLoss": [[115, null]], "QuantizedNLLLoss": [[116, null]], "QuantizedNLLLoss2d": [[117, null]], "QuantizedPReLU": [[118, null]], "QuantizedPairwiseDistance": [[119, null]], "QuantizedPixelShuffle": [[120, null]], "QuantizedPixelUnshuffle": [[121, null]], "QuantizedPoissonNLLLoss": [[122, null]], "QuantizedRNN": [[123, null]], "QuantizedRNNCell": [[124, null]], "QuantizedRReLU": [[125, null]], "QuantizedReLU": [[126, null]], "QuantizedReLU6": [[127, null]], "QuantizedReflectionPad1d": [[128, null]], "QuantizedReflectionPad2d": [[129, null]], "QuantizedReflectionPad3d": [[130, null]], "QuantizedReplicationPad1d": [[131, null]], "QuantizedReplicationPad2d": [[132, null]], "QuantizedReplicationPad3d": [[133, null]], "QuantizedSELU": [[134, null]], "QuantizedSiLU": [[135, null]], "QuantizedSigmoid": [[136, null]], "QuantizedSmoothL1Loss": [[137, null]], "QuantizedSoftMarginLoss": [[138, null]], "QuantizedSoftmax": [[139, null]], "QuantizedSoftmax2d": [[140, null]], "QuantizedSoftmin": [[141, null]], "QuantizedSoftplus": [[142, null]], "QuantizedSoftshrink": [[143, null]], "QuantizedSoftsign": [[144, null]], "QuantizedTanh": [[145, null]], "QuantizedTanhshrink": [[146, null]], "QuantizedTensor": [[159, null]], "QuantizedTensorBase": [[160, null]], "QuantizedThreshold": [[147, null]], "QuantizedTripletMarginLoss": [[148, null]], "QuantizedTripletMarginWithDistanceLoss": [[149, null]], "QuantizedUnflatten": [[150, null]], "QuantizedUnfold": [[151, null]], "QuantizedUpsample": [[152, null]], "QuantizedUpsamplingBilinear2d": [[153, null]], "QuantizedUpsamplingNearest2d": [[154, null]], "QuantizedZeroPad1d": [[155, null]], "QuantizedZeroPad2d": [[156, null]], "QuantizedZeroPad3d": [[157, null]], "Quantizers": [[178, "quantizers"]], "Quantsim and Adaround - Per Channel Quantization (PCQ)": [[201, null]], "Quick Start": [[237, "quick-start"]], "Quick Start (PyTorch)": [[242, null]], "Rank Rounding": [[226, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[209, "Re-estimate-BatchNorm-Statistics"]], "References": [[226, "references"]], "Regular AMP": [[192, "Regular-AMP"]], "Release Notes": [[237, "release-notes"]], "Release notes": [[251, null]], "Results": [[219, "results"]], "Running a quick example": [[242, "running-a-quick-example"]], "Running the notebooks": [[187, "running-the-notebooks"]], "Runtime configuration": [[246, "runtime-configuration"], [248, null]], "Sequential MSE": [[231, "sequential-mse"], [235, null]], "Set model input precision": [[234, "set-model-input-precision"]], "Set model output precision": [[234, "set-model-output-precision"]], "Set precision based on layer type": [[234, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[234, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[234, "set-precision-of-a-non-leaf-layer"]], "Setup": [[214, "setup"], [220, "setup"], [222, "setup"], [223, "setup"], [227, "setup"], [229, "setup"], [234, "setup"], [235, "setup"], [247, "setup"]], "Simulate quantization noise": [[246, "simulate-quantization-noise"]], "Spatial SVD": [[226, "spatial-svd"], [227, null]], "Start the docker container": [[240, "start-the-docker-container"]], "Starting a Bokeh server session": [[228, "starting-a-bokeh-server-session"]], "Step 1": [[214, "step-1"], [219, "step-1"], [220, "step-1"], [221, "step-1"], [222, "step-1"], [232, "step-1"], [235, "step-1"]], "Step 1 Obtain Original or QuantSim model from AIMET Export Artifacts": [[217, "step-1-obtain-original-or-quantsim-model-from-aimet-export-artifacts"]], "Step 1 Prepare callback for calibration": [[218, "step-1-prepare-callback-for-calibration"]], "Step 2": [[214, "step-2"], [219, "step-2"], [220, "step-2"], [221, "step-2"], [232, "step-2"], [235, "step-2"]], "Step 2 Generate layer-outputs": [[217, "step-2-generate-layer-outputs"]], "Step 2 Prepare callback for quantized model evaluation": [[218, "step-2-prepare-callback-for-quantized-model-evaluation"]], "Step 3": [[214, "step-3"], [219, "step-3"], [220, "step-3"], [221, "step-3"], [235, "step-3"]], "Step 3 Prepare model and callback functions": [[218, "step-3-prepare-model-and-callback-functions"]], "Step 4": [[214, "step-4"], [219, "step-4"], [220, "step-4"], [235, "step-4"], [235, "id1"]], "Step 4 Create QuantAnalyzer and run analysis": [[218, "step-4-create-quantanalyzer-and-run-analysis"]], "Step 5": [[219, "step-5"]], "Step 6": [[219, "step-6"]], "Step 7": [[219, "step-7"]], "Summary": [[192, "Summary"], [195, "Summary"], [197, "Summary"], [201, "Summary"], [209, "Summary"]], "Supported precisions for on-target inference": [[255, "supported-precisions-for-on-target-inference"]], "Techniques": [[210, "Techniques"]], "TensorFlow model guidelines": [[249, null]], "Tested platform": [[242, "tested-platform"]], "Use Case": [[226, "use-case"]], "Use Cases": [[232, "use-cases"]], "User Guide": [[237, "user-guide"]], "User flow": [[176, "user-flow"]], "Verifying the installation": [[239, "verifying-the-installation"], [242, "verifying-the-installation"]], "Visualization Tools": [[168, "visualization-tools"]], "Visualizing compression ratios": [[228, "visualizing-compression-ratios"]], "W16A16 verification": [[255, "w16a16-verification"]], "Weight SVD": [[226, "weight-svd"], [229, null]], "Weight reconstruction": [[223, "weight-reconstruction"]], "What this notebook is not": [[188, "What-this-notebook-is-not"], [192, "What-this-notebook-is-not"], [200, "What-this-notebook-is-not"], [201, "What-this-notebook-is-not"], [206, "What-this-notebook-is-not"], [209, "What-this-notebook-is-not"], [213, "What-this-notebook-is-not"]], "Winnowing": [[223, "winnowing"], [230, null], [230, "id1"]], "Workflow": [[214, "workflow"], [214, "id2"], [216, "workflow"], [217, "workflow"], [218, "workflow"], [219, "workflow"], [220, "workflow"], [221, "workflow"], [222, "workflow"], [223, "workflow"], [227, "workflow"], [229, "workflow"], [232, "workflow"], [234, "workflow"], [235, "workflow"], [244, "workflow"], [247, "workflow"], [255, "workflow"]], "aimet_onnx": [[0, "aimet-onnx"]], "aimet_onnx API": [[6, null]], "aimet_onnx.adaround": [[1, null]], "aimet_onnx.auto_quant_v2": [[3, null]], "aimet_onnx.batch_norm_fold": [[4, null]], "aimet_onnx.cross_layer_equalization": [[5, null]], "aimet_onnx.layer_output_utils": [[7, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[9, null]], "aimet_onnx.quantsim": [[10, null]], "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights": [[8, null]], "aimet_onnx.seq_mse": [[11, null]], "aimet_tensorflow": [[0, "aimet-tensorflow"]], "aimet_tensorflow API": [[19, null]], "aimet_tensorflow.adaround": [[12, null]], "aimet_tensorflow.auto_quant_v2": [[14, null]], "aimet_tensorflow.batch_norm_fold": [[16, null]], "aimet_tensorflow.compress": [[18, null]], "aimet_tensorflow.cross_layer_equalization": [[17, null]], "aimet_tensorflow.keras.bn_reestimation": [[15, null]], "aimet_tensorflow.layer_output_utils": [[20, null]], "aimet_tensorflow.mixed_precision": [[13, null]], "aimet_tensorflow.model_preparer": [[21, null]], "aimet_tensorflow.quant_analyzer": [[22, null]], "aimet_tensorflow.quantsim": [[23, null]], "aimet_torch": [[0, "aimet-torch"], [167, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[171, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[167, null]], "aimet_torch.adaround": [[24, null]], "aimet_torch.auto_quant": [[25, null]], "aimet_torch.batch_norm_fold": [[27, null]], "aimet_torch.bn_reestimation": [[26, null]], "aimet_torch.compress": [[29, null]], "aimet_torch.cross_layer_equalization": [[28, null]], "aimet_torch.layer_output_utils": [[169, null]], "aimet_torch.mixed_precision": [[174, null]], "aimet_torch.model_preparer": [[172, null]], "aimet_torch.model_validator": [[173, null]], "aimet_torch.nn": [[175, null]], "aimet_torch.peft": [[176, null]], "aimet_torch.quant_analyzer": [[177, null]], "aimet_torch.quantization": [[178, null]], "aimet_torch.quantsim": [[179, null]], "aimet_torch.quantsim.config_utils": [[170, null]], "aimet_torch.seq_mse": [[180, null]], "aimet_torch.v1": [[167, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[181, null]], "aimet_torch.v1.auto_quant": [[183, null]], "aimet_torch.v1.mixed_precision": [[182, null]], "aimet_torch.v1.quant_analyzer": [[184, null]], "aimet_torch.v1.quantsim": [[185, null]], "aimet_torch.v1.seq_mse": [[186, null]], "aimet_torch.visualization_tools": [[168, null]], "dequantize": [[163, null]], "quantize": [[164, null]], "quantize_dequantize": [[165, null]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/autoquant", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/tensorflow/adaround", "apiref/tensorflow/amp", "apiref/tensorflow/autoquant", "apiref/tensorflow/bn", "apiref/tensorflow/bnf", "apiref/tensorflow/cle", "apiref/tensorflow/compress", "apiref/tensorflow/index", "apiref/tensorflow/layer_output_generation", "apiref/tensorflow/model_preparer", "apiref/tensorflow/quant_analyzer", "apiref/tensorflow/quantsim", "apiref/torch/adaround", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "examples/index", "examples/onnx/quantization/AMP", "examples/onnx/quantization/adaround", "examples/onnx/quantization/cle", "examples/onnx/quantization/quantsim", "examples/tensorflow/quantization/keras/KerasAMP", "examples/tensorflow/quantization/keras/adaround", "examples/tensorflow/quantization/keras/autoquant", "examples/tensorflow/quantization/keras/bn_reestimation", "examples/tensorflow/quantization/keras/keras_transformer_qat", "examples/tensorflow/quantization/keras/model_preparer", "examples/tensorflow/quantization/keras/qat", "examples/tensorflow/quantization/keras/qat_range_learning", "examples/tensorflow/quantization/keras/quant_analyzer", "examples/tensorflow/quantization/keras/quantsim_adaround_pcq", "examples/tensorflow/quantization/keras/quantsim_cle", "examples/torch/compression/channel_pruning", "examples/torch/compression/spatial_svd", "examples/torch/compression/spatial_svd_channel_pruning", "examples/torch/quantization/AMP", "examples/torch/quantization/adaround", "examples/torch/quantization/autoquant", "examples/torch/quantization/bn_reestimation", "examples/torch/quantization/cle_bc", "examples/torch/quantization/qat", "examples/torch/quantization/qat_range_learning", "examples/torch/quantization/quant_analyzer", "featureguide/adaround", "featureguide/analysis tools/index", "featureguide/analysis tools/interactive_visualization", "featureguide/analysis tools/layer_output_generation", "featureguide/analysis tools/quant_analyzer", "featureguide/autoquant", "featureguide/bn", "featureguide/bnf", "featureguide/cle", "featureguide/compression/channel_pruning", "featureguide/compression/feature_guidebook", "featureguide/compression/greedy_compression_ratio_selection", "featureguide/compression/index", "featureguide/compression/spatial_svd", "featureguide/compression/visualization_compression", "featureguide/compression/weight_svd", "featureguide/compression/winnowing", "featureguide/index", "featureguide/mixed precision/amp", "featureguide/mixed precision/index", "featureguide/mixed precision/mmp", "featureguide/seq_mse", "glossary", "index", "install/basic_install", "install/index", "install/install_docker", "install/install_host", "install/quick-start", "quantsim/advanced", "quantsim/calibration", "quantsim/encoding_spec", "quantsim/index", "quantsim/qat", "quantsim/runtime_config", "quantsim/tensorflow/model_guidelines", "quantsim/torch/model_guidelines", "release_notes", "userguide/debugging_guidelines", "userguide/index", "userguide/on_target_inference", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/autoquant.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/tensorflow/adaround.rst", "apiref/tensorflow/amp.rst", "apiref/tensorflow/autoquant.rst", "apiref/tensorflow/bn.rst", "apiref/tensorflow/bnf.rst", "apiref/tensorflow/cle.rst", "apiref/tensorflow/compress.rst", "apiref/tensorflow/index.rst", "apiref/tensorflow/layer_output_generation.rst", "apiref/tensorflow/model_preparer.rst", "apiref/tensorflow/quant_analyzer.rst", "apiref/tensorflow/quantsim.rst", "apiref/torch/adaround.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "examples/index.rst", "examples/onnx/quantization/AMP.ipynb", "examples/onnx/quantization/adaround.ipynb", "examples/onnx/quantization/cle.ipynb", "examples/onnx/quantization/quantsim.ipynb", "examples/tensorflow/quantization/keras/KerasAMP.ipynb", "examples/tensorflow/quantization/keras/adaround.ipynb", "examples/tensorflow/quantization/keras/autoquant.ipynb", "examples/tensorflow/quantization/keras/bn_reestimation.ipynb", "examples/tensorflow/quantization/keras/keras_transformer_qat.ipynb", "examples/tensorflow/quantization/keras/model_preparer.ipynb", "examples/tensorflow/quantization/keras/qat.ipynb", "examples/tensorflow/quantization/keras/qat_range_learning.ipynb", "examples/tensorflow/quantization/keras/quant_analyzer.ipynb", "examples/tensorflow/quantization/keras/quantsim_adaround_pcq.ipynb", "examples/tensorflow/quantization/keras/quantsim_cle.ipynb", "examples/torch/compression/channel_pruning.ipynb", "examples/torch/compression/spatial_svd.ipynb", "examples/torch/compression/spatial_svd_channel_pruning.ipynb", "examples/torch/quantization/AMP.ipynb", "examples/torch/quantization/adaround.ipynb", "examples/torch/quantization/autoquant.ipynb", "examples/torch/quantization/bn_reestimation.ipynb", "examples/torch/quantization/cle_bc.ipynb", "examples/torch/quantization/qat.ipynb", "examples/torch/quantization/qat_range_learning.ipynb", "examples/torch/quantization/quant_analyzer.ipynb", "featureguide/adaround.rst", "featureguide/analysis tools/index.rst", "featureguide/analysis tools/interactive_visualization.rst", "featureguide/analysis tools/layer_output_generation.rst", "featureguide/analysis tools/quant_analyzer.rst", "featureguide/autoquant.rst", "featureguide/bn.rst", "featureguide/bnf.rst", "featureguide/cle.rst", "featureguide/compression/channel_pruning.rst", "featureguide/compression/feature_guidebook.rst", "featureguide/compression/greedy_compression_ratio_selection.rst", "featureguide/compression/index.rst", "featureguide/compression/spatial_svd.rst", "featureguide/compression/visualization_compression.rst", "featureguide/compression/weight_svd.rst", "featureguide/compression/winnowing.rst", "featureguide/index.rst", "featureguide/mixed precision/amp.rst", "featureguide/mixed precision/index.rst", "featureguide/mixed precision/mmp.rst", "featureguide/seq_mse.rst", "glossary.rst", "index.rst", "install/basic_install.rst", "install/index.rst", "install/install_docker.rst", "install/install_host.rst", "install/quick-start.rst", "quantsim/advanced.rst", "quantsim/calibration.rst", "quantsim/encoding_spec.rst", "quantsim/index.rst", "quantsim/qat.rst", "quantsim/runtime_config.rst", "quantsim/tensorflow/model_guidelines.rst", "quantsim/torch/model_guidelines.rst", "release_notes.rst", "userguide/debugging_guidelines.rst", "userguide/index.rst", "userguide/on_target_inference.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[236, "term-Accelerator", true]], "accuracy": [[236, "term-Accuracy", true]], "activation": [[236, "term-Activation", true]], "activation quantization": [[236, "term-Activation-Quantization", true]], "adaptermetadata (class in aimet_torch.peft)": [[176, "aimet_torch.peft.AdapterMetaData", false]], "adaround": [[236, "term-AdaRound", true]], "adaroundparameters (class in aimet_onnx.adaround.adaround_weight)": [[1, "aimet_onnx.adaround.adaround_weight.AdaroundParameters", false], [214, "aimet_onnx.adaround.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_tensorflow.keras.adaround_weight)": [[12, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false], [214, "aimet_tensorflow.keras.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.adaround.adaround_weight)": [[24, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false], [214, "aimet_torch.adaround.adaround_weight.AdaroundParameters", false]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[181, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[236, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[236, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [218, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [234, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_onnx.adaround.adaround_weight.adaround)": [[1, "aimet_onnx.adaround.adaround_weight.Adaround.apply_adaround", false], [214, "aimet_onnx.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_tensorflow.keras.adaround_weight.adaround)": [[12, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false], [214, "aimet_tensorflow.keras.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.adaround.adaround_weight.adaround)": [[24, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false], [214, "aimet_torch.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[181, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_seq_mse() (in module aimet_onnx.sequential_mse.seq_mse.sequentialmse)": [[11, "aimet_onnx.sequential_mse.seq_mse.SequentialMse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[180, "aimet_torch.seq_mse.apply_seq_mse", false], [235, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[186, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "auto (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.auto", false]], "autoquant": [[236, "term-AutoQuant", true]], "autoquantwithautomixedprecision (class in aimet_onnx.auto_quant_v2)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false]], "autoquantwithautomixedprecision (class in aimet_tensorflow.keras.auto_quant_v2)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision", false]], "batch normalization": [[236, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[236, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[236, "term-BN", true]], "callbackfunc (class in aimet_common.defs)": [[2, "aimet_common.defs.CallbackFunc", false], [13, "aimet_common.defs.CallbackFunc", false], [174, "aimet_common.defs.CallbackFunc", false], [182, "aimet_common.defs.CallbackFunc", false], [232, "aimet_common.defs.CallbackFunc", false], [232, "id0", false], [232, "id1", false]], "callbackfunc (class in aimet_common.utils)": [[177, "aimet_common.utils.CallbackFunc", false], [184, "aimet_common.utils.CallbackFunc", false], [218, "aimet_common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_fast_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[13, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false], [232, "aimet_tensorflow.keras.mixed_precision.choose_fast_mixed_precision", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [232, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_tensorflow.keras.mixed_precision)": [[13, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false], [232, "aimet_tensorflow.keras.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[174, "aimet_torch.mixed_precision.choose_mixed_precision", false], [232, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[182, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[236, "term-CNN", true]], "compress_model() (aimet_tensorflow.keras.compress.modelcompressor static method)": [[18, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false], [227, "aimet_tensorflow.keras.compress.ModelCompressor.compress_model", false]], "compression": [[236, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantsim.quantizationsimmodel method)": [[10, "aimet_onnx.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[23, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantsim.quantizationsimmodel method)": [[179, "aimet_torch.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[185, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "convolutional layer": [[236, "term-Convolutional-Layer", true]], "convolutional neural network": [[236, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[163, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[158, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[236, "term-Device", true]], "disable_lora_adapters() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.disable_lora_adapters", false]], "dlf": [[236, "term-DLF", true]], "dynamic layer fusion": [[236, "term-Dynamic-Layer-Fusion", true]], "edge device": [[236, "term-Edge-device", true]], "enable_adapter_and_load_weights() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.enable_adapter_and_load_weights", false]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [218, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[236, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[5, "aimet_onnx.cross_layer_equalization.equalize_model", false], [222, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_tensorflow.keras.cross_layer_equalization)": [[17, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false], [222, "aimet_tensorflow.keras.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[28, "aimet_torch.cross_layer_equalization.equalize_model", false], [222, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [232, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [182, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [232, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "export() (aimet_onnx.quantsim.quantizationsimmodel method)": [[10, "aimet_onnx.quantsim.QuantizationSimModel.export", false]], "export() (aimet_tensorflow.keras.quantsim.quantizationsimmodel method)": [[23, "aimet_tensorflow.keras.quantsim.QuantizationSimModel.export", false]], "export() (aimet_torch.quantsim.quantizationsimmodel method)": [[179, "aimet_torch.quantsim.QuantizationSimModel.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[185, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export_adapter_weights() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.export_adapter_weights", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_tensorflow.keras.batch_norm_fold)": [[16, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false], [221, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[27, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [221, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_scale() (in module aimet_tensorflow.keras.batch_norm_fold)": [[15, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false], [220, "aimet_tensorflow.keras.batch_norm_fold.fold_all_batch_norms_to_scale", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[4, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [221, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[100, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[161, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[162, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[180, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [235, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[186, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[236, "term-FP32", true]], "freeze_base_model() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.freeze_base_model", false]], "freeze_base_model_activation_quantizers() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.freeze_base_model_activation_quantizers", false]], "freeze_base_model_param_quantizers() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.freeze_base_model_param_quantizers", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.from_module", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[7, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [217, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_tensorflow.keras.layer_output_utils.layeroutpututil method)": [[20, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [217, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[169, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [217, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_param_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_param_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_fp_lora_layer() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.get_fp_lora_layer", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "get_quant_scheme_candidates() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false]], "get_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.get_quant_scheme_candidates", false]], "get_quantized_lora_layer() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.get_quantized_lora_layer", false]], "greedyselectionparameters (class in aimet_common.defs)": [[18, "aimet_common.defs.GreedySelectionParameters", false], [227, "aimet_common.defs.GreedySelectionParameters", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[236, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[236, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[236, "term-KL-Divergence", true]], "layer": [[236, "term-Layer", true]], "layer-wise quantization": [[236, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[7, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [217, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_tensorflow.keras.layer_output_utils)": [[20, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false], [217, "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[169, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [217, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.quantsim method)": [[179, "aimet_torch.quantsim.load_checkpoint", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[185, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lookup_quantizer() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup static method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.lookup_quantizer", false]], "lora mobilenet": [[236, "term-LoRA-MobileNet", true]], "manual (aimet_tensorflow.keras.defs.spatialsvdparameters.mode attribute)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode.manual", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [234, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[236, "term-Model", true]], "modelcompressor (class in aimet_tensorflow.keras.compress)": [[18, "aimet_tensorflow.keras.compress.ModelCompressor", false], [227, "aimet_tensorflow.keras.compress.ModelCompressor", false]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[169, "aimet_torch.layer_output_utils.NamingScheme", false], [217, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[236, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[236, "term-NNCF", true]], "node": [[236, "term-Node", true]], "normalization": [[236, "term-Normalization", true]], "onnx": [[236, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [217, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "open neural network exchange": [[236, "term-Open-Neural-Network-Exchange", true]], "optimize() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false]], "optimize() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.optimize", false]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[30, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "peftquantutils (class in aimet_torch.peft)": [[176, "aimet_torch.peft.PeftQuantUtils", false]], "per-channel quantization": [[236, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[236, "term-Post-Training-Quantization", true]], "post_training_percentile (aimet_common.defs.quantscheme attribute)": [[10, "aimet_common.defs.QuantScheme.post_training_percentile", false], [23, "aimet_common.defs.QuantScheme.post_training_percentile", false], [179, "aimet_common.defs.QuantScheme.post_training_percentile", false], [185, "aimet_common.defs.QuantScheme.post_training_percentile", false]], "post_training_tf (aimet_common.defs.quantscheme attribute)": [[10, "aimet_common.defs.QuantScheme.post_training_tf", false], [23, "aimet_common.defs.QuantScheme.post_training_tf", false], [179, "aimet_common.defs.QuantScheme.post_training_tf", false], [185, "aimet_common.defs.QuantScheme.post_training_tf", false]], "post_training_tf_enhanced (aimet_common.defs.quantscheme attribute)": [[10, "aimet_common.defs.QuantScheme.post_training_tf_enhanced", false], [23, "aimet_common.defs.QuantScheme.post_training_tf_enhanced", false], [179, "aimet_common.defs.QuantScheme.post_training_tf_enhanced", false], [185, "aimet_common.defs.QuantScheme.post_training_tf_enhanced", false]], "prepare_model() (in module aimet_tensorflow.keras.model_preparer)": [[21, "aimet_tensorflow.keras.model_preparer.prepare_model", false]], "prepare_model() (in module aimet_torch.model_preparer)": [[172, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[236, "term-Pruning", true]], "ptq": [[236, "term-PTQ", true]], "pytorch": [[236, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [217, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[236, "term-QAT", true]], "qdo": [[236, "term-QDO", true]], "qualcomm innovation center": [[236, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [218, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[177, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [218, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[184, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[236, "term-Quantization", true]], "quantization simulation": [[236, "term-Quantization-Simulation", true]], "quantization-aware training": [[236, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx.quantsim)": [[10, "aimet_onnx.quantsim.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_tensorflow.keras.quantsim)": [[23, "aimet_tensorflow.keras.quantsim.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.quantsim)": [[179, "aimet_torch.quantsim.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[185, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantize (class in aimet_torch.quantization.affine)": [[161, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[164, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[165, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantize_lora_scale_with_fixed_range() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.quantize_lora_scale_with_fixed_range", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[158, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[159, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[160, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[162, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[159, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[160, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[147, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[148, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[149, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[150, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[151, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[152, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[153, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[154, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[155, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[156, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[157, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_tensorflow.keras.amp.quantizer_groups)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantscheme (class in aimet_common.defs)": [[10, "aimet_common.defs.QuantScheme", false], [23, "aimet_common.defs.QuantScheme", false], [179, "aimet_common.defs.QuantScheme", false], [185, "aimet_common.defs.QuantScheme", false]], "quantsim": [[236, "term-QuantSim", true]], "quic": [[236, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_tensorflow.keras.bn_reestimation)": [[15, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false], [220, "aimet_tensorflow.keras.bn_reestimation.reestimate_bn_stats", false]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[26, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [220, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "replace_lora_layers_with_quantizable_layers() (aimet_torch.peft method)": [[176, "aimet_torch.peft.replace_lora_layers_with_quantizable_layers", false]], "run_inference() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false]], "run_inference() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.run_inference", false]], "save_checkpoint() (aimet_torch.quantsim method)": [[179, "aimet_torch.quantsim.save_checkpoint", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[185, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_onnx.sequential_mse.seq_mse)": [[11, "aimet_onnx.sequential_mse.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[180, "aimet_torch.seq_mse.SeqMseParams", false], [235, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[186, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_adaround_params() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false]], "set_adaround_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_adaround_params", false]], "set_bitwidth_for_lora_adapters() (aimet_torch.peft.peftquantutils method)": [[176, "aimet_torch.peft.PeftQuantUtils.set_bitwidth_for_lora_adapters", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[166, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_onnx.quantsim)": [[8, "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[170, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[30, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_mixed_precision_params() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false]], "set_mixed_precision_params() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_mixed_precision_params", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [234, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [234, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[174, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [234, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quant_scheme_candidates() (aimet_onnx.auto_quant_v2.autoquantwithautomixedprecision method)": [[3, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false], [219, "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false]], "set_quant_scheme_candidates() (aimet_tensorflow.keras.auto_quant_v2.autoquantwithautomixedprecision method)": [[14, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false], [219, "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision.set_quant_scheme_candidates", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "spatialsvdparameters (class in aimet_tensorflow.keras.defs)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters", false]], "spatialsvdparameters.automodeparams (class in aimet_tensorflow.keras.defs)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters.AutoModeParams", false]], "spatialsvdparameters.manualmodeparams (class in aimet_tensorflow.keras.defs)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters.ManualModeParams", false]], "spatialsvdparameters.mode (class in aimet_tensorflow.keras.defs)": [[18, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false], [227, "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [232, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[174, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [182, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [232, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[236, "term-Target-Hardware-Accelerator", true]], "target runtime": [[236, "term-Target-Runtime", true]], "tensorflow": [[236, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [232, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_tensorflow.keras.amp.quantizer_groups.quantizergroup method)": [[13, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false], [232, "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[174, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [182, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [232, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "torchscript": [[236, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[169, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [217, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "track_lora_meta_data() (aimet_torch.peft method)": [[176, "aimet_torch.peft.track_lora_meta_data", false]], "training_range_learning_with_tf_enhanced_init (aimet_common.defs.quantscheme attribute)": [[10, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init", false], [23, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init", false], [179, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init", false], [185, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init", false]], "training_range_learning_with_tf_init (aimet_common.defs.quantscheme attribute)": [[10, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_init", false], [23, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_init", false], [179, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_init", false], [185, "aimet_common.defs.QuantScheme.training_range_learning_with_tf_init", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[173, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[236, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[168, "aimet_torch.v2.visualization_tools.visualize_stats", false], [216, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[236, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[30, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_common.defs": [[232, 0, 1, "id1", "CallbackFunc"], [227, 0, 1, "", "GreedySelectionParameters"], [185, 0, 1, "", "QuantScheme"]], "aimet_common.defs.QuantScheme": [[185, 1, 1, "", "post_training_percentile"], [185, 1, 1, "", "post_training_tf"], [185, 1, 1, "", "post_training_tf_enhanced"], [185, 1, 1, "", "training_range_learning_with_tf_enhanced_init"], [185, 1, 1, "", "training_range_learning_with_tf_init"]], "aimet_common.utils": [[218, 0, 1, "", "CallbackFunc"]], "aimet_onnx.adaround.adaround_weight": [[214, 0, 1, "", "AdaroundParameters"]], "aimet_onnx.adaround.adaround_weight.Adaround": [[214, 2, 1, "", "apply_adaround"]], "aimet_onnx.amp.mixed_precision_algo": [[232, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[232, 3, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[232, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[232, 3, 1, "", "get_activation_quantizers"], [232, 3, 1, "", "get_active_quantizers"], [232, 3, 1, "", "get_candidate"], [232, 3, 1, "", "get_param_quantizers"], [232, 3, 1, "", "set_quantizers_to_candidate"], [232, 3, 1, "", "to_list"]], "aimet_onnx.auto_quant_v2": [[219, 0, 1, "", "AutoQuantWithAutoMixedPrecision"]], "aimet_onnx.auto_quant_v2.AutoQuantWithAutoMixedPrecision": [[219, 3, 1, "", "get_quant_scheme_candidates"], [219, 3, 1, "", "optimize"], [219, 3, 1, "", "run_inference"], [219, 3, 1, "", "set_adaround_params"], [219, 3, 1, "", "set_mixed_precision_params"], [219, 3, 1, "", "set_quant_scheme_candidates"]], "aimet_onnx.batch_norm_fold": [[221, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.cross_layer_equalization": [[222, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[217, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[217, 3, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[232, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[218, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[218, 3, 1, "", "analyze"], [218, 3, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[10, 0, 1, "", "QuantizationSimModel"], [8, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_onnx.quantsim.QuantizationSimModel": [[10, 3, 1, "", "compute_encodings"], [10, 3, 1, "", "export"]], "aimet_onnx.sequential_mse.seq_mse": [[11, 0, 1, "", "SeqMseParams"]], "aimet_onnx.sequential_mse.seq_mse.SequentialMse": [[11, 2, 1, "", "apply_seq_mse"]], "aimet_tensorflow.keras.adaround_weight": [[214, 0, 1, "", "AdaroundParameters"]], "aimet_tensorflow.keras.adaround_weight.Adaround": [[214, 2, 1, "", "apply_adaround"]], "aimet_tensorflow.keras.amp.quantizer_groups": [[232, 0, 1, "", "QuantizerGroup"]], "aimet_tensorflow.keras.amp.quantizer_groups.QuantizerGroup": [[232, 3, 1, "", "get_active_param_quantizers"], [232, 3, 1, "", "get_active_quantizers"], [232, 3, 1, "", "get_candidate"], [232, 3, 1, "", "lookup_quantizer"], [232, 3, 1, "", "set_quantizers_to_candidate"], [232, 3, 1, "", "to_list"]], "aimet_tensorflow.keras.auto_quant_v2": [[219, 0, 1, "", "AutoQuantWithAutoMixedPrecision"]], "aimet_tensorflow.keras.auto_quant_v2.AutoQuantWithAutoMixedPrecision": [[219, 3, 1, "", "get_quant_scheme_candidates"], [219, 3, 1, "", "optimize"], [219, 3, 1, "", "run_inference"], [219, 3, 1, "", "set_adaround_params"], [219, 3, 1, "", "set_mixed_precision_params"], [219, 3, 1, "", "set_quant_scheme_candidates"]], "aimet_tensorflow.keras.batch_norm_fold": [[221, 2, 1, "", "fold_all_batch_norms"], [220, 2, 1, "", "fold_all_batch_norms_to_scale"]], "aimet_tensorflow.keras.bn_reestimation": [[220, 2, 1, "", "reestimate_bn_stats"]], "aimet_tensorflow.keras.compress": [[227, 0, 1, "", "ModelCompressor"]], "aimet_tensorflow.keras.compress.ModelCompressor": [[227, 3, 1, "", "compress_model"]], "aimet_tensorflow.keras.cross_layer_equalization": [[222, 2, 1, "", "equalize_model"]], "aimet_tensorflow.keras.defs": [[227, 0, 1, "", "SpatialSvdParameters"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters": [[227, 0, 1, "", "AutoModeParams"], [227, 0, 1, "", "ManualModeParams"], [227, 0, 1, "", "Mode"]], "aimet_tensorflow.keras.defs.SpatialSvdParameters.Mode": [[227, 1, 1, "", "auto"], [227, 1, 1, "", "manual"]], "aimet_tensorflow.keras.layer_output_utils": [[217, 0, 1, "", "LayerOutputUtil"]], "aimet_tensorflow.keras.layer_output_utils.LayerOutputUtil": [[217, 3, 1, "", "generate_layer_outputs"]], "aimet_tensorflow.keras.mixed_precision": [[232, 2, 1, "", "choose_fast_mixed_precision"], [232, 2, 1, "", "choose_mixed_precision"]], "aimet_tensorflow.keras.model_preparer": [[21, 2, 1, "", "prepare_model"]], "aimet_tensorflow.keras.quantsim": [[23, 0, 1, "", "QuantizationSimModel"]], "aimet_tensorflow.keras.quantsim.QuantizationSimModel": [[23, 3, 1, "", "compute_encodings"], [23, 3, 1, "", "export"]], "aimet_torch.adaround.adaround_weight": [[214, 0, 1, "", "AdaroundParameters"]], "aimet_torch.adaround.adaround_weight.Adaround": [[214, 2, 1, "", "apply_adaround"]], "aimet_torch.amp.mixed_precision_algo": [[232, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[232, 3, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[232, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[232, 3, 1, "", "get_active_quantizers"], [232, 3, 1, "", "get_candidate"], [232, 3, 1, "", "get_input_quantizer_modules"], [232, 3, 1, "", "set_quantizers_to_candidate"], [232, 3, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[221, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[220, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.cross_layer_equalization": [[222, 2, 1, "", "equalize_model"]], "aimet_torch.layer_output_utils": [[217, 0, 1, "", "LayerOutputUtil"], [217, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[217, 3, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[217, 1, 1, "", "ONNX"], [217, 1, 1, "", "PYTORCH"], [217, 1, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[232, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[172, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[173, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[173, 3, 1, "", "add_check"], [173, 3, 1, "", "validate_model"]], "aimet_torch.nn": [[30, 0, 1, "", "QuantizationMixin"], [31, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [32, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [33, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [34, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [35, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [36, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [37, 0, 1, "", "QuantizedAlphaDropout"], [38, 0, 1, "", "QuantizedAvgPool1d"], [39, 0, 1, "", "QuantizedAvgPool2d"], [40, 0, 1, "", "QuantizedAvgPool3d"], [41, 0, 1, "", "QuantizedBCELoss"], [42, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [43, 0, 1, "", "QuantizedBatchNorm1d"], [44, 0, 1, "", "QuantizedBatchNorm2d"], [45, 0, 1, "", "QuantizedBatchNorm3d"], [46, 0, 1, "", "QuantizedBilinear"], [47, 0, 1, "", "QuantizedCELU"], [48, 0, 1, "", "QuantizedCTCLoss"], [49, 0, 1, "", "QuantizedChannelShuffle"], [50, 0, 1, "", "QuantizedCircularPad1d"], [51, 0, 1, "", "QuantizedCircularPad2d"], [52, 0, 1, "", "QuantizedCircularPad3d"], [53, 0, 1, "", "QuantizedConstantPad1d"], [54, 0, 1, "", "QuantizedConstantPad2d"], [55, 0, 1, "", "QuantizedConstantPad3d"], [56, 0, 1, "", "QuantizedConv1d"], [57, 0, 1, "", "QuantizedConv2d"], [58, 0, 1, "", "QuantizedConv3d"], [59, 0, 1, "", "QuantizedConvTranspose1d"], [60, 0, 1, "", "QuantizedConvTranspose2d"], [61, 0, 1, "", "QuantizedConvTranspose3d"], [62, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [63, 0, 1, "", "QuantizedCosineSimilarity"], [64, 0, 1, "", "QuantizedCrossEntropyLoss"], [65, 0, 1, "", "QuantizedDropout"], [66, 0, 1, "", "QuantizedDropout1d"], [67, 0, 1, "", "QuantizedDropout2d"], [68, 0, 1, "", "QuantizedDropout3d"], [69, 0, 1, "", "QuantizedELU"], [70, 0, 1, "", "QuantizedEmbedding"], [71, 0, 1, "", "QuantizedEmbeddingBag"], [72, 0, 1, "", "QuantizedFeatureAlphaDropout"], [73, 0, 1, "", "QuantizedFlatten"], [74, 0, 1, "", "QuantizedFold"], [75, 0, 1, "", "QuantizedFractionalMaxPool2d"], [76, 0, 1, "", "QuantizedFractionalMaxPool3d"], [77, 0, 1, "", "QuantizedGELU"], [78, 0, 1, "", "QuantizedGLU"], [79, 0, 1, "", "QuantizedGRU"], [80, 0, 1, "", "QuantizedGRUCell"], [81, 0, 1, "", "QuantizedGaussianNLLLoss"], [82, 0, 1, "", "QuantizedGroupNorm"], [83, 0, 1, "", "QuantizedHardshrink"], [84, 0, 1, "", "QuantizedHardsigmoid"], [85, 0, 1, "", "QuantizedHardswish"], [86, 0, 1, "", "QuantizedHardtanh"], [87, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [88, 0, 1, "", "QuantizedHuberLoss"], [89, 0, 1, "", "QuantizedInstanceNorm1d"], [90, 0, 1, "", "QuantizedInstanceNorm2d"], [91, 0, 1, "", "QuantizedInstanceNorm3d"], [92, 0, 1, "", "QuantizedKLDivLoss"], [93, 0, 1, "", "QuantizedL1Loss"], [94, 0, 1, "", "QuantizedLPPool1d"], [95, 0, 1, "", "QuantizedLPPool2d"], [96, 0, 1, "", "QuantizedLSTM"], [97, 0, 1, "", "QuantizedLSTMCell"], [98, 0, 1, "", "QuantizedLayerNorm"], [99, 0, 1, "", "QuantizedLeakyReLU"], [100, 0, 1, "", "QuantizedLinear"], [101, 0, 1, "", "QuantizedLocalResponseNorm"], [102, 0, 1, "", "QuantizedLogSigmoid"], [103, 0, 1, "", "QuantizedLogSoftmax"], [104, 0, 1, "", "QuantizedMSELoss"], [105, 0, 1, "", "QuantizedMarginRankingLoss"], [106, 0, 1, "", "QuantizedMaxPool1d"], [107, 0, 1, "", "QuantizedMaxPool2d"], [108, 0, 1, "", "QuantizedMaxPool3d"], [109, 0, 1, "", "QuantizedMaxUnpool1d"], [110, 0, 1, "", "QuantizedMaxUnpool2d"], [111, 0, 1, "", "QuantizedMaxUnpool3d"], [112, 0, 1, "", "QuantizedMish"], [113, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [114, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [115, 0, 1, "", "QuantizedMultiMarginLoss"], [116, 0, 1, "", "QuantizedNLLLoss"], [117, 0, 1, "", "QuantizedNLLLoss2d"], [118, 0, 1, "", "QuantizedPReLU"], [119, 0, 1, "", "QuantizedPairwiseDistance"], [120, 0, 1, "", "QuantizedPixelShuffle"], [121, 0, 1, "", "QuantizedPixelUnshuffle"], [122, 0, 1, "", "QuantizedPoissonNLLLoss"], [123, 0, 1, "", "QuantizedRNN"], [124, 0, 1, "", "QuantizedRNNCell"], [125, 0, 1, "", "QuantizedRReLU"], [126, 0, 1, "", "QuantizedReLU"], [127, 0, 1, "", "QuantizedReLU6"], [128, 0, 1, "", "QuantizedReflectionPad1d"], [129, 0, 1, "", "QuantizedReflectionPad2d"], [130, 0, 1, "", "QuantizedReflectionPad3d"], [131, 0, 1, "", "QuantizedReplicationPad1d"], [132, 0, 1, "", "QuantizedReplicationPad2d"], [133, 0, 1, "", "QuantizedReplicationPad3d"], [134, 0, 1, "", "QuantizedSELU"], [135, 0, 1, "", "QuantizedSiLU"], [136, 0, 1, "", "QuantizedSigmoid"], [137, 0, 1, "", "QuantizedSmoothL1Loss"], [138, 0, 1, "", "QuantizedSoftMarginLoss"], [139, 0, 1, "", "QuantizedSoftmax"], [140, 0, 1, "", "QuantizedSoftmax2d"], [141, 0, 1, "", "QuantizedSoftmin"], [142, 0, 1, "", "QuantizedSoftplus"], [143, 0, 1, "", "QuantizedSoftshrink"], [144, 0, 1, "", "QuantizedSoftsign"], [145, 0, 1, "", "QuantizedTanh"], [146, 0, 1, "", "QuantizedTanhshrink"], [147, 0, 1, "", "QuantizedThreshold"], [148, 0, 1, "", "QuantizedTripletMarginLoss"], [149, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [150, 0, 1, "", "QuantizedUnflatten"], [151, 0, 1, "", "QuantizedUnfold"], [152, 0, 1, "", "QuantizedUpsample"], [153, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [154, 0, 1, "", "QuantizedUpsamplingNearest2d"], [155, 0, 1, "", "QuantizedZeroPad1d"], [156, 0, 1, "", "QuantizedZeroPad2d"], [157, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[30, 3, 1, "", "compute_encodings"], [30, 3, 1, "", "forward"], [30, 3, 1, "", "from_module"], [30, 3, 1, "", "get_default_kernel"], [30, 3, 1, "", "get_kernel"], [30, 3, 1, "", "implements"], [30, 1, 1, "", "input_quantizers"], [30, 1, 1, "", "output_quantizers"], [30, 1, 1, "", "param_quantizers"], [30, 3, 1, "", "set_default_kernel"], [30, 3, 1, "", "set_kernel"], [30, 3, 1, "", "wrap"]], "aimet_torch.nn.QuantizedLinear": [[100, 3, 1, "", "forward"]], "aimet_torch.peft": [[176, 0, 1, "", "AdapterMetaData"], [176, 0, 1, "", "PeftQuantUtils"], [176, 3, 1, "", "replace_lora_layers_with_quantizable_layers"], [176, 3, 1, "", "track_lora_meta_data"]], "aimet_torch.peft.PeftQuantUtils": [[176, 3, 1, "", "disable_lora_adapters"], [176, 3, 1, "", "enable_adapter_and_load_weights"], [176, 3, 1, "", "export_adapter_weights"], [176, 3, 1, "", "freeze_base_model"], [176, 3, 1, "", "freeze_base_model_activation_quantizers"], [176, 3, 1, "", "freeze_base_model_param_quantizers"], [176, 3, 1, "", "get_fp_lora_layer"], [176, 3, 1, "", "get_quantized_lora_layer"], [176, 3, 1, "", "quantize_lora_scale_with_fixed_range"], [176, 3, 1, "", "set_bitwidth_for_lora_adapters"]], "aimet_torch.quant_analyzer": [[218, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[218, 3, 1, "", "analyze"], [218, 3, 1, "", "check_model_sensitivity_to_quantization"], [218, 3, 1, "", "export_per_layer_encoding_min_max_range"], [218, 3, 1, "", "export_per_layer_mse_loss"], [218, 3, 1, "", "export_per_layer_stats_histogram"], [218, 3, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [218, 3, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[158, 0, 1, "", "DequantizedTensor"], [159, 0, 1, "", "QuantizedTensor"], [160, 0, 1, "", "QuantizedTensorBase"]], "aimet_torch.quantization.DequantizedTensor": [[158, 3, 1, "", "dequantize"], [158, 3, 1, "", "quantize"], [158, 3, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[159, 3, 1, "", "dequantize"], [159, 3, 1, "", "quantize"], [159, 3, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[160, 3, 1, "", "clone"], [160, 3, 1, "", "dequantize"], [160, 3, 1, "", "detach"], [160, 3, 1, "", "new_empty"], [160, 3, 1, "", "quantize"], [160, 3, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[161, 0, 1, "", "Quantize"], [162, 0, 1, "", "QuantizeDequantize"], [163, 2, 1, "", "dequantize"], [164, 2, 1, "", "quantize"], [165, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[161, 3, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[162, 3, 1, "", "forward"]], "aimet_torch.quantization.float": [[166, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[166, 4, 1, "", "bitwidth"], [166, 3, 1, "", "compute_encodings"], [166, 3, 1, "", "forward"], [166, 3, 1, "", "get_encodings"], [166, 3, 1, "", "get_extra_state"], [166, 3, 1, "", "is_bfloat16"], [166, 3, 1, "", "is_float16"], [166, 3, 1, "", "load_state_dict"], [166, 3, 1, "", "set_extra_state"]], "aimet_torch.quantsim": [[179, 0, 1, "", "QuantizationSimModel"], [179, 3, 1, "", "load_checkpoint"], [179, 3, 1, "", "save_checkpoint"]], "aimet_torch.quantsim.QuantizationSimModel": [[179, 3, 1, "", "compute_encodings"], [179, 3, 1, "", "export"]], "aimet_torch.seq_mse": [[235, 0, 1, "", "SeqMseParams"], [235, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[235, 3, 1, "", "forward_fn"]], "aimet_torch.v1.adaround.adaround_weight": [[181, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[181, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[182, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[184, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[184, 3, 1, "", "analyze"], [184, 3, 1, "", "check_model_sensitivity_to_quantization"], [184, 3, 1, "", "export_per_layer_encoding_min_max_range"], [184, 3, 1, "", "export_per_layer_mse_loss"], [184, 3, 1, "", "export_per_layer_stats_histogram"], [184, 3, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [184, 3, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[185, 0, 1, "", "QuantizationSimModel"], [185, 3, 1, "", "load_checkpoint"], [185, 3, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[185, 3, 1, "", "compute_encodings"], [185, 3, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[186, 0, 1, "", "SeqMseParams"], [186, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[186, 3, 1, "", "forward_fn"]], "aimet_torch.v2.mixed_precision": [[234, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[234, 3, 1, "", "apply"], [234, 3, 1, "", "set_model_input_precision"], [234, 3, 1, "", "set_model_output_precision"], [234, 3, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[170, 2, 1, "", "set_activation_quantizers_to_float"], [170, 2, 1, "", "set_blockwise_quantization_for_weights"], [170, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[216, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "attribute", "Python attribute"], "2": ["py", "function", "Python function"], "3": ["py", "method", "Python method"], "4": ["py", "property", "Python property"]}, "objtypes": {"0": "py:class", "1": "py:attribute", "2": "py:function", "3": "py:method", "4": "py:property"}, "terms": {"": [1, 2, 8, 9, 10, 13, 21, 22, 23, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 166, 170, 172, 174, 175, 177, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 234, 235, 236, 241, 243, 244, 245, 246, 247, 249, 251, 252, 255], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 194, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 252, 253, 254, 255, 256], "00": [160, 164, 165, 203, 204, 205, 221, 222], "000": [214, 246], "0000": [158, 165, 178], "0000e": [160, 164, 165], "0004367042565718293": 245, "0004936049808748066": 245, "0006510374592799766": 245, "001": [188, 206, 220], "0011427081098743512": 245, "0014807": 239, "00183112e": 221, "00215936e": 221, "0030": 178, "0032": 178, "0034": 178, "00347728e": 222, "0035": 178, "0036": [162, 178], "0037": 178, "0038": 178, "0039": [162, 178], "0059": 178, "0063": 178, "0064": 178, "0068": 178, "0069": 178, "0073": 178, "0074": 178, "0078": 178, "008650330936207491": 245, "0089906234367221": 245, "00_224": [214, 221, 222], "01": [1, 12, 24, 164, 165, 181, 192, 194, 208, 214, 219, 221, 222], "010530316270887852": 245, "0115": 162, "0117": 178, "0142": 160, "01457286e": [221, 222], "0156": 178, "0158": 162, "0176": 162, "018501389771699905": 245, "0195": 178, "02": [164, 165, 221, 222], "02078857e": [221, 222], "0234": 178, "0235": 219, "026354755942277083": 239, "02635476": 239, "0273": 178, "0278355": 239, "02887694e": 222, "0293162": 239, "0295": 162, "03": [221, 222], "0312": 178, "0352": 178, "03798249e": 221, "0386": 162, "0391": 178, "04": [221, 222, 239, 242], "04025269e": 221, "0406616e": 222, "0424": 162, "0428": 220, "0430": 178, "0449": 242, "0469": 178, "0471": 162, "04721": 202, "05": [164, 165, 176, 221, 222], "0500e": [164, 165], "0508": 178, "05270951": 239, "0541903": 239, "0545e": 160, "0549": 162, "05546144023537636": 245, "05549544e": [221, 222], "05589814856648445": 245, "0564": 162, "06268782913684845": 245, "06318144500255585": 245, "0639": 162, "0667": 165, "0680": 162, "0769": 242, "0784": 162, "07906426": 239, "08": [164, 165], "080545": 239, "0819": 162, "0820258": 239, "08268175274133682": 245, "08333279937505722": 245, "0859": 166, "0861": 242, "08742931e": [221, 222], "0882": 160, "0889": 166, "0891": 166, "09": 232, "09111059e": 221, "0947": 166, "0949e": 160, "09685047e": [221, 222], "0x7f127685a598": 173, "0x7f9dd9bd90d0": 173, "0x7ff5703eff28": 173, "0x7ff577373598": 173, "1": [1, 2, 3, 10, 11, 13, 14, 18, 21, 22, 23, 24, 25, 29, 30, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 196, 223, 225, 226, 227, 229, 230, 234, 239, 240, 241, 242, 243, 244, 246, 247, 256], "10": [18, 21, 29, 30, 158, 161, 162, 164, 172, 173, 175, 176, 178, 179, 185, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 209, 211, 212, 214, 220, 223, 225, 226, 227, 229, 232, 239, 240, 241, 242, 243, 244, 247, 249, 251], "100": [2, 13, 15, 26, 171, 182, 195, 209, 220, 232, 236, 239, 242], "1000": [9, 22, 160, 177, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 223, 227, 229, 244], "10000": [12, 188, 189, 193, 201, 206, 207, 214], "1000e": [164, 165], "102": [178, 242], "1020304188132286": 245, "1024": [196, 214, 232, 244], "103": [161, 178], "10380396991968155": 245, "104": 242, "105": 242, "1050": 242, "10541902": 239, "10569119e": [221, 222], "106": 161, "1060": 162, "1068997": 239, "107": 178, "10708203e": [221, 222], "10788747668266296": 245, "108": 178, "109": 161, "109158515930176": 245, "10k": [1, 24, 181, 214], "11": [159, 160, 164, 176, 178, 205, 239, 240, 241, 245, 251], "110": [161, 178], "111": [161, 178], "11176670e": 222, "112": [214, 221, 222], "1128": 162, "114": 245, "1155": 242, "116": 242, "1176": 162, "118": 178, "119": [178, 242], "12": [164, 178, 239, 240, 241, 242, 243, 245], "12039044e": 222, "121": 242, "122": 161, "1232": 162, "126": 245, "12636379897594452": 245, "127": [159, 160, 178, 179, 242, 244, 245, 247], "128": [2, 3, 14, 25, 159, 160, 170, 172, 174, 178, 179, 182, 183, 196, 198, 199, 214, 219, 221, 222, 232, 239, 242, 243, 244, 247], "129": 161, "13": [164, 178, 242, 256], "1307": 162, "131": 161, "13177378": 239, "1333": 165, "1398": 226, "14": [164, 178, 242, 251, 256], "1406": 226, "141": 161, "143": 161, "144": 161, "145": 161, "1451239287853241": 245, "1458": 242, "146": 161, "1462666392326355": 245, "15": [164, 165, 178, 195, 196, 198, 199, 209, 211, 212, 220, 226, 247, 251], "150": 161, "1500e": [164, 165], "152": 161, "15259957e": 222, "153": 159, "155": 161, "15717569e": 221, "15812853": 239, "15e": [203, 205], "15k": [1, 24, 181, 214], "16": [2, 10, 13, 23, 30, 161, 166, 172, 174, 175, 176, 178, 179, 182, 185, 188, 192, 195, 206, 210, 214, 219, 227, 232, 239, 242, 243, 244, 245, 247, 255, 256], "1619": 242, "162": 161, "16245179e": [221, 222], "1647": 162, "16839484e": 222, "16966406e": 222, "17": [161, 178, 222, 256], "1709": 162, "172": 161, "1727": 166, "1729": 166, "1741": 162, "178": 161, "17871511e": 222, "179": 161, "18": [160, 178, 188, 189, 190, 191, 242, 256], "181": 161, "18136823e": 221, "18448329": 239, "184721499681473": 245, "186": 161, "18673885e": 221, "187": 161, "188": 161, "1889": 162, "19": [161, 178, 221, 222, 256], "1906": 202, "19186290e": [221, 222], "192": 161, "1921e": [164, 165], "194": 161, "1943": 226, "1945": 160, "1955": 226, "1977": 162, "19778645e": 221, "1_all": 241, "1e": [21, 176, 196, 197, 220, 221, 222, 244, 247], "1k": [214, 219, 220, 232, 235, 244], "1m": [189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 196, 215, 216, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 249, 250, 253, 254, 256], "20": [1, 11, 12, 21, 24, 160, 180, 181, 186, 194, 195, 196, 197, 198, 199, 209, 211, 212, 214, 220, 221, 222, 227, 235, 245, 247, 256], "200": [21, 171, 196, 197, 208], "2000": [1, 24, 159, 165, 181, 189, 193, 194, 201, 207, 208, 210, 214], "20000": [21, 196, 197], "2000e": [164, 165], "2012": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "2014": 226, "2016": 226, "2017": 226, "2019": [190, 202, 210], "203": 161, "20433941e": [221, 222], "2048": [214, 219, 220, 247], "205": 159, "2068": 242, "207": 161, "20k": 196, "21": [160, 221, 222, 245, 256], "21066449e": 221, "21083805": 239, "2118": 162, "2123188": 239, "21250950e": 221, "2137995": 239, "216": 161, "218": 161, "2196": 162, "22": [178, 221, 222, 239, 242, 256], "2205": 160, "2212": 162, "2217": 242, "22219264e": [221, 222], "224": [179, 188, 189, 190, 191, 192, 195, 198, 199, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 227, 232, 234, 235, 242, 244, 247], "225": [208, 214, 219, 232, 244], "22583652e": 221, "226": 161, "2260": 160, "2265": 162, "229": [208, 214, 219, 232, 244], "23": [159, 160, 256], "23011987e": [221, 222], "2315": 242, "23156421e": [221, 222], "2353": 162, "2355": 162, "2363": [162, 242], "237": 161, "23719281": 239, "238": 161, "24": 256, "240": 178, "2401543": 239, "241": 178, "242": 178, "24257803e": 221, "243": 178, "2431": 242, "244": 178, "245": 178, "2458": 162, "2459": 242, "246": 178, "247": 178, "248": 178, "249": 178, "25": [178, 256], "250": 178, "2500e": [164, 165], "251": 178, "252": 178, "253": 178, "254": 178, "2546": 162, "255": [158, 161, 178, 179, 244, 247], "256": [9, 178, 192, 206, 214, 218, 219, 232, 244], "2568": 162, "2592": 173, "26": [159, 178, 242, 256], "26179108e": 221, "26354757": 239, "2650282": 239, "2667": 165, "27": [242, 256], "27415752e": 222, "2771": 162, "28": [159, 160, 201, 223, 227, 229, 242, 256], "28238320e": 222, "288": [214, 222], "28990233": 239, "29": [178, 256], "291383": 239, "2921": 160, "2930528e": 222, "29590677e": [221, 222], "2b": 224, "2d": [201, 223, 226, 230, 243], "3": [2, 3, 10, 13, 14, 23, 25, 29, 158, 160, 164, 165, 169, 171, 172, 173, 174, 176, 178, 179, 180, 182, 183, 185, 186, 196, 217, 222, 223, 224, 227, 229, 230, 234, 239, 240, 241, 242, 243, 244, 247], "30": [160, 161, 178, 242, 256], "300": 206, "3000": [159, 196], "30122258e": [221, 222], "3038": 162, "30402938e": 222, "31": [1, 9, 12, 22, 24, 177, 179, 181, 184, 185, 214, 218, 242, 244, 247, 256], "31080866e": 221, "312": 159, "3137": 162, "31625706": 239, "32": [21, 161, 172, 173, 178, 179, 185, 188, 189, 190, 192, 193, 194, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 218, 219, 220, 221, 222, 232, 235, 236, 242, 244, 245, 247, 249, 252, 256], "32141271e": 221, "3216": 162, "33": [161, 203, 204, 205, 242, 256], "3333": 165, "33731920e": [221, 222], "34": 256, "34215236e": 222, "34261182": 239, "3451": 162, "3467390e": 222, "34694423e": 222, "347054": 239, "3470540046691895": 239, "35": [179, 242, 244, 247, 256], "35139937e": 222, "36": 161, "3610": 242, "36896658": 239, "37": [160, 161, 178], "3734087606114667": 239, "37757687e": 222, "38": [159, 160, 226], "38100997e": [221, 222], "3861": 242, "39": 158, "39502389e": [221, 222], "3992": 162, "3d": 243, "4": [1, 2, 3, 8, 9, 10, 11, 12, 14, 22, 23, 24, 25, 159, 160, 164, 165, 170, 171, 172, 176, 177, 178, 179, 181, 183, 184, 185, 191, 196, 200, 203, 204, 205, 208, 213, 221, 222, 223, 225, 227, 229, 230, 240, 241, 242, 243, 244, 247, 251], "40": [159, 160], "4000": [158, 165], "406": [208, 214, 219, 232, 244], "41": [158, 160], "41059163e": 222, "4157": 162, "41715762e": [221, 222], "42": [178, 242], "42083430e": 221, "4216761": 239, "4231569": 239, "4246376": 239, "42477691e": 221, "43": [178, 242], "4392": 162, "44": 160, "44408584e": [221, 222], "44632760e": 222, "4475": 162, "44803086": 239, "4495116": 239, "4499": 242, "44993666e": 222, "4549": 162, "455": [239, 242], "456": [208, 214, 219, 232, 244], "4585028e": 222, "46642041e": 222, "4667": 165, "4694": 162, "4706": 162, "47438562": 239, "4758663": 239, "4784": 162, "48": [178, 242], "48045555e": 222, "48399768e": [221, 222], "4842e": 160, "485": [208, 214, 219, 232, 244], "4863": 162, "49": [161, 178, 242], "49024737e": [221, 222], "4d": 243, "4f": [219, 220, 244, 247], "5": [160, 161, 162, 164, 165, 166, 171, 172, 175, 176, 178, 189, 190, 191, 192, 193, 196, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212, 214, 220, 221, 222, 223, 224, 227, 229, 232, 239, 240, 241, 242, 243, 247], "50": [188, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 224, 227, 232, 242], "500": [189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 207, 208, 210, 211, 212, 213, 214, 218, 223, 232, 244, 246], "5000": [194, 208], "50000": [192, 219, 227, 232, 244], "5000e": [164, 165], "5006": 228, "50074035": 239, "5022211": 239, "51": 158, "51446089e": [221, 222], "51547194e": [221, 222], "5157": 242, "51876003e": 221, "52": [158, 242], "521": 158, "5220": 162, "5255": 162, "52709514": 239, "52912678e": [221, 222], "52974629e": 221, "5301": 242, "5307": 242, "5333": [162, 165], "53403008e": [221, 222], "53976866e": [221, 222], "54": 178, "54620111e": [221, 222], "55": [161, 178, 242], "55344987": 239, "5540": 162, "5549306": 239, "55578518e": 222, "558866932988167": 245, "56": 242, "56632766e": 221, "5695": 162, "57": [159, 160, 178], "57021021e": 222, "5765e": 160, "57980466": 239, "57984132e": [221, 222], "58": 242, "5856506e": 222, "5876": 220, "5897": 162, "59": [161, 178, 242], "5962": 242, "59643137e": 221, "5e": [204, 205, 209, 211, 212], "6": [10, 21, 23, 160, 164, 165, 172, 176, 178, 179, 185, 195, 196, 197, 198, 199, 205, 207, 210, 211, 212, 221, 222, 232, 240, 241, 243, 244, 247], "60": 178, "6000": [158, 159, 160, 165], "6039": 162, "6054": 162, "6061594": 239, "60713138e": [221, 222], "6079": 162, "6086959838867188": 245, "6091": 242, "61": 178, "61087702e": 221, "6177": 162, "6196": 162, "62": 178, "6213797e": 222, "6247": 162, "62498877e": [221, 222], "63": [161, 242], "63172388e": 221, "6325141": 239, "6327": 242, "6340": 242, "64": [8, 9, 21, 161, 170, 178, 179, 201, 214, 218, 219, 232, 239, 242, 243, 244, 247, 249], "6431": 162, "64579314e": [221, 222], "65": [224, 242], "65535": 242, "6583": 247, "6588689": 239, "66": [161, 203, 204, 205, 224, 242], "6603496": 239, "6667": 165, "6695": 162, "67278278e": [221, 222], "67416465e": 222, "67510852e": [221, 222], "67677957e": [221, 222], "68": [158, 242], "68016": 247, "68522364": 239, "6867044": 239, "69": 242, "6910": 247, "69716495e": 222, "7": [160, 164, 165, 166, 176, 178, 179, 204, 205, 207, 209, 210, 211, 212, 221, 222, 230, 240, 241, 244, 247, 251], "70": 178, "7013": 244, "70130579e": 222, "70838": 247, "71": 160, "7115784": 239, "7164": 219, "71659231e": [221, 222], "7173": 244, "72": 242, "72468403e": 222, "73242160e": 221, "7333": 165, "73793316": 239, "74": 161, "74478185e": 221, "7466": 242, "75": [224, 242], "75162792e": 222, "75700430e": [221, 222], "76": 178, "76428795": 239, "77": [178, 242], "77213307e": 221, "7741": 162, "7765": 162, "7788": 242, "7793": 242, "78": 242, "7894": 162, "79": 242, "7932": 162, "8": [1, 2, 3, 8, 9, 10, 13, 14, 22, 23, 24, 25, 30, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 227, 229, 230, 232, 234, 235, 236, 239, 240, 241, 242, 243, 244, 245, 247], "80": [161, 227, 242], "8000": [162, 165], "80053532e": 221, "8078": 162, "81": 242, "81699747": 239, "81760912e": 221, "8182e": 160, "8188": 242, "81884710e": 221, "8229": 162, "83": [161, 178, 242], "83640555e": 222, "8366": 242, "83861901e": [221, 222], "84": 178, "8405": 242, "8433522": 239, "8442": 242, "85": 242, "86": [161, 242], "8606": 242, "864": [214, 221, 222], "8667": 165, "86945379e": 221, "8706": 162, "8711877": 239, "87471542e": [221, 222], "87630311e": [221, 222], "87656835e": 221, "8796": 162, "88": 242, "88260233e": [221, 222], "8836": 162, "88373709e": 222, "89": [161, 242], "89074164e": 222, "89249423e": [221, 222], "89348674e": 221, "8984": 166, "8994": 166, "8998": 166, "8bit": 251, "9": [158, 164, 165, 172, 176, 178, 203, 204, 205, 206, 207, 210, 211, 212, 213, 220, 221, 222, 227, 232, 240, 241, 251], "90": [232, 242], "90229788e": [221, 222], "9054": 242, "91": [178, 242], "9150": 242, "9157": 162, "9176": 162, "92": 178, "9216": 172, "92511864e": [221, 222], "93": [178, 242], "93232973e": [221, 222], "9333": 165, "93787616e": 221, "94": [161, 178], "9487": 162, "94877124": 239, "9490": 162, "95": 178, "95088911e": [221, 222], "95260113e": 221, "9570": 178, "95997976e": [221, 222], "96": [178, 242], "9609": 178, "96155685e": 221, "9648": 178, "9688": 178, "97": 158, "9700": 162, "9714": 242, "9727": 178, "97294299e": [221, 222], "9766": 178, "9805": 178, "9826": 242, "9844": 178, "9883": 178, "99": 242, "9904": 242, "9922": 178, "9961": [162, 178], "A": [1, 2, 3, 4, 9, 10, 13, 14, 15, 16, 18, 22, 23, 24, 25, 27, 28, 29, 170, 174, 176, 177, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 225, 226, 227, 229, 232, 234, 236, 243, 244, 246, 247, 255], "And": [188, 201, 206], "As": [30, 188, 190, 192, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 209, 210, 211, 212, 214, 218, 219, 224, 225, 232, 240, 243, 246, 250], "At": [224, 228], "But": [172, 188, 195, 196, 200, 201, 206, 209, 213, 226, 234], "By": [14, 18, 21, 29, 160, 175, 176, 179, 195, 206, 207, 209, 210, 211, 212, 214, 219, 223, 226, 227, 229, 244, 246, 247], "For": [9, 10, 21, 22, 23, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 172, 173, 175, 176, 177, 178, 179, 184, 185, 187, 188, 192, 195, 196, 197, 200, 201, 206, 209, 213, 214, 217, 218, 223, 224, 225, 226, 232, 237, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 253, 255], "If": [1, 2, 3, 9, 10, 13, 14, 18, 21, 22, 23, 24, 25, 29, 30, 100, 160, 161, 162, 163, 164, 165, 166, 167, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 223, 224, 225, 227, 228, 229, 230, 232, 234, 235, 241, 243, 244, 245, 247, 248, 249, 250, 251, 252, 255], "In": [21, 30, 160, 171, 172, 173, 175, 179, 182, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 224, 225, 226, 228, 230, 232, 233, 241, 242, 243, 244, 245, 247, 248, 249, 250, 254, 255], "It": [1, 2, 7, 9, 10, 11, 21, 171, 172, 179, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 214, 215, 217, 218, 221, 222, 223, 225, 230, 232, 235, 241, 244, 245, 247, 251], "Its": 230, "NOT": [3, 14, 25, 183, 201, 219], "No": [173, 221], "Not": [9, 22, 176, 177, 184, 187, 188, 195, 200, 201, 209, 213, 218, 225, 247], "ONE": 239, "OR": 171, "Of": [198, 199, 201, 203, 204, 205, 211, 212, 243], "On": [188, 201, 206, 245], "One": [22, 23, 189, 190, 191, 192, 200, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 218, 244, 247], "Or": [1, 10, 23, 172, 185, 214, 217, 218, 226, 244, 247], "Such": 172, "That": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212, 230], "The": [1, 2, 5, 7, 8, 9, 10, 13, 15, 18, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 239, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 254, 255], "Then": [9, 10, 23, 177, 179, 184, 185, 211, 212, 218, 226, 244, 247, 255], "There": [169, 171, 173, 185, 188, 189, 192, 193, 195, 196, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 217, 247], "These": [1, 3, 10, 14, 23, 25, 171, 173, 175, 183, 185, 188, 190, 194, 196, 201, 206, 208, 210, 214, 218, 219, 223, 224, 232, 241, 244, 246, 247, 248, 252], "To": [2, 13, 21, 30, 171, 174, 175, 176, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 220, 221, 222, 225, 227, 228, 232, 234, 235, 237, 239, 240, 242, 244, 246, 248, 252, 255], "With": [158, 197, 200, 213, 239, 247], "_": [9, 21, 22, 30, 161, 162, 165, 168, 175, 177, 178, 179, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 232, 235, 237, 239, 243, 244, 245, 246, 247, 251, 254, 255], "__________________________________________________________________________________________________": [214, 221, 222], "__getitem__": 208, "__init__": [3, 14, 21, 25, 172, 173, 175, 183, 189, 197, 208, 214, 219, 232, 244, 249, 250, 251], "__iter__": [189, 214, 219, 232, 244], "__len__": [9, 189, 208, 218], "__next__": [189, 214, 219, 232, 244], "__quant_init__": [30, 175], "__torch_function__": 172, "__version__": 241, "_batch_index": 219, "_create_sampled_data_load": 208, "_current_iter": [214, 219, 232, 244], "_data": [214, 219, 232, 244], "_dataset": 208, "_default_forward_fn": [3, 14, 25, 183, 219], "_get_unlabled_data_load": 218, "_is_encoding_frozen": 171, "_iter": 189, "_max": 166, "_module_to_wrap": 171, "_not_specifi": [179, 244, 247], "_quantizationsimmodelinterfac": [177, 179, 182, 184, 185, 218], "_quantschemepair": [3, 14, 25, 183, 219], "_remove_input_quant": 171, "_remove_output_quant": 171, "_remove_param_quant": 171, "_step": [164, 165], "_torch_data_load": 189, "_unlabel": [214, 219, 232, 244], "ab": 202, "abil": 251, "abl": [9, 21, 25, 158, 159, 160, 172, 173, 183, 189, 190, 191, 193, 194, 195, 197, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 218, 219], "about": [158, 189, 190, 191, 192, 193, 194, 197, 198, 199, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 214, 222, 237, 242, 245, 255], "abov": [172, 174, 188, 192, 200, 201, 203, 204, 205, 206, 213, 218, 223, 226, 227, 228, 229, 230, 234, 240, 241, 243, 244, 246, 255], "absolut": [10, 23, 161, 162, 179, 185, 223, 227, 229, 232, 240, 244, 247], "absolute_path_to_workspac": [187, 240], "absorpt": [190, 202, 210], "abstract": [30, 160, 175], "acc": [194, 195, 196, 200, 218, 219], "acc_top1": 219, "acccuraci": 192, "acceler": [191, 196, 198, 199, 211, 212, 223, 226, 227, 229, 236, 251], "accept": [225, 232, 251, 252, 255], "access": [171, 196, 206, 207, 210, 211, 212], "accord": [169, 179, 217, 244, 247], "accordingli": 251, "account": [225, 251, 252], "accumul": [224, 226, 227, 229, 231], "accur": [196, 214, 221], "accuraci": [1, 2, 3, 13, 14, 18, 23, 24, 25, 29, 176, 181, 182, 183, 185, 195, 196, 200, 208, 209, 213, 214, 215, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 235, 236, 237, 243, 244, 246, 247, 251, 252, 255], "achiev": [178, 189, 193, 201, 207, 224, 233, 234, 236, 243, 255], "acronym": 237, "across": [172, 176, 218, 222, 231, 236, 239, 246], "act": [2, 9, 13, 22, 174, 177, 182, 184, 188, 218, 232], "act_bw": 210, "action": [218, 230], "activ": [2, 9, 10, 13, 16, 21, 22, 23, 168, 170, 172, 174, 175, 176, 177, 180, 182, 184, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 221, 222, 231, 232, 233, 234, 235, 236, 243, 244, 245, 246, 247, 248, 249, 250, 255], "activation_bitwidth": [214, 244, 245, 247], "activation_encod": 245, "activation_quant": [2, 232], "activations_pdf": [200, 213, 218], "actual": [188, 192, 195, 197, 206, 209, 215, 217, 218, 219, 243, 255], "acuraci": [192, 232], "ad": [10, 173, 176, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 244, 245, 248, 251], "ada_model": [189, 193, 201, 207], "ada_round_data": [193, 201], "ada_rounded_model": 214, "adam": [194, 195, 196, 198, 199, 200, 218], "adapater_name_to_meta_data": 176, "adapt": [24, 26, 176, 180, 181, 186, 187, 195, 201, 209, 218, 219, 220, 235, 236, 251], "adapter1": 176, "adapter1_weight": 176, "adapter_weights_path": 176, "adaptermetadata": 176, "adaptiveavgpool1d": 31, "adaptiveavgpool2d": 32, "adaptiveavgpool3d": 33, "adaptivemaxpool1d": 34, "adaptivemaxpool2d": 35, "adaptivemaxpool3d": 36, "adaptiveround": 251, "adaround": [3, 6, 14, 19, 25, 167, 171, 183, 187, 194, 208, 214, 218, 219, 236, 251, 252], "adaround_data_load": [208, 219], "adaround_dataset": 194, "adaround_dataset_s": [194, 208, 219], "adaround_param": [3, 14, 25, 183, 194, 208, 219], "adaround_weight": [1, 12, 24, 171, 181, 189, 193, 194, 201, 207, 208, 214, 219], "adarounded_model": 214, "adaroundparamet": [1, 3, 12, 14, 24, 25, 181, 183, 189, 193, 194, 201, 207, 208, 214, 219], "add": [21, 23, 172, 173, 175, 176, 185, 191, 195, 198, 199, 203, 204, 205, 211, 212, 230, 244, 245, 246, 247, 248, 249, 250, 251], "add_check": 173, "addit": [30, 178, 179, 185, 188, 195, 196, 197, 200, 201, 206, 209, 213, 220, 243, 244, 245, 247, 248, 251, 254, 255], "addition": 243, "address": [228, 252], "adher": [244, 255], "adjac": [188, 189, 190, 191, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212, 221, 222, 231, 236, 248], "adjust": [10, 23, 168, 179, 185, 202, 210, 214, 216, 223, 224, 231, 243, 244, 247, 252], "admin": 187, "advanc": [178, 236, 251], "advantag": 235, "advis": 221, "affect": [1, 24, 181, 214, 236, 243, 246, 248], "affin": [8, 30, 158, 159, 160, 161, 162, 163, 164, 165, 170, 175, 176, 221, 222, 242, 243], "affine_q": 171, "affine_qdq": 171, "affine_quant": 171, "affinequant": [166, 171], "after": [9, 22, 166, 172, 173, 175, 177, 178, 179, 184, 185, 187, 188, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 218, 219, 220, 221, 222, 224, 226, 228, 236, 247, 251, 252], "again": [190, 201, 202, 203, 204, 205, 210, 218, 232, 247], "against": [9, 14, 22, 177, 184, 198, 199, 201, 209, 211, 212, 218, 219, 223], "aggress": 236, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256], "aim": [195, 203, 204, 205, 209, 221], "aimet": [6, 18, 19, 21, 29, 30, 171, 172, 173, 175, 176, 178, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 234, 235, 236, 239, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255], "aimet_cl": 202, "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "aimet_common_def": 227, "aimet_export_artifact": 217, "aimet_exported_model": 254, "aimet_exported_model_path": 254, "aimet_onnx": [188, 189, 190, 191, 214, 217, 218, 219, 221, 222, 232, 239, 241, 244], "aimet_spatial_svd": 227, "aimet_tensorflow": [192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 214, 217, 218, 219, 220, 221, 222, 227, 232, 239, 241, 244, 247], "aimet_tensorflow_def": 227, "aimet_torch": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 188, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 239, 240, 241, 242, 243, 244, 247, 251], "aimet_vari": 240, "algo": [182, 232], "algorithm": [2, 13, 18, 29, 182, 223, 224, 225, 226, 227, 229, 230, 245, 252], "aliasbackward0": [158, 159, 160, 161, 162, 178, 242], "align": [244, 248, 251, 255], "all": [1, 2, 4, 8, 9, 10, 13, 15, 16, 21, 22, 23, 27, 30, 100, 167, 168, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 216, 218, 220, 221, 222, 223, 225, 226, 227, 229, 231, 232, 234, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252], "all_q_modul": 171, "all_quant_wrapp": 171, "allclos": 172, "alloc": 160, "allow": [2, 3, 13, 14, 18, 21, 23, 25, 29, 158, 159, 160, 170, 172, 182, 183, 185, 188, 192, 194, 196, 206, 208, 215, 217, 219, 223, 226, 227, 228, 229, 231, 232, 233, 243, 244, 245, 246, 247, 250, 251, 252], "allow_custom_downsample_op": [29, 203, 205, 223], "allow_overwrit": [171, 214, 244, 247], "allowed_accuracy_drop": [2, 3, 13, 14, 25, 182, 183, 188, 192, 194, 206, 208, 219, 232], "alon": [176, 246], "along": [159, 176, 178, 232, 243, 246], "alpha": [176, 232], "alphadropout": 37, "alreadi": [28, 30, 178, 179, 185, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 222, 240, 244, 247, 252], "also": [1, 12, 14, 23, 24, 160, 172, 178, 179, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 215, 218, 219, 220, 221, 223, 226, 230, 232, 234, 239, 241, 243, 244, 245, 246, 247, 248, 250, 251, 254, 255], "alter": [189, 193, 201, 207], "altern": [9, 177, 184, 188, 192, 203, 204, 205, 206, 218, 241, 243], "alwai": 225, "am": [9, 22, 177, 184, 218], "among": [1, 3, 14, 24, 25, 181, 183, 214, 219], "amongst": 217, "amount": [200, 213, 232, 235], "amp": [2, 3, 13, 14, 25, 167, 174, 182, 183, 187, 219, 232, 233, 251], "amp_search_algo": [2, 13, 182, 188, 206, 232], "ampsearchalgo": [2, 13, 182, 188, 206, 232], "an": [1, 2, 3, 9, 10, 13, 14, 18, 21, 23, 25, 29, 30, 158, 159, 160, 166, 168, 170, 171, 172, 173, 175, 178, 179, 182, 183, 185, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 223, 224, 225, 226, 227, 229, 230, 232, 236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255], "analys": [200, 213, 218], "analysi": [2, 9, 13, 18, 22, 29, 177, 182, 184, 188, 192, 196, 203, 204, 205, 206, 223, 226, 227, 229, 251], "analyt": 228, "analyz": [9, 18, 22, 29, 161, 162, 175, 177, 184, 187, 194, 208, 219, 223, 227, 228, 229, 231, 234, 239, 251], "anchor": [148, 149], "andrea": 226, "andrei": 226, "andrew": 226, "ani": [2, 13, 18, 21, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 172, 173, 174, 179, 181, 182, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 223, 227, 229, 232, 234, 235, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 255], "anneal": [1, 12, 24, 181, 214], "anoth": [176, 179, 185, 196, 229, 230, 236, 244, 247, 248], "any_nam": 240, "any_tag": 240, "anyth": [2, 13, 182, 188, 192, 200, 206, 213, 232], "api": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 168, 169, 170, 171, 174, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 228, 236, 240, 245, 249, 250, 251, 254], "appear": [18, 29, 172, 173, 223, 227, 229], "append": 240, "appli": [2, 3, 8, 9, 14, 17, 25, 30, 100, 161, 162, 163, 164, 165, 170, 173, 174, 175, 180, 183, 186, 187, 188, 195, 196, 203, 204, 205, 206, 208, 209, 214, 218, 219, 220, 221, 222, 226, 228, 231, 232, 235, 236, 237, 239, 243, 244, 246, 247, 248, 251, 252], "applic": [2, 13, 182, 188, 189, 192, 193, 194, 198, 199, 200, 201, 202, 206, 207, 210, 214, 218, 219, 220, 221, 222, 227, 232, 244, 245, 247, 254, 255], "apply_adaround": [1, 12, 24, 181, 189, 193, 201, 207, 214], "apply_seq_algo": 11, "apply_seq_ms": [11, 171, 180, 186, 235], "approach": [174, 234, 246, 255], "appropri": [24, 175, 179, 181, 182, 185, 188, 192, 195, 196, 200, 201, 206, 209, 213, 214, 225, 232, 239, 243, 244, 246, 247, 252], "approx": 246, "approxim": [9, 218, 224, 235, 246], "apt": [239, 241], "ar": [1, 2, 9, 10, 11, 12, 13, 18, 21, 22, 23, 24, 25, 29, 30, 100, 161, 162, 166, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 254, 255], "arang": [164, 165, 172, 178], "arbitrari": 243, "architectur": [197, 236, 239, 241, 251], "archiv": 241, "area": [218, 231], "arg": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 170, 174, 175, 179, 197, 232, 234, 243, 244, 247, 251], "argmax": [214, 219, 232, 244, 247], "argument": [1, 2, 10, 13, 14, 23, 24, 25, 30, 160, 166, 170, 172, 173, 174, 177, 179, 180, 181, 182, 183, 184, 185, 186, 194, 195, 200, 213, 214, 217, 218, 219, 232, 235, 243, 244, 247, 254], "around": [196, 201, 218, 220], "arrai": [170, 189, 243], "arrang": 228, "art": [188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213], "arthmet": [21, 197], "artifact": [10, 174, 196, 233, 234, 240, 244, 251, 254], "arxiv": 202, "asic": 236, "ask": [30, 188, 192, 195, 196, 200, 206, 209, 213], "assert": [171, 172, 178, 197, 206, 244], "assert_array_equ": 197, "assess": 225, "assign": [30, 161, 162, 166, 175, 176, 198, 199, 232, 240, 245], "assist": 243, "associ": [2, 3, 13, 14, 16, 25, 30, 160, 173, 174, 175, 178, 182, 183, 200, 213, 219, 221, 232, 254], "assum": [8, 14, 25, 170, 183, 219, 225, 241, 243, 245], "astyp": [218, 219], "asym": [11, 180, 186, 235], "asymmetr": [161, 162, 178, 246, 248], "att": [21, 197], "attempt": [10, 222, 244], "attent": [21, 196, 197], "attn_output": [21, 197], "attribut": [13, 21, 30, 171, 172, 174, 175, 176, 178, 182, 201, 218, 232], "augment": 228, "auto": [10, 18, 29, 188, 192, 203, 204, 205, 206, 223, 227, 229, 232, 233, 244, 245], "auto_param": [203, 204, 205, 223, 227, 229], "auto_qu": [194, 208, 219], "auto_quant_v2": [19, 219], "autograd": [158, 159, 160], "autom": [172, 206, 207, 210, 211, 212, 213, 236, 250], "automat": [3, 14, 18, 25, 29, 174, 183, 187, 194, 208, 218, 223, 224, 226, 227, 229, 236, 240, 241, 243, 251], "automodeparam": [18, 29, 203, 204, 205, 223, 227, 229], "autoqu": [3, 6, 14, 25, 167, 183, 187, 219, 236, 251], "autoquantwithautomixedprecis": [3, 14, 25, 183, 219], "avail": [11, 172, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 195, 200, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 218, 235, 239, 248, 251, 252, 254], "avgpool1d": 38, "avgpool2d": [39, 234], "avgpool3d": 40, "avoid": [9, 23, 177, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 214, 218, 221, 224, 236, 244, 247, 249, 250, 251], "awai": 201, "awar": [30, 187, 214, 220, 226, 231, 235, 236, 246, 252], "axi": [178, 200, 213, 214, 218, 219, 232, 244, 245, 246], "b": [30, 161, 162, 163, 164, 165, 176, 178, 192, 227, 232], "b_": [161, 162, 163, 164, 165], "b_0": [161, 162, 163, 164, 165], "b_1": [161, 162, 163, 164, 165, 243], "b_2": 243, "b_d": [161, 162, 163, 164, 165], "b_n": 243, "back": [2, 13, 158, 159, 174, 177, 182, 184, 218, 232, 246, 247, 248], "backend": [179, 195, 233, 244, 247, 254], "backpropag": [158, 159, 160], "backslash": 251, "backward": [167, 220, 221, 244, 245, 247, 251], "bad": 21, "balanc": [226, 236], "bandwidth": 224, "base": [2, 9, 10, 22, 23, 30, 160, 161, 162, 164, 165, 166, 171, 175, 176, 177, 178, 179, 182, 184, 185, 188, 189, 192, 193, 194, 195, 201, 206, 207, 208, 209, 218, 223, 224, 225, 228, 232, 233, 235, 239, 241, 243, 244, 247, 251, 254, 255], "base_encod": 176, "base_model": 176, "baselin": [2, 13, 182, 195, 196, 208, 225, 226, 232, 247], "bash": 240, "basi": [201, 243, 245, 251], "basic": [188, 192, 195, 196, 206, 209, 232, 244, 247, 251, 254], "batch": [1, 2, 3, 9, 11, 12, 14, 15, 16, 22, 24, 25, 26, 166, 174, 180, 181, 182, 183, 186, 192, 194, 195, 203, 204, 205, 209, 214, 218, 219, 222, 227, 232, 235, 236, 244, 247, 251], "batch_cntr": [188, 189, 190, 191, 193, 198, 199, 201, 202, 207, 209, 210, 211, 212, 213, 219], "batch_data": [214, 219, 232, 244], "batch_norm": [4, 15, 16, 27, 220, 221], "batch_norm_fold": [6, 15, 19, 167, 188, 189, 190, 191, 192, 193, 195, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 220, 221, 232, 247], "batch_siz": [1, 24, 181, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 227, 232, 235, 244, 247], "batchnorm": [3, 14, 16, 21, 25, 26, 183, 187, 188, 189, 190, 191, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212, 214, 219, 220, 221, 222, 230, 251], "batchnorm1d": [27, 43, 221], "batchnorm2d": [27, 44, 173, 221, 222], "batchnorm3d": 45, "batchnrom": 209, "bc": 190, "bc_param": 210, "bceloss": 41, "bcewithlogitsloss": 42, "becaus": [21, 172, 197, 209, 255], "becom": 251, "becuas": [21, 197, 217], "been": [21, 30, 158, 168, 173, 194, 195, 196, 201, 208, 216, 230, 236, 241, 242, 243, 246, 251, 254], "befor": [1, 2, 9, 10, 21, 30, 168, 171, 175, 178, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 216, 217, 218, 219, 220, 221, 222, 226, 232, 244, 246, 247, 249, 251, 255], "begin": [21, 161, 162, 163, 164, 165, 172, 173, 197, 237, 255], "behav": [30, 100, 175, 251, 252], "behavior": [171, 172, 175, 188, 201, 206, 207, 210, 211, 212, 221, 237, 246, 248, 255], "being": [13, 29, 171, 172, 173, 177, 184, 218, 220, 223, 227, 229, 232, 243, 245, 249, 250, 251], "below": [21, 30, 161, 162, 164, 165, 171, 174, 175, 176, 178, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 222, 223, 225, 226, 228, 232, 234, 239, 240, 241, 242, 243, 245, 246, 248, 249, 250, 252, 255], "benefici": [188, 192, 195, 196, 200, 201, 206, 213, 218], "benefit": [233, 243, 245, 246], "bert": 252, "best": [3, 14, 25, 174, 183, 194, 201, 208, 219, 224, 226, 231, 234, 239, 246], "beta": [1, 12, 24, 181, 214, 251], "better": [189, 193, 198, 199, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 219, 226, 243, 247, 255], "between": [1, 2, 9, 12, 18, 22, 24, 29, 169, 171, 174, 175, 177, 181, 182, 184, 188, 201, 206, 214, 215, 217, 218, 223, 227, 229, 231, 232, 235, 236, 243, 244, 246, 248, 255], "bfloat16": 166, "bia": [30, 166, 171, 172, 173, 175, 176, 179, 190, 195, 202, 221, 222, 223, 242, 244, 247, 248, 251], "bias": [188, 190, 202, 206, 210, 221], "bias_correct": 210, "bilinear": 46, "bin": [176, 240, 241, 254], "binari": [2, 13, 182, 188, 206, 232, 254], "binary_classifi": 21, "binary_fil": 254, "binary_file_nam": 254, "bit": [1, 2, 9, 13, 22, 24, 166, 170, 177, 178, 181, 182, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 218, 231, 233, 236, 239, 242, 243, 245, 246, 251, 252, 255], "bitop": [2, 13, 182, 232], "bitwidth": [1, 2, 3, 8, 9, 10, 12, 13, 14, 22, 23, 24, 25, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 188, 189, 192, 193, 201, 206, 207, 214, 218, 219, 222, 232, 233, 236, 239, 243, 244, 245, 246, 247], "biwidth": [189, 193, 201, 207], "blankevoort": 226, "block": [8, 25, 161, 162, 163, 164, 165, 170, 171, 183, 196, 219, 240, 246, 251], "block_group": [170, 243], "block_siz": [8, 161, 162, 163, 164, 165, 170, 178, 243], "blockwis": [8, 170, 178, 246], "bn": [4, 15, 16, 26, 27, 188, 189, 190, 191, 193, 195, 198, 199, 201, 202, 206, 207, 210, 211, 212, 218, 220, 221, 231, 236, 251], "bn1": [21, 173], "bn2": 21, "bn_conv1": [214, 221, 222], "bn_num_batch": [15, 220], "bn_re_estimation_dataset": [15, 220], "bn_reestim": [167, 195, 209, 220], "bnf": 214, "bokeh": 232, "bool": [1, 2, 3, 10, 13, 14, 18, 23, 25, 29, 160, 161, 162, 164, 165, 166, 170, 172, 173, 174, 176, 179, 182, 183, 185, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 219, 223, 227, 229, 232, 234, 243, 244, 247], "boolean": [30, 174, 234], "both": [21, 22, 164, 165, 171, 172, 175, 177, 184, 188, 192, 199, 205, 206, 211, 212, 218, 221, 226, 227, 228, 230, 232, 235, 239, 242, 243, 244, 245, 246, 247, 248, 252, 254, 255], "bottleneck": 252, "box": 251, "bq": [170, 243, 251], "branch": [172, 187, 248], "break": [188, 189, 190, 191, 192, 193, 198, 199, 201, 202, 207, 209, 210, 211, 212, 213, 219, 227, 232, 244, 247, 251], "bridg": 233, "british": 226, "broken": 245, "browser": 187, "bruteforc": [13, 188, 206, 232], "buffer": 166, "bug": [245, 251], "bugfix": 251, "build": [21, 171, 197, 241], "built": [2, 30, 174, 175, 182, 195, 196, 232, 239], "bw": [2, 8, 13, 25, 170, 174, 176, 179, 182, 183, 185, 219, 232, 243, 244, 247, 250], "bw_output": 250, "c": [178, 224, 241], "c_": 178, "cach": [2, 3, 13, 14, 25, 182, 183, 188, 192, 206, 219, 232, 241], "cache_id": [3, 14, 25, 183, 219], "calcul": [2, 3, 10, 13, 22, 23, 25, 175, 179, 182, 183, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 218, 219, 225, 232, 244, 246, 247, 251], "calibr": [9, 10, 22, 161, 162, 168, 175, 177, 179, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 215, 216, 232, 233, 235, 236, 242, 243, 246, 247, 251, 252], "calibration_batch": [232, 247], "calibration_data_load": [175, 244], "calibration_data_s": 244, "calibration_dataset": [214, 220, 244, 247], "calibration_dataset_s": [194, 208, 219], "call": [2, 3, 9, 11, 13, 14, 15, 21, 25, 30, 100, 158, 159, 160, 166, 172, 174, 175, 177, 178, 182, 183, 184, 189, 190, 191, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 210, 211, 212, 213, 218, 219, 220, 223, 227, 229, 232, 234, 243, 244, 245, 246, 249, 251, 254, 255], "call_funct": 172, "callabl": [1, 2, 3, 13, 14, 18, 24, 25, 26, 29, 30, 170, 172, 173, 174, 177, 179, 180, 181, 182, 183, 184, 186, 214, 218, 219, 220, 223, 227, 229, 232, 235, 243, 244, 247], "callback": [2, 9, 10, 13, 14, 18, 22, 23, 29, 174, 177, 182, 184, 185, 192, 194, 196, 198, 199, 200, 203, 204, 205, 213, 219, 223, 227, 229, 232, 235, 246, 247], "callbackfunc": [2, 9, 13, 22, 174, 177, 182, 184, 188, 192, 200, 206, 213, 218, 232], "callbak": [188, 206], "can": [1, 2, 7, 9, 10, 12, 13, 20, 21, 23, 24, 27, 28, 158, 160, 161, 162, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 239, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255], "candid": [2, 3, 11, 13, 14, 18, 25, 29, 174, 180, 182, 183, 186, 188, 192, 206, 219, 223, 225, 226, 227, 229, 232, 235], "cannot": [161, 162, 172, 173, 188, 189, 190, 191, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212], "capabl": [175, 228, 239, 242], "captur": [7, 20, 169, 172, 179, 189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213, 215, 217, 244, 247], "card": [239, 242], "care": 226, "carefulli": 236, "carri": [158, 159, 160], "case": [164, 165, 170, 171, 172, 173, 175, 176, 196, 197, 200, 213, 214, 218, 219, 220, 225, 234, 242, 243, 245, 248, 250, 251, 255], "cast": 166, "cat": [178, 241], "categor": [192, 193, 198, 199, 201, 214, 219, 220, 227, 232, 244, 247], "categorical_crossentropi": [198, 199], "categoricalaccuraci": [194, 195, 200, 214, 218, 219, 220, 244, 247], "categoricalcrossentropi": [194, 195, 200, 214, 218, 219, 220, 244, 247], "caus": [21, 203, 204, 205, 226, 248, 252], "caution": 214, "cd": 187, "cdot": [161, 162, 163, 164, 165], "ceil": [1, 24, 181, 214, 219, 232, 244], "cell": [188, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "celu": 47, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "center_crop": [192, 232], "centercrop": [206, 208, 214, 219, 232, 244], "certain": [172, 177, 184, 206, 207, 210, 211, 212, 213, 218, 226, 232, 234, 243, 248, 250, 251], "challeng": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "chang": [2, 13, 160, 168, 171, 172, 176, 178, 179, 182, 188, 189, 190, 191, 192, 195, 196, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 216, 218, 220, 221, 222, 226, 230, 232, 233, 234, 235, 244, 245, 247, 248, 250, 251], "channel": [8, 9, 18, 22, 29, 170, 175, 176, 177, 184, 187, 188, 189, 190, 191, 202, 204, 206, 207, 209, 210, 211, 212, 213, 218, 219, 220, 221, 222, 224, 225, 227, 229, 230, 231, 236, 243, 245, 246, 248, 251, 252], "channel_index": [9, 22, 177, 184, 218], "channel_index_0": [200, 213], "channel_index_1": [200, 213], "channel_index_n": [200, 213], "channel_prun": [29, 203, 205, 223, 227, 229], "channel_pruning_auto_mod": 223, "channel_pruning_manual_mod": 223, "channelpruningparamet": [29, 203, 205, 223, 227, 229], "channelshuffl": 49, "characterist": [188, 206, 209, 213], "chart": 252, "check": [21, 25, 172, 173, 178, 183, 188, 192, 195, 196, 200, 206, 209, 213, 215, 217, 219, 240, 250], "check_model_sensitivity_to_quant": [9, 22, 177, 184, 218], "checker": [173, 251], "checkpoint": [179, 180, 185, 186, 235], "checkpoints_config": [180, 186, 235], "choic": [200, 201, 213, 218, 226, 239, 240, 245, 246], "choos": [10, 185, 188, 192, 203, 204, 205, 206, 214, 223, 224, 226, 232, 236, 241, 243, 244, 248, 251], "choose_fast_mixed_precis": [13, 192, 232], "choose_mixed_precis": [2, 13, 174, 182, 188, 192, 206, 232], "chosen": [187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 240], "chunk": 246, "cin": 178, "circularpad1d": 50, "circularpad2d": 51, "circularpad3d": 52, "cl": [30, 190, 202, 210, 251], "clamp": [161, 162, 164, 165, 166, 246], "class": [1, 2, 3, 7, 9, 10, 11, 12, 13, 14, 18, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 166, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 221, 222, 223, 227, 229, 232, 234, 235, 243, 244, 247, 248, 249, 250], "class_nam": [192, 197, 227, 232], "classif": [21, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 226, 235, 236], "classifi": 21, "classmethod": 30, "cle": [5, 17, 28, 187, 214, 218, 219, 222, 251, 252], "cle_applied_model": [202, 222], "clean": 232, "clean_start": [2, 13, 182, 188, 192, 206, 232], "clear": 255, "clear_sess": 195, "clearli": 251, "clip": [195, 221, 246, 248], "clone": [160, 187, 196], "clone_lay": 196, "close": [224, 246], "closer": [189, 193, 201, 207], "cloud": [236, 240], "cmp_re": 232, "cnn": 236, "cnt": [192, 227, 232], "code": [188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 209, 210, 211, 212, 213, 214, 222, 239, 240, 242, 243], "codelinaro": 240, "collect": [3, 9, 22, 25, 168, 177, 183, 184, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 210, 211, 212, 215, 216, 218, 219, 225, 236, 246], "color": 230, "column": 225, "com": [187, 239, 240, 241], "combin": [2, 13, 172, 182, 188, 192, 206, 219, 224, 226, 232, 236, 240, 243, 251], "come": [226, 233, 243, 247], "command": [187, 228, 239, 240, 241, 242, 254], "common": [171, 176, 178, 188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 240, 241, 243, 251, 252], "commonli": [1, 24, 178, 181, 214, 236], "comp": [18, 29, 223, 227, 229], "comp_stat": [203, 204], "compar": [2, 13, 172, 182, 188, 192, 195, 196, 197, 200, 206, 209, 213, 218, 232, 233, 236, 243, 246, 247, 255], "comparison": [189, 190, 191, 207, 210, 211, 212, 215, 217], "compat": [167, 170, 192, 195, 196, 232, 239, 242, 243, 244, 245, 250, 251], "compil": [1, 10, 194, 195, 196, 198, 199, 200, 214, 218, 219, 220, 244, 247, 251, 255], "complet": [2, 13, 182, 188, 192, 203, 204, 205, 206, 227, 232, 241, 252, 254], "complex": [1, 10, 23, 185, 196, 214, 218, 234, 244, 247], "compli": [206, 207, 210, 211, 212, 213], "compon": [171, 178, 244, 251], "compos": [174, 206, 208, 214, 219, 232, 234, 242, 244], "compress": [19, 167, 187, 230, 236, 237, 251], "compress_model": [18, 29, 203, 204, 205, 223, 227, 228, 229], "compress_schem": [18, 29, 203, 204, 205, 223, 227, 229], "compressed_model": [203, 204, 223, 227, 229], "compressionschem": [18, 29, 203, 204, 205, 223, 227, 229], "compressionstat": [18, 29, 223, 227, 229], "compressor": [18, 29, 223, 227, 229], "compris": [188, 192, 206, 225], "compromis": [203, 204, 205], "compuat": [3, 14, 25, 183, 219], "comput": [1, 2, 3, 9, 10, 13, 14, 22, 23, 25, 29, 30, 100, 166, 176, 177, 179, 180, 182, 183, 184, 185, 186, 189, 190, 191, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 210, 211, 212, 213, 214, 217, 218, 219, 221, 223, 226, 227, 228, 229, 230, 232, 235, 236, 239, 241, 242, 243, 245, 246], "computation": 228, "compute_encod": [10, 23, 30, 159, 160, 161, 162, 166, 168, 171, 174, 175, 178, 179, 180, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 214, 216, 220, 232, 234, 235, 242, 244, 247], "computeencod": 239, "concat": 251, "concept": 171, "concret": 172, "concrete_arg": [25, 172, 183, 219], "condit": [172, 173], "confer": 226, "config": [2, 9, 22, 23, 174, 177, 179, 180, 182, 184, 185, 186, 188, 201, 206, 218, 220, 232, 234, 235, 244, 247, 248, 251], "config_fil": [3, 9, 10, 12, 14, 22, 23, 25, 177, 179, 183, 184, 185, 195, 200, 201, 213, 214, 218, 219, 220, 234, 235, 242, 244, 247], "config_util": [167, 243], "configr": 176, "configur": [1, 3, 9, 10, 12, 14, 18, 22, 24, 25, 29, 166, 174, 176, 177, 178, 181, 183, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 202, 206, 207, 209, 210, 211, 212, 214, 218, 219, 223, 227, 229, 234, 242, 243, 244, 245, 251], "confirm": 242, "conflict": [174, 234], "connect": [214, 221, 222, 223, 226, 229, 236], "connectedgraph": [173, 251], "consecut": [176, 190, 202, 210, 222], "consid": [196, 203, 205, 227, 229, 252, 255], "consist": [169, 170, 171, 188, 206, 217, 225, 232, 243, 246], "constant": [172, 191, 198, 211, 212, 225], "constantpad2d": [53, 54], "constantpad3d": 55, "constrain": [236, 243], "constraint": [243, 245], "construct": [217, 240, 241, 251], "constructor": [7, 10, 20, 169, 172, 185, 217, 244, 250], "consum": [194, 208, 219, 226, 233, 245], "contain": [2, 9, 13, 22, 29, 30, 158, 166, 168, 172, 173, 174, 175, 177, 182, 184, 189, 190, 191, 193, 194, 198, 199, 200, 202, 203, 204, 205, 207, 208, 210, 211, 212, 213, 216, 218, 222, 223, 227, 229, 232, 236, 237, 239, 242, 244, 245, 246, 248, 254], "container_nam": 240, "content": 236, "context": [30, 175, 254], "contigu": 251, "continu": [2, 21, 173, 179, 182, 185, 188, 206, 232, 240, 241, 247, 251, 252], "contrast": [21, 171, 201], "contribut": [218, 232, 239, 252], "control": [25, 172, 175, 178, 183, 219, 246], "conv": [1, 4, 8, 12, 15, 16, 24, 27, 170, 172, 181, 190, 195, 202, 210, 214, 220, 221, 222, 227, 229, 230, 234, 243, 248, 251], "conv1": [21, 172, 173, 179, 195, 203, 204, 205, 214, 221, 222, 223, 227, 229, 244, 247, 250], "conv1_relu": [214, 221, 222], "conv1d": [27, 56, 221, 251], "conv2": [21, 170, 172, 195, 223, 227, 229, 243, 245, 250], "conv2d": [16, 21, 27, 30, 57, 170, 171, 172, 173, 175, 179, 195, 196, 201, 214, 221, 222, 223, 226, 230, 243, 244, 245, 247, 251], "conv2d_1": 245, "conv2d_lay": 201, "conv2dnormactiv": [221, 222, 242], "conv2dtranspos": [16, 221], "conv3d": 58, "conv_1": [21, 197, 243], "conv_2": [21, 197], "conv_weight": 221, "conv_weight_arrai": 221, "conv_weight_nam": 221, "conveni": 240, "convent": [248, 251], "converg": [236, 247], "convers": [250, 251], "convert": [2, 13, 21, 158, 171, 172, 174, 179, 182, 192, 206, 214, 219, 221, 222, 236, 244, 247, 249, 254, 255], "convert_to_pb": [23, 196, 244, 247], "convinplacelinear": 176, "convolut": [21, 170, 188, 189, 190, 191, 193, 197, 198, 199, 201, 202, 203, 205, 206, 207, 210, 211, 212, 220, 221, 222, 223, 224, 226, 227, 229, 230, 231, 236, 243, 252], "convtranspose1d": [59, 251], "convtranspose2d": [27, 60, 221], "convtranspose3d": 61, "copi": [21, 23, 160, 166, 185, 187, 189, 206, 207, 210, 211, 212, 240, 244, 246, 247], "copy_": 171, "corp": 232, "correct": [14, 25, 183, 190, 201, 202, 208, 213, 219, 232, 234, 236, 244, 247], "correct_bia": 210, "correct_predict": [214, 219, 232, 244], "correctli": [242, 251], "correl": [188, 206, 232], "correspond": [1, 2, 4, 7, 9, 12, 13, 15, 16, 20, 22, 24, 27, 166, 169, 173, 174, 175, 176, 177, 179, 180, 181, 182, 184, 185, 186, 194, 201, 208, 214, 217, 218, 220, 221, 223, 230, 231, 232, 235, 239, 240, 241, 243, 244, 246, 247], "cosineembeddingloss": 62, "cosinesimilar": 63, "cost": [18, 29, 203, 205, 223, 225, 226, 227, 229, 231, 233, 246, 247], "cost_metr": [18, 29, 203, 204, 205, 223, 227, 229], "costmetr": [18, 29, 203, 204, 205, 223, 227, 229], "could": [1, 10, 23, 171, 185, 188, 189, 190, 191, 193, 195, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 230, 244, 245, 247, 250], "count_param": 197, "counter": [23, 185, 195, 209, 244, 247], "counterpart": [175, 243], "cours": [198, 199, 201, 203, 204, 205, 211, 212], "cout": 178, "cover": [188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 243, 246], "cp": [223, 224, 225, 230], "cp310": [239, 240, 241], "cp_comp_stat": 205, "cpu": [28, 160, 172, 179, 185, 188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 219, 220, 221, 222, 232, 234, 235, 236, 239, 240, 242, 244, 247, 251], "cpuexecutionprovid": [188, 189, 190, 191], "creat": [2, 3, 9, 10, 11, 13, 14, 18, 21, 23, 25, 29, 30, 166, 168, 172, 174, 175, 176, 178, 179, 182, 183, 185, 187, 194, 196, 200, 203, 204, 205, 208, 213, 214, 216, 217, 219, 220, 223, 226, 227, 229, 233, 234, 235, 236, 242, 244, 246, 247, 251, 254, 255], "create_quantsim_and_encod": [9, 218], "critic": 192, "crop_length": [192, 232], "cropped_imag": [192, 232], "cross": [3, 5, 14, 17, 25, 28, 183, 187, 214, 218, 219, 252], "cross_layer_equ": [6, 19, 167, 190, 202, 210, 222], "crossentropyloss": [64, 220, 247], "ctcloss": 48, "ctivations_pdf": [9, 22, 177, 184, 218], "cu118": [239, 241], "cu121": [239, 240, 241], "cuda": [1, 3, 7, 10, 188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 232, 234, 235, 239, 241, 242, 244, 247, 251], "cudaexecutionprovid": [188, 189, 190, 191], "cudnn": [239, 242], "cudnn_conv_algo_search": [188, 189, 190, 191], "culprit": 218, "cumul": [194, 208], "current": [13, 18, 21, 23, 29, 30, 160, 166, 173, 174, 182, 195, 196, 197, 206, 213, 223, 227, 229, 232, 233, 244, 247, 251, 255, 256], "curv": [2, 13, 182, 188, 192, 206, 225, 232], "custom": [1, 10, 21, 23, 30, 172, 175, 188, 194, 196, 208, 214, 236, 240, 244, 245, 246, 247, 251, 252, 255], "custom_function_not_to_be_trac": 172, "custom_object": [23, 244, 247], "customdataload": [214, 219, 232, 244], "custommodel": 172, "custommodul": 172, "d": [161, 162, 163, 164, 165, 214], "dangl": [244, 247], "dark": [188, 189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213], "data": [1, 2, 3, 9, 10, 11, 12, 13, 18, 22, 23, 24, 25, 26, 29, 158, 159, 160, 168, 171, 172, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 223, 227, 229, 231, 232, 235, 236, 243, 244, 246, 247, 251, 252], "data_load": [1, 2, 3, 11, 24, 25, 29, 168, 174, 180, 181, 182, 183, 186, 188, 189, 190, 191, 193, 194, 195, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 219, 220, 223, 232, 235, 244, 247], "data_loader_wrapp": [13, 192, 232], "data_set": [12, 193, 201, 214], "data_typ": [25, 171, 179, 183, 185, 219, 244, 247], "datacent": 240, "dataload": [2, 3, 13, 24, 25, 26, 174, 180, 181, 182, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 232, 235, 244, 247], "dataloader_wrapp": [192, 232], "dataloadermnist": 223, "dataparallel": 247, "dataset": [2, 3, 9, 13, 14, 15, 22, 25, 26, 177, 179, 182, 183, 184, 187, 196, 214, 218, 219, 220, 227, 232, 235, 242, 244, 246, 247, 255], "dataset_dir": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 227, 232], "dataset_train": [198, 199], "dataset_valid": [198, 199], "datasetv2": [12, 14, 15, 22, 214, 218, 219, 220], "datatyp": [3, 14, 25, 183, 219, 245], "de": [178, 246], "deb": 241, "debian": [239, 241], "debug": [171, 192, 215, 217, 245, 251], "decai": 226, "decid": [180, 186, 188, 189, 190, 191, 192, 193, 201, 203, 204, 205, 206, 207, 210, 211, 212, 213, 228, 232, 235, 240, 255], "decim": [203, 204, 205, 223, 227, 229], "decis": 255, "declar": 30, "decode_predict": [192, 227, 232], "decompos": [226, 227, 229], "decomposit": [204, 205, 226, 227, 229], "decompress": [8, 170, 243], "decompressed_bw": [8, 170, 243], "decor": 30, "decreas": 236, "dedic": 236, "deep": [221, 226, 233, 236], "deepcopi": 189, "def": [2, 10, 13, 18, 21, 23, 29, 30, 171, 172, 173, 174, 179, 182, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 223, 227, 229, 232, 235, 242, 244, 247, 249, 250], "default": [1, 2, 3, 9, 10, 11, 12, 13, 14, 18, 21, 22, 23, 24, 25, 29, 30, 160, 161, 162, 164, 165, 166, 168, 172, 174, 175, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 221, 222, 223, 225, 226, 227, 228, 229, 232, 234, 235, 236, 241, 242, 244, 246, 247, 251], "default_activation_bw": [9, 10, 188, 189, 190, 191, 214, 218, 232, 244], "default_beta_rang": [1, 12, 24, 181, 214], "default_bitwidth": 232, "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "default_config_fil": [1, 24, 181, 214], "default_config_per_channel": [195, 248], "default_data_typ": [10, 23, 179, 185, 244, 247], "default_forward_fn": [180, 186, 235], "default_num_iter": [1, 12, 24, 181, 189, 193, 201, 207, 208, 214], "default_output_bw": [10, 22, 23, 177, 179, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 220, 232, 234, 235, 242, 244, 247], "default_param_bw": [1, 9, 10, 12, 22, 23, 24, 177, 179, 181, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 220, 232, 234, 235, 242, 244, 247], "default_quant_schem": [1, 12, 24, 181, 189, 193, 201, 207, 214], "default_reg_param": [1, 12, 24, 181, 214], "default_warm_start": [1, 12, 24, 181, 214], "defin": [2, 13, 21, 30, 160, 172, 173, 175, 178, 182, 192, 197, 200, 201, 203, 204, 205, 207, 210, 211, 212, 213, 218, 223, 227, 229, 232, 240, 243, 244, 245, 246, 248, 249, 250], "definit": [2, 13, 29, 30, 169, 171, 172, 174, 176, 182, 185, 188, 192, 195, 196, 200, 201, 206, 207, 210, 211, 212, 213, 217, 223, 227, 229, 232, 244, 246, 249, 250], "degrad": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212], "degre": [227, 229], "deleg": 178, "delet": [2, 13, 182, 188, 192, 206, 218, 232], "deliber": [188, 192, 200, 201, 206, 213], "delta": [9, 21, 22, 177, 184, 196, 197, 218, 239, 246], "demand": 236, "demonstr": [187, 200, 213], "denot": [227, 230], "dens": [16, 21, 195, 196, 197, 221, 249, 251], "depend": [158, 159, 160, 172, 187, 201, 224, 239, 240, 241, 248, 251, 255], "depict": 245, "deploi": [246, 253], "deploy": [236, 237, 254, 255], "deprec": [179, 244, 247, 251], "depth": [214, 220, 222, 251, 252], "depthwis": 251, "depthwise_conv2d": 222, "depthwiseconv2d": [16, 221, 222], "dequant": [158, 159, 160, 162, 165, 166, 178, 236, 239, 251, 255], "dequantizedtensor": [159, 160, 162, 178, 242], "deriv": [161, 162, 164, 165, 178, 189, 192, 206, 214, 232, 245, 246], "descend": 166, "describ": [166, 171, 187, 201, 226, 228, 232, 239, 240, 241, 242, 243, 245, 246, 248, 252], "descript": [175, 188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "design": [173, 188, 192, 197, 200, 201, 206, 213, 214, 236], "desir": [9, 23, 160, 177, 178, 179, 184, 185, 188, 192, 203, 204, 205, 206, 218, 224, 226, 227, 232, 233, 243, 244, 247, 252, 254, 255], "detach": 160, "detail": [25, 172, 183, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 219, 223, 225, 228, 234, 237, 240, 241, 243, 244, 246, 252, 254, 255], "detect": 226, "determin": [1, 9, 10, 22, 23, 166, 170, 175, 177, 184, 185, 188, 196, 206, 208, 214, 218, 219, 231, 235, 236, 243, 244, 245, 247], "determinist": 172, "dev": [240, 241], "develop": [168, 216, 236, 240, 241, 243], "devic": [1, 3, 7, 10, 14, 24, 25, 160, 172, 176, 179, 181, 182, 183, 185, 188, 189, 190, 191, 195, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 219, 220, 221, 222, 232, 234, 235, 236, 237, 242, 244, 246, 247, 254, 255], "dfrac": 246, "diagnost": 252, "diagram": [196, 246], "dict": [2, 3, 9, 10, 13, 18, 22, 23, 25, 29, 166, 169, 172, 174, 175, 176, 177, 182, 183, 184, 185, 217, 218, 219, 223, 227, 229, 232, 234, 244, 247, 250], "dictionari": [9, 13, 18, 22, 29, 166, 177, 179, 184, 185, 194, 195, 197, 218, 223, 225, 226, 227, 229, 232, 244, 247, 248], "dicuss": 197, "did": [201, 240], "didn": 209, "diff": 251, "differ": [18, 29, 171, 172, 176, 188, 192, 195, 196, 201, 203, 204, 205, 206, 211, 212, 222, 223, 225, 226, 227, 229, 231, 232, 233, 234, 235, 236, 241, 243, 246, 252, 253, 254], "dim": [170, 178, 206, 243], "dimens": [8, 170, 175, 227, 229, 243, 246, 252], "dir": [10, 195, 200, 201, 202, 241, 244], "dir_path": [7, 169, 217], "direct": [217, 221, 232, 237, 243, 245, 246, 251, 255], "directli": [9, 15, 195, 209, 218, 220, 234, 246], "directori": [2, 3, 7, 9, 10, 13, 14, 18, 20, 22, 25, 29, 169, 176, 177, 182, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 240, 241, 244, 247, 254], "directrori": 232, "disabl": [2, 9, 22, 171, 175, 176, 177, 179, 180, 184, 185, 186, 192, 203, 205, 218, 221, 225, 226, 232, 235, 244, 246, 247, 248], "disable_lora_adapt": 176, "discard": 218, "disciplin": 214, "discrep": 246, "discuss": [187, 192, 203, 204, 205, 214], "disk": [7, 20, 169, 217], "displai": [187, 192, 196, 218, 228], "display_comp_ratio_plot": 228, "display_eval_scor": 228, "dist": [240, 241], "distinct": 173, "distribut": [18, 29, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 222, 223, 227, 229, 236, 246, 251, 252], "divbackward0": 178, "diverg": 236, "divid": [224, 243, 247], "divis": [178, 236, 243], "dlc": 254, "dlc_path": 254, "dlf": 236, "do": [1, 13, 21, 24, 172, 173, 176, 179, 181, 188, 189, 190, 191, 192, 193, 194, 195, 197, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 218, 221, 226, 232, 234, 240, 241, 242, 244, 246, 247, 255], "do_constant_fold": [188, 189, 190, 191, 221], "do_not_trace_m": 172, "doc": [172, 176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 240, 251], "docker": 237, "docker_container_nam": 240, "docker_image_nam": 240, "docker_run_command": 240, "dockerfil": 240, "docstr": 243, "document": [176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 202, 206, 207, 208, 209, 210, 211, 212, 213, 217, 221, 228, 232, 239, 243, 245, 246, 251, 254, 255], "doe": [21, 30, 172, 182, 188, 189, 190, 191, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 218, 223, 225, 227, 229, 246, 250, 251, 252, 255], "doesn": [30, 188, 189, 190, 191, 193, 194, 202, 203, 204, 205, 206, 207, 210, 211, 212, 244, 247, 255], "doesnt": [174, 234], "don": [30, 172, 176, 192, 195, 196, 200, 201, 213, 218], "done": [161, 162, 178, 192, 197, 201, 209, 226, 248, 249, 251], "down": [201, 245, 255], "download": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 237, 239, 241], "download_url": [240, 241], "downsampl": [203, 205], "downstream": [230, 245, 251], "dpkg": 241, "dq_output": 30, "drastic": 225, "drawback": 246, "driver": [227, 239, 242], "drop": [2, 3, 13, 14, 25, 175, 182, 183, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 218, 219, 224, 226, 231, 232, 246, 247, 252], "dropout": [21, 65, 196, 197], "dropout1": [21, 197], "dropout1d": 66, "dropout2": [21, 197], "dropout2d": 67, "dropout3d": 68, "dtype": [158, 159, 160, 166, 170, 171, 172, 174, 176, 234, 243, 245, 251], "due": [21, 173, 243, 251], "dummi": [3, 9, 10, 24, 25, 27, 28, 169, 173, 177, 179, 181, 182, 183, 184, 185, 188, 200, 206, 213, 214, 217, 218, 219, 221, 222, 244, 247], "dummy_data": 218, "dummy_input": [3, 9, 10, 24, 25, 27, 28, 168, 169, 176, 177, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 232, 234, 235, 242, 243, 244, 247, 250], "dummy_input_dict": 217, "dump": [195, 251], "duplic": [160, 172], "dure": [1, 2, 3, 10, 12, 13, 14, 18, 21, 23, 24, 25, 29, 30, 166, 168, 175, 179, 181, 182, 183, 185, 189, 191, 193, 195, 198, 199, 200, 201, 202, 203, 204, 205, 207, 209, 211, 212, 213, 214, 215, 216, 219, 223, 226, 227, 228, 229, 232, 236, 237, 244, 246, 247, 248, 251, 255], "dynam": [166, 172, 236, 246, 251], "dynamic_ax": [188, 189, 190, 191, 214, 219, 221, 232, 244], "e": [1, 9, 10, 23, 179, 185, 188, 192, 195, 200, 201, 206, 209, 213, 214, 218, 231, 232, 235, 244, 245, 247], "each": [1, 2, 9, 10, 12, 13, 21, 22, 24, 30, 168, 173, 175, 177, 178, 181, 182, 184, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 232, 234, 236, 243, 244, 245, 246, 248, 252, 254], "earli": [2, 13, 182, 232], "easi": [234, 251, 255], "easier": 171, "easili": [178, 223, 227, 229], "ed": 214, "edg": [236, 237], "edit": [188, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 245], "effect": [1, 12, 15, 23, 24, 26, 175, 176, 181, 185, 191, 196, 198, 199, 211, 212, 214, 218, 220, 231, 236, 243, 244, 247, 255], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 195, 209, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "efficientnetb4": 251, "effort": [3, 14, 25, 174, 183, 194, 208, 219, 234, 255], "eights_pdf": [9, 22, 177, 184, 218], "either": [18, 29, 170, 174, 176, 178, 188, 193, 201, 203, 204, 205, 206, 223, 227, 229, 230, 232, 234, 240, 243, 250], "element": 245, "elementari": 196, "elementwis": [172, 175, 251], "elimin": [221, 236, 246], "els": [2, 13, 172, 173, 182, 188, 189, 190, 191, 192, 193, 195, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 219, 220, 221, 222, 227, 232, 234, 235, 242, 244, 247], "elu": 69, "embed": [21, 70, 172, 179, 185, 196, 197, 226, 244, 247, 251, 252], "embed_dim": [21, 196, 197], "embedding_lay": [21, 197], "embeddingbag": 71, "embodi": 236, "empir": [194, 208], "emploi": [236, 237], "empti": 248, "emul": [237, 246], "enabl": [2, 3, 8, 9, 13, 14, 18, 22, 25, 29, 166, 171, 174, 176, 177, 180, 182, 183, 184, 186, 192, 199, 201, 203, 205, 206, 212, 219, 220, 223, 227, 228, 229, 231, 232, 235, 236, 243, 246, 248, 251, 254, 255], "enable_adapter_and_load_weight": 176, "enable_convert_op_reduct": [13, 174, 182, 192, 206, 232], "enable_onnx_check": [179, 185, 244, 247], "enable_per_layer_mse_loss": [9, 22, 200, 213, 218], "enc": 30, "encapsul": [2, 13, 174, 177, 182, 184, 218, 232], "encod": [1, 2, 3, 9, 10, 12, 13, 14, 22, 23, 24, 25, 30, 158, 159, 160, 161, 162, 166, 171, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 193, 194, 195, 196, 198, 199, 201, 202, 207, 208, 210, 211, 212, 214, 215, 217, 219, 231, 232, 234, 235, 236, 239, 243, 247, 251, 254], "encoding_analyz": [161, 162, 166], "encoding_file_path": [244, 247], "encoding_path": [189, 193, 194, 201, 207, 208, 214, 219], "encoding_vers": 243, "encodinganalyz": [161, 162, 166], "encodinganalyzerforpython": 239, "encodingbas": [158, 159, 166], "encodingmismatchinfo": 244, "encount": [30, 196], "encourag": [21, 172, 249, 250], "end": [15, 161, 162, 163, 164, 165, 172, 173, 187, 195, 196, 198, 199, 203, 204, 205, 209, 211, 212, 214, 219, 220, 226, 232, 235, 237, 244], "end_beta": [1, 12, 24, 181, 214], "end_idx": 219, "enforc": 166, "engin": [217, 221, 232, 237, 240, 243, 245, 246, 251, 255], "enhanc": [9, 22, 176, 177, 184, 200, 213, 218, 244, 246, 251], "enough": [192, 203, 204, 205, 214], "ensur": [175, 201, 217, 220, 225, 232, 241, 242, 251, 252, 255], "enter": [30, 175, 219], "entir": [9, 22, 176, 177, 178, 184, 188, 189, 190, 191, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 223, 226, 227, 229, 246], "entireti": [21, 201], "entri": [25, 179, 183, 185, 219, 244, 247, 248], "entrypoint": 240, "enum": [1, 2, 13, 18, 24, 29, 169, 179, 181, 182, 185, 188, 192, 195, 196, 206, 209, 214, 217, 223, 227, 229, 232, 244, 247], "enumer": [10, 18, 23, 29, 169, 179, 185, 197, 217, 223, 227, 229, 232, 244, 247], "environ": [187, 188, 192, 194, 195, 198, 199, 206, 213, 232, 236, 239, 241], "envsetup": [240, 241], "ep": [221, 222], "epoch": [189, 190, 191, 193, 194, 195, 196, 198, 199, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 220, 223, 226, 227, 229, 247], "epsilon": [21, 196, 197, 251], "equal": [3, 5, 14, 17, 25, 28, 159, 160, 170, 178, 183, 187, 193, 197, 214, 218, 219, 224, 225, 243, 246, 252], "equalize_model": [5, 17, 28, 190, 202, 210, 222], "equat": [161, 162, 163, 164, 165, 243, 246], "equival": [25, 30, 164, 165, 166, 170, 174, 179, 182, 183, 185, 188, 201, 206, 207, 210, 211, 212, 213, 219, 232, 243, 244, 247, 250, 254], "erorr": 192, "error": [3, 14, 21, 25, 30, 172, 178, 179, 183, 192, 219, 232, 234, 236, 244, 246, 247, 252], "especi": [188, 201, 206, 236, 247, 252, 255], "essenti": [188, 192, 195, 196, 206, 209, 243], "estim": [187, 246, 251, 255], "esults_dir": [9, 22, 177, 184, 218], "etc": [188, 192, 195, 196, 201, 206, 224, 236, 240, 245, 251], "eval": [2, 3, 9, 13, 14, 18, 22, 25, 29, 172, 174, 177, 180, 182, 183, 184, 186, 188, 189, 190, 191, 192, 195, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 232, 234, 235, 242, 244, 247], "eval_callback": [3, 9, 14, 18, 22, 25, 29, 177, 183, 184, 194, 195, 200, 203, 204, 205, 208, 213, 218, 219, 223, 227, 229, 232], "eval_callback_factori": [188, 206], "eval_callback_fn": 218, "eval_callback_for_phase1": [2, 13, 182, 188, 206, 232], "eval_callback_for_phase2": [2, 13, 182, 188, 206, 232], "eval_callback_for_phase_1": 232, "eval_callback_for_phase_2": 232, "eval_callback_phase1": 192, "eval_callback_phase2": 192, "eval_data_load": [214, 219, 232, 244], "eval_data_s": 244, "eval_dataset": [194, 195, 214, 218, 219, 220, 244, 247], "eval_dataset_s": [194, 195, 208, 219], "eval_func": [192, 200, 227, 232], "eval_iter": [18, 29, 203, 204, 205, 223, 227, 229], "eval_scor": [9, 18, 22, 29, 177, 184, 218, 223, 227, 229], "evalcallbackfactori": [2, 174, 182, 188, 206, 232], "evalfunct": 227, "evalu": [2, 3, 9, 13, 14, 18, 22, 25, 29, 174, 177, 182, 183, 184, 187, 194, 196, 208, 214, 219, 220, 223, 225, 226, 227, 228, 229, 232, 235, 242, 246, 255], "evaluate_accuraci": 206, "evaluate_model": [223, 227, 229], "even": [30, 178, 188, 192, 195, 196, 201, 206, 232, 233, 234, 244], "evenli": 243, "eventu": 234, "everi": [1, 7, 9, 12, 20, 22, 24, 169, 177, 178, 181, 184, 188, 189, 190, 191, 192, 195, 196, 198, 199, 200, 201, 203, 204, 205, 206, 209, 211, 212, 213, 214, 215, 217, 218, 225, 226, 246, 247], "evlauat": 192, "ex": 243, "exactli": [14, 30, 100, 166, 175, 200, 213, 219, 246], "examin": 172, "exampl": [8, 18, 29, 30, 158, 159, 160, 161, 162, 164, 165, 166, 168, 170, 173, 174, 175, 176, 178, 179, 180, 186, 196, 197, 208, 214, 216, 222, 224, 230, 234, 236, 240, 241, 243, 244, 246, 247, 248, 249, 250, 251, 252, 255], "exce": [168, 216], "exceed": [168, 216], "except": [21, 180, 186, 188, 189, 190, 191, 197, 200, 213, 214, 219, 221, 222, 232, 234, 235, 243, 244], "exchang": 236, "exclud": [25, 172, 173, 177, 180, 183, 184, 186, 218, 219, 235, 251], "exclus": [166, 170, 243], "execut": [2, 3, 13, 14, 25, 172, 179, 182, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 219, 221, 222, 228, 232, 241, 242, 244, 247], "exercis": [188, 195, 200, 201, 206, 209, 213], "exist": [30, 100, 166, 175, 179, 185, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 232, 243, 244, 247], "exist_ok": [188, 189, 192, 193, 195, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "exit": [2, 3, 13, 14, 25, 30, 175, 182, 183, 219, 232], "expand": [243, 251], "expand_dim": 219, "expanded_conv_depthwis": [214, 222], "expans": [226, 243], "expect": [2, 3, 9, 14, 18, 22, 23, 24, 25, 29, 168, 172, 173, 174, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 195, 203, 204, 205, 206, 209, 214, 216, 218, 219, 223, 226, 227, 229, 232, 234, 235, 244, 247, 249, 255], "experi": [18, 29, 170, 189, 193, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212, 223, 226, 227, 229, 239, 243], "experienc": 239, "experiment": [226, 243, 251], "experss": [21, 197], "expert": [194, 208], "explain": [188, 192, 195, 196, 200, 206, 209, 213, 223, 226, 251], "explan": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212, 237], "explicitli": [3, 14, 25, 183, 219, 230], "expon": [166, 170, 243], "exponent_bit": [166, 170, 243], "export": [9, 10, 22, 23, 25, 169, 171, 174, 176, 177, 179, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 196, 198, 199, 201, 202, 206, 207, 208, 210, 211, 212, 214, 215, 218, 219, 220, 221, 222, 226, 232, 233, 234, 235, 240, 241, 245, 246, 250, 251, 254], "export_adapter_weight": 176, "export_model": [176, 179, 185, 244, 247], "export_param": [188, 189, 190, 191], "export_per_layer_encoding_min_max_rang": [9, 22, 177, 184, 218], "export_per_layer_mse_loss": [9, 22, 177, 184, 218], "export_per_layer_stats_histogram": [9, 22, 177, 184, 218], "export_to_torchscript": [179, 185, 244, 247], "exported_model": 243, "expos": [172, 214, 218], "express": [18, 29, 223, 227, 229], "extend": [171, 251], "extens": [167, 175, 187, 240, 241, 254], "extra": [166, 175, 251], "extract": [189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213, 236, 244], "extrem": [188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 246], "ey": 218, "f": [172, 173, 178, 195, 208, 214, 219, 220, 221, 222, 232, 239, 240, 241, 244, 247], "f0": 224, "facebook": 236, "facilit": 237, "fact": 243, "factor": [195, 196, 198, 199, 203, 204, 205, 209, 211, 212, 224, 226, 243], "factori": [2, 13, 174, 182, 232], "fail": [172, 173, 174, 179, 188, 189, 190, 191, 214, 219, 221, 222, 232, 234, 244, 247, 251], "fair": 236, "fairli": [192, 195, 196, 200, 201, 213, 244], "fake": [162, 165, 166, 174, 182, 188, 189, 190, 191, 193, 195, 196, 198, 199, 201, 202, 206, 207, 208, 209, 210, 211, 212, 232, 244, 251], "fakequ": [179, 185, 244, 247], "fall": [203, 204, 205, 225, 248], "fallback": 254, "fals": [2, 3, 8, 10, 13, 14, 18, 21, 23, 25, 29, 30, 158, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 176, 178, 179, 182, 183, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 219, 221, 222, 223, 227, 229, 232, 234, 239, 242, 243, 244, 245, 247, 248], "famili": [234, 236], "familiar": 187, "far": [195, 209], "farther": [189, 193, 207], "fashion": [201, 243], "fast": [13, 232], "faster": [188, 194, 195, 201, 206, 208, 232, 251], "favor": 243, "fc": [226, 229], "fc1": [172, 245], "fc2": 172, "featur": [8, 21, 168, 171, 172, 173, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 219, 220, 221, 222, 226, 228, 232, 234, 236, 242, 246, 249, 250, 251, 252], "featurealphadropout": 72, "feed": [21, 196, 197, 246], "feel": [195, 196, 209, 234], "feez": [180, 186, 235], "few": [188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 220, 224], "fewer": 226, "ff_dim": [21, 196, 197], "ffn": [21, 197], "ffn_output": [21, 197], "field": [2, 166, 182, 188, 206, 232, 245], "figur": [214, 223, 225, 227, 228, 229, 230, 252, 255], "file": [1, 2, 3, 9, 10, 12, 14, 18, 22, 23, 24, 25, 29, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 194, 196, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 223, 227, 229, 232, 234, 235, 239, 240, 241, 243, 244, 246, 247, 251, 254], "file_path": [179, 185, 214, 219, 221, 222, 232, 244], "filenam": [1, 10, 12, 23, 24, 176, 179, 181, 185, 188, 189, 190, 191, 214, 220, 235, 244, 247], "filename_prefix": [1, 10, 12, 23, 24, 176, 179, 181, 185, 188, 189, 192, 193, 195, 201, 202, 206, 207, 208, 209, 210, 211, 212, 214, 220, 232, 235, 244, 247], "filename_prefix_encod": [176, 179, 185, 244, 247], "fill": [25, 160, 179, 183, 185, 219, 244, 247], "filter": [21, 201, 236], "final": [18, 21, 29, 173, 180, 186, 188, 192, 195, 196, 200, 206, 209, 213, 223, 224, 225, 227, 229, 232, 235, 241, 243, 247, 252], "find": [2, 13, 16, 173, 174, 180, 182, 185, 186, 188, 189, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 218, 221, 222, 225, 233, 234, 235, 244, 246, 247], "find_pkg_url_str": [240, 241], "fine": [18, 23, 29, 176, 179, 185, 189, 190, 191, 193, 194, 196, 198, 199, 202, 207, 208, 210, 211, 212, 223, 224, 227, 229, 237, 244, 255], "finer": [166, 178, 243], "finetun": [189, 190, 193, 194, 195, 201, 202, 203, 204, 205, 207, 209, 211, 212], "finetuned_accuraci": [209, 211, 212], "finetuned_accuracy_bn_reestim": 209, "finetuned_model": [203, 204], "finish": [198, 199, 211, 212], "first": [21, 24, 172, 175, 178, 180, 181, 186, 195, 196, 197, 200, 201, 203, 204, 205, 209, 213, 214, 226, 227, 235, 240, 247, 251, 252], "fit": [18, 29, 195, 196, 198, 199, 209, 220, 223, 225, 227, 229, 247], "five": [190, 210], "fix": [173, 188, 189, 190, 191, 232, 245, 246, 251], "flag": [2, 3, 13, 14, 25, 168, 171, 172, 173, 174, 179, 182, 183, 185, 188, 192, 206, 216, 219, 232, 234, 244, 247], "flatten": [73, 172, 195], "flexibl": [167, 188, 206], "float": [1, 2, 3, 9, 10, 11, 12, 13, 14, 18, 22, 23, 24, 25, 29, 30, 158, 159, 166, 170, 177, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 223, 227, 229, 232, 233, 236, 237, 243, 244, 245, 246, 247, 251, 252, 255], "float16": [166, 170, 171, 243], "float32": [218, 219], "float_fallback": 254, "floatencod": 166, "floatquant": [166, 171], "floatquantizedequant": 171, "flow": [25, 172, 183, 219, 246, 252], "fo": 201, "focu": 209, "fold": [3, 4, 14, 15, 16, 25, 27, 74, 183, 192, 214, 218, 219, 220, 222, 232, 236, 251], "fold_all_batch_norm": [16, 27, 192, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212, 221, 232, 247], "fold_all_batch_norms_to_scal": [15, 195, 209, 220], "fold_all_batch_norms_to_weight": [4, 188, 189, 190, 191, 221], "folded_model": 221, "folder": [200, 213, 218], "follow": [6, 7, 9, 10, 11, 19, 20, 21, 22, 23, 30, 100, 169, 171, 172, 173, 174, 175, 176, 177, 179, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 220, 222, 223, 224, 225, 226, 227, 229, 230, 233, 234, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], "footprint": 236, "forall_": [161, 162, 163, 164, 165], "forc": [188, 189, 190, 191, 203, 204, 205, 206, 207, 210, 211, 212, 213], "form": [21, 189, 200, 213], "format": [1, 9, 12, 22, 23, 24, 160, 176, 177, 179, 181, 184, 185, 214, 218, 219, 232, 235, 236, 237, 243, 244, 247, 249, 251, 254, 255], "former": 250, "forward": [2, 3, 9, 10, 13, 14, 21, 22, 23, 24, 25, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 166, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 191, 192, 195, 196, 197, 198, 199, 200, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 232, 235, 240, 244, 247, 250, 251, 252, 255], "forward_fn": [1, 2, 3, 14, 24, 25, 26, 174, 180, 181, 182, 183, 186, 188, 189, 195, 206, 209, 214, 219, 220, 232, 235], "forward_one_batch": [188, 206], "forward_pass": [214, 232, 235, 242], "forward_pass_arg": 244, "forward_pass_call_back": [192, 232], "forward_pass_callback": [1, 2, 9, 10, 13, 22, 23, 177, 179, 182, 184, 185, 188, 189, 190, 191, 193, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 232, 244, 247], "forward_pass_callback_2": [13, 232], "forward_pass_callback_arg": [1, 10, 23, 179, 185, 188, 189, 190, 191, 192, 193, 195, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 214, 232, 244, 247], "forward_pass_callback_fn": 218, "found": [171, 197, 246, 247], "four": 246, "fp16": [174, 234, 251], "fp32": [2, 7, 9, 13, 20, 22, 169, 174, 176, 177, 180, 182, 184, 186, 195, 196, 213, 217, 218, 232, 235, 236, 246, 247, 251, 254, 255], "fp32_acccuraci": 192, "fp32_layer_output": 217, "fp32_layer_output_util": 217, "fp32_output": [2, 232], "fp_qdq": 171, "fp_quantiz": 171, "frac": [161, 162, 163, 164, 165, 166, 178, 246], "fraction": 232, "fractionalmaxpool2d": 75, "fractionalmaxpool3d": 76, "framework": [0, 187, 236, 237, 239, 240, 241, 242, 244, 246, 248, 254, 255], "free": [190, 195, 196, 202, 209, 210, 234], "freez": [171, 176, 189, 193, 201, 207, 235], "freeze_base_model": 176, "freeze_base_model_activation_quant": 176, "freeze_base_model_param_quant": 176, "freeze_encod": 171, "fresh": 239, "friendli": [167, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 219, 255], "from": [2, 3, 13, 14, 21, 24, 25, 26, 30, 159, 160, 161, 162, 166, 167, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 232, 234, 235, 236, 237, 240, 241, 242, 243, 244, 245, 246, 247, 248, 251, 252, 254], "from_modul": 30, "from_tensor_slic": 218, "front": [2, 3, 13, 14, 25, 182, 183, 188, 192, 206, 219], "frozen": 201, "full": [188, 189, 190, 191, 192, 193, 195, 198, 199, 202, 206, 207, 209, 210, 211, 212, 227, 232, 233, 236, 249, 250], "fulli": [167, 226, 229, 244, 245, 251], "func": [2, 13, 174, 177, 182, 184, 218, 232], "func_callback_arg": [2, 13, 174, 177, 182, 184, 188, 206, 218, 232], "func_wrapp": [192, 227, 232], "function": [1, 2, 3, 6, 9, 10, 11, 13, 14, 18, 19, 21, 22, 23, 24, 25, 26, 29, 30, 158, 159, 160, 164, 165, 166, 168, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 192, 193, 196, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 216, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 232, 235, 236, 243, 244, 246, 247, 249, 250, 251, 254, 255], "function_nam": [203, 204, 205], "functional_callback": 196, "functional_model": [195, 196, 197], "functional_model_weight_ord": 197, "functional_op": 173, "fundament": 171, "furhter": [194, 208], "furo": 251, "further": [158, 161, 162, 163, 164, 165, 172, 178, 205, 223, 226, 246, 255], "furthermor": 21, "fuse": [195, 246, 248], "fuse_bn_into_conv": 221, "fusion": 236, "futur": [168, 179, 216, 244, 247], "fx": [25, 172, 183, 219], "g": [1, 10, 23, 179, 185, 188, 192, 195, 200, 201, 206, 209, 213, 214, 218, 232, 240, 244, 245, 247], "gain": [189, 193, 198, 199, 201, 202, 203, 204, 205, 207, 210, 211, 212, 214, 223, 224, 226], "gap": 233, "gather": 244, "gaussiannllloss": 81, "gelu": 77, "gemm": [8, 195, 248], "gener": [7, 10, 20, 30, 161, 162, 163, 164, 165, 169, 176, 178, 182, 187, 188, 192, 196, 206, 208, 214, 218, 220, 221, 222, 226, 232, 234, 235, 236, 242, 243, 244, 245, 246, 247, 248, 251, 252, 254], "generate_layer_output": [7, 20, 169, 217], "get": [2, 13, 18, 29, 166, 172, 174, 176, 182, 188, 189, 190, 191, 192, 195, 196, 197, 200, 217, 226, 227, 229, 232, 233, 234, 235, 239, 241, 244, 247], "get_activation_quant": [2, 232], "get_active_param_quant": [13, 232], "get_active_quant": [2, 13, 174, 182, 232], "get_available_provid": [188, 189, 190, 191], "get_calibration_and_eval_data_load": 244, "get_candid": [2, 13, 174, 182, 232], "get_data_loader_wrapp": [192, 232], "get_default_kernel": 30, "get_devic": 208, "get_encod": 166, "get_eval_func": [192, 227, 232], "get_extra_st": 166, "get_fp_lora_lay": 176, "get_input": [188, 189, 190, 191, 214, 219, 232, 244], "get_input_quantizer_modul": [174, 182, 232], "get_kernel": 30, "get_model": [192, 249], "get_offset": 178, "get_original_models_weights_in_functional_model_ord": 197, "get_param_quant": [2, 232], "get_path_for_per_channel_config": [214, 219, 220, 234, 235, 242, 244], "get_peft_model": 176, "get_pre_processed_input": 217, "get_quant_scheme_candid": [3, 14, 25, 183, 219], "get_quantized_lora_lay": 176, "get_scal": [160, 178], "get_subclass_model_with_functional_lay": 21, "get_text_classificaiton_model": 21, "get_top5_acc": 192, "get_val_dataload": [188, 189, 190, 191, 203, 205, 206, 207, 209, 210, 211, 212, 213], "get_val_dataset": [193, 194, 195, 200, 201, 202], "get_weight": [197, 221, 222], "git": [187, 240], "github": [187, 239, 240, 241], "give": [192, 196, 200, 201, 203, 204, 205, 213, 214, 226, 232, 251], "given": [2, 3, 5, 10, 13, 14, 17, 18, 23, 24, 25, 26, 28, 29, 30, 168, 174, 175, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 216, 219, 220, 222, 223, 225, 226, 227, 229, 232, 235, 243, 244, 245, 247, 255], "global": [235, 236, 252], "globalaveragepooling1d": [21, 196, 197], "glu": 78, "gnu": 241, "gnupg2": 241, "go": [21, 188, 192, 195, 196, 200, 201, 206, 209, 213, 251, 255], "goal": [3, 14, 25, 183, 188, 206, 218, 219], "good": [21, 176, 195, 196, 198, 199, 203, 204, 205, 209, 211, 212, 226, 234], "googl": 236, "got": [4, 15, 27, 172, 220, 221], "gpu": [203, 204, 205, 207, 209, 210, 211, 212, 213, 217, 223, 227, 229, 236, 239, 240, 241, 242, 251], "grad_fn": [158, 159, 160, 161, 162, 178, 242], "gradient": [158, 159, 160, 244, 247, 251], "grant": 187, "granular": [18, 29, 196, 203, 204, 205, 223, 226, 227, 229, 243, 252], "graph": [21, 24, 25, 160, 172, 179, 181, 183, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 203, 205, 206, 207, 209, 210, 211, 212, 213, 214, 219, 221, 222, 227, 228, 232, 234, 244, 245, 246, 247, 251, 254, 255], "graphmodul": 172, "greater": [18, 29, 223, 224, 225, 227, 229, 243], "greedi": [18, 29, 223, 226, 227, 228, 229], "greedili": [188, 192, 206], "greedy_param": [203, 204, 205, 223, 227, 229], "greedy_select_param": [18, 29, 203, 204, 205, 223, 227], "greedymixedprecisionalgo": [13, 174, 182, 192, 206, 232], "greedyselectionparamet": [18, 29, 203, 204, 205, 223, 227, 229], "green": 230, "grep": 240, "grid": [11, 178, 180, 186, 235, 243, 246], "group": [2, 8, 13, 170, 174, 182, 188, 206, 240, 243, 246, 248, 251], "groupedblockquantizedequant": 243, "groupnorm": 82, "gru": [79, 251], "grucel": 80, "guarante": 21, "guid": [187, 188, 192, 194, 206, 207, 208, 210, 211, 212, 213, 214, 232, 240, 251], "guidebook": 226, "guidelin": [172, 206, 207, 210, 211, 212, 213, 214, 244, 247, 255], "h": [187, 229, 230, 241], "h5": [217, 254], "ha": [1, 9, 12, 21, 24, 25, 30, 158, 160, 171, 172, 173, 176, 181, 182, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 224, 225, 228, 230, 234, 236, 242, 243, 244, 246, 251], "had": [249, 250], "half": 224, "hand": [188, 206], "handl": [3, 14, 15, 25, 26, 183, 219, 220, 239, 242, 243, 246, 251], "hard": 172, "hardshrink": 83, "hardsigmoid": 84, "hardswish": 85, "hardtanh": 86, "hardwar": [179, 236, 237, 244, 246, 247, 255], "hat": 246, "have": [9, 21, 22, 30, 168, 169, 170, 171, 172, 173, 177, 178, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 222, 226, 232, 234, 236, 241, 243, 244, 245, 247, 251, 252, 254], "hba": [190, 202, 210], "he": 226, "head": [21, 196, 197], "heavi": [168, 216, 228], "height": [188, 189, 190, 191, 196, 206, 207, 209, 210, 211, 212, 213, 219, 221, 222, 227, 229, 230], "held": [30, 175, 211, 212], "help": [171, 173, 174, 176, 179, 185, 190, 200, 202, 210, 213, 218, 225, 226, 232, 233, 234, 244, 247, 248, 252, 255], "helper": [174, 178, 182, 195, 209, 232], "hen": [14, 25, 183, 219], "here": [1, 24, 171, 176, 178, 181, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 209, 210, 211, 212, 213, 214, 221, 222, 227, 244, 247, 248, 255], "heterogen": 255, "heurist": [194, 208], "hidden": [21, 196, 197], "hide": 234, "high": [2, 5, 13, 17, 28, 171, 182, 189, 190, 191, 193, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212, 222, 224, 225, 231, 232, 236, 237, 251], "higher": [1, 24, 181, 182, 188, 192, 203, 204, 205, 206, 214, 223, 225, 227, 229, 231, 232, 233, 243, 246, 247, 252, 255], "highest": [3, 14, 25, 183, 219, 225, 255], "highlight": [228, 245], "hingeembeddingloss": 87, "histogram": [9, 22, 177, 184, 200, 213, 215, 246, 251], "histogram_freq": 196, "historgram": [9, 218], "histori": [195, 196, 198, 199], "hold": [158, 159, 160, 175, 220, 234, 246, 248, 252], "home": 240, "honor": [223, 227, 229], "hood": 171, "hook": 246, "hope": [188, 192, 195, 196, 201, 206, 209], "hopefulli": 197, "host": [228, 240, 242, 251], "hostnam": 240, "hotspot": [9, 22, 177, 184, 218, 231], "how": [2, 13, 173, 175, 176, 178, 179, 182, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 226, 227, 229, 232, 234, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 251, 252, 253, 255], "howev": [21, 171, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 223, 227, 229, 239, 246, 249, 251], "html": [9, 22, 168, 172, 177, 184, 187, 200, 213, 215, 216, 218, 239, 240, 241, 251], "htp": 254, "http": [172, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 228, 239, 240, 241], "hub": [217, 221, 232, 237, 243, 245, 246, 251, 255], "huberloss": 88, "huggingfac": 176, "hx": [79, 80, 96, 97, 123, 124], "hxwx5": 230, "hxwx8": 230, "hyper": [189, 193, 195, 196, 198, 199, 202, 203, 204, 205, 207, 209, 210, 211, 212, 220, 247, 255], "hyperparamet": 247, "i": [1, 2, 3, 5, 8, 9, 10, 13, 14, 18, 21, 22, 23, 24, 25, 28, 29, 30, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], "i_": [161, 162, 163, 164, 165], "i_0": [161, 162, 163, 164, 165], "i_d": [161, 162, 163, 164, 165], "iccv": [190, 202, 210, 226], "id": [1, 3, 7, 14, 25, 183, 214, 217, 219, 228, 240], "ideal": [192, 200, 201, 213, 251], "idempot": 160, "ident": [197, 221, 249, 250], "identifi": [170, 173, 187, 218, 223, 230, 231, 232, 240, 241, 243, 244, 246, 251, 252, 255], "ie": 234, "ieee": [166, 226], "ignor": [2, 18, 29, 172, 182, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 223, 227, 229, 232, 244, 255], "ignore_quant_ops_list": [1, 24, 181, 214], "illustr": [201, 203, 204, 205, 214, 223, 225, 227, 228, 229, 230, 246, 255], "ilsvrc": [214, 219, 232, 244], "ilsvrc2012": [188, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "imag": [9, 22, 177, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 232, 235, 236, 239, 241, 244, 247], "image_bw": 250, "image_dataset": 218, "image_dataset_from_directori": [192, 193, 198, 199, 201, 214, 219, 220, 227, 232, 244, 247], "image_height": [193, 201], "image_net_config": [188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213], "image_net_data_load": [188, 189, 190, 191, 203, 205, 207, 209, 210, 211, 212, 213], "image_net_dataset": [193, 194, 195, 200, 201, 202], "image_net_evalu": [188, 189, 190, 191, 193, 194, 195, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "image_net_train": [203, 204, 205, 209, 210, 211, 212], "image_rgb": 250, "image_s": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 219, 220, 227, 232, 244, 247], "image_width": [193, 201], "imageclassificationevalu": 235, "imagefold": [206, 208, 244], "imagenet": [187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 219, 220, 221, 222, 227, 232, 235, 244, 247], "imagenet_dataset": [208, 214, 219, 220, 244, 247], "imagenet_dir": [198, 199], "imagenetdataload": [188, 189, 190, 191, 200, 203, 205, 207, 209, 210, 211, 212, 213], "imagenetdatapipelin": [188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "imagenetdataset": [193, 194, 195, 200, 201, 202], "imagenetevalu": [188, 189, 190, 191, 193, 194, 195, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "imagenettrain": [203, 204, 205, 209, 210, 211, 212], "images_dir": 209, "images_mean": 206, "images_std": 206, "imagin": 201, "imdb": 196, "img": [192, 227, 232], "img_height": [192, 232], "img_width": [192, 232], "immedi": [190, 194, 202, 208, 210], "impact": [188, 201, 206, 220, 225, 236, 252], "implement": [7, 20, 23, 30, 169, 178, 185, 187, 188, 206, 217, 218, 219, 244, 247, 251, 252], "impli": [188, 206, 232], "import": [1, 12, 21, 23, 24, 30, 158, 159, 160, 161, 162, 164, 165, 166, 167, 170, 171, 172, 173, 175, 176, 178, 179, 181, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 236, 239, 241, 242, 243, 244, 247, 249, 255], "improv": [189, 190, 191, 193, 194, 195, 196, 198, 199, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 214, 221, 224, 226, 231, 233, 241, 246, 247, 251, 252, 253, 255], "in1": 234, "in2": 234, "in_channel": [170, 243], "in_eval_mod": 208, "in_featur": [30, 171, 175], "in_plac": [23, 179, 185, 244, 247], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "includ": [2, 3, 14, 25, 168, 182, 183, 216, 218, 219, 221, 225, 228, 232, 236, 237, 239, 241, 245, 246, 247, 248, 251, 255], "include_top": [192, 193, 201, 202], "incompat": [195, 196], "incorrect": [174, 234, 251], "incorrectli": [251, 255], "increas": [18, 29, 188, 201, 206, 223, 225, 227, 229, 231, 246], "incur": 218, "independ": [172, 236, 252], "index": [9, 22, 175, 177, 184, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 251], "index_0": [200, 213], "index_1": [200, 213], "index_n": [200, 213], "indic": [18, 29, 109, 110, 111, 174, 175, 178, 179, 194, 201, 206, 208, 223, 224, 227, 229, 230, 234, 244, 245, 247], "indirect": [188, 206, 232], "individu": [9, 22, 177, 184, 218, 225, 237, 243, 246, 248], "induc": 246, "infer": [1, 3, 12, 14, 23, 24, 25, 181, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 221, 224, 227, 232, 234, 235, 236, 243, 244, 246, 247, 251], "inferencesess": [3, 188, 189, 190, 191, 217, 218, 219, 244], "influenc": 246, "info": [16, 173, 192, 221, 251], "inform": [2, 13, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 171, 173, 182, 188, 192, 206, 209, 232, 237, 242, 245, 251], "inherit": [21, 30, 100, 175], "init": [176, 221, 222], "initi": [7, 10, 23, 161, 162, 166, 175, 178, 179, 185, 188, 189, 193, 199, 200, 201, 206, 207, 211, 212, 213, 214, 217, 221, 222, 244], "initial_accuraci": [208, 219], "initializd": 175, "inner": 252, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "inp_data": 227, "inp_symmetri": [11, 180, 186, 235], "inplac": [166, 173, 221, 222], "input": [2, 3, 7, 8, 9, 10, 11, 13, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 209, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 229, 230, 232, 235, 236, 242, 243, 244, 246, 247, 248, 249, 250, 251, 254], "input1": [46, 62, 105], "input2": [46, 62, 105], "input_": [161, 162, 163, 164, 165], "input_1": [214, 221, 222], "input_batch": 217, "input_channel": [8, 170, 243], "input_data": [188, 189, 190, 191, 206, 207, 209, 210, 211, 212, 213, 219, 244], "input_dim": [21, 196, 197], "input_dlc": 254, "input_inst": [7, 20, 169, 217], "input_lay": [21, 197], "input_length": 48, "input_list": 254, "input_nam": [179, 185, 188, 189, 190, 191, 214, 219, 221, 232, 244, 247], "input_network": 254, "input_op_nam": [18, 227], "input_q": 178, "input_qdq": 178, "input_qtzr": 30, "input_quant": [13, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 174, 175, 179, 182, 232, 242, 244, 247], "input_shap": [13, 27, 28, 29, 172, 188, 189, 190, 191, 192, 193, 197, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 214, 218, 219, 221, 222, 223, 227, 229, 232, 234, 244, 249], "input_tensor": [21, 172, 192, 193, 201, 202], "inputlay": [21, 214, 221, 222], "inputs_batch": [188, 189, 190, 191, 206, 207, 209, 210, 211, 212, 213, 244], "insert": [172, 188, 189, 190, 191, 193, 195, 196, 198, 199, 201, 202, 203, 205, 206, 207, 209, 210, 211, 212, 244, 246, 255], "insid": [21, 30, 172, 175, 196, 197, 227, 234], "insight": [228, 252], "inspect": [196, 226], "instabl": [195, 209], "instal": [187, 198, 199, 202, 208, 210, 211, 212, 228, 236, 251], "instanc": [7, 20, 30, 169, 172, 173, 179, 185, 217, 228, 244, 255], "instancenorm1d": 89, "instancenorm2d": 90, "instancenorm3d": 91, "instanti": [176, 178, 188, 200, 201, 206, 209, 213, 228, 232, 242, 243, 248, 250], "instead": [166, 172, 173, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 227, 230, 243, 249, 250], "instruct": [187, 237, 239, 240, 241, 251, 253, 254], "int": [1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 22, 23, 24, 25, 26, 29, 59, 60, 61, 109, 110, 111, 160, 161, 162, 163, 164, 165, 166, 170, 171, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 192, 194, 195, 200, 203, 204, 205, 206, 208, 214, 217, 218, 219, 220, 223, 227, 229, 232, 235, 243, 244, 245, 247], "int16": [174, 188, 192, 206, 219, 232, 234, 246, 255], "int4": [174, 219, 234, 251, 255], "int8": [159, 160, 174, 188, 189, 190, 191, 192, 201, 206, 207, 210, 211, 212, 219, 232, 234, 236, 246, 247, 255], "int_multipli": 30, "intact": 232, "integ": [1, 10, 23, 160, 164, 165, 170, 178, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 232, 236, 243, 244, 245, 247, 255], "integr": [3, 14, 25, 176, 183, 219], "intel": [239, 242], "intellig": 226, "intend": [187, 218, 223, 227, 229, 234, 236, 245], "interact": [168, 171, 236], "intercept": 246, "interchang": 243, "interest": [9, 177, 184, 218], "interfac": [167, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 239, 251], "intermedi": [2, 7, 13, 20, 25, 169, 179, 182, 183, 185, 188, 192, 206, 215, 217, 219, 232, 236, 244, 247, 251], "intern": [3, 10, 14, 21, 25, 171, 183, 185, 189, 193, 196, 197, 201, 207, 219, 226, 244, 248], "interpol": [13, 188, 206, 225, 232], "interpret": [244, 247], "introduc": [175, 222, 232, 243, 246, 248], "invalid": [172, 243], "invoc": [203, 204, 205], "invok": [1, 9, 10, 22, 175, 177, 179, 184, 185, 201, 214, 218, 226, 228, 244, 247], "involv": [2, 13, 171, 182, 188, 192, 206, 220, 222, 232, 244, 252], "io": [174, 234], "ip": 187, "ipynb": 187, "is_avail": [203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 219, 220, 221, 222, 232, 234, 235, 242, 247], "is_bfloat16": 166, "is_float16": 166, "is_initi": [30, 161, 162, 166, 175, 178], "is_input_quant": [195, 248], "is_leaf_modul": 172, "is_output_quant": [195, 248], "is_quant": [195, 248], "is_symmetr": [195, 239, 245, 248], "is_train": [188, 189, 190, 191, 192, 203, 205, 207, 209, 210, 211, 212, 213, 232], "is_unsigned_symmetr": 171, "isinst": [170, 243], "isol": 246, "issu": [21, 173, 215, 217, 220, 228, 234, 251, 252, 255], "item": [166, 197, 200, 213, 232, 244, 247, 254], "iter": [1, 3, 9, 12, 18, 24, 25, 29, 181, 183, 188, 189, 190, 191, 192, 193, 194, 195, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 218, 219, 223, 227, 229, 232, 244], "its": [2, 9, 13, 21, 30, 100, 158, 166, 174, 175, 182, 187, 188, 189, 190, 191, 193, 194, 195, 197, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 218, 223, 227, 230, 232, 234, 236, 237, 246, 248], "itself": [200, 209, 213, 217, 226, 227], "j_": [161, 162, 163, 164, 165], "j_0": [161, 162, 163, 164, 165], "j_d": [161, 162, 163, 164, 165], "jaderberg": 226, "jan": 226, "jenkin": 240, "jian": 226, "jianhua": 226, "jit": 250, "job": [195, 196, 198, 199, 203, 204, 205, 209, 211, 212, 254], "join": [189, 193, 198, 199, 201, 206, 207, 208, 214, 219, 221, 222, 223, 227, 229, 232, 244], "jointli": [199, 211, 212], "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 195, 200, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "jupyt": [187, 237, 251], "just": [188, 192, 195, 196, 200, 201, 206, 211, 212, 213, 230, 246, 251], "k": [178, 208, 229], "kaim": 226, "kd": 251, "keep": [172, 191, 198, 232, 233, 248], "kei": [166, 174, 189, 190, 191, 193, 197, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212, 234, 241, 245], "kept": [13, 232, 244, 247, 252], "kera": [12, 13, 14, 16, 17, 18, 20, 21, 22, 23, 187, 192, 193, 194, 198, 199, 200, 201, 202, 214, 217, 218, 219, 220, 221, 222, 227, 232, 237, 244, 247, 249, 251], "keraslayeroutput": [20, 217], "kernel": [30, 201, 223, 227, 229, 243, 251], "kernel_s": [21, 172, 173, 179, 201, 221, 222, 242, 244, 247], "key_dim": [21, 196, 197], "keyring_1": 241, "keyword": [30, 160, 173], "kill": 240, "kl": 236, "kldivloss": 92, "know": [30, 188, 195, 196, 201, 206, 209, 255], "knowledg": 245, "known": [173, 243, 246, 255], "kullback": 236, "kuzmin": 226, "kwarg": [21, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 173, 174, 175, 179, 197, 232, 244, 247, 251], "l1": [11, 180, 186, 234, 235], "l1loss": 93, "l2": 234, "lab": 236, "label": [188, 192, 193, 194, 195, 198, 199, 201, 206, 208, 214, 218, 219, 220, 227, 232, 244, 247], "label_dataset": 218, "label_mod": [192, 193, 198, 199, 201, 214, 219, 220, 227, 232, 244, 247], "labeled_data": 219, "labeled_data_load": 219, "lambda": [25, 170, 174, 182, 183, 192, 193, 194, 195, 196, 197, 201, 214, 218, 219, 220, 232, 243, 244, 247, 251], "laptop": [236, 237], "larg": [176, 192, 224, 226, 227, 229, 247], "larger": [227, 229, 243], "last": [13, 232], "lastli": 244, "latenc": [224, 233, 251, 255], "later": [179, 185, 188, 195, 196, 200, 201, 209, 213, 239, 242], "latest": [174, 234, 239, 240, 241, 242, 251], "launch": 187, "layer": [1, 3, 4, 5, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 100, 168, 169, 170, 173, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 192, 196, 203, 204, 205, 214, 216, 219, 220, 221, 223, 224, 227, 228, 229, 230, 233, 235, 236, 243, 244, 246, 247, 248, 249, 250, 251], "layer1": [200, 213], "layer2": [200, 213], "layer_nam": [9, 22, 177, 184, 218], "layer_output_util": [6, 19, 167, 217], "layern": [200, 213], "layernorm": [21, 98, 196, 197], "layernorm1": [21, 197], "layernorm2": [21, 197], "layeroutpututil": [7, 20, 169, 217], "layers_to_exclud": 173, "layout": [160, 172], "lceil": [161, 162, 164, 165, 166, 246], "lead": [9, 202, 210, 214, 218, 222, 243, 252], "leaf": [172, 174, 251], "leakyrelu": 99, "learn": [10, 23, 171, 179, 185, 188, 189, 190, 191, 193, 194, 195, 196, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 221, 222, 226, 233, 235, 236, 241, 244, 247, 251, 255], "learnabl": [161, 162], "learnedgrid": 251, "learnedgridquant": 171, "learning_r": [203, 204, 205, 209, 211, 212, 220, 247], "learning_rate_schedul": [203, 204, 205, 209, 211, 212], "least": [182, 189, 193, 207], "leav": 214, "left": [161, 162, 163, 164, 165, 166, 178, 188, 189, 190, 191, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 225, 230, 232, 246], "legaci": [239, 251], "leibler": 236, "len": [1, 24, 172, 181, 189, 192, 196, 206, 208, 214, 219, 227, 232, 244, 247], "length": [170, 175, 243], "leq": [161, 162, 163, 164, 165], "less": [192, 206, 223, 225, 232, 233, 236, 243, 246, 248], "lesser": [188, 206], "let": [172, 188, 192, 195, 196, 201, 206], "level": [1, 2, 3, 4, 5, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 24, 25, 26, 27, 28, 29, 170, 171, 172, 174, 177, 178, 180, 181, 182, 183, 184, 185, 186, 189, 191, 192, 193, 196, 198, 199, 203, 204, 205, 207, 211, 212, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 232, 233, 234, 235, 237, 243, 244, 246, 247, 251, 252], "leverag": 243, "lfloor": [161, 162, 163, 164, 165, 166], "lib": [240, 241], "libjpeg": 241, "liblapack": 239, "libpymo": [171, 239], "libqnnhtp": 254, "libqnnmodeldlc": 254, "librari": [1, 10, 176, 214, 236, 240, 241, 244, 254], "lie": 243, "light": [188, 189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213], "like": [21, 171, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 226, 231, 232, 233, 237, 243, 245, 246, 248, 254, 255], "limit": [188, 189, 190, 191, 193, 194, 195, 196, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 251], "line": [240, 241, 244, 254], "linear": [1, 4, 12, 15, 16, 24, 27, 30, 100, 170, 171, 172, 173, 175, 176, 181, 214, 220, 221, 222, 223, 231, 243, 251], "linear1": [170, 243], "linear_1": 243, "link": [187, 241], "linux": [239, 241], "list": [1, 2, 3, 4, 7, 10, 13, 14, 15, 16, 18, 20, 21, 24, 25, 27, 28, 29, 59, 60, 61, 109, 110, 111, 160, 164, 165, 166, 169, 170, 172, 173, 174, 175, 177, 180, 181, 182, 183, 184, 186, 188, 192, 195, 197, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 225, 227, 229, 234, 235, 240, 241, 243, 244, 248], "list_of_module_comp_ratio_pair": [18, 29, 223, 227, 229], "listen": 228, "lite": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "liter": [174, 234], "littl": [171, 189, 193, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212, 226], "llm": 251, "ln": 241, "load": [23, 172, 173, 176, 179, 185, 188, 189, 190, 191, 196, 214, 217, 219, 220, 221, 222, 223, 226, 227, 229, 232, 235, 236, 244, 247, 251, 254], "load_checkpoint": [179, 185], "load_data": 196, "load_dataset": [214, 219, 220, 232, 235, 244], "load_encod": [214, 244, 247], "load_encodings_to_sim": [217, 244, 247], "load_model": [188, 189, 190, 191, 214, 217, 219, 221, 222, 232, 244], "load_state_dict": 166, "loader": [1, 2, 3, 9, 11, 24, 25, 26, 174, 180, 181, 182, 183, 186, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 205, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 232, 235, 244, 247], "local": [187, 228, 235, 239, 240, 241], "localresponsenorm": 101, "locat": [188, 192, 195, 200, 201, 206, 209, 213, 240], "log": [173, 174, 192, 195, 196, 198, 199, 218, 232, 234], "log_2": 166, "log_dir": [195, 196, 198, 199], "log_fil": [174, 234], "log_input": 122, "log_prob": 48, "logdir": 196, "logger": 173, "logic": [2, 13, 30, 100, 175, 182, 188, 189, 190, 191, 203, 204, 205, 206, 207, 210, 211, 212, 213, 232, 251], "logit": [206, 208, 232, 247], "logsigmoid": 102, "logsoftmax": 103, "long": 243, "longer": [171, 179, 203, 204, 205, 239, 244, 247, 255], "look": [21, 188, 192, 195, 196, 197, 200, 201, 206, 213, 219, 237, 254], "lookup": 197, "lookup_quant": [13, 232], "loop": [172, 188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 247, 252], "lora": [176, 236, 251], "lora_a": 176, "lora_alpha": 176, "lora_b": 176, "lora_config": 176, "lora_dropout": 176, "lora_modul": 176, "lora_weights_after_adaptation_for_adapter1": 176, "loraconfig": 176, "lose": 230, "loss": [1, 9, 11, 12, 22, 24, 158, 177, 180, 181, 184, 186, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 207, 210, 211, 212, 214, 219, 220, 226, 233, 235, 237, 244, 246, 247], "loss_fn": [11, 180, 186, 195, 220, 235, 247], "lost": 246, "low": [171, 178, 201, 203, 205, 210, 226, 231, 236, 244, 251], "lower": [1, 24, 181, 188, 192, 201, 206, 214, 222, 225, 232, 233, 236, 243, 252, 255], "lowest": 223, "lpbq": [170, 243, 251], "lppool1d": 94, "lppool2d": 95, "lr": [220, 247], "lstm": [96, 251], "lstmcell": 97, "lsvrc": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "lt": 239, "m": [187, 196, 229, 239, 240, 241, 242], "mac": [18, 29, 188, 203, 204, 205, 206, 223, 224, 226, 227, 229, 231, 232], "machin": [226, 236, 239, 240, 241, 251, 255], "made": [172, 174, 206, 207, 210, 211, 212, 234, 236, 248, 251], "magnitud": 223, "mai": [2, 3, 14, 21, 25, 30, 158, 159, 160, 168, 171, 172, 175, 176, 182, 183, 188, 190, 192, 195, 196, 200, 201, 202, 206, 209, 210, 213, 216, 218, 219, 220, 222, 226, 232, 243, 245, 251, 255], "main": [220, 248], "maintain": [189, 193, 207, 219, 225, 226, 236], "major": [226, 245], "make": [174, 175, 195, 196, 197, 201, 209, 234, 236, 248, 249, 250, 251, 255], "makedir": [188, 189, 192, 193, 195, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "man": 241, "manag": [239, 242], "mandatori": 217, "manditori": 21, "mani": [172, 188, 192, 195, 196, 200, 201, 203, 204, 205, 206, 213, 214, 224, 251], "manner": [3, 14, 25, 183, 219, 222], "mantissa": [166, 170, 243], "mantissa_bit": [166, 170, 243], "manual": [18, 29, 161, 162, 171, 174, 203, 204, 205, 219, 223, 226, 227, 229, 239, 245, 251], "manual_param": [223, 227, 229], "manual_se": 206, "manualmodeparam": [18, 29, 223, 227, 229], "manylinux_2_34_x86_64": [239, 241], "map": [13, 14, 23, 30, 159, 164, 165, 173, 175, 176, 192, 193, 194, 195, 197, 201, 214, 218, 219, 220, 225, 232, 236, 243, 244, 245, 246, 247, 248, 251], "map_loc": 217, "marginrankingloss": 105, "marku": 226, "mask": 30, "maskedadd": 30, "match": [166, 179, 185, 197, 215, 217, 218, 223, 226, 227, 229, 230, 243, 244, 247, 248, 252], "math": [214, 219, 232, 244, 255], "mathemat": [178, 188, 201, 206, 236, 250], "matmul": [8, 251], "matmul_8": 173, "matrix": 201, "matter": [30, 213], "max": [9, 22, 161, 162, 166, 168, 171, 176, 177, 184, 191, 198, 199, 201, 215, 216, 226, 239, 244, 245, 246, 251, 254], "max_epoch": [203, 204, 205, 209, 211, 212], "maximum": [2, 3, 10, 13, 14, 22, 23, 25, 164, 165, 166, 179, 182, 183, 185, 188, 192, 193, 194, 195, 200, 201, 202, 206, 213, 218, 219, 232, 244, 247], "maxlen": [21, 196, 197], "maxpool1d": 106, "maxpool2d": 107, "maxpool3d": 108, "maxpooling2d": 195, "maxunpool1d": 109, "maxunpool2d": 110, "maxunpool3d": 111, "mean": [26, 175, 188, 192, 195, 196, 197, 200, 201, 203, 204, 205, 206, 208, 209, 210, 213, 214, 219, 220, 232, 244, 246, 248], "meaning": 192, "measur": [2, 9, 13, 22, 29, 177, 182, 184, 188, 203, 204, 205, 206, 218, 223, 227, 229, 232, 236], "mechan": [23, 172, 178, 185, 214, 234, 244, 247], "meet": [3, 14, 25, 182, 183, 188, 192, 206, 219, 224, 225, 232, 255], "member": [11, 248], "memori": [18, 29, 160, 176, 203, 204, 205, 223, 224, 226, 227, 229, 230, 231, 233, 236, 251, 255], "memory_format": 160, "mention": 243, "merg": 236, "messag": 192, "met": [2, 13, 182, 188, 192, 206, 232, 255], "meta": 176, "meta_data": 176, "metapackag": 187, "method": [3, 7, 14, 20, 23, 25, 30, 100, 160, 169, 171, 172, 174, 175, 179, 182, 183, 185, 189, 190, 191, 193, 194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 217, 219, 225, 226, 231, 232, 234, 235, 236, 243, 244, 246, 247, 252], "methodologi": 255, "metric": [18, 29, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 218, 219, 220, 223, 227, 229, 232, 235, 244, 246, 247, 255], "mha": [196, 251], "might": [2, 13, 182, 188, 189, 192, 193, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 218, 232, 241, 245, 251, 252], "migrat": [167, 244, 251], "mimick": 171, "min": [9, 22, 161, 162, 168, 171, 176, 177, 184, 191, 198, 199, 201, 215, 216, 239, 245, 246, 251, 254], "min_max_rang": [200, 213, 218], "minim": [10, 23, 179, 180, 185, 186, 188, 191, 192, 198, 199, 206, 211, 212, 222, 231, 232, 233, 235, 236, 237, 244, 246, 247], "minima": 235, "minimum": [10, 23, 164, 165, 172, 179, 185, 200, 213, 235, 244, 247, 251], "minmaxencodinganalyz": 166, "minor": [245, 251], "miou": [188, 206], "mish": 112, "mismatch": [178, 201, 244], "miss": [21, 166, 173, 215, 217, 245, 254], "missing_kei": 166, "mix": [2, 3, 13, 14, 25, 174, 182, 183, 187, 219, 249, 251, 255], "mixed_precis": [6, 19, 167, 188, 192, 206, 232, 234], "mixed_precision_algo": [2, 174, 182, 188, 192, 206, 232], "mixed_preision_quant_model": 232, "mixedprecisionconfigur": [174, 234], "mixin": [30, 175], "mkdir": 187, "ml": [191, 196, 198, 199, 211, 212, 226], "mmp": 233, "mmp_log": [174, 234], "mnist": [223, 227, 229], "mnist_after_bn_re_estimation_qat_range_learn": 195, "mnist_torch_model": 223, "mnist_trained_on_gpu": [223, 227, 229], "mnt": 240, "mobil": [236, 237], "mobilenet": [214, 220, 235, 236], "mobilenet_v2": [214, 219, 220, 221, 222, 232, 234, 235, 242, 244, 247], "mobilenet_v2_weight": [214, 219, 221, 222, 232, 235, 244], "mobilenetv2": [214, 219, 220, 221, 222, 242, 244, 247], "mobilenetv2_1": [214, 221, 222], "mode": [3, 10, 14, 18, 23, 25, 29, 172, 179, 183, 185, 203, 204, 205, 219, 223, 227, 229, 244, 248, 251], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 251, 253, 254, 255, 256], "model_after_qat": [196, 198, 199], "model_input": [173, 195], "model_or_pipelin": [214, 235], "model_output": 195, "model_prepar": [19, 167, 197, 206, 207, 209, 210, 211, 212, 213, 218, 221, 222, 251], "model_prepare_requir": [25, 183, 219], "model_preparer_elementwise_add_exampl": 172, "model_preparer_functional_exampl": 172, "model_preparer_reused_exampl": 172, "model_preparer_subclassed_model_with_functional_lay": 21, "model_preparer_two_subclassed_lay": 21, "model_torch": 217, "model_transform": 172, "model_valid": 167, "model_weights_in_correct_ord": 197, "modelcompressor": [18, 29, 203, 204, 205, 223, 227, 229], "modelprepar": [21, 172, 197, 206, 207, 210, 211, 212, 213], "modelproto": [1, 4, 5, 7, 9, 10, 214, 217, 218, 221, 222, 244], "modelvalid": 173, "modelwithelementwiseaddop": 172, "modelwithfunctionallinear": 173, "modelwithfunctionalrelu": 172, "modelwithnontorchfunct": 172, "modelwithoutfunctionallinear": 173, "modelwithoutreusednod": 173, "modelwithreusednod": 173, "modelwithreusedrelu": 172, "modif": [206, 207, 210, 211, 212], "modifi": [23, 172, 176, 179, 185, 188, 189, 190, 191, 193, 194, 195, 196, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 219, 221, 222, 230, 232, 241, 244, 246, 247, 251], "modul": [1, 2, 8, 13, 18, 24, 25, 26, 27, 28, 29, 30, 100, 166, 168, 169, 170, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 230, 232, 234, 235, 242, 243, 244, 247, 249, 250, 251, 255], "module_cl": 30, "module_classes_to_exclud": 172, "module_to_exclud": 172, "modulecompratiopair": [18, 29, 223, 227, 229], "moduledict": [30, 171, 175, 179, 242, 244, 247], "modulelist": [30, 171, 175, 179, 242, 244, 247], "modules_to_exclud": [25, 172, 180, 183, 186, 219, 235], "modules_to_ignor": [18, 29, 177, 184, 203, 204, 205, 218, 223, 227, 229], "momentum": [220, 221, 222], "monitor": 218, "monoton": [18, 29, 223, 225, 227, 229], "more": [1, 10, 18, 23, 24, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 167, 171, 173, 175, 181, 182, 185, 188, 192, 195, 196, 197, 200, 201, 206, 209, 213, 214, 218, 222, 223, 225, 226, 227, 228, 229, 232, 233, 234, 236, 244, 245, 246, 247, 248, 251, 252, 254, 255], "most": [178, 196, 236, 239, 240, 243, 248], "move": [187, 247], "movi": 196, "mp": [174, 234], "mp_configur": 234, "mse": [9, 11, 22, 171, 177, 180, 184, 186, 215, 246], "mseloss": 104, "much": [203, 204, 205, 230], "mul": 176, "multi": [196, 251], "multigpu": 251, "multiheadattent": [21, 196, 197, 251], "multilabelmarginloss": 113, "multilabelsoftmarginloss": 114, "multimarginloss": 115, "multipl": [18, 25, 27, 28, 29, 160, 169, 172, 173, 174, 175, 179, 183, 185, 200, 213, 217, 219, 221, 222, 223, 226, 227, 229, 234, 241, 243, 244, 246, 247, 251], "multipli": [175, 188, 206, 224, 226, 227, 229, 231], "must": [21, 160, 166, 170, 173, 175, 189, 190, 191, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 210, 211, 212, 213, 214, 218, 219, 220, 224, 230, 235, 243, 244, 248], "mutual": [166, 170, 243], "my_functional_model": 21, "myfunctionalmodel": 21, "n": [9, 14, 22, 177, 178, 184, 218, 219, 229, 243], "nagel": 226, "name": [1, 2, 9, 13, 18, 21, 22, 30, 168, 169, 174, 175, 176, 177, 179, 182, 184, 185, 188, 189, 190, 191, 195, 197, 200, 213, 214, 216, 217, 218, 219, 221, 222, 227, 228, 232, 234, 240, 241, 244, 245, 246, 247, 251, 254], "name_": [9, 22, 177, 184, 200, 213, 218], "name_to_module_dict": 176, "name_to_quantizer_dict": [2, 13, 174, 182, 232], "namedtupl": 166, "namespac": [167, 171, 181, 182, 183, 184, 185, 186], "naming_schem": [169, 217], "namingschem": [169, 217], "nativ": [175, 240], "navig": 187, "na\u00efv": 231, "nconv": 221, "ndarrai": [2, 3, 7, 9, 10, 13, 197, 214, 217, 218, 219, 232, 244], "nearest": [3, 10, 14, 22, 23, 25, 183, 185, 189, 192, 193, 196, 198, 199, 201, 202, 207, 210, 214, 218, 219, 244, 247], "necessari": [158, 159, 160, 179, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 209, 210, 211, 212, 213, 218, 221, 223, 227, 228, 229, 243, 244, 247, 255], "necessarili": [188, 192, 206, 232], "necessit": 239, "need": [1, 9, 18, 20, 21, 22, 23, 24, 29, 169, 174, 176, 177, 181, 184, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 220, 221, 222, 223, 227, 229, 232, 233, 234, 235, 236, 239, 241, 243, 244, 245, 246, 247, 248, 249, 251, 254, 255], "neg": [148, 149, 164, 165, 188, 195, 196, 201, 206, 220, 246], "negat": [189, 193, 207], "net": [187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 254], "network": [21, 175, 194, 196, 197, 201, 208, 219, 224, 225, 226, 228, 236, 242, 246, 247, 251, 252], "neural": [188, 194, 201, 206, 208, 219, 224, 226, 236, 246, 247, 252], "neuron": 236, "new": [16, 21, 160, 161, 162, 171, 172, 176, 179, 185, 193, 197, 198, 199, 201, 202, 206, 213, 221, 232, 234, 237, 239, 243, 245, 249, 250, 251], "new_empti": 160, "next": [176, 188, 190, 192, 195, 196, 201, 206, 209, 214, 222, 241, 242, 244, 246, 247, 251, 252], "next_conv_weight": 222, "nfolded_model": 221, "night": [189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213], "nllloss": 116, "nllloss2d": 117, "nmodel": [221, 222], "nn": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 167, 168, 170, 171, 172, 173, 174, 179, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 223, 227, 229, 232, 234, 235, 242, 243, 244, 247, 250, 251, 255], "nncf": 236, "nnext": 222, "no_grad": [173, 179, 206, 207, 208, 210, 211, 212, 213, 214, 232, 235, 242, 244, 247], "node": [23, 25, 172, 179, 182, 183, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 210, 211, 212, 219, 221, 222, 235, 236, 244, 247, 250, 251], "noffset": 178, "nois": [10, 23, 179, 185, 218, 222, 244, 247, 248, 255], "noisi": [195, 209], "non": [172, 174, 180, 186, 194, 208, 228, 235, 246, 251, 254], "none": [1, 2, 3, 7, 9, 10, 12, 13, 14, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 59, 60, 61, 71, 79, 80, 96, 97, 100, 109, 110, 111, 123, 124, 160, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 228, 229, 232, 234, 235, 239, 240, 241, 242, 243, 244, 247], "norm": [16, 192, 202, 214, 218, 222, 232, 251], "normal": [16, 208, 214, 218, 219, 221, 232, 236, 244, 251], "note": [2, 3, 9, 10, 13, 14, 21, 22, 23, 25, 171, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 218, 219, 223, 227, 228, 229, 232, 234, 235, 239, 240, 241, 243, 244, 247], "note1": [188, 206, 209, 213], "note2": [188, 206, 209, 213], "notebook": [189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 251], "noth": [18, 29, 223, 227, 229], "notic": [168, 216, 251], "notimplementederror": 30, "now": [21, 172, 173, 178, 179, 188, 189, 190, 191, 192, 195, 196, 197, 198, 199, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 219, 221, 222, 232, 244, 247, 251], "np": [2, 3, 21, 192, 197, 214, 218, 219, 227, 232, 239, 244], "nprepared_model": [221, 222], "nprev": 222, "nscale": 178, "num": [164, 165], "num_batch": [1, 9, 11, 12, 22, 24, 26, 180, 181, 186, 189, 190, 193, 194, 195, 200, 201, 207, 208, 209, 210, 213, 214, 218, 219, 220, 235, 244], "num_bias_correct_sampl": 210, "num_calibration_sampl": [214, 219, 220, 232, 244, 247], "num_candid": [11, 180, 186, 235], "num_class": 218, "num_comp_ratio_candid": [18, 29, 203, 204, 205, 223, 227, 229], "num_epoch": 220, "num_eval_sampl": [219, 232, 244], "num_head": [21, 196, 197], "num_iter": [192, 227, 232], "num_of_sampl": 219, "num_quant_sampl": 210, "num_reconstruction_sampl": [29, 203, 205, 223], "num_sampl": [2, 174, 182, 194, 195, 206, 208, 214, 218, 232], "num_samples_for_phase_1": [3, 14, 25, 183, 219], "num_samples_for_phase_2": [3, 14, 25, 183, 219], "num_step": [164, 165], "num_word": 196, "num_work": [188, 189, 190, 191, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 219, 220, 235], "number": [1, 2, 3, 9, 10, 11, 12, 14, 15, 18, 21, 22, 23, 24, 25, 26, 29, 30, 164, 165, 166, 170, 172, 174, 175, 180, 181, 182, 183, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 218, 219, 220, 223, 225, 226, 227, 228, 229, 230, 232, 235, 236, 240, 243, 244, 246, 247, 251], "numer": [169, 217], "numpi": [188, 189, 190, 191, 197, 214, 218, 219, 227, 232, 239, 244], "numpy_help": [221, 222], "nupi": [13, 232], "nvidia": [236, 239, 240, 241, 242], "o": [188, 189, 192, 193, 194, 195, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 219, 221, 222, 223, 227, 229, 232, 239, 244], "object": [1, 2, 3, 8, 9, 10, 11, 13, 14, 23, 25, 30, 158, 159, 160, 166, 168, 170, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 188, 194, 195, 200, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 223, 227, 229, 232, 233, 234, 235, 237, 243, 244, 246, 247, 255], "observ": [30, 166, 175, 179, 201, 209, 218, 226, 244, 246, 247], "obtain": [7, 169, 218, 224, 245, 254, 255], "obvious": [223, 227, 229], "occur": [168, 216, 245], "occurr": [9, 22, 177, 184, 218, 223], "oct": 226, "off": [1, 12, 23, 24, 181, 185, 188, 192, 206, 214, 220, 232, 244, 247, 255], "offer": [9, 177, 184, 194, 208, 218, 219, 222, 239, 255], "offset": [9, 22, 30, 71, 161, 162, 163, 164, 165, 177, 178, 184, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 218, 239, 242, 243, 244, 245, 246, 247, 251, 254], "offset_": [161, 162, 163, 164, 165], "often": [219, 221, 222, 226, 247], "older": [234, 239], "omit": [225, 240, 248], "onboard": 255, "onc": [30, 160, 173, 176, 178, 191, 196, 198, 199, 203, 204, 205, 210, 211, 212, 218, 223, 226, 234, 247, 249, 250, 254, 255], "one": [24, 25, 27, 172, 173, 174, 176, 178, 179, 181, 182, 183, 185, 187, 188, 189, 190, 191, 192, 193, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 219, 221, 223, 226, 227, 228, 229, 234, 239, 240, 241, 242, 243, 244, 245, 247, 251], "ones": [160, 242], "ones_lik": [161, 162], "onli": [2, 9, 10, 13, 23, 158, 159, 160, 164, 165, 168, 170, 171, 172, 174, 175, 177, 178, 179, 182, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 216, 217, 218, 221, 226, 227, 228, 230, 232, 234, 235, 239, 240, 241, 243, 244, 246, 247, 248, 249, 250, 251, 255], "onnx": [0, 1, 2, 4, 6, 7, 9, 10, 25, 169, 174, 179, 183, 185, 187, 214, 217, 218, 219, 221, 222, 232, 234, 236, 237, 239, 240, 241, 243, 244, 245, 246, 247, 248, 250, 251, 254, 255], "onnx_encoding_path": 244, "onnx_export_arg": [25, 169, 179, 183, 185, 217, 219, 244, 247], "onnx_file_nam": 250, "onnx_model": 218, "onnx_util": 217, "onnxexportapiarg": [25, 169, 179, 183, 185, 217, 219, 244, 247], "onnxmodel": [3, 9, 218, 219], "onnxruntim": [188, 189, 190, 191, 217, 218, 219, 241, 244, 251], "onnxruntime_v": 241, "onnxsim": [188, 189, 190, 191, 214, 217, 218, 219, 221, 222, 232, 244], "op": [1, 2, 3, 10, 12, 13, 16, 18, 23, 24, 25, 173, 179, 181, 182, 183, 185, 188, 189, 190, 191, 193, 195, 196, 198, 199, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 214, 219, 221, 227, 244, 246, 247, 248, 251, 254], "op_list": [195, 248], "op_typ": [2, 8, 182, 188, 195, 206, 232, 248], "op_type_map": 173, "open": [195, 236], "oper": [8, 18, 21, 30, 160, 171, 172, 173, 175, 188, 191, 192, 197, 198, 199, 206, 211, 212, 217, 222, 227, 231, 232, 235, 236, 242, 244, 246, 247, 248, 250, 251, 252, 255], "oppos": [190, 202, 210], "opset": 251, "opset_vers": [179, 185, 244, 247], "opt": 192, "optim": [1, 2, 3, 10, 12, 13, 14, 23, 24, 25, 29, 166, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 226, 228, 232, 235, 236, 237, 241, 244, 246, 247, 251, 255], "optimized_accuraci": [208, 219], "option": [1, 2, 3, 9, 10, 11, 12, 13, 14, 18, 22, 23, 24, 25, 26, 29, 30, 160, 161, 162, 163, 164, 165, 166, 170, 172, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 223, 227, 229, 232, 235, 237, 239, 240, 241, 243, 244, 246, 247, 248, 251, 254, 255], "optmiz": [188, 206], "orang": 230, "order": [30, 100, 173, 175, 189, 195, 196, 197, 198, 199, 200, 203, 204, 205, 207, 209, 211, 212, 213, 223, 234, 243, 247, 248, 249, 250, 254], "org": [172, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 239, 240, 241], "org_top1": 232, "organ": [236, 253], "origin": [18, 21, 29, 30, 167, 171, 172, 175, 180, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 218, 223, 224, 225, 227, 228, 229, 231, 232, 235, 236, 246, 251], "original_model": [21, 197], "original_model_weight": 197, "ort": [188, 189, 190, 191, 219, 244], "oscil": 220, "other": [160, 170, 171, 172, 174, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 225, 234, 237, 239, 242, 243, 246, 250, 251, 252], "otherwis": [3, 10, 14, 161, 162, 164, 165, 170, 173, 176, 179, 185, 189, 190, 191, 193, 194, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 219, 222, 243, 244, 247, 252], "our": [176, 195, 196, 209], "out": [9, 21, 22, 23, 161, 162, 163, 164, 165, 166, 172, 176, 177, 178, 179, 184, 185, 197, 215, 218, 239, 242, 244, 247, 251], "out1": [21, 197, 234], "out2": 234, "out3": 234, "out_": [161, 162, 163, 164, 165], "out_channel": [170, 243], "out_featur": [30, 171, 175], "outlier": [218, 246], "outlin": [224, 255], "output": [2, 3, 7, 8, 9, 10, 13, 14, 18, 20, 21, 22, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162, 164, 166, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 226, 227, 229, 230, 231, 232, 235, 236, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254], "output_bw": [3, 14, 25, 176, 183, 219], "output_dim": [21, 196, 197], "output_dir": 254, "output_dir_path": 254, "output_dlc": 254, "output_encod": 30, "output_nam": [179, 185, 188, 189, 190, 191, 214, 219, 221, 232, 244, 247], "output_op_nam": [13, 18, 227, 232], "output_path": 254, "output_qtzr": 30, "output_quant": [13, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 174, 175, 179, 182, 232, 242, 244, 247], "output_s": [59, 60, 61, 109, 110, 111], "output_shap": 197, "outsid": 248, "over": [3, 9, 21, 25, 164, 165, 175, 178, 183, 218, 219, 223, 225, 226, 227, 229, 231, 232], "overal": [224, 225, 252, 253], "overfit": 235, "overhead": [192, 203, 205, 206, 246], "overlin": [162, 165], "overload": [164, 165, 179, 244, 247], "overrid": [25, 172, 179, 183, 185, 206, 207, 210, 211, 212, 219, 244, 247], "overridden": [30, 175, 248], "overview": 171, "overwri": [244, 247], "overwriiten": [244, 247], "overwritten": 171, "own": [218, 219], "p": [188, 206, 240], "p1": 234, "p2": 234, "packag": [167, 187, 241, 242, 251], "pad": [21, 172, 173, 179, 195, 221, 222, 242, 244, 247], "pad_sequ": 196, "page": [224, 237, 239, 240, 241, 242, 243, 246, 255], "pair": [4, 15, 18, 27, 29, 220, 221, 223, 227, 229], "pairwisedist": 119, "param": [1, 2, 11, 12, 13, 18, 22, 24, 29, 30, 100, 173, 174, 175, 176, 177, 180, 181, 182, 184, 186, 188, 189, 190, 191, 193, 194, 195, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 223, 227, 229, 232, 233, 234, 235, 243], "param_bitwidth": [214, 244, 245, 247], "param_bw": [3, 14, 25, 176, 183, 219], "param_bw_override_list": [1, 24, 181, 214], "param_encod": [171, 245], "param_nam": [9, 22, 177, 184, 218], "param_name_": [9, 22, 177, 184, 200, 213, 218], "param_quant": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 171, 175, 176, 179, 242, 243, 244, 247], "paramet": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 223, 226, 227, 229, 231, 232, 234, 235, 236, 243, 244, 245, 248, 251, 254, 255], "parameter_quant": [2, 13, 174, 182, 232], "parent": [30, 100, 175, 232], "pareto": [2, 3, 13, 14, 25, 182, 183, 188, 192, 206, 219], "pareto_front": 219, "pareto_front_list": [188, 206, 232], "pars": [24, 179, 181, 185, 214, 244, 247], "part": [9, 21, 22, 177, 184, 218, 219, 226, 243, 244], "partial": [25, 172, 183, 219, 244, 247], "particular": [170, 173, 188, 192, 206, 232, 243, 248], "pass": [1, 2, 3, 9, 10, 13, 14, 21, 22, 23, 24, 25, 26, 30, 100, 166, 169, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 227, 228, 232, 234, 235, 236, 243, 244, 246, 247, 250, 251, 252, 254, 255], "pass_calibration_data": [188, 189, 190, 191, 193, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 220, 244, 247], "passwd": 240, "past": [187, 251], "patch": 245, "path": [1, 2, 3, 9, 10, 12, 13, 14, 18, 22, 23, 24, 25, 29, 168, 172, 174, 176, 177, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 240, 241, 244, 247, 254], "path_to_imagenet": [232, 244, 247], "pathlik": [244, 247], "pattern": [226, 251], "pb": 254, "pcq": [9, 22, 177, 184, 218], "pcq_quantsim_config": 201, "pdf": [9, 22, 177, 184, 218, 251], "peft": [167, 251], "peft_util": 176, "peftquantutil": 176, "pendyam": 226, "per": [8, 9, 18, 22, 29, 169, 170, 175, 176, 177, 184, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 217, 220, 221, 222, 223, 227, 228, 229, 231, 232, 235, 236, 245, 246, 248, 251], "per_channel_quant": [175, 195, 201, 245, 248], "per_layer_mse_loss": [200, 213, 218], "per_layer_quant_dis": [200, 213, 218], "per_layer_quant_en": [200, 213, 218], "per_sample_weight": 71, "percentag": [189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213, 236], "percentil": [10, 23, 179, 185, 244, 247], "perform": [2, 3, 4, 5, 9, 11, 13, 14, 17, 22, 24, 25, 26, 28, 30, 100, 161, 162, 166, 175, 176, 177, 180, 181, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 194, 196, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 213, 214, 215, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 235, 236, 239, 241, 242, 243, 244, 245, 246, 247, 251, 253, 255], "perform_per_layer_analysis_by_disabling_quant": [9, 218], "perform_per_layer_analysis_by_disabling_quant_wrapp": [22, 177, 184, 218], "perform_per_layer_analysis_by_enabling_quant": [9, 218], "perform_per_layer_analysis_by_enabling_quant_wrapp": [22, 177, 184, 218], "perhap": [188, 195, 200, 201, 206, 209, 213], "period": [1, 12, 24, 181, 214], "permit": 243, "persist": 166, "person": 236, "perspect": [188, 201, 206], "phase": [2, 3, 13, 14, 25, 174, 182, 183, 188, 192, 206, 219, 226], "phase1": [2, 13, 182, 188, 206, 232], "phase1_optim": [2, 13, 182, 188, 206, 232], "phase2": 182, "phase2_revers": 182, "phone": [236, 237], "php": [187, 188, 192, 195, 200, 201, 206, 209, 213], "pick": [2, 13, 21, 182, 188, 192, 206, 224, 225, 232], "pickl": [18, 29, 223, 227, 229], "pictur": [189, 190, 191, 192, 193, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213], "piec": [21, 172], "pillow": 241, "pin": [160, 251], "pin_memori": [160, 172, 206], "pink": 230, "pinpoint": 218, "pip": [187, 228, 239, 240, 241, 242, 251], "pip3": 241, "pipelin": [25, 183, 196, 198, 199, 217, 219, 246, 251, 252, 255], "pitr": 226, "pixelshuffl": 120, "pixelunshuffl": 121, "place": [2, 5, 13, 23, 24, 28, 179, 181, 182, 185, 188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 222, 232, 243, 244, 247, 248, 251], "placement": [188, 189, 190, 191, 203, 204, 205, 206, 207, 210, 211, 212, 213, 251], "plai": 201, "plan": 240, "platform": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212, 236, 237, 239], "pleas": [167, 171, 173, 176, 179, 185, 188, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 209, 213, 218, 220, 223, 227, 229, 239, 244, 247, 251, 254, 255], "plot": [2, 13, 168, 182, 188, 192, 200, 206, 213, 216, 218, 232], "pmatrix": [161, 162, 163, 164, 165], "point": [2, 9, 13, 21, 22, 30, 158, 159, 170, 171, 176, 177, 179, 182, 184, 185, 188, 189, 190, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 215, 218, 221, 222, 232, 233, 236, 237, 243, 245, 246, 251, 252, 255], "pointer": [192, 195, 196, 200, 201, 213], "poissonnllloss": 122, "pool": [192, 193, 195, 201, 202], "pop": 197, "popul": 245, "port": [228, 240, 251], "port_id": 240, "portabl": 236, "portion": 21, "pos_emb": [21, 197], "posit": [21, 148, 149, 164, 165, 195, 196, 197], "possess": 235, "possibl": [2, 3, 4, 10, 13, 14, 21, 23, 25, 173, 179, 182, 183, 185, 188, 192, 194, 197, 206, 208, 218, 219, 221, 225, 232, 243, 244, 247, 248, 249, 250, 252], "post": [3, 14, 25, 183, 189, 190, 191, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 218, 219, 222, 226, 231, 236, 237, 244, 245, 246, 247, 251], "post1": 241, "post_training_percentil": [10, 23, 179, 185, 244, 247], "post_training_tf": [1, 9, 10, 12, 22, 23, 24, 168, 177, 179, 181, 184, 185, 188, 192, 193, 195, 196, 198, 201, 202, 206, 209, 210, 214, 216, 218, 219, 244, 245, 247], "post_training_tf_enhanc": [1, 3, 9, 10, 12, 14, 22, 23, 24, 25, 177, 179, 181, 183, 184, 185, 188, 189, 190, 191, 192, 195, 196, 200, 206, 207, 209, 210, 211, 213, 214, 218, 219, 244, 245, 247], "potenti": [195, 209, 228], "power": [233, 236, 251, 252], "pp": 226, "practic": [188, 192, 194, 195, 196, 200, 201, 203, 204, 205, 206, 208, 213, 226, 244], "pre": [187, 215, 217, 218, 219, 223, 226, 231, 235, 239, 247, 251], "preced": [220, 230, 231], "precis": [2, 3, 13, 14, 25, 161, 162, 163, 164, 165, 166, 174, 182, 183, 187, 189, 190, 191, 193, 195, 196, 198, 199, 200, 201, 202, 207, 209, 210, 211, 212, 213, 219, 236, 245, 251, 252], "precomput": 244, "pred": [192, 206, 227, 232], "pred_label": [214, 219, 232, 244], "pred_prob": [214, 219, 232, 244], "predefin": [194, 208], "predict": [13, 192, 200, 217, 218, 227, 232, 236, 244], "prefac": 241, "prefer": [167, 171, 239, 244], "prefix": [1, 12, 23, 24, 175, 176, 179, 181, 185, 214, 244, 247], "prelu": [118, 251], "prepar": [21, 25, 172, 176, 183, 194, 219, 221, 222, 232, 239, 249, 250, 251], "prepare_model": [21, 172, 197, 206, 207, 209, 210, 211, 212, 213, 218, 221, 222], "prepared_model": [172, 218, 221, 222], "prepend": [175, 187], "preprocess": [192, 193, 196, 198, 199, 201, 214, 219, 220, 221, 227, 232, 244, 247], "preprocess_input": [192, 193, 198, 199, 201, 202, 214, 219, 220, 227, 232, 244, 247], "prerequisit": 228, "presenc": 247, "present": [169, 171, 173, 176, 197, 209, 217, 243], "preserv": [166, 172, 214], "preserve_format": 160, "preset": 232, "pretrain": [188, 189, 190, 191, 192, 193, 198, 199, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 232, 234, 244, 246, 247], "pretti": [223, 227, 229], "prev": 222, "prev_conv_weight": 222, "prevent": [171, 172, 223], "previou": [2, 13, 18, 29, 178, 182, 188, 192, 206, 210, 223, 224, 227, 229, 232, 236, 239, 240, 252, 254], "primari": 237, "print": [30, 164, 165, 171, 172, 173, 175, 178, 179, 188, 189, 190, 191, 195, 196, 197, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 218, 219, 220, 221, 222, 223, 227, 229, 232, 239, 241, 242, 244, 247], "prior": [2, 13, 182, 188, 192, 206, 207, 210, 211, 212, 232], "privileg": [187, 241], "probabl": 236, "problem": [172, 239, 251, 252], "problemat": [172, 252], "proce": [3, 14, 25, 183, 219, 221, 241, 255], "procedur": [190, 202, 203, 205, 210, 225, 228, 239, 241, 251], "proceed": [188, 189, 190, 191, 214, 219, 221, 222, 232, 244], "process": [188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 210, 211, 212, 217, 219, 223, 225, 226, 227, 234, 236, 241, 243, 246], "processor": [236, 239, 242], "produc": [3, 14, 25, 158, 159, 168, 172, 183, 188, 200, 201, 206, 213, 215, 216, 218, 219, 225, 232, 236, 243, 245, 246], "product": [224, 236, 237], "profil": [174, 232, 254], "progbar": [193, 198, 199, 201, 202], "progbar_stat_upd": [193, 198, 199, 201, 202], "progress": 228, "project": 236, "prone": [219, 234], "pronounc": 233, "propag": [230, 234, 251], "propagate_encod": [25, 179, 183, 185, 219, 244, 247], "proper": 251, "properli": [161, 162, 217, 251], "properti": [166, 171], "provid": [1, 2, 6, 9, 13, 19, 21, 22, 24, 25, 30, 166, 171, 173, 176, 177, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 221, 224, 225, 226, 227, 228, 232, 234, 235, 236, 243, 244, 245, 246, 247, 252, 254, 255], "proxi": 172, "prune": [29, 187, 204, 224, 225, 227, 229, 230, 231, 236, 251], "pt": 176, "pt_model": [188, 189, 190, 191, 214, 219, 221, 222, 232, 244], "pth": [23, 176, 179, 185, 217, 223, 227, 229, 244, 247], "ptq": [3, 14, 25, 183, 194, 208, 218, 219, 222, 226, 231, 236, 246, 247, 251], "public": [167, 244, 251], "pure": [21, 250], "purpos": [188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 236, 243], "put": [182, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "pypi": [240, 241], "python": [21, 236, 239, 240, 241, 242, 251], "python3": [187, 239, 240, 241, 242, 251], "pythonpath": [187, 217], "pytorch": [0, 25, 167, 169, 172, 173, 175, 178, 179, 180, 183, 185, 186, 187, 193, 194, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 236, 237, 239, 240, 241, 243, 244, 246, 247, 248, 251, 254, 255], "q": [30, 158, 159, 160, 161, 162, 164, 165, 166, 171, 175, 178, 242, 246], "q_modul": 171, "q_output": 30, "qadd": 175, "qairt": 254, "qat": [1, 12, 24, 181, 187, 188, 191, 196, 201, 206, 214, 220, 236, 246, 251, 252], "qat2": 251, "qc": 251, "qc_quantize_op": 251, "qcquantizeop": [2, 232], "qcquantizewrapp": [15, 171, 220, 251], "qdo": 236, "qdq": [162, 166, 255], "qlinear": [30, 171, 175], "qmax": [161, 162, 164, 165, 178, 179, 242, 244, 246, 247], "qmin": [161, 162, 164, 165, 178, 179, 242, 244, 246, 247], "qmodul": 171, "qmul": 175, "qmult": 30, "qnn": [201, 202, 233, 251, 254], "qol": 251, "qsim": 195, "qtzr": 178, "quad": [161, 162, 163, 164, 165, 246], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256], "qualiti": [188, 206], "quant": [1, 2, 3, 9, 10, 14, 22, 23, 24, 25, 177, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 208, 209, 210, 211, 212, 214, 218, 219, 232, 235, 244, 247, 251], "quant_analyz": [6, 19, 167, 171, 200, 213, 218], "quant_analyzer_result": 218, "quant_dequ": 158, "quant_schem": [3, 9, 10, 14, 22, 23, 25, 168, 177, 179, 183, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 235, 239, 242, 244, 245, 247], "quant_sim": 234, "quant_sim_model": [179, 185, 244], "quant_stats_visu": [168, 216], "quant_wrapp": 171, "quantanalyz": [9, 22, 171, 177, 184, 215, 251], "quantiz": [1, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 19, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 165, 166, 167, 168, 170, 172, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 194, 197, 203, 204, 205, 208, 214, 216, 217, 220, 221, 222, 224, 226, 228, 232, 233, 234, 235, 236, 239, 242, 244, 245, 248, 250, 251], "quantizablemultiheadattent": 251, "quantizaion": 231, "quantizaiton": 8, "quantization_overrid": 254, "quantization_tf": 239, "quantizationdatatyp": [2, 3, 10, 13, 14, 23, 25, 171, 174, 179, 182, 183, 185, 188, 192, 206, 219, 232, 244, 247], "quantizationmixin": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 175], "quantizationmod": 239, "quantizationsim": [188, 189, 190, 191, 193, 194, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "quantizationsimmodel": [2, 3, 8, 9, 10, 11, 13, 14, 15, 22, 23, 25, 30, 168, 170, 171, 174, 176, 177, 178, 179, 180, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 232, 234, 235, 242, 243, 244, 245, 246, 247, 248, 251, 254], "quantizationsimmodelv1": 171, "quantizationsimmodelv2": 171, "quantize_lora_scale_with_fixed_rang": 176, "quantized_": [214, 235], "quantized_callback": [195, 196, 198, 199], "quantized_dlc": 254, "quantized_linear": 30, "quantized_mobilenet_v2": [214, 220, 244, 247], "quantized_mobilenetv2": 247, "quantized_model": [179, 244, 247], "quantized_repr": [30, 158, 159, 160], "quantizedadd": 175, "quantizedconv2d": [171, 175, 179, 242, 243, 244, 247], "quantizedequant": [158, 171, 175, 176, 178, 179, 239, 242, 243, 244, 247], "quantizedlinear": [30, 171, 175, 243], "quantizedmaskedadd": 30, "quantizedmultipli": [30, 175], "quantizedrelu": 171, "quantizedsoftmax": 175, "quantizedtensor": [30, 158, 160, 161, 178], "quantizer_arg": 245, "quantizer_config": [13, 232], "quantizer_group": [2, 13, 174, 182, 232], "quantizer_info": [2, 232], "quantizer_nam": [13, 232], "quantizerbas": [30, 175, 178], "quantizergroup": [2, 3, 13, 14, 25, 174, 182, 183, 219, 232], "quantparam": 210, "quantschem": [1, 3, 9, 10, 12, 14, 22, 23, 24, 25, 168, 177, 179, 181, 183, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 232, 235, 242, 244, 247], "quantsim": [1, 6, 7, 9, 12, 19, 20, 22, 24, 167, 168, 169, 171, 174, 176, 177, 181, 184, 187, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 206, 207, 209, 210, 211, 212, 214, 215, 216, 218, 220, 232, 233, 234, 235, 236, 237, 242, 243, 247, 251, 255], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "quantsim_layer_output": 217, "quantsim_layer_output_util": 217, "quatiz": 255, "quic": [187, 236, 239, 240, 241], "quick": [178, 197, 201, 203, 205, 239], "quickli": [188, 192, 200, 201, 206, 213, 237, 242], "r": [2, 9, 13, 22, 176, 177, 182, 184, 201, 218, 232], "rais": [21, 30, 166, 197, 214, 219, 232, 244], "rand": [173, 206, 207, 209, 210, 211, 212, 213, 217, 218], "randn": [30, 159, 160, 161, 162, 172, 173, 175, 179, 188, 189, 190, 191, 208, 214, 218, 219, 220, 221, 222, 232, 234, 235, 239, 242, 244, 247], "random": [21, 197, 208, 218, 219, 223, 232, 239, 242], "random_input": [21, 197], "random_split": 244, "randperm": 206, "rang": [9, 21, 22, 164, 165, 168, 172, 177, 178, 184, 185, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 201, 202, 206, 207, 208, 209, 210, 211, 214, 215, 216, 220, 222, 225, 231, 232, 236, 244, 246, 247, 251, 252], "rank": [18, 29, 176, 227, 229], "rank_select": 229, "rank_select_schem": [29, 229], "rankselectschem": [29, 223, 227, 229], "rare": 251, "rate": [21, 195, 196, 197, 198, 199, 203, 204, 205, 209, 211, 212, 226, 247], "rather": [172, 195, 209, 235, 248], "ratio": [18, 29, 203, 204, 205, 223, 224, 227, 229], "rceil": [164, 165], "re": [2, 13, 178, 180, 182, 186, 187, 188, 197, 206, 232, 235, 251], "re_estimation_dataset": 195, "re_estimation_dataset_s": 195, "reach": [194, 208, 219], "read": 218, "reader": [188, 195, 200, 201, 206, 209, 213], "readi": [23, 178, 179, 185, 188, 189, 190, 191, 192, 195, 196, 198, 199, 201, 203, 204, 205, 206, 207, 210, 211, 212, 213, 244, 247, 252, 255], "readili": [188, 192, 195, 200, 201, 206, 209, 213], "readvariableop": 245, "real": [158, 159, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 228, 232, 242], "realiz": [174, 234], "realli": [188, 195, 200, 201, 209, 213], "reason": [173, 189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212], "recalcul": [195, 209], "receiv": 172, "recent": [240, 255], "recogn": [173, 246], "recommend": [1, 2, 9, 10, 21, 24, 178, 181, 188, 189, 190, 191, 192, 193, 194, 197, 201, 207, 208, 214, 218, 220, 221, 222, 224, 232, 239, 240, 242, 243, 244, 252, 255], "recomput": [176, 251], "reconstruct": [1, 12, 24, 181, 203, 205, 214], "record": [9, 22, 160, 177, 184, 218], "recov": [190, 202, 203, 204, 205, 210, 252], "recurr": 251, "recurs": 251, "redefin": 173, "redesign": 251, "reduc": [175, 176, 188, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 222, 226, 230, 231, 233, 236, 243, 246, 251, 252, 255], "reduct": [203, 204, 205, 224, 255], "redund": [221, 226], "reestim": [15, 26, 195, 220], "reestimate_bn_stat": [15, 26, 195, 209, 220], "ref": 241, "refer": [169, 171, 176, 180, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 217, 232, 235, 243, 244, 245, 246, 247, 254, 255], "reflect": [198, 199, 211, 212, 246], "reflectionpad1d": 128, "reflectionpad2d": 129, "reflectionpad3d": 130, "regard": [2, 13, 182, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213, 232], "regist": [30, 166, 188, 251], "regress": 223, "regular": [1, 12, 13, 23, 24, 30, 178, 179, 181, 185, 188, 196, 214, 232, 244, 247], "rel": [13, 18, 29, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 223, 224, 227, 229, 232, 252], "relat": [171, 223, 227, 229, 237, 246], "relationship": 243, "releas": [187, 239, 240, 241], "release_tag": [187, 240, 241], "relev": [203, 204, 205, 233], "reli": [171, 188, 189, 190, 191, 192, 195, 196, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "reload": 210, "relu": [21, 126, 171, 172, 173, 195, 196, 197, 214, 221, 222, 230, 245, 248, 249, 250, 251], "relu1": [21, 173, 250], "relu2": [21, 173, 249, 250], "relu6": [127, 221, 222], "remain": [171, 180, 186, 214, 235, 251], "remov": [15, 25, 26, 172, 183, 197, 219, 220, 221, 223, 230, 236, 240, 244, 246, 247, 251], "reorder": 197, "reorgan": 251, "repeat": [176, 201, 223, 255], "replac": [171, 172, 175, 176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 241, 243, 246, 247, 251], "replace_lora_layers_with_quantizable_lay": 176, "replaced_module_typ": 176, "replicationpad1d": 131, "replicationpad2d": 132, "replicationpad3d": 133, "repo": [240, 241], "report": [234, 251, 255], "repositori": [187, 240, 241], "repres": [1, 2, 3, 9, 10, 14, 18, 21, 22, 23, 25, 158, 159, 160, 166, 174, 175, 176, 177, 179, 182, 183, 184, 185, 200, 213, 214, 218, 219, 223, 225, 227, 229, 232, 234, 236, 242, 244, 245, 246, 247, 250, 254], "represent": [158, 159, 160, 166, 178, 197, 236, 245, 246, 251, 254], "reqs_deb_common": 241, "reqs_deb_onnx_common": 241, "reqs_deb_onnx_gpu": 241, "reqs_deb_tf_gpu": 241, "reqs_deb_torch_common": 241, "reqs_deb_torch_gpu": 241, "requant": 221, "request": 234, "requir": [2, 18, 21, 29, 30, 169, 171, 172, 176, 178, 179, 185, 188, 189, 192, 197, 200, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 217, 218, 222, 223, 224, 226, 227, 228, 229, 232, 234, 235, 236, 239, 240, 241, 243, 244, 245, 246, 247, 248, 250, 251, 255], "requires_grad": [159, 160, 172, 244, 247], "requires_grad_": 171, "rerun": 173, "resblock": 21, "research": 236, "resembl": 21, "resid": [240, 246, 251], "residu": 223, "resiz": [206, 214, 219, 232, 244], "resnet": [179, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 224, 227, 232, 244, 247], "resnet18": [179, 188, 189, 190, 191, 193, 194, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 244, 247], "resnet18_after_adaround": 189, "resnet18_after_cle_bc": [207, 208, 210], "resnet18_after_qat": [209, 211, 212], "resnet18_mixed_precis": [188, 206], "resnet50": [192, 193, 194, 198, 199, 200, 201, 202, 218, 227, 232], "resnet50_after_adaround": 193, "resnet50_after_amp": 192, "resnet50_after_cl": 202, "resnet50_pcq_adaround": 201, "resolv": [173, 251], "resort": 252, "resourc": [188, 195, 196, 197, 201, 206, 209, 236], "respecit": [188, 206], "respect": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 175, 178, 180, 186, 198, 199, 218, 232, 235, 249, 251], "respond": [200, 213], "respons": [2, 13, 182, 188, 192, 206, 226, 232, 236], "ressembl": 197, "rest": [193, 198, 199, 201, 202, 251, 252], "restor": [179, 185, 203, 204, 205, 246, 252], "restrict": 243, "resu": 21, "result": [2, 3, 9, 13, 14, 21, 22, 25, 30, 158, 159, 160, 177, 178, 179, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 220, 221, 222, 223, 224, 226, 227, 229, 232, 235, 243, 244, 246, 247, 248], "results_dir": [2, 3, 9, 13, 14, 22, 25, 177, 182, 183, 184, 188, 192, 200, 206, 213, 218, 219, 232], "retain": [232, 236], "retrain": [236, 250, 255], "retriev": 30, "retrieve_context": 254, "retuern": 232, "return": [1, 2, 3, 4, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 29, 30, 158, 159, 160, 161, 162, 166, 168, 169, 170, 172, 173, 174, 176, 177, 178, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 225, 227, 229, 232, 243, 244, 246, 247, 249, 250], "reus": [172, 173, 188, 206, 249, 250], "reveal": 252, "revert": 252, "review": 196, "revis": 245, "revisit": 224, "rewrit": 173, "rfloor": [161, 162, 163, 164, 165, 166, 246], "rgb": [214, 219, 232, 244, 250], "rgb_output": 250, "right": [161, 162, 163, 164, 165, 166, 175, 178, 188, 192, 194, 196, 206, 208, 230, 232, 246], "rm": 240, "rnn": [123, 251], "rnncell": 124, "ro": 240, "robust": [235, 251, 255], "root": [206, 208, 240, 241, 251], "rough": [188, 206], "roughli": [25, 174, 182, 183, 219, 232], "round": [1, 3, 10, 12, 14, 18, 22, 23, 24, 25, 29, 181, 183, 185, 187, 201, 218, 219, 227, 229, 236, 244, 246, 247, 251], "round_mod": 210, "round_nearest": 239, "rounding_mod": [3, 10, 14, 22, 23, 25, 179, 183, 185, 192, 193, 196, 198, 199, 201, 202, 218, 219, 244, 247], "roundingmod": 239, "routin": [188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 244], "rrelu": 125, "rtype": [13, 21, 158, 159, 160, 174, 182, 232], "rule": [23, 232, 234, 243, 244, 247, 248], "run": [2, 3, 9, 10, 13, 14, 18, 21, 22, 23, 25, 26, 29, 161, 162, 171, 172, 173, 174, 175, 177, 178, 179, 182, 183, 184, 185, 189, 190, 191, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 217, 219, 220, 223, 226, 227, 228, 229, 232, 233, 234, 236, 237, 240, 241, 243, 244, 245, 246, 247, 251, 254, 255], "run_forward_pass": [179, 244, 247], "run_infer": [3, 14, 25, 183, 208, 219], "runtim": [23, 30, 178, 179, 185, 188, 189, 190, 191, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212, 217, 218, 221, 223, 224, 226, 227, 229, 232, 236, 237, 240, 243, 244, 245, 247, 251, 253, 254, 255], "runtimeerror": [166, 179, 244, 247], "s_1": 243, "s_2": 243, "s_n": 243, "safe": 160, "safetensor": 176, "sai": [172, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 224, 232, 234], "sake": [190, 210, 217], "same": [9, 21, 22, 25, 30, 100, 158, 159, 160, 169, 171, 172, 173, 174, 175, 176, 177, 179, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212, 217, 218, 219, 222, 231, 234, 243, 244, 247, 248], "sampl": [1, 2, 3, 9, 10, 14, 22, 23, 25, 168, 174, 175, 177, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 221, 222, 223, 227, 232, 235, 242, 244, 245, 246, 247], "sampled_dataset": [194, 195, 200], "sandeep": 226, "saniti": 178, "satisfactori": [214, 220, 235, 252, 255], "satisfi": [172, 174, 189, 194, 208, 219, 234, 243], "saurabh": 226, "save": [1, 2, 3, 7, 9, 10, 12, 13, 14, 18, 20, 22, 23, 24, 25, 29, 168, 169, 176, 177, 179, 181, 182, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 223, 227, 229, 232, 235, 244, 246, 247, 254], "save_checkpoint": [179, 185], "save_dir": [20, 217], "save_path": [168, 216], "saved_eval_scores_dict": [18, 29, 223, 227, 229], "saw": 201, "scalar": [9, 14, 22, 177, 184, 218, 219], "scale": [15, 30, 159, 160, 161, 162, 163, 164, 165, 166, 176, 178, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 218, 220, 222, 231, 236, 239, 242, 243, 244, 245, 246, 247, 251, 254], "scale_": [161, 162, 163, 164, 165, 178], "scale_max": 176, "scale_min": 176, "scenario": [189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 207, 210, 211, 212, 213, 230], "schedul": [203, 204, 205, 209, 211, 212, 247], "scheme": [1, 3, 9, 10, 12, 14, 18, 22, 23, 24, 25, 29, 169, 177, 179, 181, 183, 184, 185, 188, 192, 195, 196, 200, 203, 204, 205, 206, 209, 213, 214, 217, 218, 219, 223, 226, 227, 229, 236, 243, 244, 247, 251], "scope": 172, "score": [2, 3, 9, 13, 14, 18, 22, 25, 29, 177, 182, 183, 184, 188, 189, 190, 191, 192, 194, 195, 208, 209, 218, 219, 223, 225, 226, 227, 228, 229, 232], "script": [209, 240, 241], "sdk": [188, 201, 206, 237], "search": [2, 3, 10, 11, 13, 14, 23, 25, 179, 180, 182, 183, 185, 186, 188, 195, 196, 198, 199, 203, 204, 205, 206, 209, 211, 212, 219, 231, 232, 235, 244, 247, 248, 251, 255], "searcher": 226, "sec": [188, 192, 201, 206, 232], "second": [14, 21, 24, 175, 179, 180, 181, 186, 200, 213, 214, 219, 221, 232, 235, 243, 244, 247], "section": [2, 173, 182, 188, 195, 206, 208, 232, 240, 241, 243, 246, 248, 253], "see": [18, 21, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 167, 179, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 223, 226, 227, 228, 229, 237, 239, 242, 243, 244, 246, 248, 251, 252, 253], "seed": 223, "seem": 226, "seen": [200, 213, 218], "select": [3, 10, 14, 18, 23, 25, 29, 179, 183, 185, 188, 192, 206, 218, 219, 224, 227, 228, 229, 230, 236, 239, 240, 241, 244, 246, 247, 248, 254], "select_param": [29, 229], "self": [14, 21, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 172, 173, 189, 197, 208, 214, 219, 232, 244, 249, 250], "selu": 134, "sens": 175, "sensit": [2, 9, 13, 22, 177, 182, 184, 188, 190, 192, 202, 206, 210, 215, 220, 225, 231, 233, 246, 251, 255], "sentiment": 196, "separ": [1, 12, 21, 23, 24, 171, 172, 173, 179, 181, 185, 192, 214, 218, 220, 244, 247, 251, 252], "separableconv2d": 251, "seq": 235, "seq_ms": [6, 167, 171, 235], "seqms": [231, 235], "seqmseparam": [11, 180, 186, 235], "sequanti": 197, "sequenc": [179, 194, 196, 208, 214, 219, 231, 244, 247, 248, 255], "sequenti": [11, 21, 171, 180, 186, 195, 196, 197, 221, 222, 242, 248, 249, 251, 252], "sequential_ms": 11, "sequentialms": 11, "seri": [25, 174, 179, 183, 185, 194, 208, 219, 234, 244, 247, 254], "serial": 254, "serializetostr": [188, 189, 190, 191, 217], "serv": [175, 214, 218, 219, 228], "servic": 234, "sess": [188, 189, 190, 191, 227], "session": [3, 18, 188, 189, 190, 191, 214, 217, 218, 219, 227, 232, 244], "set": [1, 2, 3, 8, 9, 10, 12, 13, 14, 18, 21, 22, 25, 29, 30, 160, 166, 168, 170, 171, 172, 173, 174, 175, 176, 177, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 218, 219, 220, 223, 224, 225, 226, 227, 229, 232, 233, 236, 240, 241, 243, 244, 245, 246, 247, 248, 252, 253, 255], "set_activation_quantizers_to_float": [170, 243], "set_adaround_param": [3, 14, 25, 183, 194, 208, 219], "set_and_freeze_param_encod": [189, 193, 201, 207, 214], "set_bitwidth_for_lora_adapt": 176, "set_blockwise_quantization_for_weight": [170, 243], "set_default_kernel": 30, "set_export_param": [25, 183, 219], "set_extra_st": 166, "set_grouped_blockwise_quantization_for_weight": [6, 170, 243], "set_kernel": 30, "set_mixed_precision_param": [3, 14, 25, 183, 219], "set_model_input_precis": [174, 234], "set_model_output_precis": [174, 234], "set_model_preparer_param": [25, 183, 219], "set_precis": [174, 234], "set_quant_scheme_candid": [3, 14, 25, 183, 219], "set_quantizers_to_candid": [2, 13, 174, 182, 232], "set_rang": [158, 159], "set_transform": [214, 219, 232, 244], "set_verbos": [192, 232], "settabl": 246, "setup": [171, 221, 232, 239, 242], "sever": [173, 175, 218, 224, 235, 249, 250, 251], "sgd": [220, 244, 247], "sh": [240, 241], "shall": 245, "shape": [13, 21, 27, 28, 29, 158, 159, 160, 161, 162, 166, 170, 171, 172, 173, 175, 176, 178, 179, 188, 189, 190, 191, 195, 196, 197, 201, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 221, 222, 223, 227, 229, 232, 242, 243, 244, 247, 249], "share": [173, 175], "sharp": 226, "sharpli": [203, 204, 205], "shell": 240, "shift": [202, 210], "should": [1, 9, 10, 13, 18, 21, 22, 23, 29, 30, 100, 160, 167, 169, 171, 172, 175, 176, 177, 179, 180, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 219, 223, 224, 227, 229, 232, 235, 239, 242, 243, 244, 245, 247, 249, 250, 254, 255], "shouldn": 178, "show": [173, 176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 222, 225, 243, 252], "showcas": [200, 213], "shown": [171, 176, 200, 213, 218, 222, 230, 239, 240, 242, 250, 252], "shuffl": [192, 193, 198, 199, 201, 214, 219, 220, 223, 227, 232, 244, 247], "side": 230, "sigmoid": [21, 136, 172], "sign": [164, 165, 171, 178, 246], "signal": 246, "signatur": [18, 29, 30, 164, 165, 179, 189, 203, 204, 205, 218, 219, 223, 227, 229, 244, 247], "signific": [214, 243, 252], "significantli": 251, "silu": 135, "sim": [1, 2, 8, 9, 11, 13, 15, 22, 23, 168, 170, 171, 174, 176, 177, 179, 180, 182, 184, 185, 186, 192, 195, 196, 201, 208, 214, 216, 218, 219, 220, 232, 234, 235, 242, 243, 244, 247], "sim1": 171, "sim2": 171, "sim_model": [193, 198, 199, 200, 201, 202, 207, 209, 210, 211, 212, 213], "simd": 241, "similar": [176, 178, 222, 236, 239, 242, 243, 246], "similarli": [188, 192, 200, 201, 206, 209, 213, 243, 244, 252, 255], "simpl": [172, 192, 195, 196, 200, 201, 213, 218, 223, 227, 229, 244], "simpler": 171, "simpli": [1, 9, 10, 22, 23, 172, 177, 184, 185, 188, 192, 195, 196, 201, 206, 209, 214, 218, 244, 247], "simplic": 217, "simplif": [222, 244], "simplifi": [1, 2, 9, 10, 171, 214, 217, 218, 219, 221, 222, 232, 244, 254], "simuat": [189, 190, 193, 198, 199, 201, 202, 207, 210, 211, 212], "simul": [10, 23, 166, 170, 175, 176, 179, 185, 187, 195, 196, 203, 204, 205, 213, 214, 215, 216, 217, 221, 232, 233, 235, 236, 243, 244, 245, 247, 248, 250, 251], "sinc": [176, 179, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 224, 232, 235, 243, 244, 246, 247], "singl": [2, 3, 7, 14, 20, 25, 169, 172, 174, 182, 183, 200, 201, 213, 217, 218, 219, 223, 225, 226, 227, 229, 232, 243, 246, 251], "singular": [204, 205, 226, 227, 229], "situat": 218, "six": 248, "size": [8, 9, 21, 160, 161, 162, 163, 164, 165, 170, 173, 175, 178, 189, 193, 194, 195, 196, 197, 201, 207, 214, 218, 227, 229, 232, 233, 236, 243, 244, 255], "skew": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212], "skip": [1, 24, 25, 180, 181, 183, 186, 190, 202, 210, 214, 219, 220, 223, 235, 240, 244, 247], "skipped_optim": 221, "slight": 201, "slightli": [188, 206], "slim": 251, "slow": 226, "small": [179, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 220, 235, 244, 247], "smaller": [1, 2, 13, 24, 181, 182, 188, 201, 206, 214, 226, 227, 229, 232, 252], "smoothl1loss": 137, "snapdragon": [188, 201, 206], "snippet": [172, 188, 206, 242, 243, 245], "snpe": [201, 202, 251], "so": [1, 9, 10, 23, 24, 171, 172, 173, 175, 179, 181, 185, 188, 189, 192, 193, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 209, 211, 212, 213, 214, 218, 221, 223, 225, 227, 229, 236, 241, 244, 247, 254], "softmarginloss": 138, "softmax": [21, 139, 175, 196, 197, 206], "softmax2d": 140, "softmin": 141, "softplu": [142, 172], "softshrink": 143, "softsign": 144, "softwar": [236, 237, 251], "solut": [192, 206, 225, 232, 247, 252], "some": [18, 21, 29, 171, 172, 175, 178, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 222, 223, 225, 227, 229, 230, 232, 244, 246, 247, 252, 255], "someth": [1, 10, 23, 185, 214, 218, 226, 244, 247], "sometim": [188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 206, 207, 210, 211, 212, 218, 223, 226], "soon": 219, "sort": 197, "sourc": [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 236, 240, 241, 243, 244, 247, 252], "space": [29, 223, 227, 229, 232, 243, 246], "spars": 251, "sparse_categorical_crossentropi": 196, "spatial": [18, 29, 187, 223, 224, 225, 229, 231, 251], "spatial_svd": [29, 204, 205, 223, 227, 229], "spatial_svd_auto_mod": 227, "spatial_svd_manual_mod": 227, "spatialsvdparamet": [18, 29, 204, 205, 223, 227, 229], "special": [25, 172, 183, 219, 236], "specif": [9, 18, 21, 23, 25, 29, 30, 170, 173, 175, 177, 179, 183, 184, 185, 188, 189, 192, 193, 195, 197, 201, 206, 207, 209, 218, 219, 223, 227, 229, 243, 244, 247, 248, 251, 254, 255], "specifi": [1, 2, 9, 14, 18, 22, 23, 24, 29, 161, 162, 163, 164, 165, 166, 174, 177, 179, 181, 182, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 223, 224, 225, 226, 227, 228, 229, 232, 234, 240, 243, 244, 245, 246, 247, 248], "speed": [18, 29, 176, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 223, 226, 227, 229, 247, 251], "speedup": [180, 186, 188, 201, 206, 221, 235], "split": [180, 186, 214, 219, 220, 232, 235, 243, 244, 246, 247], "sqnr": [2, 3, 11, 14, 25, 174, 180, 182, 183, 186, 188, 192, 206, 219, 232, 235, 246], "sqrt": 172, "squar": 246, "squeez": [195, 206], "ssvd": 224, "ssvd_comp_stat": 205, "ssvd_compressed_model": 205, "ssvd_cp_compressed_model": 205, "ssvd_cp_finetuned_model": 205, "ssvd_finetuned_model": 205, "stabl": [172, 195, 209, 214], "stack": [214, 219, 232, 244], "stand": 251, "standalon": [214, 251], "standard": [13, 166, 172, 175, 191, 196, 198, 199, 211, 212, 232, 236], "start": [1, 2, 12, 13, 21, 24, 164, 165, 172, 173, 181, 182, 187, 188, 192, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 209, 211, 212, 213, 214, 219, 224, 232, 239, 244, 246, 248, 255], "start_beta": [1, 12, 24, 181, 214], "start_i": [192, 232], "start_x": [192, 232], "starting_op_nam": [13, 232], "stat": [9, 26, 168, 216, 218, 220, 223, 227, 229], "statatist": 209, "state": [166, 180, 186, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 232, 235], "state_dict": 166, "stateless": 250, "statement": 172, "static": [13, 18, 21, 29, 172, 173, 223, 227, 229, 232], "static_patch_count": 21, "staticgridperchannelquant": 171, "staticgridquant": 171, "staticgridquantwrapp": 171, "staticmethod": [172, 188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213], "statist": [9, 18, 22, 26, 29, 161, 162, 166, 168, 175, 177, 179, 184, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 215, 216, 220, 223, 227, 229, 231, 244, 247], "std": [206, 208, 214, 219, 232, 244], "step": [9, 11, 13, 22, 164, 165, 172, 174, 176, 177, 180, 184, 186, 187, 188, 190, 191, 192, 194, 195, 196, 200, 201, 206, 209, 213, 223, 224, 225, 226, 233, 234, 239, 240, 241, 242, 246, 247, 252, 253, 254, 255], "still": [167, 171, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 239, 243, 249, 252, 255], "stochast": [22, 23, 185, 218, 244, 247], "stop": [1, 12, 24, 181, 214, 219], "stopiter": [214, 219, 232, 244], "storag": [243, 246], "store": [1, 12, 23, 24, 159, 174, 176, 179, 181, 185, 188, 192, 206, 214, 233, 234, 243, 244, 247], "str": [1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 14, 18, 20, 22, 23, 24, 25, 166, 168, 169, 172, 174, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 192, 197, 214, 216, 217, 218, 219, 227, 232, 234, 235, 244, 247, 250], "str_idx": 219, "stream": [214, 219, 220, 235], "strict": [8, 166, 174, 234, 244, 246, 247, 248], "strict_symmetr": [195, 239, 248], "strict_valid": [3, 14, 25, 183, 219], "strictli": 166, "stride": [160, 172, 173, 179, 221, 222, 242, 244, 247], "strike": 226, "string": [174, 234, 240, 245, 248], "strongli": [21, 172, 178, 189, 193, 201, 207], "structur": [25, 172, 175, 183, 200, 205, 213, 219, 226, 234, 236, 245], "style": 201, "sub": [200, 213], "subbackward0": 178, "subclass": [21, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 178, 195, 196, 244, 249, 251], "subdirectori": [188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "subfold": [188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "sublay": 173, "submit": 254, "submodul": [21, 250], "subpackag": [30, 251], "subsequ": [158, 188, 190, 201, 202, 206, 214, 219, 221, 222, 236, 248, 251], "subset": [9, 22, 177, 179, 184, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 220, 230, 234, 243, 244, 247], "subsetrandomsampl": 208, "subsidiari": 237, "substanti": 245, "substep": 240, "success": [194, 208], "successfulli": [179, 217, 244, 247], "sudo": [187, 241], "suffic": [189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212], "suffici": [218, 246, 247], "suffix": 240, "suggest": [23, 185, 203, 205, 225, 226, 244, 247], "suit": [194, 208, 219, 236], "suitabl": 236, "sum": [192, 206, 208, 214, 219, 227, 232, 244, 247], "summari": [196, 214, 221, 222, 226, 251], "sun": 226, "super": [21, 30, 172, 173, 197, 214, 219, 232, 244, 251], "supergroup": [195, 246, 251], "support": [1, 2, 9, 10, 12, 13, 21, 22, 23, 24, 29, 30, 172, 173, 174, 177, 179, 180, 181, 182, 184, 185, 186, 188, 192, 195, 196, 197, 206, 209, 211, 212, 214, 218, 221, 223, 224, 226, 227, 229, 231, 232, 235, 237, 239, 240, 242, 243, 244, 246, 248, 249, 250, 251, 252], "supported_kernel": [2, 182, 188, 206, 232], "supported_kernel_op": [174, 182, 232], "suppos": [2, 13, 182, 188, 192, 206, 232], "svd": [18, 29, 187, 223, 224, 225, 231, 251], "switch": 243, "symbol": [21, 172], "symbolic_trac": [25, 172, 183, 219], "symfp": [11, 180, 186, 235], "symmetr": [10, 30, 158, 159, 160, 161, 162, 170, 171, 175, 176, 178, 179, 197, 242, 243, 244, 245, 246, 247, 248, 251], "symmetri": [11, 171, 180, 186, 235], "symqt": [11, 180, 186, 235], "syntax": 245, "systemat": 236, "t": [2, 13, 25, 30, 172, 176, 178, 179, 180, 182, 183, 186, 188, 192, 195, 196, 200, 201, 206, 209, 213, 218, 219, 232, 235, 240, 244, 247, 248, 255], "tabl": [168, 187, 216, 228, 240], "tag": [187, 240, 241], "take": [2, 3, 13, 14, 25, 29, 30, 170, 173, 174, 178, 179, 182, 183, 188, 192, 194, 195, 197, 200, 201, 202, 203, 204, 205, 206, 209, 213, 214, 219, 220, 223, 225, 226, 227, 229, 230, 232, 234, 243, 244, 247, 251, 252, 255], "taken": [21, 230], "tanh": 145, "tanhshrink": 146, "tap": [22, 177, 184, 218], "tar": [29, 229], "target": [10, 18, 21, 23, 29, 41, 42, 48, 62, 64, 81, 87, 88, 92, 93, 104, 105, 113, 114, 115, 116, 117, 122, 137, 138, 176, 179, 182, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 217, 219, 221, 223, 224, 225, 226, 227, 229, 232, 235, 236, 244, 245, 246, 247, 251, 252], "target_comp_ratio": [18, 29, 203, 204, 205, 223, 227, 229], "target_data": [188, 189, 190, 191, 206, 207, 209, 210, 211, 212, 213], "target_length": 48, "target_modul": 176, "task": [188, 189, 190, 191, 192, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 228, 236, 245], "taxonomi": 226, "tbd": 216, "teach": 187, "techiqu": 210, "techniqu": [3, 14, 25, 183, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 218, 219, 221, 222, 223, 224, 227, 229, 232, 233, 236, 237, 246, 247, 251, 252, 255], "technologi": [236, 237], "tell": 201, "temporari": [214, 219, 221, 222, 232, 244], "temporarili": 171, "tend": 214, "tensor": [2, 3, 10, 13, 14, 20, 21, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 188, 190, 197, 200, 201, 202, 206, 210, 211, 212, 213, 214, 217, 218, 219, 221, 222, 223, 227, 229, 232, 234, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252], "tensor_nam": 245, "tensor_quant": 171, "tensorboard": [195, 196, 198, 199], "tensorflow": [0, 19, 187, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 213, 214, 217, 218, 219, 220, 221, 222, 227, 232, 236, 237, 239, 240, 241, 243, 244, 246, 247, 248, 251, 254, 255], "tensorquant": [13, 232], "tensorquantizationsimforpython": 239, "term": [179, 227, 229, 236, 237, 244, 247], "termin": 187, "test": [9, 22, 177, 184, 195, 196, 197, 218, 241], "text": [21, 161, 162, 163, 164, 165, 197], "textclassif": 197, "textrm": 246, "tf": [2, 9, 12, 15, 17, 18, 21, 22, 177, 184, 188, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 206, 209, 213, 214, 217, 218, 219, 220, 222, 227, 232, 240, 244, 246, 249, 251], "tf_cpp_min_log_level": [192, 194, 195, 198, 199, 232], "tf_dataset": [193, 201, 202], "tf_enhanc": [23, 188, 192, 195, 196, 206, 209, 244, 247], "tfencod": 171, "tflite": [188, 201, 206], "tfoplambda": [21, 197, 251], "than": [1, 18, 24, 29, 171, 172, 173, 181, 182, 185, 188, 189, 193, 195, 196, 206, 207, 209, 214, 223, 224, 227, 229, 232, 234, 235, 243, 245, 247, 248, 251], "thei": [172, 173, 188, 189, 190, 191, 193, 196, 197, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 228, 232, 243, 249, 252], "them": [3, 14, 21, 23, 25, 166, 171, 172, 173, 175, 176, 183, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 226, 234, 236, 244, 247], "theme": 251, "themselv": [243, 247], "thereaft": 232, "therebi": [188, 206, 233], "therefor": [2, 13, 176, 182, 188, 192, 195, 196, 197, 206, 221, 224, 232], "theta_": [161, 162], "thi": [1, 2, 3, 7, 9, 10, 13, 14, 18, 20, 21, 22, 23, 24, 25, 29, 30, 100, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 234, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], "thing": [189, 190, 191, 193, 194, 201, 202, 203, 204, 205, 207, 210, 211, 212, 254], "those": [9, 177, 184, 197, 218, 234, 248], "though": [30, 178, 188, 192, 195, 196, 201, 206, 233, 243, 244, 248], "three": [172, 188, 203, 204, 205, 206, 219, 224], "threshold": [147, 168, 216, 219], "through": [2, 21, 22, 160, 171, 172, 174, 175, 177, 182, 184, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 214, 218, 222, 232, 234, 235, 244, 246, 247], "throughout": [236, 248], "throw": [30, 178, 179, 244, 247], "thrown": [244, 247], "thu": [158, 159, 160, 222], "tijmen": 226, "till": [2, 13, 182, 188, 192, 206, 232], "time": [18, 29, 160, 172, 173, 192, 194, 200, 208, 210, 213, 214, 219, 223, 226, 227, 228, 229, 232, 234, 243, 245, 247], "tmp": [3, 9, 14, 22, 25, 177, 183, 184, 195, 200, 213, 214, 218, 219, 220, 221, 222, 232, 244, 247], "tmp_dir": 176, "tmpdir": 176, "to_arrai": [221, 222], "to_list": [2, 13, 174, 182, 232], "todo": [174, 234], "togeth": [201, 232, 243], "toi": 218, "token": [21, 196, 197, 252], "token_and_position_embed": 197, "token_emb": [21, 197], "tokenandpositionembed": [21, 197], "toler": [194, 208, 219, 224], "tolist": 206, "too": [203, 205, 243], "tool": [22, 177, 184, 197, 218, 222, 230, 233, 236, 251, 254, 255], "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256], "top": [1, 2, 3, 4, 5, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 24, 25, 26, 27, 28, 29, 170, 174, 177, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 227, 228, 229, 232, 234, 235, 243, 244, 247, 255], "top1": [192, 208, 227, 232], "top1_accuraci": 206, "top5": 192, "topk": [206, 208], "torch": [14, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 170, 171, 172, 173, 174, 175, 176, 178, 179, 182, 183, 185, 187, 188, 189, 190, 191, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 240, 241, 242, 243, 244, 247, 250, 251, 255], "torch_stabl": [239, 240, 241], "torchscript": [25, 169, 179, 183, 185, 217, 219, 236, 244, 247], "torchvis": [179, 188, 189, 190, 191, 193, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 232, 234, 235, 242, 244, 247], "total": [192, 197, 225, 227, 232, 244, 246], "total_sampl": [214, 219, 232, 244], "totensor": [206, 208, 214, 219, 232, 244], "touch": 21, "toward": 252, "tpu": 236, "tqdm": [206, 208, 219, 232, 244, 247], "trace": [21, 25, 168, 172, 183, 216, 219, 250], "traceabl": [172, 250], "traceback": 172, "traceerror": 172, "tracer": 172, "track": [176, 200, 213, 218], "track_lora_meta_data": 176, "track_meta_data": 176, "track_running_stat": [221, 222], "trade": [1, 12, 24, 181, 188, 192, 206, 214, 232], "tradeoff": [188, 192, 206, 232, 236], "train": [3, 9, 10, 14, 15, 18, 21, 22, 23, 25, 26, 29, 177, 179, 183, 184, 185, 187, 188, 192, 197, 206, 208, 214, 215, 217, 218, 219, 220, 222, 223, 226, 227, 229, 231, 232, 235, 236, 237, 244, 246, 251, 252], "train_dataload": [179, 244, 247], "train_dataset": 195, "train_dataset_s": 195, "train_flag": [29, 223, 227, 229], "train_load": [209, 223], "train_model": [29, 223, 227, 229], "trainabl": [199, 212], "trainer": [18, 29, 187, 203, 204, 205, 209, 211, 212, 223, 227, 229], "training_range_learning_with_tf_enhanced_init": [10, 23, 179, 185, 244, 247], "training_range_learning_with_tf_init": [10, 23, 179, 185, 195, 199, 209, 212, 220, 235, 242, 244, 247], "trainingmod": [188, 191], "transact": 226, "transform": [21, 172, 197, 206, 207, 208, 210, 211, 212, 213, 214, 219, 232, 243, 244, 251], "transformer_block": [21, 197], "transformerblock": [21, 197], "transit": [232, 255], "translat": 201, "transpos": 178, "trap": 235, "travers": 234, "tri": [203, 204, 205, 226], "tripletmarginloss": 148, "tripletmarginwithdistanceloss": 149, "triumph": 234, "true": [1, 2, 3, 10, 13, 14, 18, 23, 25, 29, 30, 159, 160, 161, 162, 166, 170, 171, 172, 173, 174, 175, 176, 178, 179, 182, 183, 185, 188, 189, 190, 191, 192, 193, 195, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 239, 242, 243, 244, 245, 247, 248], "truli": 172, "try": [18, 29, 188, 189, 190, 191, 192, 201, 203, 204, 205, 206, 214, 219, 221, 222, 223, 224, 226, 227, 229, 232, 244, 252], "tune": [18, 23, 29, 176, 179, 185, 189, 190, 191, 193, 194, 196, 198, 199, 202, 207, 208, 210, 211, 212, 223, 224, 227, 229, 237, 244, 255], "tupl": [1, 2, 3, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20, 22, 23, 24, 25, 27, 28, 29, 97, 160, 161, 162, 163, 164, 165, 169, 170, 173, 174, 177, 178, 179, 181, 182, 183, 184, 185, 188, 192, 200, 206, 213, 214, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 243, 244, 247, 250], "turn": 248, "tweak": 176, "twice": 21, "two": [13, 21, 29, 171, 172, 173, 174, 182, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 210, 211, 212, 213, 218, 225, 226, 227, 229, 232, 234, 236, 243, 246, 247, 254], "txt": [174, 234, 241, 254], "type": [1, 2, 3, 4, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 29, 30, 158, 159, 160, 161, 162, 166, 168, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 188, 192, 194, 195, 197, 200, 203, 204, 205, 206, 208, 211, 212, 214, 216, 218, 219, 220, 221, 222, 223, 226, 227, 228, 229, 232, 235, 236, 240, 242, 243, 244, 247, 248, 251, 254], "typeerror": 172, "typic": [21, 30, 175, 188, 189, 193, 194, 195, 196, 198, 199, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 221, 224, 236, 243, 246, 255], "u": [196, 201, 240], "ubuntu": [239, 240, 242], "ubuntu22": 251, "ubuntu2204": 241, "uint8": 158, "unchang": [180, 186, 235, 244, 247], "uncompress": 224, "under": [2, 13, 168, 171, 182, 188, 206, 216, 218, 232, 248, 252], "undergo": 236, "underli": [30, 178, 252], "understand": [171, 178, 188, 192, 195, 196, 197, 200, 201, 206, 209, 213], "undo": [15, 26, 220], "uneven": 252, "unexpect": 166, "unexpected_kei": 166, "unflatten": 150, "unfold": 151, "unid": [3, 14, 25, 183, 219], "uniniti": [160, 214, 235], "uninstal": 241, "unintuit": [3, 14, 25, 183, 219], "union": [7, 8, 9, 16, 18, 20, 21, 23, 24, 25, 27, 28, 29, 169, 170, 173, 174, 177, 179, 181, 182, 183, 184, 185, 214, 217, 218, 219, 221, 222, 223, 227, 229, 234, 243, 244, 247], "uniqu": [201, 240], "unit": 236, "unknown": 224, "unlabel": [3, 9, 14, 25, 183, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 210, 211, 212, 213, 214, 218, 219, 232, 244], "unlabeled_data": 219, "unlabeled_data_load": [214, 218, 219, 232], "unlabeled_dataset": [22, 194, 195, 200, 214, 218, 219, 220], "unlabeled_dataset_iter": [9, 218], "unlabeled_imagenet_data_load": 208, "unlabeled_imagenet_dataset": 208, "unlabeleddatasetwrapp": [194, 208], "unlabelled_data_load": 219, "unless": [3, 14, 25, 30, 179, 183, 185, 219, 230, 240, 244, 247], "unlik": [166, 209, 221], "unmodifi": 225, "unnecessari": [188, 201, 206, 221, 230, 251], "unrol": [172, 251], "unsign": [246, 248], "unsigned_symmetr": [195, 239, 248], "unsimplifi": [188, 189, 190, 191, 214, 219, 221, 222, 232, 244], "until": [3, 14, 25, 161, 162, 179, 183, 194, 208, 219, 220, 244, 247], "untouch": [244, 247], "unus": 240, "unwrap": 197, "up": [1, 10, 12, 18, 21, 23, 24, 29, 174, 176, 181, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 220, 223, 226, 227, 229, 230, 232, 234, 236, 243, 244, 247, 248], "updat": [166, 187, 189, 191, 193, 197, 198, 199, 201, 202, 207, 208, 210, 211, 212, 220, 228, 241, 244, 245, 246, 247, 251], "updatestat": 239, "upgrad": [167, 241, 251], "upon": [15, 26, 30, 175, 220, 254], "upsampl": 152, "upsamplingbilinear2d": 153, "upsamplingnearest2d": 154, "upstream": [223, 230], "upto": [188, 195, 200, 201, 206, 209, 213, 232], "url": [18, 29, 187, 223, 227, 228, 229, 240, 241], "us": [1, 2, 3, 7, 8, 9, 10, 12, 13, 14, 15, 18, 20, 21, 22, 23, 24, 25, 26, 29, 30, 158, 159, 161, 162, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 224, 228, 231, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255], "usabl": 245, "usag": [173, 176, 178, 236, 245, 251, 254], "use_all_amp_candid": [2, 182, 188, 206, 232], "use_cuda": [1, 3, 10, 18, 29, 188, 189, 190, 191, 192, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 219, 223, 227, 229, 239, 244], "use_embedded_encod": [179, 185, 244, 247], "use_monotonic_fit": [18, 29, 223, 227, 229], "use_safetensor": 176, "use_strict_symmetr": 171, "use_symmetric_encod": [10, 171, 244], "user": [1, 2, 3, 10, 13, 14, 15, 18, 21, 23, 24, 25, 29, 30, 167, 171, 172, 174, 178, 179, 181, 182, 183, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 223, 226, 227, 229, 232, 234, 239, 240, 241, 243, 244, 245, 247, 249, 250, 251], "user_onnx_lib": [1, 10, 214, 244], "userflow": [180, 186, 235], "usr": [240, 241], "usual": [220, 226, 246, 247], "util": [7, 9, 21, 171, 173, 176, 177, 184, 188, 189, 190, 191, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 232, 234, 235, 242, 243, 244, 247, 251], "v": [1, 2, 12, 13, 24, 181, 182, 188, 192, 200, 206, 213, 214, 232, 240, 243, 246], "v1": [171, 192, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 232, 239, 251], "v2": [30, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 170, 174, 179, 216, 234, 243, 244, 247, 251], "val": [188, 189, 190, 191, 198, 199, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "val_dataset": 200, "val_transform": 208, "valid": [2, 9, 13, 22, 173, 177, 182, 184, 188, 192, 195, 196, 198, 199, 200, 201, 206, 208, 209, 213, 214, 218, 219, 223, 227, 229, 232, 235, 243, 244, 251], "validate_example_model": 173, "validate_for_missing_modul": 173, "validate_for_reused_modul": 173, "validate_model": 173, "validation_check": 173, "validation_d": [192, 227, 232], "validation_data": [195, 196, 198, 199], "valu": [1, 2, 9, 10, 13, 14, 18, 22, 23, 24, 29, 30, 159, 160, 161, 162, 164, 165, 166, 168, 169, 170, 172, 174, 176, 177, 179, 181, 182, 184, 185, 188, 189, 190, 192, 193, 194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 229, 232, 234, 236, 239, 242, 243, 244, 245, 246, 247, 248, 251], "value_qtzr": 30, "vanilla": 196, "var": [26, 81, 220], "vari": [2, 13, 182, 224, 232], "variabl": [18, 21, 29, 161, 162, 172, 187, 223, 225, 227, 229, 240, 241], "varianc": [195, 209, 222], "variant": [236, 239, 241, 242, 251], "variant_str": 240, "varieti": 222, "varint": 240, "variou": [2, 18, 29, 174, 182, 194, 200, 208, 213, 223, 226, 227, 229, 231, 232, 240, 243, 246, 251, 252, 255], "vector": [189, 193, 201, 207], "vedaldi": 226, "venic": 226, "ver": 251, "verbos": 192, "veri": [189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 207, 208, 210, 211, 212, 213, 218, 224, 226, 232], "verifi": [21, 172, 207, 210, 211, 212, 255], "versa": [176, 188, 192, 206, 225, 232, 246, 252], "version": [30, 100, 168, 171, 172, 175, 179, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 216, 217, 236, 240, 241, 242, 244, 246, 247, 251], "via": [21, 25, 183, 219, 224, 236, 239, 246, 251, 254], "vice": [176, 188, 192, 206, 225, 232, 246, 252], "view": [168, 173, 178, 215, 216, 228, 232, 237, 239], "view_a": 208, "viewabl": 187, "vision": 226, "visit": 241, "visual": [18, 29, 196, 200, 213, 218, 223, 226, 227, 229, 232, 240, 251], "visualization_tool": [167, 216], "visualization_url": [18, 29, 223, 227, 229], "visualize_stat": [168, 216], "visualizecompress": 228, "vocab_s": [21, 196, 197], "vol": 226, "volum": 230, "w": [2, 9, 13, 22, 177, 182, 184, 195, 218, 219, 229, 230, 232, 240, 244, 251], "w16a16": 251, "w4a16": 251, "w4a8": [219, 251, 255], "w4fp16": 251, "w8a16": [219, 244, 255], "w8a8": [219, 247, 255], "wa": [18, 29, 158, 176, 178, 188, 189, 192, 193, 195, 196, 197, 201, 203, 205, 206, 207, 209, 217, 223, 226, 227, 229, 232, 234, 245, 251], "wai": [171, 178, 179, 180, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200, 201, 202, 206, 207, 210, 211, 212, 213, 217, 235], "want": [2, 7, 10, 13, 23, 30, 169, 172, 179, 182, 185, 188, 192, 195, 196, 200, 201, 206, 213, 217, 232, 239, 240, 241, 244, 247], "warm": [1, 12, 24, 181, 214], "warn": 173, "we": [1, 2, 7, 13, 21, 169, 171, 172, 173, 175, 176, 178, 182, 188, 189, 192, 193, 194, 195, 196, 197, 200, 201, 203, 205, 206, 207, 208, 209, 213, 214, 217, 219, 221, 222, 224, 226, 227, 232, 233, 240, 244, 246, 247, 252], "websit": [224, 237], "websocket": 228, "weight": [1, 2, 4, 8, 9, 12, 13, 21, 22, 24, 27, 29, 30, 168, 170, 171, 173, 174, 175, 176, 177, 178, 179, 181, 182, 184, 188, 189, 190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 222, 224, 227, 231, 232, 234, 235, 236, 242, 243, 244, 245, 246, 247, 248, 255], "weight_bw": 210, "weight_decai": 220, "weight_info": 197, "weight_nam": 197, "weight_q": 178, "weight_qdq": 178, "weight_svd": [29, 223, 227, 229], "weight_svd_auto_mod": 229, "weight_svd_manual_mod": 229, "weights_in_correct_ord": 197, "weights_pdf": [200, 213, 218], "weightsvdparamet": [29, 223, 227, 229], "well": [158, 173, 176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 214, 218, 226, 232, 243], "were": [176, 194, 201, 208, 217, 224, 230, 244, 245, 248, 251], "weren": 172, "wget": 241, "what": [197, 228], "whatev": 240, "wheel": [240, 241], "wheel_file_nam": [240, 241], "when": [2, 3, 9, 13, 18, 21, 22, 23, 25, 29, 30, 166, 172, 175, 177, 179, 180, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 218, 219, 220, 223, 226, 227, 228, 229, 230, 232, 233, 234, 235, 240, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255], "whenev": [221, 250], "where": [1, 2, 12, 13, 14, 18, 23, 24, 29, 161, 162, 163, 164, 165, 166, 172, 176, 178, 179, 181, 182, 185, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 223, 225, 227, 228, 229, 230, 232, 236, 240, 241, 244, 245, 246, 247, 250, 251, 254, 255], "wherea": [178, 182, 188, 206, 246], "wherein": [7, 169, 217], "whether": [25, 166, 172, 173, 174, 183, 188, 189, 190, 191, 193, 201, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 217, 219, 225, 234, 244, 247], "which": [1, 2, 3, 7, 8, 12, 13, 14, 18, 20, 21, 24, 25, 29, 30, 158, 159, 160, 161, 162, 164, 165, 166, 169, 170, 172, 173, 174, 175, 176, 178, 179, 181, 182, 183, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 239, 243, 244, 245, 246, 248, 251, 254, 255], "while": [8, 18, 29, 166, 175, 176, 188, 189, 190, 191, 192, 193, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 223, 225, 227, 229, 243, 244, 252, 255], "whl": [239, 240, 241], "who": 245, "whole": [14, 194, 195, 219, 243, 246], "whose": [168, 169, 170, 172, 216, 217, 243, 248, 254], "why": [188, 201, 206, 252], "wide": [221, 222, 236], "width": [9, 22, 177, 184, 188, 189, 190, 191, 201, 206, 207, 209, 210, 211, 212, 213, 218, 219, 221, 222, 227, 229, 230, 231, 233, 245, 246, 252, 255], "wildcard": 178, "wise": [9, 22, 177, 180, 184, 186, 218, 220, 235, 236, 251, 252], "wiseconv2d": [214, 222], "wish": [214, 219, 221, 222, 232, 234, 244], "within": [30, 158, 159, 160, 175, 196, 218, 224, 236, 240, 246, 251, 255], "without": [3, 14, 23, 25, 158, 160, 166, 168, 176, 179, 183, 185, 189, 193, 194, 196, 198, 199, 201, 202, 207, 208, 210, 211, 212, 216, 219, 230, 235, 240, 244, 247, 255], "won": [25, 172, 180, 183, 186, 219, 235, 248], "word": [160, 196, 243], "work": [10, 13, 23, 160, 171, 173, 185, 187, 188, 189, 191, 192, 193, 194, 195, 196, 198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 222, 226, 232, 243, 244, 247, 251], "workaround": 172, "workflow": [224, 254], "workspac": [187, 240], "world": 242, "wors": 226, "worth": 178, "would": [21, 160, 171, 174, 188, 192, 195, 200, 201, 206, 209, 213, 227, 234, 243, 246, 248, 251], "wrap": [30, 171, 172, 196, 200, 213], "wrap_linear": 171, "wrapped_module_nam": [22, 177, 184, 218], "wrapper": [22, 177, 184, 192, 196, 206, 207, 210, 211, 212, 218, 223, 227, 229, 232, 251], "write": [174, 189, 190, 191, 193, 198, 199, 202, 207, 210, 211, 212, 234, 243, 244, 247], "written": [179, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 244, 247], "wrt": [13, 232], "wsl2": 251, "www": 187, "x": [21, 158, 159, 160, 166, 167, 172, 173, 175, 188, 189, 190, 191, 192, 193, 196, 197, 200, 201, 206, 207, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 224, 227, 232, 239, 242, 244, 246, 247, 249, 250, 251], "x1": [63, 119, 172], "x2": [63, 119, 172, 249, 250], "x86": [239, 241, 242], "x86_64": [239, 241], "x_c": 166, "x_dq": 159, "x_q": [159, 160], "x_qdq": 158, "x_train": 196, "x_val": 196, "xarg": 241, "xhat": 246, "xiangyu": 226, "xint": 246, "xx": 245, "y": [172, 192, 193, 200, 201, 213, 214, 218, 219, 220, 232, 241, 244, 247], "y_train": 196, "y_val": 196, "ye": [226, 241], "yet": [188, 192, 195, 196, 201, 206, 244], "yield": [2, 3, 9, 14, 24, 25, 26, 174, 180, 181, 182, 183, 186, 195, 209, 214, 218, 219, 220, 224, 232, 235, 246, 252], "yihui": 226, "you": [9, 18, 25, 29, 30, 167, 171, 172, 177, 178, 179, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 232, 234, 235, 239, 240, 241, 244, 246, 248, 251, 252, 254, 255], "your": [9, 25, 30, 167, 171, 172, 173, 177, 183, 184, 187, 188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 217, 218, 219, 220, 224, 226, 228, 231, 239, 241, 244, 246, 247, 255], "your_imagenet_validation_data_path": [214, 219, 220, 244, 247], "yy": 245, "zero": [1, 12, 24, 176, 181, 214, 242, 246, 251], "zero_grad": [220, 247], "zeropad1d": 155, "zeropad2d": 156, "zeropad3d": 157, "zhang": 226, "zip": [192, 218, 227, 232], "zisserman": 226, "zou": 226, "zz": 245, "\u00aa": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00b2": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00b3": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00b5": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00b9": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00ba": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00bc": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00bd": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u00be": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u03c9": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u210e": 227, "\u215b": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u215c": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u215d": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\u215e": [217, 221, 232, 237, 243, 245, 246, 251, 254, 255], "\ud835\udc58": [227, 229], "\ud835\udc5a": 227, "\ud835\udc5b": 227, "\ud835\udc64": 227}, "titles": ["AIMET API", "aimet_onnx.adaround", "aimet_onnx.mixed_precision", "aimet_onnx.auto_quant_v2", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.quantsim.set_grouped_blockwise_quantization_for_weights", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.seq_mse", "aimet_tensorflow.adaround", "aimet_tensorflow.mixed_precision", "aimet_tensorflow.auto_quant_v2", "aimet_tensorflow.keras.bn_reestimation", "aimet_tensorflow.batch_norm_fold", "aimet_tensorflow.cross_layer_equalization", "aimet_tensorflow.compress", "aimet_tensorflow API", "aimet_tensorflow.layer_output_utils", "aimet_tensorflow.model_preparer", "aimet_tensorflow.quant_analyzer", "aimet_tensorflow.quantsim", "aimet_torch.adaround", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "Examples", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quantization simulation", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Quantization-Aware Training with a Keras Transformer Model", "Keras Model Preparer", "Quantization-aware training", "Quantization-Aware training with range learning", "Quant Analyzer", "Quantsim and Adaround - Per Channel Quantization (PCQ)", "Cross-Layer Equalization with QuantSim", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization and Bias Correction", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "Adaptive rounding", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Optimization techniques", "Automatic mixed precision", "Mixed precision", "Manual mixed precision", "Sequential MSE", "Glossary", "AIMET Documentation", "&lt;no title&gt;", "Installation", "AIMET installation in Docker", "AIMET manual installation and setup", "Quick Start (PyTorch)", "Advanced", "Calibration", "Encoding Format Specification", "Quantization simulation guide", "Quantization-aware training", "Runtime configuration", "TensorFlow model guidelines", "PyTorch model guidelines", "Release notes", "Quantization debugging guidelines", "Quantization user guide", "On-target inference", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [192, 245, 251], "1": [171, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 222, 232, 235, 245, 248, 251, 252, 255], "13": 251, "16": 251, "17": 251, "18": 251, "19": 251, "2": [171, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 232, 235, 245, 248, 251, 252, 255], "20": 251, "21": 251, "22": 251, "23": 251, "24": 251, "25": 251, "26": 251, "27": 251, "28": 251, "29": 251, "3": [187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 218, 219, 220, 221, 232, 235, 245, 248, 252, 255], "30": 251, "31": 251, "32": 251, "33": 251, "34": 251, "35": 251, "4": [188, 189, 190, 192, 193, 194, 195, 197, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 214, 218, 219, 220, 232, 235, 245, 248, 252], "5": [194, 195, 209, 218, 219, 245, 248, 251, 252], "6": [219, 245, 252], "7": [219, 252], "8": 252, "For": [189, 190, 191, 193, 194, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212], "No": 255, "On": [253, 254], "accuraci": [188, 189, 190, 191, 192, 193, 194, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212], "activ": [218, 252], "adapt": [189, 193, 207, 214, 231], "adaround": [1, 12, 24, 181, 189, 193, 201, 207], "advanc": 243, "affin": [171, 178], "ai": 254, "aimet": [0, 217, 228, 237, 240, 241, 242, 256], "aimet_onnx": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "aimet_tensorflow": [0, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "aimet_torch": [0, 24, 25, 26, 27, 28, 29, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186], "algorithm": [188, 192, 206, 232], "altern": 239, "amp": [188, 192, 206], "an": [188, 189, 190, 191, 240], "analysi": [200, 213, 215, 218, 231, 232, 252], "analyz": [200, 213, 215, 218], "api": [0, 6, 19, 21, 167, 172, 173, 175, 176, 178, 188, 192, 206, 214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 237, 243, 244, 247], "appli": [189, 190, 193, 194, 200, 201, 202, 207, 210, 213, 234, 255], "artifact": 217, "auto_qu": [25, 183], "auto_quant_v2": [3, 14], "automat": [188, 192, 206, 219, 231, 232, 233], "autoqu": [194, 208], "awar": [195, 196, 198, 199, 209, 211, 212, 247, 255], "base": 234, "baselin": [188, 189, 190, 191, 192, 193, 194, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212], "batch": [188, 189, 190, 191, 193, 198, 199, 201, 202, 206, 207, 210, 211, 212, 220, 221, 231], "batch_norm_fold": [4, 16, 27], "batchnorm": [195, 209], "bc": 210, "between": 197, "bia": 210, "bit": 232, "block": [178, 243], "blockwis": 243, "bn_reestim": [15, 26], "bokeh": 228, "brows": 187, "build": [239, 240], "calibr": [218, 244], "call": [188, 192, 206], "callback": [188, 195, 206, 218, 244], "case": [226, 232], "channel": [178, 201, 203, 205, 223, 226], "check": 252, "choos": [239, 240], "cle": [190, 202, 210], "code": [21, 171, 172, 187, 217, 218, 219, 221, 223, 227, 229, 232, 235], "compil": 254, "complementari": 214, "compress": [18, 29, 203, 204, 205, 223, 224, 225, 226, 227, 228, 229, 231], "comput": [175, 188, 192, 206, 244, 247], "confid": 252, "config_util": 170, "configur": [175, 246, 248], "constant": [194, 195, 208], "contain": 240, "context": [214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235], "convers": 254, "convert": [188, 189, 190, 191, 197, 232], "correct": 210, "cp": 226, "creat": [188, 189, 190, 191, 192, 193, 195, 197, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 218, 232], "creation": 244, "cross": [190, 202, 210, 222, 231], "cross_layer_equ": [5, 17, 28], "dataset": [188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "debug": [252, 253], "default": 248, "defin": [188, 194, 195, 206, 208], "definit": [10, 23], "deploi": 255, "dequant": 163, "dequantizedtensor": 158, "descript": 218, "design": 228, "detail": 218, "determin": [189, 190, 191, 193, 194, 198, 199, 201, 202, 207, 210, 211, 212, 246], "differ": 197, "direct": 254, "disabl": [200, 213], "discuss": 197, "docker": [239, 240], "document": [237, 256], "download": [187, 240], "enabl": [200, 213, 218], "encod": [175, 188, 192, 200, 206, 213, 218, 244, 245, 246], "engin": 254, "enum": [10, 23], "environ": 240, "equal": [190, 202, 210, 222, 231], "error": 218, "estim": [195, 209, 220, 231], "evalu": [188, 189, 190, 191, 192, 193, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 218, 244, 247], "exampl": [21, 171, 172, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213, 217, 218, 219, 221, 223, 227, 229, 232, 235, 237, 242, 245], "execut": 254, "explor": 225, "export": [195, 209, 217, 243, 244, 247], "fake": 192, "faq": 226, "fast": 192, "featur": [224, 237, 255], "file": [245, 248], "find": [192, 232], "fine": [203, 204, 205, 226, 247], "fix": 252, "float": 171, "floatquantizedequant": 166, "flow": [176, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "fold": [188, 189, 190, 191, 193, 195, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 221, 231], "format": 245, "fp16": 255, "fp32": [188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 252], "from": [171, 217, 239], "front": 232, "function": [188, 194, 195, 197, 206, 208, 218], "gener": [215, 217], "get": [193, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212], "glossari": [236, 237], "gpu": 247, "granular": 246, "greedi": 225, "group": 232, "guid": [171, 237, 246, 253], "guidebook": 224, "guidelin": [249, 250, 252, 253], "helper": [194, 208], "histogram": 218, "host": 239, "how": [171, 225, 230, 248], "hub": 254, "hyper": 214, "i": [188, 192, 200, 201, 206, 209, 213], "imag": 240, "individu": 252, "infer": [253, 254, 255], "inform": [189, 190, 191, 193, 194, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212], "initi": 247, "input": 234, "insert": 192, "instal": [237, 239, 240, 241, 242], "instanti": [189, 190, 191, 192, 193, 194, 195, 198, 199, 202, 203, 204, 205, 207, 210, 211, 212], "interact": [215, 216], "kera": [15, 195, 196, 197], "layer": [188, 189, 190, 191, 193, 195, 197, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 215, 217, 218, 222, 225, 226, 231, 232, 234, 252], "layer_output_util": [7, 20, 169], "leaf": 234, "learn": [199, 212], "learnedgrid": 171, "limit": [21, 172, 197], "list": 232, "load": [192, 193, 194, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "loss": [200, 213, 218], "low": 243, "manual": [233, 234, 241], "max": [200, 213, 218], "mean": 218, "method": 192, "migrat": 171, "min": [200, 213, 218], "mix": [188, 192, 206, 231, 232, 233, 234], "mixed_precis": [2, 13, 174, 182], "mmp": 234, "mode": 247, "model": [188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 217, 218, 226, 234, 247, 249, 250, 252], "model_input": 248, "model_output": 248, "model_prepar": [21, 172], "model_valid": 173, "modifi": 248, "modul": [171, 175], "more": [189, 190, 191, 193, 194, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212], "move": 171, "mse": [200, 213, 218, 231, 235], "multi": 247, "next": [189, 193, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 219], "nn": 175, "nois": 246, "non": 234, "norm": [190, 210, 220, 221, 231], "normal": [188, 189, 191, 193, 198, 199, 201, 202, 206, 207, 211, 212], "note": [226, 237, 251], "notebook": [187, 188, 192, 200, 201, 206, 209, 213, 237], "obtain": 217, "old": 239, "onnx": [188, 189, 190, 191], "op": [192, 232], "optim": 231, "option": [226, 234], "origin": [197, 217], "output": [215, 217, 234], "overal": [188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "overhead": 232, "overview": [225, 226, 228, 230, 246, 248], "packag": [239, 240], "param": 248, "paramet": [188, 192, 206, 214, 246, 247], "pareto": 232, "pcq": 201, "pdf": [200, 213], "peft": 176, "per": [178, 200, 201, 213, 218, 225, 226, 243, 252], "perform": [195, 198, 199, 209, 211, 212, 232, 252], "phase": 232, "pipelin": [188, 189, 190, 191, 193, 194, 195, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210, 211, 212, 213], "platform": 242, "post": 255, "power": 243, "prebuilt": 240, "precis": [188, 192, 206, 231, 232, 233, 234, 255], "prepar": [195, 197, 218], "prerequisit": [214, 218, 219, 220, 235, 239, 240, 241], "pretrain": [194, 200, 208], "procedur": 223, "process": 171, "profil": 234, "prune": [203, 205, 223, 226], "ptq": 255, "pypi": 239, "pytorch": [188, 189, 190, 191, 242, 245, 250], "qat": [195, 198, 199, 209, 211, 212, 247, 255], "qualcomm": 254, "quant": [200, 213], "quant_analyz": [9, 22, 177, 184], "quantanalyz": [200, 213, 218], "quantiz": [161, 164, 171, 175, 178, 188, 189, 190, 191, 192, 193, 195, 196, 198, 199, 200, 201, 202, 206, 207, 209, 210, 211, 212, 213, 215, 218, 219, 231, 237, 243, 246, 247, 252, 253, 254, 255], "quantizationmixin": 30, "quantizationsim": 195, "quantize_dequant": 165, "quantizedadaptiveavgpool1d": 31, "quantizedadaptiveavgpool2d": 32, "quantizedadaptiveavgpool3d": 33, "quantizedadaptivemaxpool1d": 34, "quantizedadaptivemaxpool2d": 35, "quantizedadaptivemaxpool3d": 36, "quantizedalphadropout": 37, "quantizedavgpool1d": 38, "quantizedavgpool2d": 39, "quantizedavgpool3d": 40, "quantizedbatchnorm1d": 43, "quantizedbatchnorm2d": 44, "quantizedbatchnorm3d": 45, "quantizedbceloss": 41, "quantizedbcewithlogitsloss": 42, "quantizedbilinear": 46, "quantizedcelu": 47, "quantizedchannelshuffl": 49, "quantizedcircularpad1d": 50, "quantizedcircularpad2d": 51, "quantizedcircularpad3d": 52, "quantizedconstantpad1d": 53, "quantizedconstantpad2d": 54, "quantizedconstantpad3d": 55, "quantizedconv1d": 56, "quantizedconv2d": 57, "quantizedconv3d": 58, "quantizedconvtranspose1d": 59, "quantizedconvtranspose2d": 60, "quantizedconvtranspose3d": 61, "quantizedcosineembeddingloss": 62, "quantizedcosinesimilar": 63, "quantizedcrossentropyloss": 64, "quantizedctcloss": 48, "quantizeddropout": 65, "quantizeddropout1d": 66, "quantizeddropout2d": 67, "quantizeddropout3d": 68, "quantizedelu": 69, "quantizedembed": 70, "quantizedembeddingbag": 71, "quantizedequant": 162, "quantizedfeaturealphadropout": 72, "quantizedflatten": 73, "quantizedfold": 74, "quantizedfractionalmaxpool2d": 75, "quantizedfractionalmaxpool3d": 76, "quantizedgaussiannllloss": 81, "quantizedgelu": 77, "quantizedglu": 78, "quantizedgroupnorm": 82, "quantizedgru": 79, "quantizedgrucel": 80, "quantizedhardshrink": 83, "quantizedhardsigmoid": 84, "quantizedhardswish": 85, "quantizedhardtanh": 86, "quantizedhingeembeddingloss": 87, "quantizedhuberloss": 88, "quantizedinstancenorm1d": 89, "quantizedinstancenorm2d": 90, "quantizedinstancenorm3d": 91, "quantizedkldivloss": 92, "quantizedl1loss": 93, "quantizedlayernorm": 98, "quantizedleakyrelu": 99, "quantizedlinear": 100, "quantizedlocalresponsenorm": 101, "quantizedlogsigmoid": 102, "quantizedlogsoftmax": 103, "quantizedlppool1d": 94, "quantizedlppool2d": 95, "quantizedlstm": 96, "quantizedlstmcel": 97, "quantizedmarginrankingloss": 105, "quantizedmaxpool1d": 106, "quantizedmaxpool2d": 107, "quantizedmaxpool3d": 108, "quantizedmaxunpool1d": 109, "quantizedmaxunpool2d": 110, "quantizedmaxunpool3d": 111, "quantizedmish": 112, "quantizedmseloss": 104, "quantizedmultilabelmarginloss": 113, "quantizedmultilabelsoftmarginloss": 114, "quantizedmultimarginloss": 115, "quantizednllloss": 116, "quantizednllloss2d": 117, "quantizedpairwisedist": 119, "quantizedpixelshuffl": 120, "quantizedpixelunshuffl": 121, "quantizedpoissonnllloss": 122, "quantizedprelu": 118, "quantizedreflectionpad1d": 128, "quantizedreflectionpad2d": 129, "quantizedreflectionpad3d": 130, "quantizedrelu": 126, "quantizedrelu6": 127, "quantizedreplicationpad1d": 131, "quantizedreplicationpad2d": 132, "quantizedreplicationpad3d": 133, "quantizedrnn": 123, "quantizedrnncel": 124, "quantizedrrelu": 125, "quantizedselu": 134, "quantizedsigmoid": 136, "quantizedsilu": 135, "quantizedsmoothl1loss": 137, "quantizedsoftmarginloss": 138, "quantizedsoftmax": 139, "quantizedsoftmax2d": 140, "quantizedsoftmin": 141, "quantizedsoftplu": 142, "quantizedsoftshrink": 143, "quantizedsoftsign": 144, "quantizedtanh": 145, "quantizedtanhshrink": 146, "quantizedtensor": 159, "quantizedtensorbas": 160, "quantizedthreshold": 147, "quantizedtripletmarginloss": 148, "quantizedtripletmarginwithdistanceloss": 149, "quantizedunflatten": 150, "quantizedunfold": 151, "quantizedupsampl": 152, "quantizedupsamplingbilinear2d": 153, "quantizedupsamplingnearest2d": 154, "quantizedzeropad1d": 155, "quantizedzeropad2d": 156, "quantizedzeropad3d": 157, "quantsim": [8, 10, 23, 170, 179, 185, 201, 202, 217, 244, 246], "quantwrapp": 171, "quick": [237, 242], "rang": [199, 200, 212, 213, 218], "rank": 226, "ratio": [225, 226, 228], "re": [195, 209, 220, 231], "recommend": 247, "reconstruct": 223, "reduc": 232, "reestim": 209, "refer": [175, 178, 226, 237], "regular": 192, "relat": 187, "releas": [237, 251], "result": 219, "round": [189, 193, 207, 214, 226, 231], "run": [187, 188, 192, 206, 208, 218, 242], "runtim": [246, 248], "scheme": 246, "score": [193, 198, 199, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212], "scratch": 239, "sdk": 254, "select": [223, 225, 226], "sensit": [218, 232, 252], "seq_ms": [11, 180, 186], "sequenti": [231, 235], "server": [187, 228], "session": 228, "set": 234, "set_grouped_blockwise_quantization_for_weight": 8, "setup": [214, 220, 222, 223, 227, 229, 234, 235, 240, 241, 247], "show": 197, "sim": [188, 189, 190, 191, 193, 198, 199, 202, 206, 207, 209, 210, 211, 212], "similar": 197, "simplifi": [188, 189, 190, 191], "simul": [188, 189, 190, 191, 192, 193, 198, 199, 201, 202, 206, 207, 209, 210, 211, 212, 237, 246, 255], "sourc": 239, "spatial": [204, 205, 226, 227], "specif": 245, "specifi": 255, "squar": 218, "start": [228, 237, 240, 242], "staticgrid": 171, "statist": [200, 209, 213, 218], "step": [189, 193, 198, 199, 202, 203, 204, 205, 207, 208, 210, 211, 212, 214, 217, 218, 219, 220, 221, 222, 232, 235], "structur": 248, "subclass": 197, "summari": [192, 195, 197, 201, 209], "supergroup": 248, "support": [247, 255], "svd": [204, 205, 226, 227, 229], "target": [253, 254, 255], "techniqu": [210, 214, 226, 231], "tensorflow": [245, 249], "test": 242, "thi": [188, 192, 200, 201, 206, 209, 213], "tool": [168, 215, 231], "train": [189, 190, 191, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 207, 209, 210, 211, 212, 213, 247, 255], "transform": 196, "tune": [203, 204, 205, 226, 247], "type": 234, "up": 245, "us": [171, 203, 204, 205, 223, 226, 227, 229, 232], "user": [176, 237, 253], "v": 171, "v1": [167, 181, 182, 183, 184, 185, 186], "valid": [189, 190, 191, 193, 194, 202, 203, 204, 205, 207, 210, 211, 212], "variant": 240, "verif": 255, "verifi": [239, 242], "version": [239, 245, 256], "visual": [168, 215, 216, 228, 252], "visualization_tool": 168, "w16a16": 255, "weight": [218, 223, 226, 229, 252], "what": [188, 192, 200, 201, 206, 209, 213], "width": 232, "winnow": [223, 230], "work": [225, 230], "workflow": [214, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 232, 234, 235, 244, 246, 247, 252, 253, 255], "wrapper": [200, 213], "x": 171, "your": 240}})