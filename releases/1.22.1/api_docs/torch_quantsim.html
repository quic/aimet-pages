

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>AIMET PyTorch Quantization SIM API &#8212; AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.22.1</title>
    <link rel="stylesheet" href="../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="../user_guide/index.html">AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.22.1</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../user_guide/index.html">
              <img class="logo" src="../_static/brain_logo.png" alt="Logo"/>
            </a></p>
  <h3><a href="../user_guide/index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">AIMET PyTorch Quantization SIM API</a><ul>
<li><a class="reference internal" href="#top-level-api">Top-level API</a></li>
<li><a class="reference internal" href="#enum-definition">Enum Definition</a></li>
<li><a class="reference internal" href="#code-example-quantization-aware-training-qat">Code Example - Quantization Aware Training (QAT)</a></li>
</ul>
</li>
</ul>

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="aimet-pytorch-quantization-sim-api">
<span id="api-torch-quantsim"></span><h1>AIMET PyTorch Quantization SIM API<a class="headerlink" href="#aimet-pytorch-quantization-sim-api" title="Permalink to this headline">¶</a></h1>
<p>AIMET Quantization Sim requires PyTorch model definition to follow certain guidelines. These guidelines are described
in detail here. <a class="reference internal" href="torch_model_guidelines.html#api-torch-model-guidelines"><span class="std std-ref">Model Guidelines</span></a></p>
<p>AIMET provides Model Preparer API to allow user to prepare PyTorch model for AIMET Quantization features. The API and
usage examples are described in detail here. <a class="reference internal" href="torch_model_preparer.html#api-torch-model-preparer"><span class="std std-ref">Model Preparer API</span></a></p>
<p>AIMET also includes a Model Validator utility to allow user to check their model definition. Please see the API and
usage examples for this utility here. <a class="reference internal" href="torch_model_validator.html#api-torch-model-validator"><span class="std std-ref">Model Validator API</span></a></p>
<div class="section" id="top-level-api">
<h2>Top-level API<a class="headerlink" href="#top-level-api" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="aimet_torch.quantsim.QuantizationSimModel">
<em class="property">class </em><code class="sig-prename descclassname">aimet_torch.quantsim.</code><code class="sig-name descname">QuantizationSimModel</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">dummy_input</em>, <em class="sig-param">quant_scheme=&lt;QuantScheme.post_training_tf_enhanced: 2&gt;</em>, <em class="sig-param">rounding_mode='nearest'</em>, <em class="sig-param">default_output_bw=8</em>, <em class="sig-param">default_param_bw=8</em>, <em class="sig-param">in_place=False</em>, <em class="sig-param">config_file=None</em>, <em class="sig-param">default_data_type=&lt;QuantizationDataType.int: 1&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.quantsim.QuantizationSimModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements mechanism to add quantization simulations ops to a model. This allows for off-target simulation of
inference accuracy. Also allows the model to be fine-tuned to counter the effects of quantization.</p>
<p>Constructor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>) – Model to add simulation ops to</p></li>
<li><p><strong>dummy_input</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>]) – Dummy input to the model. Used to parse model graph. If the model has more than one input,
pass a tuple. User is expected to place the tensors on the appropriate device.</p></li>
<li><p><strong>quant_scheme</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#aimet_common.defs.QuantScheme" title="aimet_common.defs.QuantScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantScheme</span></code></a>]) – Quantization scheme. The Quantization scheme is used to compute the Quantization encodings.
There are multiple schemes available. Please refer the QuantScheme enum definition.</p></li>
<li><p><strong>rounding_mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Rounding mode. Supported options are ‘nearest’ or ‘stochastic’</p></li>
<li><p><strong>default_output_bw</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Default bitwidth (4-31) to use for quantizing all layer inputs and outputs</p></li>
<li><p><strong>default_param_bw</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Default bitwidth (4-31) to use for quantizing all layer parameters</p></li>
<li><p><strong>in_place</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, then the given ‘model’ is modified in-place to add quant-sim nodes.
Only suggested use of this option is when the user wants to avoid creating a copy of the model</p></li>
<li><p><strong>config_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Path to Configuration file for model quantizers</p></li>
<li><p><strong>default_data_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationDataType</span></code>) – Default data type to use for quantizing all layer inputs, outputs and parameters.
Possible options are QuantizationDataType.int and QuantizationDataType.float.
Note that the mode default_data_type=QuantizationDataType.float is only supported with
default_output_bw=16 and default_param_bw=16</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>The following API can be used to Compute Encodings for Model</strong></p>
<dl class="method">
<dt id="aimet_torch.quantsim.QuantizationSimModel.compute_encodings">
<code class="sig-prename descclassname">QuantizationSimModel.</code><code class="sig-name descname">compute_encodings</code><span class="sig-paren">(</span><em class="sig-param">forward_pass_callback</em>, <em class="sig-param">forward_pass_callback_args</em><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.quantsim.QuantizationSimModel.compute_encodings" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes encodings for all quantization sim nodes in the model. It is also used to find initial encodings for
Range Learning</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_pass_callback</strong> – A callback function that simply runs forward passes on the model. This callback
function should use representative data for the forward pass, so the calculated encodings work for all
data samples. This callback internally chooses the number of data samples it wants to use for calculating
encodings.</p></li>
<li><p><strong>forward_pass_callback_args</strong> – These argument(s) are passed to the forward_pass_callback as-is. Up to
the user to determine the type of this parameter. E.g. could be simply an integer representing the number
of data samples to use. Or could be a tuple of parameters or an object representing something more complex.
If set to None, forward_pass_callback will be invoked with no parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>The following APIs can be used to save and restore the quantized model</strong></p>
<dl class="method">
<dt id="aimet_torch.quantsim.save_checkpoint">
<code class="sig-prename descclassname">quantsim.</code><code class="sig-name descname">save_checkpoint</code><span class="sig-paren">(</span><em class="sig-param">file_path</em><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.quantsim.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>This API provides a way for the user to save a checkpoint of the quantized model which can
be loaded at a later point to continue fine-tuning e.g.
See also load_checkpoint()</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>quant_sim_model</strong> (<a class="reference internal" href="#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code></a>) – QuantizationSimModel to save checkpoint for</p></li>
<li><p><strong>file_path</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Path to the file where you want to save the checkpoint</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
<dl class="method">
<dt id="aimet_torch.quantsim.load_checkpoint">
<code class="sig-prename descclassname">quantsim.</code><code class="sig-name descname">load_checkpoint</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.quantsim.load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the quantized model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_path</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Path to the file where you want to save the checkpoint</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#aimet_torch.quantsim.QuantizationSimModel" title="aimet_torch.quantsim.QuantizationSimModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizationSimModel</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A new instance of the QuantizationSimModel created after loading the checkpoint</p>
</dd>
</dl>
</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>The following API can be used to Export the Model to target</strong></p>
<dl class="method">
<dt id="aimet_torch.quantsim.QuantizationSimModel.export">
<code class="sig-prename descclassname">QuantizationSimModel.</code><code class="sig-name descname">export</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">filename_prefix</em>, <em class="sig-param">dummy_input</em>, <em class="sig-param">onnx_export_args=&lt;aimet_torch.onnx_utils.OnnxExportApiArgs object&gt;</em>, <em class="sig-param">propagate_encodings=False</em><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.quantsim.QuantizationSimModel.export" title="Permalink to this definition">¶</a></dt>
<dd><p>This method exports out the quant-sim model so it is ready to be run on-target.</p>
<p>Specifically, the following are saved</p>
<ol class="arabic simple">
<li><p>The sim-model is exported to a regular PyTorch model without any simulation ops</p></li>
<li><p>The quantization encodings are exported to a separate JSON-formatted file that can
then be imported by the on-target runtime (if desired)</p></li>
<li><p>Optionally, An equivalent model in ONNX format is exported. In addition, nodes in the ONNX model are named
the same as the corresponding PyTorch module names. This helps with matching ONNX node to their quant
encoding from #2.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – path where to store model pth and encodings</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Prefix to use for filenames of the model pth and encodings files</p></li>
<li><p><strong>dummy_input</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>]) – Dummy input to the model. Used to parse model graph. It is required for the dummy_input to
be placed on CPU.</p></li>
<li><p><strong>onnx_export_args</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">OnnxExportApiArgs</span></code>]) – optional export argument with onnx specific overrides if not provide export via
torchscript graph</p></li>
<li><p><strong>propagate_encodings</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, encoding entries for intermediate ops (when one PyTorch ops results in
multiple ONNX nodes) are filled with the same BW and data_type as the output tensor for that series of
ops.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Encoding format is described in the <a class="reference internal" href="quantization_encoding_specification.html#api-quantization-encoding-spec"><span class="std std-ref">Quantization Encoding Specification</span></a></p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="enum-definition">
<h2>Enum Definition<a class="headerlink" href="#enum-definition" title="Permalink to this headline">¶</a></h2>
<p><strong>Quant Scheme Enum</strong></p>
<dl class="class">
<dt id="aimet_common.defs.QuantScheme">
<em class="property">class </em><code class="sig-prename descclassname">aimet_common.defs.</code><code class="sig-name descname">QuantScheme</code><a class="headerlink" href="#aimet_common.defs.QuantScheme" title="Permalink to this definition">¶</a></dt>
<dd><p>Enumeration of Quant schemes</p>
<dl class="attribute">
<dt id="aimet_common.defs.QuantScheme.post_training_percentile">
<code class="sig-name descname">post_training_percentile</code><em class="property"> = 6</em><a class="headerlink" href="#aimet_common.defs.QuantScheme.post_training_percentile" title="Permalink to this definition">¶</a></dt>
<dd><p>For a Tensor, adjusted minimum and maximum values are selected based on the percentile value passed.
The Quantization encodings are calculated using the adjusted minimum and maximum value.</p>
</dd></dl>

<dl class="attribute">
<dt id="aimet_common.defs.QuantScheme.post_training_tf">
<code class="sig-name descname">post_training_tf</code><em class="property"> = 1</em><a class="headerlink" href="#aimet_common.defs.QuantScheme.post_training_tf" title="Permalink to this definition">¶</a></dt>
<dd><p>For a Tensor, the absolute minimum and maximum value of the Tensor are used to compute the Quantization
encodings.</p>
</dd></dl>

<dl class="attribute">
<dt id="aimet_common.defs.QuantScheme.post_training_tf_enhanced">
<code class="sig-name descname">post_training_tf_enhanced</code><em class="property"> = 2</em><a class="headerlink" href="#aimet_common.defs.QuantScheme.post_training_tf_enhanced" title="Permalink to this definition">¶</a></dt>
<dd><p>For a Tensor, searches and selects the optimal minimum and maximum value that minimizes the Quantization Noise.
The Quantization encodings are calculated using the selected minimum and maximum value.</p>
</dd></dl>

<dl class="attribute">
<dt id="aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init">
<code class="sig-name descname">training_range_learning_with_tf_enhanced_init</code><em class="property"> = 4</em><a class="headerlink" href="#aimet_common.defs.QuantScheme.training_range_learning_with_tf_enhanced_init" title="Permalink to this definition">¶</a></dt>
<dd><p>For a Tensor, the encoding values are initialized with the post_training_tf_enhanced scheme. Then, the encodings
are learned during training.</p>
</dd></dl>

<dl class="attribute">
<dt id="aimet_common.defs.QuantScheme.training_range_learning_with_tf_init">
<code class="sig-name descname">training_range_learning_with_tf_init</code><em class="property"> = 3</em><a class="headerlink" href="#aimet_common.defs.QuantScheme.training_range_learning_with_tf_init" title="Permalink to this definition">¶</a></dt>
<dd><p>For a Tensor, the encoding values are initialized with the post_training_tf scheme. Then, the encodings are
learned during training.</p>
</dd></dl>

</dd></dl>

<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="code-example-quantization-aware-training-qat">
<h2>Code Example - Quantization Aware Training (QAT)<a class="headerlink" href="#code-example-quantization-aware-training-qat" title="Permalink to this headline">¶</a></h2>
<p>This example shows how to use AIMET to perform QAT (Quantization-aware training). QAT is an
AIMET feature adding quantization simulation ops (also called fake quantization ops sometimes) to a trained ML model and
using a standard training pipeline to fine-tune or train the model for a few epochs. The resulting model should show
improved accuracy on quantized ML accelerators.</p>
<p>Simply referred to as QAT - quantization parameters like per-tensor scale/offsets for activations are computed once.
During fine-tuning, the model weights are updated to minimize the effects of quantization in the forward pass, keeping
the quantization parameters constant.</p>
<p><strong>Required imports</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.cuda</span>

</pre></div>
</div>
<p><strong>Load the PyTorch Model</strong></p>
<p>For this example, we are going to load a pretrained ResNet18 model from torchvision. Similarly, you can load any
pretrained PyTorch model instead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet18</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

</pre></div>
</div>
<p><strong>Prepare the model for Quantization simulation</strong></p>
<p>AIMET quantization simulation requires the user’s model definition to follow certain guidelines. For example,
functionals defined in forward pass should be changed to equivalent torch.nn.Module. AIMET user guide lists all these
guidelines. The following ModelPreparer API uses new graph transformation feature available in PyTorch 1.9+ version and
automates model definition changes required to comply with the above guidelines.</p>
<p>For more details, please refer:  <a class="reference internal" href="torch_model_preparer.html#api-torch-model-preparer"><span class="std std-ref">Model Preparer API</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span> <span class="nn">aimet_torch.model_preparer</span> <span class="kn">import</span> <span class="n">prepare_model</span>
    <span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

</pre></div>
</div>
<p><strong>Create the Quantization Simulation Model</strong></p>
<p>Now we use AIMET to create a QuantizationSimModel. This basically means that AIMET will insert fake quantization ops in
the model graph and will configure them. A few of the parameters are explained here</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="kn">from</span> <span class="nn">aimet_common.defs</span> <span class="kn">import</span> <span class="n">QuantScheme</span>
    <span class="kn">from</span> <span class="nn">aimet_torch.quantsim</span> <span class="kn">import</span> <span class="n">QuantizationSimModel</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
    <span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="n">quant_sim</span> <span class="o">=</span> <span class="n">QuantizationSimModel</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="o">=</span><span class="n">dummy_input</span><span class="p">,</span>
                                     <span class="n">quant_scheme</span><span class="o">=</span><span class="n">QuantScheme</span><span class="o">.</span><span class="n">post_training_tf_enhanced</span><span class="p">,</span>
                                     <span class="n">default_param_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">default_output_bw</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                     <span class="n">config_file</span><span class="o">=</span><span class="s1">&#39;../../TrainingExtensions/common/src/python/aimet_common/quantsim_config/&#39;</span>
                                                 <span class="s1">&#39;default_config.json&#39;</span><span class="p">)</span>

</pre></div>
</div>
<p><strong>An example User created function that is called back from compute_encodings()</strong></p>
<p>Even though AIMET has added ‘quantizer’ nodes to the model graph, the model is not ready to be used yet. Before we can
use the sim model for inference or training, we need to find appropriate scale/offset quantization parameters for each
‘quantizer’ node. For activation quantization nodes, we need to pass unlabeled data samples through the model to collect
range statistics which will then let AIMET calculate appropriate scale/offset quantization parameters. This process is
sometimes referred to as calibration. AIMET simply refers to it as ‘computing encodings’.</p>
<p>So we create a routine to pass unlabeled data samples through the model. This should be fairly simple - use the existing
train or validation data loader to extract some samples and pass them to the model. We don’t need to compute any
loss metric etc. So we can just ignore the model output for this purpose. A few pointers regarding the data samples</p>
<p>In practice, we need a very small percentage of the overall data samples for computing encodings. For example,
the training dataset for ImageNet has 1M samples. For computing encodings we only need 500 or 1000 samples.</p>
<p>It may be beneficial if the samples used for computing encoding are well distributed. It’s not necessary that all
classes need to be covered etc. since we are only looking at the range of values at every layer activation. However,
we definitely want to avoid an extreme scenario like all ‘dark’ or ‘light’ samples are used - e.g. only using pictures
captured at night might not give ideal results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pass_calibration_data</span><span class="p">(</span><span class="n">sim_model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The User of the QuantizationSimModel API is expected to write this function based on their data set.</span>
<span class="sd">    This is not a working function and is provided only as a guideline.</span>

<span class="sd">    :param sim_model:</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># User action required</span>
    <span class="c1"># The following line of code is an example of how to use the ImageNet data&#39;s validation data loader.</span>
    <span class="c1"># Replace the following line with your own dataset&#39;s validation data loader.</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">get_val_dataloader</span><span class="p">()</span>

    <span class="c1"># User action required</span>
    <span class="c1"># For computing the activation encodings, around 1000 unlabelled data samples are required.</span>
    <span class="c1"># Edit the following 2 lines based on your batch size.</span>
    <span class="c1"># batch_size * max_batch_counter should be 1024</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">max_batch_counter</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="n">sim_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">current_batch_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>

            <span class="n">inputs_batch</span> <span class="o">=</span> <span class="n">input_data</span>  <span class="c1"># labels are ignored</span>
            <span class="n">sim_model</span><span class="p">(</span><span class="n">inputs_batch</span><span class="p">)</span>

            <span class="n">current_batch_counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">current_batch_counter</span> <span class="o">==</span> <span class="n">max_batch_counter</span><span class="p">:</span>
                <span class="k">break</span>
</pre></div>
</div>
<p><strong>Compute the Quantization Encodings</strong></p>
<p>Now we call AIMET to use the above routine to pass data through the model and then subsequently compute the quantization
encodings. Encodings here refer to scale/offset quantization parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">quant_sim</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">(</span><span class="n">pass_calibration_data</span><span class="p">,</span> <span class="n">forward_pass_callback_args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

</pre></div>
</div>
<p><strong>Finetune the Quatization Simulation Model</strong></p>
<p>To perform quantization aware training (QAT), we simply train the model for a few more epochs (typically 15-20). As with
any training job, hyper-parameters need to be searched for optimal results. Good starting points are to use a learning
rate on the same order as the ending learning rate when training the original model, and to drop the learning rate by a
factor of 10 every 5 epochs or so.</p>
<p>For the purpose of this example, we are going to train only for 1 epoch. But feel free to change these parameters as you
see fit.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># User action required</span>
    <span class="c1"># The following line of code illustrates that the model is getting finetuned.</span>
    <span class="c1"># Replace the following finetune() unction with your pipeline&#39;s finetune() function.</span>
    <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">finetune</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-7</span><span class="p">,</span> <span class="n">learning_rate_schedule</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
                                  <span class="n">use_cuda</span><span class="o">=</span><span class="n">use_cuda</span><span class="p">)</span>

    <span class="c1"># Determine simulated accuracy</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">ImageNetDataPipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

</pre></div>
</div>
<p><strong>Export the model</strong></p>
<p>So we have an improved model after QAT. Now the next step would be to actually take this model to target. For this
purpose, we need to export the model with the updated weights without the fake quant ops. We also to export the
encodings (scale/offset quantization parameters) that were updated during training since we employed QAT.
AIMET QuantizationSimModel provides an export API for this purpose.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># Export the model which saves pytorch model without any simulation nodes and saves encodings file for both</span>
    <span class="c1"># activations and parameters in JSON format</span>
    <span class="n">quant_sim</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;./&#39;</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="s1">&#39;quantized_resnet18&#39;</span><span class="p">,</span> <span class="n">dummy_input</span><span class="o">=</span><span class="n">dummy_input</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="../user_guide/index.html">AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.22.1</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Qualcomm Innovation Center, Inc..
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.1.
    </div>
  </body>
</html>