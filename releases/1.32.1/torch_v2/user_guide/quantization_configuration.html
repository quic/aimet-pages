<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantization Simulation Configuration &mdash; AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.32.1</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="../torch_docs/index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        tf-torch-cpu_1.32.1
      </div>

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      AIMET Variant: <span class="rst-current-version-name"> PyTorch </span>
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Documentation Versions</dt>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/user_guide/index.html">Universal</a></dd>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/torch_v2/torch_docs/index.html">PyTorch</a></dd>
        
      </dl>
    </div>
  </div>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/tutorials/quickstart_guide.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantized_modules.html">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/encoding_analyzer.html">Encoding Analyzers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.MinMaxEncodingAnalyzer.html">MinMaxEncodingAnalyzer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.SqnrEncodingAnalyzer.html">SqnrEncodingAnalyzer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/generated/aimet_torch.v2.quantization.encoding_analyzer.PercentileEncodingAnalyzer.html">PercentileEncodingAnalyzer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.fake_quantization_mixin.html">FakeQuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/nn.quantization_mixin.html">nn.QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/affine/index.html">quantization.affine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.Quantize.html">Quantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.QuantizeDequantize.html">QuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.quantize_.html">quantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.quantize_dequantize.html">quantize_dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_docs/api/quantization/affine/generated/aimet_torch.v2.quantization.affine.dequantize.html">dequantize</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../torch_docs/api/quantization/float/index.html">quantization.float</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../torch_docs/index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../torch_docs/index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quantization Simulation Configuration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/quantization_configuration.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="quantization-simulation-configuration">
<span id="ug-quantsim-config"></span><h1>Quantization Simulation Configuration<a class="headerlink" href="#quantization-simulation-configuration" title="Permalink to this heading"></a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>AIMET allows the configuration of quantizer placement and settings in accordance with a set of rules specified in a json configuration file, applied when the Quantization Simulation API is called.</p>
<p>Settings such as quantizer enablement, per channel quantization, symmetric quantization, and specifying fused ops when quantizing can be configurated.
The general use case for this file would be for users to match the quantization rules for a particular runtime they would like to simulate.</p>
<p>For examples on how to provide a specific configuration file to AIMET Quantization Simulation,
refer to the API docs for <span class="xref std std-doc">PyTorch Quantsim</span>, <span class="xref std std-doc">TensorFlow Quantsim</span>, and <span class="xref std std-doc">Keras Quantsim</span>.</p>
<p>It is advised for the user to begin with the default configuration file under</p>
<p>aimet_common/quantsim_config/default_config.json</p>
<p>For most users of AIMET, no additional changes to the default configuration file should be needed.</p>
</div>
<div class="section" id="configuration-file-structure">
<h2>Configuration File Structure<a class="headerlink" href="#configuration-file-structure" title="Permalink to this heading"></a></h2>
<p>The configuration file contains six main sections, in increasing amounts of specificity:</p>
<img alt="../_images/quantsim_config_file.png" src="../_images/quantsim_config_file.png" />
<p>Rules defined in a more general section can be overruled by subsequent rules defined in a more specific case.
For example, one may specify in “defaults” for no layers to be quantized, but then turn on quantization for specific layers in the “op_type” section.</p>
</div>
<div class="section" id="how-to-configure-individual-configuration-file-sections">
<h2>How to configure individual Configuration File Sections<a class="headerlink" href="#how-to-configure-individual-configuration-file-sections" title="Permalink to this heading"></a></h2>
<p>When working with a new runtime with different rules, or for experimental purposes, users can refer to this section to understand how to configure individual sections in a configuration file.</p>
<ol class="arabic">
<li><p><strong>defaults</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;defaults&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;ops&quot;</span><span class="p">:</span> <span class="p">{</span>                                <span class="c1"># Required dictionary, but can be empty</span>
        <span class="s2">&quot;is_output_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>      <span class="c1"># Optional: Possible settings: True</span>
        <span class="s2">&quot;is_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;False&quot;</span>             <span class="c1"># Optional: Possible settings: True, False</span>
    <span class="p">},</span>
    <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>                             <span class="c1"># Required dictionary, but can be empty</span>
        <span class="s2">&quot;is_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>             <span class="c1"># Optional: Possible settings: True, False</span>
        <span class="s2">&quot;is_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span>              <span class="c1"># Optional: Possible settings: True, False</span>
    <span class="p">},</span>
    <span class="s2">&quot;strict_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;False&quot;</span><span class="p">,</span>            <span class="c1"># Optional: Possible settings: True, False</span>
    <span class="s2">&quot;unsigned_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>           <span class="c1"># Optional: Possible settings: True, False</span>
    <span class="s2">&quot;per_channel_quantization&quot;</span><span class="p">:</span> <span class="s2">&quot;False&quot;</span>     <span class="c1"># Optional: Possible settings: True, False</span>
    <span class="p">},</span>
</pre></div>
</div>
<p>In the defaults section, it is required to include an “ops” dictionary and a “params” dictionary (though these dictionaries may be empty).</p>
<p>The “ops” dictionary holds settings that will apply to all activation quantizers in the model.
In this section, the following settings are available:</p>
<blockquote>
<div><ul>
<li><dl>
<dt>is_output_quantized:</dt><dd><p>An optional parameter. If included, it must be set to “True”.
Including this setting will turn on all output activation quantizers by default.
If not specified, all activation quantizers will start off as disabled.</p>
<p>For cases when the runtime quantizes input activations, we typically see this only done for certain op types.
Configuring these settings for specific op types is covered in sections further below.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>is_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will place all activation quantizers in symmetric mode by default.
A “False” setting, or omitting the parameter altogether, will set all activation quantizers to asymmetric mode by default.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>The “params” dictionary holds settings that will apply to all parameter quantizers in the model.
In this section, the following settings are available:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>is_quantized:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will turn on all parameter quantizers by default.
A “False” setting, or omitting the parameter altogether, will disable all parameter quantizers by default.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>is_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will place all parameter quantizers in symmetric mode by default.
A “False” setting, or omitting the parameter altogether, will set all parameter quantizers to asymmetric mode by default.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>Aside from the “ops” and “params” dictionary, additional settings governing quantizers in the model are available:</p>
<ul class="simple">
<li><dl class="simple">
<dt>strict_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
When set to “True”, quantizers which are configured in symmetric mode will use strict symmetric quantization.
When set to “False” or omitting the parameter altogether, quantizers which are configured in symmetric mode will not use strict symmetric quantization.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>unsigned_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
When set to “True”, quantizers which are configured in symmetric mode will use unsigned symmetric quantization when available.
When set to “False” or omitting the parameter altogether, quantizers which are configured in symmetric mode will not use unsigned symmetric quantization.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>per_channel_quantization:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
When set to “True”, parameter quantizers will use per channel quantization as opposed to per tensor quantization.
When set to “False” or omitting the parameter altogether, parameter quantizers will use per tensor quantization.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><p><strong>params</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>                         <span class="c1"># Can specify 0 or more param types</span>
        <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;is_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>     <span class="c1"># Optional: Possible settings: True, False</span>
            <span class="s2">&quot;is_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span>      <span class="c1"># Optional: Possible settings: True, False</span>
        <span class="p">}</span>
    <span class="p">},</span>
</pre></div>
</div>
<p>In the params section, settings can be configured for certain types of parameters throughout the model.
For example, adding settings for “weight” will affect all parameters of type “weight” in the model.
Currently supported parameter types include:</p>
<blockquote>
<div><ul class="simple">
<li><p>weight</p></li>
<li><p>bias</p></li>
</ul>
</div></blockquote>
<p>For each parameter type, the following settings are available:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>is_quantized:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will turn on all parameter quantizers of that type.
A “False” setting, will disable all parameter quantizers of that type.
By omitting the setting, the parameter will fall back to the setting specified by the defaults section.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>is_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will place all parameter quantizers of that type in symmetric mode.
A “False” setting will place all parameter quantizers of that type in asymmetric mode.
By omitting the setting, the parameter will fall back to the setting specified by the defaults section.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div></blockquote>
</li>
<li><p><strong>op_type</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s2">&quot;op_type&quot;</span><span class="p">:</span> <span class="p">{</span>                                <span class="c1"># Can specify 0 or more ONNX op types</span>
        <span class="s2">&quot;Gemm&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;is_input_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>       <span class="c1"># Optional: Possible settings: True</span>
            <span class="s2">&quot;is_output_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;False&quot;</span><span class="p">,</span>     <span class="c1"># Optional: Possible settings: True, False</span>
            <span class="s2">&quot;per_channel_quantization&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span> <span class="c1"># Optional: Possible settings: True, False</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">{</span>                         <span class="c1"># Optional, can specify 1 or more param types</span>
                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;is_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span><span class="p">,</span>     <span class="c1"># Optional: Possible settings: True, False</span>
                    <span class="s2">&quot;is_symmetric&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span>      <span class="c1"># Optional: Possible settings: True, False</span>
                <span class="p">}</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">},</span>
</pre></div>
</div>
<p>In the op type section, settings affecting particular op types can be specified.
The configuration file recognizes ONNX op types, and will internally map the type to a PyTorch or TensorFlow op type
depending on which framework is used.</p>
<p>For each op type, the following settings are available:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>is_input_quantized:</dt><dd><p>An optional parameter. If included, it must be set to “True”.
Including this setting will turn on input quantization for all ops of this op type.
Omitting the setting will keep input quantization disabled for all ops of this op type.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>is_output_quantized:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will turn on output quantization for all ops of this op type.
A “False” setting will disable output quantization for all ops of this op type.
By omitting the setting, output quantizers of this op type will fall back to the setting specified by the defaults section.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>is_symmetric:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
A “True” setting will place all quantizers of this op type in symmetric mode.
A “False” setting will place all quantizers of this op type in asymmetric mode.
By omitting the setting, quantizers of this op type will fall back to the setting specified by the defaults section.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>per_channel_quantization:</dt><dd><p>An optional parameter. If included, possible settings include “True” and “False”.
When set to “True”, parameter quantizers of this op type will use per channel quantization as opposed to per tensor quantization.
When set to “False”, parameter quantizers of this op type will use per tensor quantization.
By omitting the setting, parameter quantizers of this op type will fall back to the setting specified by the defaults section.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>For a particular op type, settings for particular parameter types can also be specified.
For example, specifying settings for weight parameters of a Conv op type will affect only Conv weights and not weights
of Gemm op types.</p>
<p>To specify settings for param types of this op type, include a “params” dictionary under the op type.
Settings for this section follow the same convention as settings for parameter types in the preceding “params” section, however will only affect parameters for this op type.</p>
</div></blockquote>
</li>
<li><p><strong>supergroups</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s2">&quot;supergroups&quot;</span><span class="p">:</span> <span class="p">[</span>    <span class="c1"># Can specify 0 or more supergroup lists made up of ONNX op types</span>
        <span class="p">{</span>
            <span class="s2">&quot;op_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;Relu&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;op_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;Clip&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;op_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Add&quot;</span><span class="p">,</span> <span class="s2">&quot;Relu&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;op_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;Relu&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">],</span>
</pre></div>
</div>
<p>Supergroups are a sequence of operations which are fused during quantization, meaning no quantization noise is introduced between members of the supergroup.
For example, specifying [“Conv, “Relu”] as a supergroup disables quantization between any adjacent Conv and Relu ops in the model.</p>
<p>When searching for supergroups in the model, only sequential groups of ops with no branches in between will be matched with supergroups defined in the list.
Using [“Conv”, “Relu”] as an example, if there was a Conv op in the model whose output is used by both a Relu op and a second op, the supergroup would not take effect for these Conv and Relu ops.</p>
<p>To specify supergroups in the config file, add each entry as a list of op type strings.
The configuration file recognizes ONNX op types, and will internally map the types to PyTorch or TensorFlow op types depending on which framework is used.</p>
</div></blockquote>
</li>
<li><p><strong>model_input</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s2">&quot;model_input&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;is_input_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span>    <span class="c1"># Optional: Possible settings: True</span>
    <span class="p">},</span>
</pre></div>
</div>
<p>The “model_input” section is used to configure the quantization of inputs to the model.
In this section, the following setting is available:</p>
<ul class="simple">
<li><dl class="simple">
<dt>is_input_quantized:</dt><dd><p>An optional parameter. If included, it must be set to “True”.
Including this setting will turn on quantization for input quantizers to the model.
Omitting the setting will keep input quantizers set to whatever setting they were in as a result of applying configurations from earlier sections.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><p><strong>model_output</strong>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="s2">&quot;model_output&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;is_output_quantized&quot;</span><span class="p">:</span> <span class="s2">&quot;True&quot;</span>   <span class="c1"># Optional: Possible settings: True</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>The “model_output” section is used to configure the quantization of outputs of the model.
In this section, the following setting is available:</p>
<ul class="simple">
<li><dl class="simple">
<dt>is_output_quantized:</dt><dd><p>An optional parameter. If included, it must be set to “True”.
Including this setting will turn on quantization for output quantizers of the model.
Omitting the setting will keep output quantizers set to whatever setting they were in as a result of applying configurations from earlier sections.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
</ol>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>