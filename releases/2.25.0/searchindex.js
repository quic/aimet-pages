Search.setIndex({"alltitles": {"1. Define Constants and Helper functions": [[246, "1.-Define-Constants-and-Helper-functions"]], "1. Example evaluation and training pipeline": [[239, "1.-Example-evaluation-and-training-pipeline"], [247, "1.-Example-evaluation-and-training-pipeline"], [251, "1.-Example-evaluation-and-training-pipeline"]], "1. Example evaluation pipeline": [[236, "1.-Example-evaluation-pipeline"], [244, "1.-Example-evaluation-pipeline"]], "1. FP32 confidence checks": [[227, "fp32-confidence-checks"]], "1. Instantiate the example training and validation pipeline": [[237, "1.-Instantiate-the-example-training-and-validation-pipeline"], [238, "1.-Instantiate-the-example-training-and-validation-pipeline"], [240, "1.-Instantiate-the-example-training-and-validation-pipeline"], [241, "1.-Instantiate-the-example-training-and-validation-pipeline"], [242, "1.-Instantiate-the-example-training-and-validation-pipeline"], [243, "1.-Instantiate-the-example-training-and-validation-pipeline"], [245, "1.-Instantiate-the-example-training-and-validation-pipeline"], [248, "1.-Instantiate-the-example-training-and-validation-pipeline"], [249, "1.-Instantiate-the-example-training-and-validation-pipeline"], [250, "1.-Instantiate-the-example-training-and-validation-pipeline"]], "1. Run the notebook server": [[235, "run-the-notebook-server"]], "1. Sensitivity to weight and activation quantization": [[206, "sensitivity-to-weight-and-activation-quantization"]], "1. Set Random Seeds and Instantiate ImageNet Data Loaders": [[252, "1.-Set-Random-Seeds-and-Instantiate-ImageNet-Data-Loaders"]], "1. Versioning": [[216, "versioning"]], "1. defaults": [[225, "defaults"]], "1. meta-llama/Llama-3.2-1B-Instruct": [[254, "meta-llama-llama-3-2-1b-instruct"]], "1.13.0": [[202, "id63"]], "1.16.0": [[202, "id62"]], "1.16.1": [[202, "id61"]], "1.16.2": [[202, "id60"]], "1.17.0": [[202, "id59"]], "1.18.0": [[202, "id58"]], "1.19.1": [[202, "id57"]], "1.20.0": [[202, "id56"]], "1.21.0": [[202, "id55"]], "1.22.0": [[202, "id54"]], "1.22.1": [[202, "id53"]], "1.22.2": [[202, "id52"]], "1.23.0": [[202, "id51"]], "1.24.0": [[202, "id50"]], "1.25.0": [[202, "id49"]], "1.26.0": [[202, "id48"]], "1.27.0": [[202, "id47"]], "1.28.0": [[202, "id46"]], "1.29.0": [[202, "id45"]], "1.30.0": [[202, "id44"]], "1.31.0": [[202, "id43"]], "1.32.0": [[202, "id42"]], "1.33.0": [[202, "id41"]], "1.33.5": [[202, "id40"]], "1.34.0": [[202, "id39"]], "1.35.0": [[202, "id38"]], "1.35.1": [[202, "id37"]], "2. Convert an FP32 PyTorch model to ONNX, simplify & then evaluate baseline FP32 accuracy": [[236, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [237, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [238, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"], [240, "2.-Convert-an-FP32-PyTorch-model-to-ONNX,-simplify-&-then-evaluate-baseline-FP32-accuracy"]], "2. Create W4A8 QuantizationSimModel with Vision Transformer (ViT)": [[252, "2.-Create-W4A8-QuantizationSimModel-with-Vision-Transformer-(ViT)"]], "2. Download the example notebooks and related code": [[235, "download-the-example-notebooks-and-related-code"]], "2. Load FP32 model": [[247, "2.-Load-FP32-model"]], "2. Load a pretrained FP32 model": [[246, "2.-Load-a-pretrained-FP32-model"]], "2. Load the model": [[239, "2.-Load-the-model"], [251, "2.-Load-the-model"]], "2. Load the model and evaluate to get a baseline FP32 accuracy score": [[241, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [242, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [243, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [244, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [245, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [248, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [249, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"], [250, "2.-Load-the-model-and-evaluate-to-get-a-baseline-FP32-accuracy-score"]], "2. Per-layer quantizer enablement": [[206, "per-layer-quantizer-enablement"]], "2. Version 0.6.1": [[216, "version-0-6-1"]], "2. Weights or activations quantization": [[227, "weights-or-activations-quantization"]], "2. meta-llama/Llama-3.2-3B-Instruct": [[254, "meta-llama-llama-3-2-3b-instruct"]], "2. params": [[225, "params"]], "2.0.0": [[202, "id36"]], "2.1. Encoding specification": [[216, "encoding-specification"]], "2.1.0": [[202, "id35"]], "2.10.0": [[202, "id22"]], "2.11.0": [[202, "id20"]], "2.12.0": [[202, "id19"]], "2.13.0": [[202, "id18"]], "2.14.0": [[202, "id16"]], "2.15.0": [[202, "id14"]], "2.16.0": [[202, "id13"]], "2.17.0": [[202, "id12"]], "2.18.0": [[202, "id11"]], "2.19.0": [[202, "id10"]], "2.2.0": [[202, "id34"]], "2.20.0": [[202, "id7"]], "2.21.0": [[202, "id6"]], "2.22.0": [[202, "id5"]], "2.23.0": [[202, "id3"]], "2.24.0": [[202, "id2"]], "2.25.0": [[202, "id1"]], "2.3.0": [[202, "id33"]], "2.4.0": [[202, "id32"]], "2.5.0": [[202, "id31"]], "2.6.0": [[202, "id30"]], "2.7.0": [[202, "id29"]], "2.8.0": [[202, "id25"]], "2.9.0": [[202, "id23"]], "3. Apply QuantAnalyzer to the model": [[239, "3.-Apply-QuantAnalyzer-to-the-model"], [251, "3.-Apply-QuantAnalyzer-to-the-model"]], "3. Compress the model and fine-tune": [[241, "3.-Compress-the-model-and-fine-tune"], [242, "3.-Compress-the-model-and-fine-tune"], [243, "3.-Compress-the-model-and-fine-tune"]], "3. Create a quantization simulation model": [[236, "3.-Create-a-quantization-simulation-model"], [244, "3.-Create-a-quantization-simulation-model"]], "3. Create a quantization simulation model and Perform QAT": [[247, "3.-Create-a-quantization-simulation-model-and-Perform-QAT"]], "3. Create a quantization simulation model and determine quantized accuracy": [[237, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [238, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [240, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [245, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [248, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [249, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"], [250, "3.-Create-a-quantization-simulation-model-and-determine-quantized-accuracy"]], "3. Evaluate Initial Accuracy Before QAT": [[252, "3.-Evaluate-Initial-Accuracy-Before-QAT"]], "3. Fixing weight quantization": [[227, "fixing-weight-quantization"]], "3. Per-layer encodings min-max range": [[206, "per-layer-encodings-min-max-range"]], "3. Qwen/Qwen2.5-0.5B-Instruct": [[254, "qwen-qwen2-5-0-5b-instruct"]], "3. Run AutoQuant": [[246, "3.-Run-AutoQuant"]], "3. Run the notebooks": [[235, "run-the-notebooks"]], "3. Version 1.0.0": [[216, "version-1-0-0"]], "3. supergroups": [[225, "supergroups"]], "3.1. Encoding specification": [[216, "id1"]], "4. Apply AdaRound": [[237, "4.-Apply-AdaRound"]], "4. Apply Adaround": [[245, "4.-Apply-Adaround"]], "4. Apply CLE": [[238, "4.-Apply-CLE"], [248, "4.-Apply-CLE"]], "4. Fixing activation quantization": [[227, "fixing-activation-quantization"]], "4. Per-layer statistics histogram": [[206, "per-layer-statistics-histogram"]], "4. Perform BatchNorm Reestimation": [[247, "4.-Perform-BatchNorm-Reestimation"]], "4. Perform QAT": [[249, "4.-Perform-QAT"], [250, "4.-Perform-QAT"]], "4. Qwen/Qwen2.5-1.5B-Instruct": [[254, "qwen-qwen2-5-1-5b-instruct"]], "4. Run AMP algorithm on the quantized model": [[236, "4.-Run-AMP-algorithm-on-the-quantized-model"], [244, "4.-Run-AMP-algorithm-on-the-quantized-model"]], "4. Run QAT and Evaluate Post-QAT Accuracy": [[252, "4.-Run-QAT-and-Evaluate-Post-QAT-Accuracy"]], "4. model_input": [[225, "model-input"]], "5. Export Model": [[247, "5.-Export-Model"]], "5. Per-layer mean-square-error loss": [[206, "per-layer-mean-square-error-loss"]], "5. Performing per-layer analysis": [[227, "performing-per-layer-analysis"]], "5. Qwen/Qwen3-4B": [[254, "qwen-qwen3-4b"]], "5. model_output": [[225, "model-output"]], "6. Visualizing sensitive layers": [[227, "visualizing-sensitive-layers"]], "6. microsoft/Phi-3.5-mini-instruct": [[254, "microsoft-phi-3-5-mini-instruct"]], "7. Fixing individual quantizers": [[227, "fixing-individual-quantizers"]], "8. Quantize the model": [[227, "quantize-the-model"]], "AIMET API": [[0, null]], "AIMET Documentation": [[184, null]], "AIMET documentation versions": [[260, null]], "AIMET features": [[257, "aimet-features"], [258, null]], "AIMET visualization": [[213, null]], "API": [[163, "api"], [164, "api"], [169, "api"], [189, "api"], [190, "api"], [191, "api"], [192, "api"], [193, "api"], [194, "api"], [196, "api"], [200, "api"], [201, "api"], [204, "api"], [205, "api"], [206, "api"], [207, "api"], [208, "api"], [212, "api"], [214, "api"], [218, "api"], [219, "api"], [221, "api"], [222, "api"], [223, "api"], [224, "api"]], "API Reference": [[184, "api-reference"]], "API reference": [[166, "api-reference"], [171, "api-reference"]], "Accuracy improvement tools": [[258, "accuracy-improvement-tools"]], "Accuracy-vs-Performance Tradeoff": [[255, "accuracy-vs-performance-tradeoff"]], "AdaScale": [[190, null], [195, "adascale"]], "Adaptive Rounding (AdaRound)": [[237, null], [245, null]], "Adaptive rounding": [[189, null], [195, "adaptive-rounding"]], "Affine quantizers": [[171, "affine-quantizers"]], "Alternative packages": [[187, "alternative-packages"]], "Analysis descriptions": [[206, "analysis-descriptions"]], "Analysis tools": [[203, null], [217, "analysis-tools"]], "Apply LPBQ": [[218, "apply-lpbq"]], "AutoQuant": [[246, null]], "Automatic Mixed-Precision (AMP)": [[236, null], [244, null]], "Automatic mixed precision": [[219, null], [220, "automatic-mixed-precision"]], "Automatic quantization": [[191, null], [195, "automatic-quantization"]], "Batch norm folding": [[193, null], [195, "batch-norm-folding"]], "Batch norm re-estimation": [[192, null], [195, "batch-norm-re-estimation"]], "Blockwise Quantization": [[207, null], [217, "blockwise-quantization"]], "Browse the notebooks": [[235, "browse-the-notebooks"]], "Build AIMET documentation": [[186, "build-aimet-documentation"]], "Build AIMET wheel and run unit tests": [[186, "build-aimet-wheel-and-run-unit-tests"], [186, "id2"]], "Build and run docker container locally": [[186, "build-and-run-docker-container-locally"]], "Building from source": [[186, null], [187, "building-from-source"]], "CLE": [[248, "CLE"]], "Calibration Callback": [[199, "calibration-callback"]], "Call AMP API": [[236, "Call-AMP-API"], [244, "Call-AMP-API"]], "Channel pruning": [[208, null]], "Channel pruning (CP)": [[211, "channel-pruning-cp"]], "Channel selection": [[208, "channel-selection"]], "Choose and install a package": [[187, "choose-and-install-a-package"]], "Code Examples": [[162, "code-examples"], [163, "code-examples"]], "Code example": [[208, "code-example"], [212, "code-example"], [214, "code-example"]], "Compilation": [[253, "compilation"]], "Compile and install pip package dependencies": [[186, "compile-and-install-pip-package-dependencies"]], "Compressing using Spatial SVD": [[212, "compressing-using-spatial-svd"]], "Compression": [[211, null], [213, "compression"], [217, "compression"]], "Compression features Guidebook": [[209, null]], "Compression ratio selection": [[210, "compression-ratio-selection"], [211, "compression-ratio-selection"]], "Compression using Channel Pruning": [[208, "compression-using-channel-pruning"]], "Compression using Weight SVD": [[214, "compression-using-weight-svd"]], "Compute Encodings": [[236, "Compute-Encodings"], [244, "Compute-Encodings"]], "Computing encodings": [[166, "computing-encodings"]], "Configuration": [[166, "configuration"]], "Configuration file structure": [[225, "configuration-file-structure"]], "Contact Us": [[254, "contact-us"]], "Context": [[189, "context"], [190, "context"], [191, "context"], [192, "context"], [193, "context"], [194, "context"], [196, "context"], [198, "context"], [199, "context"], [200, "context"], [201, "context"], [204, "context"], [205, "context"], [206, "context"], [208, "context"], [212, "context"], [214, "context"], [219, "context"], [222, "context"]], "Conversion": [[253, "conversion"]], "Create Quantization Sim Model": [[236, "Create-Quantization-Sim-Model"], [244, "Create-Quantization-Sim-Model"], [247, "Create-Quantization-Sim-Model"]], "Create QuantizationSimModel": [[199, "create-quantizationsimmodel"]], "Create a new uv environment with Python 3.10": [[186, "create-a-new-uv-environment-with-python-3-10"]], "Create the Quantization Sim Model": [[238, "Create-the-Quantization-Sim-Model"], [240, "Create-the-Quantization-Sim-Model"], [245, "Create-the-Quantization-Sim-Model"], [248, "Create-the-Quantization-Sim-Model"], [249, "Create-the-Quantization-Sim-Model"], [250, "Create-the-Quantization-Sim-Model"]], "Cross-Layer Equalization": [[238, null], [248, null]], "Cross-layer equalization": [[194, null], [195, "cross-layer-equalization"]], "Data type": [[216, "id5"]], "Dataset": [[236, "Dataset"], [237, "Dataset"], [238, "Dataset"], [239, "Dataset"], [240, "Dataset"], [241, "Dataset"], [242, "Dataset"], [243, "Dataset"], [244, "Dataset"], [245, "Dataset"], [246, "Dataset"], [247, "Dataset"], [248, "Dataset"], [249, "Dataset"], [250, "Dataset"], [251, "Dataset"]], "Debugging guidelines": [[257, "debugging-guidelines"]], "Debugging workflow": [[227, "debugging-workflow"]], "Define callback functions for AMP": [[236, "Define-callback-functions-for-AMP"], [244, "Define-callback-functions-for-AMP"]], "Deployment paths": [[258, "deployment-paths"]], "DequantizedTensor": [[147, null]], "Design": [[213, "design"]], "Detailed Workflow": [[255, "detailed-workflow"]], "Determine quantization parameters (encodings)": [[256, "determine-quantization-parameters-encodings"]], "Docker environment": [[186, "docker-environment"]], "Encoding Format Specification": [[216, null]], "Encoding dictionary structure": [[216, "id3"]], "Encoding min/max ranges": [[239, "Encoding-min/max-ranges"], [251, "Encoding-min/max-ranges"]], "Encoding type": [[216, "id4"]], "Evaluate": [[254, "evaluate"]], "Example Notebooks": [[184, "example-notebooks"], [235, null]], "Executing blockwise quantization": [[207, "executing-blockwise-quantization"]], "Execution": [[192, "execution"], [194, "execution"], [253, "execution"]], "Export API": [[258, "export-api"]], "Export tools": [[258, "export-tools"]], "Exported Encodings": [[256, "exported-encodings"]], "External resources": [[182, null]], "FAQs": [[211, "faqs"], [254, "faqs"]], "FloatQuantizeDequantize": [[155, null]], "Fold Batch Norm layers": [[238, "Fold-Batch-Norm-layers"], [248, "Fold-Batch-Norm-layers"]], "Fold Batch Normalization layers": [[236, "Fold-Batch-Normalization-layers"], [240, "Fold-Batch-Normalization-layers"], [244, "Fold-Batch-Normalization-layers"], [245, "Fold-Batch-Normalization-layers"], [249, "Fold-Batch-Normalization-layers"], [250, "Fold-Batch-Normalization-layers"]], "Fold BatchNorm Layers": [[247, "Fold-BatchNorm-Layers"]], "For more information": [[237, "For-more-information"], [238, "For-more-information"], [240, "For-more-information"], [241, "For-more-information"], [242, "For-more-information"], [243, "For-more-information"], [245, "For-more-information"], [246, "For-more-information"], [248, "For-more-information"], [249, "For-more-information"], [250, "For-more-information"]], "General guidelines": [[259, "general-guidelines"]], "Get Started": [[185, "get-started"]], "Glossary": [[183, null], [184, "glossary"]], "Greedy compression ratio selection": [[210, null]], "How it works": [[210, "how-it-works"]], "How quantization simulation works": [[256, "how-quantization-simulation-works"]], "How to modify configuration file": [[225, "how-to-modify-configuration-file"]], "How to use aimet_torch 1.x": [[162, "how-to-use-aimet-torch-1-x"]], "How winnowing works": [[215, "how-winnowing-works"]], "Install uv": [[186, "install-uv"]], "Installation": [[187, null]], "Installing AIMET": [[188, "installing-aimet"]], "Interactive visualization": [[203, "interactive-visualization"], [204, null]], "Layer output generation": [[203, "layer-output-generation"], [205, null]], "Limitations": [[163, "limitations"]], "Lite mixed precision": [[220, "lite-mixed-precision"], [221, null]], "LoRa Training": [[198, "lora-training"]], "Low-Power Blockwise Quantization": [[217, "low-power-blockwise-quantization"]], "Low-Power Blockwise Quantization (LPBQ)": [[218, null]], "Manual mixed precision": [[220, "manual-mixed-precision"], [222, null]], "Migration Process": [[162, "migration-process"]], "Migration guide": [[162, null]], "Min-Max (also called \u201cTF\u201d in AIMET)": [[256, "min-max-also-called-tf-in-aimet"]], "Mixed Precision": [[217, "mixed-precision"]], "Mixed Precision Algorithm": [[219, "mixed-precision-algorithm"]], "Mixed precision": [[220, null], [258, "mixed-precision"]], "Model compression": [[211, "model-compression"]], "Model compression using channel pruning": [[241, null]], "Model compression using spatial SVD": [[242, null]], "Model compression using spatial SVD and channel pruning": [[243, null]], "Model guidelines": [[226, "model-guidelines"]], "Moving from QuantWrapper to Quantized Modules": [[162, "moving-from-quantwrapper-to-quantized-modules"]], "Moving from StaticGrid and LearnedGrid Quantizer to Affine and Float Quantizer": [[162, "moving-from-staticgrid-and-learnedgrid-quantizer-to-affine-and-float-quantizer"]], "NOTE": [[211, null]], "NVIDIA CUDA support": [[186, "nvidia-cuda-support"]], "Next steps": [[237, "Next-steps"], [241, "Next-steps"], [242, "Next-steps"], [243, "Next-steps"], [245, "Next-steps"], [246, "Next-steps"], [248, "Next-steps"], [249, "Next-steps"], [250, "Next-steps"]], "Next: Deploying the model": [[255, "next-deploying-the-model"]], "Next: deploying the model": [[259, "next-deploying-the-model"]], "Old versions": [[187, "old-versions"]], "OmniQuant": [[195, "omniquant"], [196, null]], "On-target inference": [[253, null], [257, "on-target-inference"]], "Optional techniques": [[211, "optional-techniques"]], "Overall flow": [[236, "Overall-flow"], [237, "Overall-flow"], [238, "Overall-flow"], [239, "Overall-flow"], [240, "Overall-flow"], [241, "Overall-flow"], [242, "Overall-flow"], [243, "Overall-flow"], [244, "Overall-flow"], [245, "Overall-flow"], [246, "Overall-flow"], [247, "Overall-flow"], [248, "Overall-flow"], [249, "Overall-flow"], [250, "Overall-flow"], [251, "Overall-flow"]], "Overview": [[184, "overview"], [210, "overview"], [211, "overview"], [213, "overview"], [215, "overview"], [225, "overview"], [256, "overview"]], "PDF of statistics": [[239, "PDF-of-statistics"], [251, "PDF-of-statistics"]], "PTQ": [[198, "ptq"]], "Parameters for AMP algorithm": [[236, "Parameters-for-AMP-algorithm"], [244, "Parameters-for-AMP-algorithm"]], "Per-block quantization": [[171, "per-block-quantization"]], "Per-channel quantization": [[171, "per-channel-quantization"]], "Per-layer MSE loss": [[239, "Per-layer-MSE-loss"], [251, "Per-layer-MSE-loss"]], "Per-layer analysis by enabling/disabling quantization wrappers": [[239, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"], [251, "Per-layer-analysis-by-enabling/disabling-quantization-wrappers"]], "Per-layer exploration": [[210, "per-layer-exploration"]], "Per-layer fine-tuning": [[211, "per-layer-fine-tuning"]], "Perform QAT": [[247, "Perform-QAT"]], "Performance Summary": [[254, "performance-summary"]], "Phase 0: Find quantizer groups": [[219, "phase-0-find-quantizer-groups"]], "Phase 1: Perform sensitivity analysis": [[219, "phase-1-perform-sensitivity-analysis"]], "Phase 2: Create a Pareto-front list": [[219, "phase-2-create-a-pareto-front-list"]], "Phase 3: Reduce Convert overhead": [[219, "phase-3-reduce-convert-overhead"]], "Post Training Quantization": [[217, "post-training-quantization"], [223, null]], "Post Training Quantization Techniques": [[184, "post-training-quantization-techniques"], [195, null]], "Post-training quantization": [[258, "post-training-quantization"]], "Prerequisites": [[187, "prerequisites"], [189, "prerequisites"], [190, "prerequisites"], [191, "prerequisites"], [192, "prerequisites"], [196, "prerequisites"], [200, "prerequisites"], [201, "prerequisites"], [206, "prerequisites"], [221, "prerequisites"], [222, "prerequisites"], [223, "prerequisites"]], "Procedure": [[190, "procedure"], [191, "procedure"], [193, "procedure"], [196, "procedure"], [200, "procedure"], [201, "procedure"], [208, "procedure"], [219, "procedure"], [259, "procedure"]], "PyPI": [[187, "pypi"]], "PyTorch model guidelines": [[226, null]], "QW-LoRa": [[197, "qw-lora"], [198, null]], "QWA-LoRa": [[197, "qwa-lora"], [199, null]], "Qualcomm\u00ae AI Engine Direct SDK": [[253, "qualcommreg-ai-engine-direct-sdk"]], "Qualcomm\u00ae AI hub": [[253, "qualcommreg-ai-hub"]], "Quant Analyzer": [[239, null], [251, null]], "QuantSim workflow": [[256, "quantsim-workflow"]], "Quantization": [[253, "quantization"]], "Quantization Aware Training": [[217, "quantization-aware-training"]], "Quantization analyzer": [[203, "quantization-analyzer"], [206, null]], "Quantization debugging guidelines": [[227, null]], "Quantization granularity": [[256, "quantization-granularity"]], "Quantization recipes for LLMs": [[254, null]], "Quantization schemes": [[256, "quantization-schemes"]], "Quantization simulation": [[240, null], [258, "quantization-simulation"]], "Quantization simulation guide": [[256, null]], "Quantization user guide": [[257, null]], "Quantization workflow": [[255, null], [257, "quantization-workflow"], [259, null]], "Quantization-Aware Training (QAT)": [[252, null]], "Quantization-Aware Training with BatchNorm Re-estimation": [[247, null]], "Quantization-aware training": [[224, null], [249, null], [258, "quantization-aware-training"]], "Quantization-aware training with range learning": [[250, null]], "QuantizationMixin": [[19, null]], "Quantize": [[150, null], [254, "quantize"]], "Quantize a small model quickly with AIMET": [[188, "quantize-a-small-model-quickly-with-aimet"]], "Quantize and Update Base Model Weights": [[198, "quantize-and-update-base-model-weights"]], "QuantizeDequantize": [[151, null]], "Quantized LoRa": [[195, "quantized-lora"], [197, null]], "Quantized modules": [[166, "quantized-modules"]], "QuantizedAdaptiveAvgPool1d": [[20, null]], "QuantizedAdaptiveAvgPool2d": [[21, null]], "QuantizedAdaptiveAvgPool3d": [[22, null]], "QuantizedAdaptiveMaxPool1d": [[23, null]], "QuantizedAdaptiveMaxPool2d": [[24, null]], "QuantizedAdaptiveMaxPool3d": [[25, null]], "QuantizedAlphaDropout": [[26, null]], "QuantizedAvgPool1d": [[27, null]], "QuantizedAvgPool2d": [[28, null]], "QuantizedAvgPool3d": [[29, null]], "QuantizedBCELoss": [[30, null]], "QuantizedBCEWithLogitsLoss": [[31, null]], "QuantizedBatchNorm1d": [[32, null]], "QuantizedBatchNorm2d": [[33, null]], "QuantizedBatchNorm3d": [[34, null]], "QuantizedBilinear": [[35, null]], "QuantizedCELU": [[36, null]], "QuantizedCTCLoss": [[37, null]], "QuantizedChannelShuffle": [[38, null]], "QuantizedCircularPad1d": [[39, null]], "QuantizedCircularPad2d": [[40, null]], "QuantizedCircularPad3d": [[41, null]], "QuantizedConstantPad1d": [[42, null]], "QuantizedConstantPad2d": [[43, null]], "QuantizedConstantPad3d": [[44, null]], "QuantizedConv1d": [[45, null]], "QuantizedConv2d": [[46, null]], "QuantizedConv3d": [[47, null]], "QuantizedConvTranspose1d": [[48, null]], "QuantizedConvTranspose2d": [[49, null]], "QuantizedConvTranspose3d": [[50, null]], "QuantizedCosineEmbeddingLoss": [[51, null]], "QuantizedCosineSimilarity": [[52, null]], "QuantizedCrossEntropyLoss": [[53, null]], "QuantizedDropout": [[54, null]], "QuantizedDropout1d": [[55, null]], "QuantizedDropout2d": [[56, null]], "QuantizedDropout3d": [[57, null]], "QuantizedELU": [[58, null]], "QuantizedEmbedding": [[59, null]], "QuantizedEmbeddingBag": [[60, null]], "QuantizedFeatureAlphaDropout": [[61, null]], "QuantizedFlatten": [[62, null]], "QuantizedFold": [[63, null]], "QuantizedFractionalMaxPool2d": [[64, null]], "QuantizedFractionalMaxPool3d": [[65, null]], "QuantizedGELU": [[66, null]], "QuantizedGLU": [[67, null]], "QuantizedGRU": [[68, null]], "QuantizedGRUCell": [[69, null]], "QuantizedGaussianNLLLoss": [[70, null]], "QuantizedGroupNorm": [[71, null]], "QuantizedHardshrink": [[72, null]], "QuantizedHardsigmoid": [[73, null]], "QuantizedHardswish": [[74, null]], "QuantizedHardtanh": [[75, null]], "QuantizedHingeEmbeddingLoss": [[76, null]], "QuantizedHuberLoss": [[77, null]], "QuantizedInstanceNorm1d": [[78, null]], "QuantizedInstanceNorm2d": [[79, null]], "QuantizedInstanceNorm3d": [[80, null]], "QuantizedKLDivLoss": [[81, null]], "QuantizedL1Loss": [[82, null]], "QuantizedLPPool1d": [[83, null]], "QuantizedLPPool2d": [[84, null]], "QuantizedLSTM": [[85, null]], "QuantizedLSTMCell": [[86, null]], "QuantizedLayerNorm": [[87, null]], "QuantizedLeakyReLU": [[88, null]], "QuantizedLinear": [[89, null]], "QuantizedLocalResponseNorm": [[90, null]], "QuantizedLogSigmoid": [[91, null]], "QuantizedLogSoftmax": [[92, null]], "QuantizedMSELoss": [[93, null]], "QuantizedMarginRankingLoss": [[94, null]], "QuantizedMaxPool1d": [[95, null]], "QuantizedMaxPool2d": [[96, null]], "QuantizedMaxPool3d": [[97, null]], "QuantizedMaxUnpool1d": [[98, null]], "QuantizedMaxUnpool2d": [[99, null]], "QuantizedMaxUnpool3d": [[100, null]], "QuantizedMish": [[101, null]], "QuantizedMultiLabelMarginLoss": [[102, null]], "QuantizedMultiLabelSoftMarginLoss": [[103, null]], "QuantizedMultiMarginLoss": [[104, null]], "QuantizedNLLLoss": [[105, null]], "QuantizedNLLLoss2d": [[106, null]], "QuantizedPReLU": [[107, null]], "QuantizedPairwiseDistance": [[108, null]], "QuantizedPixelShuffle": [[109, null]], "QuantizedPixelUnshuffle": [[110, null]], "QuantizedPoissonNLLLoss": [[111, null]], "QuantizedRNN": [[112, null]], "QuantizedRNNCell": [[113, null]], "QuantizedRReLU": [[114, null]], "QuantizedReLU": [[115, null]], "QuantizedReLU6": [[116, null]], "QuantizedReflectionPad1d": [[117, null]], "QuantizedReflectionPad2d": [[118, null]], "QuantizedReflectionPad3d": [[119, null]], "QuantizedReplicationPad1d": [[120, null]], "QuantizedReplicationPad2d": [[121, null]], "QuantizedReplicationPad3d": [[122, null]], "QuantizedSELU": [[123, null]], "QuantizedSiLU": [[124, null]], "QuantizedSigmoid": [[125, null]], "QuantizedSmoothL1Loss": [[126, null]], "QuantizedSoftMarginLoss": [[127, null]], "QuantizedSoftmax": [[128, null]], "QuantizedSoftmax2d": [[129, null]], "QuantizedSoftmin": [[130, null]], "QuantizedSoftplus": [[131, null]], "QuantizedSoftshrink": [[132, null]], "QuantizedSoftsign": [[133, null]], "QuantizedTanh": [[134, null]], "QuantizedTanhshrink": [[135, null]], "QuantizedTensor": [[148, null]], "QuantizedTensorBase": [[149, null]], "QuantizedThreshold": [[136, null]], "QuantizedTripletMarginLoss": [[137, null]], "QuantizedTripletMarginWithDistanceLoss": [[138, null]], "QuantizedUnflatten": [[139, null]], "QuantizedUnfold": [[140, null]], "QuantizedUpsample": [[141, null]], "QuantizedUpsamplingBilinear2d": [[142, null]], "QuantizedUpsamplingNearest2d": [[143, null]], "QuantizedZeroPad1d": [[144, null]], "QuantizedZeroPad2d": [[145, null]], "QuantizedZeroPad3d": [[146, null]], "Quantizer Args structure": [[216, "id6"]], "Quantizers": [[171, "quantizers"]], "Quick Start": [[188, null], [254, "quick-start"]], "Qwen/Qwen2.5-0.5B-Instruct": [[232, null]], "Qwen/Qwen2.5-1.5B-Instruct": [[233, null]], "Qwen/Qwen3-4B": [[234, null]], "Rank Rounding": [[211, "rank-rounding"]], "Re-estimate BatchNorm Statistics": [[247, "Re-estimate-BatchNorm-Statistics"]], "Recipes": [[254, "recipes"]], "References": [[211, "references"]], "Release Notes": [[184, "release-notes"]], "Release notes": [[202, null]], "Run QWA-LoRa": [[199, "run-qwa-lora"]], "Running the notebooks": [[235, "running-the-notebooks"]], "Runtime configuration": [[225, null], [256, "runtime-configuration"]], "Sequential MSE": [[195, "sequential-mse"], [200, null]], "Set environment variables to build desired AIMET wheel": [[186, "set-environment-variables-to-build-desired-aimet-wheel"], [186, "id1"]], "Set model input precision": [[222, "set-model-input-precision"]], "Set model output precision": [[222, "set-model-output-precision"]], "Set precision based on layer type": [[222, "set-precision-based-on-layer-type"]], "Set precision of a leaf layer": [[222, "set-precision-of-a-leaf-layer"]], "Set precision of a non-leaf layer": [[222, "set-precision-of-a-non-leaf-layer"]], "Setup": [[189, "setup"], [190, "setup"], [192, "setup"], [194, "setup"], [196, "setup"], [198, "setup"], [199, "setup"], [200, "setup"], [201, "setup"], [208, "setup"], [212, "setup"], [214, "setup"], [222, "setup"]], "Signal-to-Quantization-Noise": [[256, "signal-to-quantization-noise"]], "Simulate quantization noise": [[256, "simulate-quantization-noise"]], "Spatial SVD": [[211, "spatial-svd"], [212, null]], "SpinQuant": [[195, "spinquant"], [201, null]], "Starting a Bokeh server session": [[213, "starting-a-bokeh-server-session"]], "Step 1": [[189, "step-1"], [190, "step-1"], [191, "step-1"], [192, "step-1"], [193, "step-1"], [196, "step-1"], [200, "step-1"], [201, "step-1"], [219, "step-1"]], "Step 1 Importing libraries": [[206, "step-1-importing-libraries"]], "Step 1: Applying MMP API options": [[222, "step-1-applying-mmp-api-options"]], "Step 1: Creating a QuantSim model": [[221, "step-1-creating-a-quantsim-model"], [223, "step-1-creating-a-quantsim-model"]], "Step 1: Find baseline precision": [[255, "step-1-find-baseline-precision"]], "Step 1: Importing the API": [[205, "step-1-importing-the-api"]], "Step 1: Setup": [[224, "step-1-setup"]], "Step 1: Trying FP16 precision (no quantization)": [[259, "step-1-trying-fp16-precision-no-quantization"]], "Step 2": [[189, "step-2"], [190, "step-2"], [191, "step-2"], [192, "step-2"], [193, "step-2"], [196, "step-2"], [200, "step-2"], [201, "step-2"], [219, "step-2"]], "Step 2 Preparing calibration callback": [[206, "step-2-preparing-calibration-callback"]], "Step 2: Applying the profile": [[222, "step-2-applying-the-profile"]], "Step 2: Compute initial quantization parameters": [[224, "step-2-compute-initial-quantization-parameters"]], "Step 2: Computing encodings": [[221, "step-2-computing-encodings"]], "Step 2: Creating a calibration callback": [[223, "step-2-creating-a-calibration-callback"]], "Step 2: Loading a model": [[205, "step-2-loading-a-model"]], "Step 2: Use lite mixed precision": [[255, "step-2-use-lite-mixed-precision"]], "Step 2: Verifying W16A16 quantization": [[259, "step-2-verifying-w16a16-quantization"]], "Step 3": [[189, "step-3"], [190, "step-3"], [191, "step-3"], [192, "step-3"], [193, "step-3"], [196, "step-3"], [200, "step-3"], [201, "step-3"]], "Step 3 Preparing evaluation callback": [[206, "step-3-preparing-evaluation-callback"]], "Step 3. Reducing precision": [[259, "step-3-reducing-precision"]], "Step 3: Computing encodings": [[223, "step-3-computing-encodings"]], "Step 3: Evaluation of w8a8 base precision": [[221, "step-3-evaluation-of-w8a8-base-precision"]], "Step 3: Obtaining inputs": [[205, "step-3-obtaining-inputs"]], "Step 3: Run quantization-aware training": [[224, "step-3-run-quantization-aware-training"]], "Step 3: Use Automatic Mixed Precision (AMP)": [[255, "step-3-use-automatic-mixed-precision-amp"]], "Step 4": [[189, "step-4"], [190, "step-4"], [191, "step-4"], [192, "step-4"], [196, "step-4"], [200, "step-4"], [201, "step-4"]], "Step 4 Preparing model": [[206, "step-4-preparing-model"]], "Step 4. Restoring accuracy": [[259, "step-4-restoring-accuracy"]], "Step 4: Evaluation": [[223, "step-4-evaluation"]], "Step 4: Generating layer outputs": [[205, "step-4-generating-layer-outputs"]], "Step 4: Perform sensitivity analysis": [[221, "step-4-perform-sensitivity-analysis"]], "Step 4: Use advanced Post-Training Quantization (PTQ) techniques": [[255, "step-4-use-advanced-post-training-quantization-ptq-techniques"]], "Step 5": [[190, "step-5"], [191, "step-5"], [196, "step-5"], [200, "step-5"], [201, "step-5"]], "Step 5 Creating QuantAnalyzer": [[206, "step-5-creating-quantanalyzer"]], "Step 5: Apply precision adjustment": [[221, "step-5-apply-precision-adjustment"]], "Step 5: Exporting the model": [[223, "step-5-exporting-the-model"]], "Step 5: Use Quantization-Aware Training (QAT)": [[255, "step-5-use-quantization-aware-training-qat"]], "Step 6": [[191, "step-6"]], "Step 6 Running the analysis": [[206, "step-6-running-the-analysis"]], "Step 6: Recompute encodings": [[221, "step-6-recompute-encodings"]], "Step 7": [[191, "step-7"]], "Step 7: Evaluation of w8a8_mixed precision": [[221, "step-7-evaluation-of-w8a8-mixed-precision"]], "Summary": [[247, "Summary"]], "Supported platform": [[185, "supported-platform"]], "Supported precisions for on-target inference": [[258, "supported-precisions-for-on-target-inference"]], "System Requirements": [[254, "system-requirements"]], "Techniques": [[184, "techniques"], [217, null], [248, "Techniques"]], "Terminology": [[169, "terminology"]], "Tested platform": [[188, "tested-platform"]], "Top level structure": [[216, "id2"]], "Training Callback": [[199, "training-callback"]], "Tutorials": [[184, "tutorials"], [228, null]], "Typical recommendations": [[224, "typical-recommendations"]], "UV environment": [[186, "uv-environment"]], "Use Case": [[211, "use-case"]], "Use Cases": [[219, "use-cases"]], "User flow": [[169, "user-flow"]], "Variants of QAT": [[224, "variants-of-qat"]], "Verifying the installation": [[187, "verifying-the-installation"], [188, "verifying-the-installation"]], "Visualization Tools": [[159, "visualization-tools"]], "Visualizing compression ratios": [[213, "visualizing-compression-ratios"]], "Weight SVD": [[211, "weight-svd"], [214, null]], "Weight reconstruction": [[208, "weight-reconstruction"]], "What is AIMET?": [[185, null]], "What this notebook is not": [[236, "What-this-notebook-is-not"], [239, "What-this-notebook-is-not"], [244, "What-this-notebook-is-not"], [247, "What-this-notebook-is-not"], [251, "What-this-notebook-is-not"]], "Winnowing": [[208, "winnowing"], [215, null], [215, "id1"]], "Workflow": [[189, "workflow"], [189, "id2"], [190, "workflow"], [191, "workflow"], [192, "workflow"], [193, "workflow"], [194, "workflow"], [196, "workflow"], [198, "workflow"], [199, "workflow"], [200, "workflow"], [201, "workflow"], [204, "workflow"], [205, "workflow"], [206, "workflow"], [208, "workflow"], [212, "workflow"], [214, "workflow"], [219, "workflow"], [221, "workflow"], [222, "workflow"], [223, "workflow"], [224, "workflow"]], "Workflow Overview": [[254, "workflow-overview"]], "aimet_onnx API": [[5, null]], "aimet_onnx.apply_adaround": [[1, null]], "aimet_onnx.apply_seq_mse": [[11, null]], "aimet_onnx.batch_norm_fold": [[3, null]], "aimet_onnx.cross_layer_equalization": [[4, null]], "aimet_onnx.layer_output_utils": [[6, null]], "aimet_onnx.lite_mp": [[7, null]], "aimet_onnx.mixed_precision": [[2, null]], "aimet_onnx.quant_analyzer": [[9, null]], "aimet_onnx.quantsim": [[10, null]], "aimet_onnx.quantsim.set_lpbq_for_params": [[8, null]], "aimet_torch": [[158, "aimet-torch"]], "aimet_torch 1.x vs aimet_torch 2": [[162, "aimet-torch-1-x-vs-aimet-torch-2"]], "aimet_torch API": [[158, null]], "aimet_torch.adaround": [[12, null]], "aimet_torch.auto_quant": [[14, null]], "aimet_torch.batch_norm_fold": [[16, null]], "aimet_torch.bn_reestimation": [[15, null]], "aimet_torch.compress": [[18, null]], "aimet_torch.cross_layer_equalization": [[17, null]], "aimet_torch.experimental.adascale": [[13, null]], "aimet_torch.experimental.omniquant": [[167, null]], "aimet_torch.experimental.spinquant": [[174, null]], "aimet_torch.layer_output_utils": [[160, null]], "aimet_torch.mixed_precision": [[165, null]], "aimet_torch.model_preparer": [[163, null]], "aimet_torch.model_validator": [[164, null]], "aimet_torch.nn": [[166, null]], "aimet_torch.onnx.export": [[168, null]], "aimet_torch.peft": [[169, null]], "aimet_torch.quant_analyzer": [[170, null]], "aimet_torch.quantization": [[171, null]], "aimet_torch.quantsim": [[172, null]], "aimet_torch.quantsim.config_utils": [[161, null]], "aimet_torch.seq_mse": [[173, null]], "aimet_torch.utils": [[175, null]], "aimet_torch.v1": [[158, "aimet-torch-v1"]], "aimet_torch.v1.adaround": [[176, null]], "aimet_torch.v1.auto_quant": [[178, null]], "aimet_torch.v1.mixed_precision": [[177, null]], "aimet_torch.v1.quant_analyzer": [[179, null]], "aimet_torch.v1.quantsim": [[180, null]], "aimet_torch.v1.seq_mse": [[181, null]], "aimet_torch.visualization_tools": [[159, null]], "dequantize": [[152, null]], "get_backend": [[156, null]], "meta-llama/Llama-3.2-1B-Instruct": [[229, null]], "meta-llama/Llama-3.2-3B-Instruct": [[230, null]], "microsoft/Phi-3.5-mini-instruct": [[231, null]], "quantize": [[153, null]], "quantize_dequantize": [[154, null]], "set_backend": [[157, null]], "\ud83c\udfc1 Conclusion": [[252, "\ud83c\udfc1-Conclusion"]], "\ud83d\udcc1 Before Getting Started: Prepare the ImageNet Dataset": [[252, "\ud83d\udcc1-Before-Getting-Started:-Prepare-the-ImageNet-Dataset"]]}, "docnames": ["apiref/index", "apiref/onnx/adaround", "apiref/onnx/amp", "apiref/onnx/bnf", "apiref/onnx/cle", "apiref/onnx/index", "apiref/onnx/layer_output_generation", "apiref/onnx/litemp", "apiref/onnx/lpbq", "apiref/onnx/quant_analyzer", "apiref/onnx/quantsim", "apiref/onnx/seq_mse", "apiref/torch/adaround", "apiref/torch/adascale", "apiref/torch/autoquant", "apiref/torch/bn", "apiref/torch/bnf", "apiref/torch/cle", "apiref/torch/compress", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d", "apiref/torch/generated/aimet_torch.nn.QuantizedELU", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten", "apiref/torch/generated/aimet_torch.nn.QuantizedFold", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d", "apiref/torch/generated/aimet_torch.nn.QuantizedMish", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize", "apiref/torch/generated/aimet_torch.quantization.get_backend", "apiref/torch/generated/aimet_torch.quantization.set_backend", "apiref/torch/index", "apiref/torch/interactive_visualization", "apiref/torch/layer_output_generation", "apiref/torch/lpbq", "apiref/torch/migration_guide", "apiref/torch/model_preparer", "apiref/torch/model_validator", "apiref/torch/mp", "apiref/torch/nn", "apiref/torch/omniquant", "apiref/torch/onnx", "apiref/torch/peft_lora", "apiref/torch/quant_analyzer", "apiref/torch/quantization", "apiref/torch/quantsim", "apiref/torch/seq_mse", "apiref/torch/spinquant", "apiref/torch/utils", "apiref/torch/v1/adaround", "apiref/torch/v1/amp", "apiref/torch/v1/autoquant", "apiref/torch/v1/quant_analyzer", "apiref/torch/v1/quantsim", "apiref/torch/v1/seq_mse", "external/index", "glossary", "index", "overview/index", "overview/install/build_from_source", "overview/install/index", "overview/install/quick-start", "ptq_techniques/adaround", "ptq_techniques/adascale", "ptq_techniques/autoquant", "ptq_techniques/bn", "ptq_techniques/bnf", "ptq_techniques/cle", "ptq_techniques/index", "ptq_techniques/omniquant", "ptq_techniques/quantized_LoRa/index", "ptq_techniques/quantized_LoRa/qw_lora", "ptq_techniques/quantized_LoRa/qwa_lora", "ptq_techniques/seq_mse", "ptq_techniques/spinquant", "release_notes", "techniques/analysis_tools/index", "techniques/analysis_tools/interactive_visualization", "techniques/analysis_tools/layer_output_generation", "techniques/analysis_tools/quant_analyzer", "techniques/blockwise", "techniques/compression/channel_pruning", "techniques/compression/feature_guidebook", "techniques/compression/greedy_compression_ratio_selection", "techniques/compression/index", "techniques/compression/spatial_svd", "techniques/compression/visualization_compression", "techniques/compression/weight_svd", "techniques/compression/winnowing", "techniques/encoding_spec", "techniques/index", "techniques/lpbq", "techniques/mixed_precision/amp", "techniques/mixed_precision/index", "techniques/mixed_precision/litemp", "techniques/mixed_precision/mmp", "techniques/ptq", "techniques/qat", "techniques/runtime_config", "techniques/torch/model_guidelines", "tutorials/debugging_guidelines", "tutorials/index", "tutorials/models/llama-3.2-1b", "tutorials/models/llama-3.2-3b", "tutorials/models/phi-3.5-mini", "tutorials/models/qwen-2.5-0.5b", "tutorials/models/qwen-2.5-1.5b", "tutorials/models/qwen-3-4b", "tutorials/notebooks", "tutorials/notebooks/onnx/quantization/AMP", "tutorials/notebooks/onnx/quantization/adaround", "tutorials/notebooks/onnx/quantization/cle", "tutorials/notebooks/onnx/quantization/quant_analyzer", "tutorials/notebooks/onnx/quantization/quantsim", "tutorials/notebooks/torch/compression/channel_pruning", "tutorials/notebooks/torch/compression/spatial_svd", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning", "tutorials/notebooks/torch/quantization/AMP", "tutorials/notebooks/torch/quantization/adaround", "tutorials/notebooks/torch/quantization/autoquant", "tutorials/notebooks/torch/quantization/bn_reestimation", "tutorials/notebooks/torch/quantization/cle", "tutorials/notebooks/torch/quantization/qat", "tutorials/notebooks/torch/quantization/qat_range_learning", "tutorials/notebooks/torch/quantization/quant_analyzer", "tutorials/notebooks/torch/v2/qat", "tutorials/on_target_inference", "tutorials/quantization_recipe", "tutorials/quantization_workflow", "tutorials/quantsim", "userguide/index", "userguide/quantization_tools", "userguide/quantization_workflow", "versions"], "envversion": {"nbsphinx": 4, "sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1}, "filenames": ["apiref/index.rst", "apiref/onnx/adaround.rst", "apiref/onnx/amp.rst", "apiref/onnx/bnf.rst", "apiref/onnx/cle.rst", "apiref/onnx/index.rst", "apiref/onnx/layer_output_generation.rst", "apiref/onnx/litemp.rst", "apiref/onnx/lpbq.rst", "apiref/onnx/quant_analyzer.rst", "apiref/onnx/quantsim.rst", "apiref/onnx/seq_mse.rst", "apiref/torch/adaround.rst", "apiref/torch/adascale.rst", "apiref/torch/autoquant.rst", "apiref/torch/bn.rst", "apiref/torch/bnf.rst", "apiref/torch/cle.rst", "apiref/torch/compress.rst", "apiref/torch/generated/aimet_torch.nn.QuantizationMixin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAdaptiveMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedAvgPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBCEWithLogitsLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBatchNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedBilinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCTCLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedChannelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCircularPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConstantPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConv3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedConvTranspose3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCosineSimilarity.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedCrossEntropyLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedDropout3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbedding.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedEmbeddingBag.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFeatureAlphaDropout.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFlatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedFractionalMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGRUCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGaussianNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedGroupNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardsigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardswish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHardtanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHingeEmbeddingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedHuberLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedInstanceNorm3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedKLDivLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLPPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTM.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLSTMCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLayerNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLeakyReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLinear.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLocalResponseNorm.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedLogSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMSELoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMarginRankingLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxPool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMaxUnpool3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMish.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedMultiMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedNLLLoss2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPairwiseDistance.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelShuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPixelUnshuffle.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedPoissonNLLLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNN.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRNNCell.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedRReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReLU6.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReflectionPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedReplicationPad3d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSELU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSiLU.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSigmoid.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSmoothL1Loss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmax2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftmin.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftplus.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedSoftsign.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanh.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTanhshrink.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedThreshold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnflatten.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUnfold.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsample.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingBilinear2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedUpsamplingNearest2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad1d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad2d.rst", "apiref/torch/generated/aimet_torch.nn.QuantizedZeroPad3d.rst", "apiref/torch/generated/aimet_torch.quantization.DequantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensor.rst", "apiref/torch/generated/aimet_torch.quantization.QuantizedTensorBase.rst", "apiref/torch/generated/aimet_torch.quantization.affine.Quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.QuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize.rst", "apiref/torch/generated/aimet_torch.quantization.affine.quantize_dequantize.rst", "apiref/torch/generated/aimet_torch.quantization.float.FloatQuantizeDequantize.rst", "apiref/torch/generated/aimet_torch.quantization.get_backend.rst", "apiref/torch/generated/aimet_torch.quantization.set_backend.rst", "apiref/torch/index.rst", "apiref/torch/interactive_visualization.rst", "apiref/torch/layer_output_generation.rst", "apiref/torch/lpbq.rst", "apiref/torch/migration_guide.rst", "apiref/torch/model_preparer.rst", "apiref/torch/model_validator.rst", "apiref/torch/mp.rst", "apiref/torch/nn.rst", "apiref/torch/omniquant.rst", "apiref/torch/onnx.rst", "apiref/torch/peft_lora.rst", "apiref/torch/quant_analyzer.rst", "apiref/torch/quantization.rst", "apiref/torch/quantsim.rst", "apiref/torch/seq_mse.rst", "apiref/torch/spinquant.rst", "apiref/torch/utils.rst", "apiref/torch/v1/adaround.rst", "apiref/torch/v1/amp.rst", "apiref/torch/v1/autoquant.rst", "apiref/torch/v1/quant_analyzer.rst", "apiref/torch/v1/quantsim.rst", "apiref/torch/v1/seq_mse.rst", "external/index.rst", "glossary.rst", "index.rst", "overview/index.rst", "overview/install/build_from_source.rst", "overview/install/index.rst", "overview/install/quick-start.rst", "ptq_techniques/adaround.rst", "ptq_techniques/adascale.rst", "ptq_techniques/autoquant.rst", "ptq_techniques/bn.rst", "ptq_techniques/bnf.rst", "ptq_techniques/cle.rst", "ptq_techniques/index.rst", "ptq_techniques/omniquant.rst", "ptq_techniques/quantized_LoRa/index.rst", "ptq_techniques/quantized_LoRa/qw_lora.rst", "ptq_techniques/quantized_LoRa/qwa_lora.rst", "ptq_techniques/seq_mse.rst", "ptq_techniques/spinquant.rst", "release_notes.rst", "techniques/analysis_tools/index.rst", "techniques/analysis_tools/interactive_visualization.rst", "techniques/analysis_tools/layer_output_generation.rst", "techniques/analysis_tools/quant_analyzer.rst", "techniques/blockwise.rst", "techniques/compression/channel_pruning.rst", "techniques/compression/feature_guidebook.rst", "techniques/compression/greedy_compression_ratio_selection.rst", "techniques/compression/index.rst", "techniques/compression/spatial_svd.rst", "techniques/compression/visualization_compression.rst", "techniques/compression/weight_svd.rst", "techniques/compression/winnowing.rst", "techniques/encoding_spec.rst", "techniques/index.rst", "techniques/lpbq.rst", "techniques/mixed_precision/amp.rst", "techniques/mixed_precision/index.rst", "techniques/mixed_precision/litemp.rst", "techniques/mixed_precision/mmp.rst", "techniques/ptq.rst", "techniques/qat.rst", "techniques/runtime_config.rst", "techniques/torch/model_guidelines.rst", "tutorials/debugging_guidelines.rst", "tutorials/index.rst", "tutorials/models/llama-3.2-1b.rst", "tutorials/models/llama-3.2-3b.rst", "tutorials/models/phi-3.5-mini.rst", "tutorials/models/qwen-2.5-0.5b.rst", "tutorials/models/qwen-2.5-1.5b.rst", "tutorials/models/qwen-3-4b.rst", "tutorials/notebooks.rst", "tutorials/notebooks/onnx/quantization/AMP.ipynb", "tutorials/notebooks/onnx/quantization/adaround.ipynb", "tutorials/notebooks/onnx/quantization/cle.ipynb", "tutorials/notebooks/onnx/quantization/quant_analyzer.ipynb", "tutorials/notebooks/onnx/quantization/quantsim.ipynb", "tutorials/notebooks/torch/compression/channel_pruning.ipynb", "tutorials/notebooks/torch/compression/spatial_svd.ipynb", "tutorials/notebooks/torch/compression/spatial_svd_channel_pruning.ipynb", "tutorials/notebooks/torch/quantization/AMP.ipynb", "tutorials/notebooks/torch/quantization/adaround.ipynb", "tutorials/notebooks/torch/quantization/autoquant.ipynb", "tutorials/notebooks/torch/quantization/bn_reestimation.ipynb", "tutorials/notebooks/torch/quantization/cle.ipynb", "tutorials/notebooks/torch/quantization/qat.ipynb", "tutorials/notebooks/torch/quantization/qat_range_learning.ipynb", "tutorials/notebooks/torch/quantization/quant_analyzer.ipynb", "tutorials/notebooks/torch/v2/qat.ipynb", "tutorials/on_target_inference.rst", "tutorials/quantization_recipe.rst", "tutorials/quantization_workflow.rst", "tutorials/quantsim.rst", "userguide/index.rst", "userguide/quantization_tools.rst", "userguide/quantization_workflow.rst", "versions.rst"], "indexentries": {"accelerator": [[183, "term-Accelerator", true]], "accuracy": [[183, "term-Accuracy", true]], "activation": [[183, "term-Activation", true]], "activation quantization": [[183, "term-Activation-Quantization", true]], "adaround": [[183, "term-AdaRound", true]], "adaroundparameters (class in aimet_torch.v1.adaround.adaround_weight)": [[176, "aimet_torch.v1.adaround.adaround_weight.AdaroundParameters", false]], "add_check() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[164, "aimet_torch.model_validator.model_validator.ModelValidator.add_check", false]], "ai model efficiency toolkit": [[183, "term-AI-Model-Efficiency-Toolkit", true]], "aimet": [[183, "term-AIMET", true]], "analyze() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false], [206, "aimet_onnx.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.analyze", false]], "analyze() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.analyze", false]], "apply() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[165, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false], [222, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.apply", false]], "apply_adaround() (in module aimet_torch.v1.adaround.adaround_weight.adaround)": [[176, "aimet_torch.v1.adaround.adaround_weight.Adaround.apply_adaround", false]], "apply_adascale() (in module aimet_torch.experimental.adascale)": [[13, "aimet_torch.experimental.adascale.apply_adascale", false], [190, "aimet_torch.experimental.adascale.apply_adascale", false]], "apply_omniquant() (in module aimet_torch.experimental.omniquant)": [[167, "aimet_torch.experimental.omniquant.apply_omniquant", false], [196, "aimet_torch.experimental.omniquant.apply_omniquant", false]], "apply_seq_mse() (in module aimet_onnx)": [[11, "aimet_onnx.apply_seq_mse", false], [200, "aimet_onnx.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.seq_mse)": [[173, "aimet_torch.seq_mse.apply_seq_mse", false], [200, "aimet_torch.seq_mse.apply_seq_mse", false]], "apply_seq_mse() (in module aimet_torch.v1.seq_mse)": [[181, "aimet_torch.v1.seq_mse.apply_seq_mse", false]], "apply_spinquant() (in module aimet_torch.experimental.spinquant)": [[174, "aimet_torch.experimental.spinquant.apply_spinquant", false], [201, "aimet_torch.experimental.spinquant.apply_spinquant", false]], "autoquant": [[183, "term-AutoQuant", true]], "batch normalization": [[183, "term-Batch-Normalization", true]], "batch normalization folding (bn folding)": [[183, "term-Batch-Normalization-Folding-BN-Folding", true]], "bitwidth (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.bitwidth", false]], "bn": [[183, "term-BN", true]], "callbackfunc (class in aimet_onnx.common.defs)": [[2, "aimet_onnx.common.defs.CallbackFunc", false], [219, "aimet_onnx.common.defs.CallbackFunc", false]], "callbackfunc (class in aimet_torch.common.defs)": [[165, "aimet_torch.common.defs.CallbackFunc", false], [177, "aimet_torch.common.defs.CallbackFunc", false], [219, "aimet_torch.common.defs.CallbackFunc", false]], "callbackfunc (class in aimet_torch.common.utils)": [[170, "aimet_torch.common.utils.CallbackFunc", false], [179, "aimet_torch.common.utils.CallbackFunc", false], [206, "aimet_torch.common.utils.CallbackFunc", false]], "check_model_sensitivity_to_quantization() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "check_model_sensitivity_to_quantization() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.check_model_sensitivity_to_quantization", false]], "choose_mixed_precision() (in module aimet_onnx.mixed_precision)": [[2, "aimet_onnx.mixed_precision.choose_mixed_precision", false], [219, "aimet_onnx.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.mixed_precision)": [[165, "aimet_torch.mixed_precision.choose_mixed_precision", false], [219, "aimet_torch.mixed_precision.choose_mixed_precision", false]], "choose_mixed_precision() (in module aimet_torch.v1.mixed_precision)": [[177, "aimet_torch.v1.mixed_precision.choose_mixed_precision", false]], "clone() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.clone", false]], "cnn": [[183, "term-CNN", true]], "compression": [[183, "term-Compression", true]], "compute_encodings() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.compute_encodings", false]], "compute_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.compute_encodings", false]], "compute_encodings() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[180, "aimet_torch.v1.quantsim.QuantizationSimModel.compute_encodings", false]], "compute_encodings() (in module aimet_onnx)": [[10, "aimet_onnx.compute_encodings", false]], "convolutional layer": [[183, "term-Convolutional-Layer", true]], "convolutional neural network": [[183, "term-Convolutional-Neural-Network", true]], "dequantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.dequantize", false]], "dequantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.dequantize", false]], "dequantize() (in module aimet_torch.quantization.affine)": [[152, "aimet_torch.quantization.affine.dequantize", false]], "dequantizedtensor (class in aimet_torch.quantization)": [[147, "aimet_torch.quantization.DequantizedTensor", false]], "detach() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.detach", false]], "device": [[183, "term-Device", true]], "dlf": [[183, "term-DLF", true]], "dynamic layer fusion": [[183, "term-Dynamic-Layer-Fusion", true]], "edge device": [[183, "term-Edge-device", true]], "enable_per_layer_mse_loss() (aimet_onnx.quant_analyzer.quantanalyzer method)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false], [206, "aimet_onnx.quant_analyzer.QuantAnalyzer.enable_per_layer_mse_loss", false]], "encoding": [[183, "term-Encoding", true]], "equalize_model() (in module aimet_onnx.cross_layer_equalization)": [[4, "aimet_onnx.cross_layer_equalization.equalize_model", false], [194, "aimet_onnx.cross_layer_equalization.equalize_model", false]], "equalize_model() (in module aimet_torch.cross_layer_equalization)": [[17, "aimet_torch.cross_layer_equalization.equalize_model", false], [194, "aimet_torch.cross_layer_equalization.equalize_model", false]], "evalcallbackfactory (class in aimet_onnx.amp.mixed_precision_algo)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false], [219, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory", false]], "evalcallbackfactory (class in aimet_torch.amp.mixed_precision_algo)": [[165, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [177, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false], [219, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory", false]], "exponent_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.exponent_bits", false]], "export() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.export", false]], "export() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.export", false]], "export() (aimet_torch.quantsim.quantizationsimmodelonnxexporter method)": [[172, "aimet_torch.quantsim.QuantizationSimModelOnnxExporter.export", false]], "export() (aimet_torch.v1.quantsim.quantizationsimmodel method)": [[180, "aimet_torch.v1.quantsim.QuantizationSimModel.export", false]], "export() (in module aimet_torch.onnx)": [[168, "aimet_torch.onnx.export", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_encoding_min_max_range() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_encoding_min_max_range", false]], "export_per_layer_mse_loss() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_mse_loss() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_mse_loss", false]], "export_per_layer_stats_histogram() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "export_per_layer_stats_histogram() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.export_per_layer_stats_histogram", false]], "floatquantizedequantize (class in aimet_torch.quantization.float)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize", false]], "fold_all_batch_norms() (in module aimet_torch.batch_norm_fold)": [[16, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false], [193, "aimet_torch.batch_norm_fold.fold_all_batch_norms", false]], "fold_all_batch_norms_to_weight() (in module aimet_onnx.batch_norm_fold)": [[3, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false], [193, "aimet_onnx.batch_norm_fold.fold_all_batch_norms_to_weight", false]], "fold_param_quantizers() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.fold_param_quantizers", false]], "forward() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad1d method)": [[39, "aimet_torch.nn.QuantizedCircularPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad2d method)": [[40, "aimet_torch.nn.QuantizedCircularPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedcircularpad3d method)": [[41, "aimet_torch.nn.QuantizedCircularPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad1d method)": [[42, "aimet_torch.nn.QuantizedConstantPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad2d method)": [[43, "aimet_torch.nn.QuantizedConstantPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedconstantpad3d method)": [[44, "aimet_torch.nn.QuantizedConstantPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedhardsigmoid method)": [[73, "aimet_torch.nn.QuantizedHardsigmoid.forward", false]], "forward() (aimet_torch.nn.quantizedhardswish method)": [[74, "aimet_torch.nn.QuantizedHardswish.forward", false]], "forward() (aimet_torch.nn.quantizedlinear method)": [[89, "aimet_torch.nn.QuantizedLinear.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad1d method)": [[117, "aimet_torch.nn.QuantizedReflectionPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad2d method)": [[118, "aimet_torch.nn.QuantizedReflectionPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedreflectionpad3d method)": [[119, "aimet_torch.nn.QuantizedReflectionPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad1d method)": [[120, "aimet_torch.nn.QuantizedReplicationPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad2d method)": [[121, "aimet_torch.nn.QuantizedReplicationPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedreplicationpad3d method)": [[122, "aimet_torch.nn.QuantizedReplicationPad3d.forward", false]], "forward() (aimet_torch.nn.quantizedthreshold method)": [[136, "aimet_torch.nn.QuantizedThreshold.forward", false]], "forward() (aimet_torch.nn.quantizedtripletmarginwithdistanceloss method)": [[138, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss.forward", false]], "forward() (aimet_torch.nn.quantizedunflatten method)": [[139, "aimet_torch.nn.QuantizedUnflatten.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad1d method)": [[144, "aimet_torch.nn.QuantizedZeroPad1d.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad2d method)": [[145, "aimet_torch.nn.QuantizedZeroPad2d.forward", false]], "forward() (aimet_torch.nn.quantizedzeropad3d method)": [[146, "aimet_torch.nn.QuantizedZeroPad3d.forward", false]], "forward() (aimet_torch.quantization.affine.quantize method)": [[150, "aimet_torch.quantization.affine.Quantize.forward", false]], "forward() (aimet_torch.quantization.affine.quantizedequantize method)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize.forward", false]], "forward() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.forward", false]], "forward_fn() (aimet_torch.seq_mse.seqmseparams method)": [[173, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false], [200, "aimet_torch.seq_mse.SeqMseParams.forward_fn", false]], "forward_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[181, "aimet_torch.v1.seq_mse.SeqMseParams.forward_fn", false]], "fp32": [[183, "term-FP32", true]], "from_encodings() (aimet_torch.quantization.float.floatquantizedequantize class method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.from_encodings", false]], "from_module() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.from_module", false]], "from_onnx_qdq() (aimet_onnx.quantizationsimmodel class method)": [[10, "aimet_onnx.QuantizationSimModel.from_onnx_qdq", false]], "from_str() (aimet_onnx.common.defs.quantscheme class method)": [[10, "aimet_onnx.common.defs.QuantScheme.from_str", false]], "from_str() (aimet_torch.common.defs.quantscheme class method)": [[172, "aimet_torch.common.defs.QuantScheme.from_str", false], [180, "aimet_torch.common.defs.QuantScheme.from_str", false]], "generate_layer_outputs() (aimet_onnx.layer_output_utils.layeroutpututil method)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [205, "aimet_onnx.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "generate_layer_outputs() (aimet_torch.layer_output_utils.layeroutpututil method)": [[160, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false], [205, "aimet_torch.layer_output_utils.LayerOutputUtil.generate_layer_outputs", false]], "get_activation_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_activation_quantizers", false]], "get_active_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_active_quantizers() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_active_quantizers", false]], "get_backend() (in module aimet_torch.quantization)": [[156, "aimet_torch.quantization.get_backend", false]], "get_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_candidate", false]], "get_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.get_default_kernel", false]], "get_encodings() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_encodings", false]], "get_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.get_extra_state", false]], "get_input_quantizer_modules() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup.get_input_quantizer_modules", false]], "get_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.get_kernel", false]], "get_loss_fn() (aimet_torch.seq_mse.seqmseparams method)": [[173, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false], [200, "aimet_torch.seq_mse.SeqMseParams.get_loss_fn", false]], "get_loss_fn() (aimet_torch.v1.seq_mse.seqmseparams method)": [[181, "aimet_torch.v1.seq_mse.SeqMseParams.get_loss_fn", false]], "get_param_quantizers() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.get_param_quantizers", false]], "ignore() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.ignore", false]], "ignore_unknown_modules() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.ignore_unknown_modules", false]], "implements() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.implements", false]], "inference": [[183, "term-Inference", true]], "input_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.input_quantizers", false]], "int8": [[183, "term-INT8", true]], "is_bfloat16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_bfloat16", false]], "is_float16() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.is_float16", false]], "kl divergence": [[183, "term-KL-Divergence", true]], "layer": [[183, "term-Layer", true]], "layer-wise quantization": [[183, "term-Layer-wise-quantization", true]], "layeroutpututil (class in aimet_onnx.layer_output_utils)": [[6, "aimet_onnx.layer_output_utils.LayerOutputUtil", false], [205, "aimet_onnx.layer_output_utils.LayerOutputUtil", false]], "layeroutpututil (class in aimet_torch.layer_output_utils)": [[160, "aimet_torch.layer_output_utils.LayerOutputUtil", false], [205, "aimet_torch.layer_output_utils.LayerOutputUtil", false]], "load_checkpoint() (aimet_torch.v1.quantsim method)": [[180, "aimet_torch.v1.quantsim.load_checkpoint", false]], "load_state_dict() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.load_state_dict", false]], "lora mobilenet": [[183, "term-LoRA-MobileNet", true]], "mantissa_bits (aimet_torch.quantization.float.floatquantizedequantize property)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.mantissa_bits", false]], "mixedprecisionconfigurator (class in aimet_torch.v2.mixed_precision)": [[165, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false], [222, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator", false]], "model": [[183, "term-Model", true]], "modelvalidator (class in aimet_torch.model_validator.model_validator)": [[164, "aimet_torch.model_validator.model_validator.ModelValidator", false]], "named_qmodules() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.named_qmodules", false]], "named_quantizer_parameters() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.named_quantizer_parameters", false]], "named_quantizers() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.named_quantizers", false]], "namingscheme (class in aimet_torch.layer_output_utils)": [[160, "aimet_torch.layer_output_utils.NamingScheme", false], [205, "aimet_torch.layer_output_utils.NamingScheme", false]], "neural network compression framework": [[183, "term-Neural-Network-Compression-Framework", true]], "new_empty() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.new_empty", false]], "nncf": [[183, "term-NNCF", true]], "node": [[183, "term-Node", true]], "normalization": [[183, "term-Normalization", true]], "onnx": [[183, "term-ONNX", true]], "onnx (aimet_torch.layer_output_utils.namingscheme attribute)": [[160, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false], [205, "aimet_torch.layer_output_utils.NamingScheme.ONNX", false]], "onnx (aimet_torch.quantizationsimmodel property)": [[172, "aimet_torch.QuantizationSimModel.onnx", false]], "open neural network exchange": [[183, "term-Open-Neural-Network-Exchange", true]], "output_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.output_quantizers", false]], "param_quantizers (aimet_torch.nn.quantizationmixin attribute)": [[19, "aimet_torch.nn.QuantizationMixin.param_quantizers", false]], "per-channel quantization": [[183, "term-Per-channel-Quantization", true]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_disabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_disabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.quant_analyzer.quantanalyzer method)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "perform_per_layer_analysis_by_enabling_quant_wrappers() (aimet_torch.v1.quant_analyzer.quantanalyzer method)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer.perform_per_layer_analysis_by_enabling_quant_wrappers", false]], "post-training quantization": [[183, "term-Post-Training-Quantization", true]], "prepare_model() (in module aimet_torch.model_preparer)": [[163, "aimet_torch.model_preparer.prepare_model", false]], "pruning": [[183, "term-Pruning", true]], "ptq": [[183, "term-PTQ", true]], "pytorch": [[183, "term-PyTorch", true]], "pytorch (aimet_torch.layer_output_utils.namingscheme attribute)": [[160, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false], [205, "aimet_torch.layer_output_utils.NamingScheme.PYTORCH", false]], "qat": [[183, "term-QAT", true]], "qdo": [[183, "term-QDO", true]], "qmodules() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.qmodules", false]], "qualcomm innovation center": [[183, "term-Qualcomm-Innovation-Center", true]], "quantanalyzer (class in aimet_onnx.quant_analyzer)": [[9, "aimet_onnx.quant_analyzer.QuantAnalyzer", false], [206, "aimet_onnx.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.quant_analyzer)": [[170, "aimet_torch.quant_analyzer.QuantAnalyzer", false], [206, "aimet_torch.quant_analyzer.QuantAnalyzer", false]], "quantanalyzer (class in aimet_torch.v1.quant_analyzer)": [[179, "aimet_torch.v1.quant_analyzer.QuantAnalyzer", false]], "quantization": [[183, "term-Quantization", true]], "quantization simulation": [[183, "term-Quantization-Simulation", true]], "quantization-aware training": [[183, "term-Quantization-Aware-Training", true]], "quantizationmixin (class in aimet_torch.nn)": [[19, "aimet_torch.nn.QuantizationMixin", false]], "quantizationsimmodel (class in aimet_onnx)": [[10, "aimet_onnx.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch)": [[172, "aimet_torch.QuantizationSimModel", false]], "quantizationsimmodel (class in aimet_torch.v1.quantsim)": [[180, "aimet_torch.v1.quantsim.QuantizationSimModel", false]], "quantizationsimmodelonnxexporter (class in aimet_torch.quantsim)": [[172, "aimet_torch.quantsim.QuantizationSimModelOnnxExporter", false]], "quantize (class in aimet_torch.quantization.affine)": [[150, "aimet_torch.quantization.affine.Quantize", false]], "quantize() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantize", false]], "quantize() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantize", false]], "quantize() (in module aimet_torch.quantization.affine)": [[153, "aimet_torch.quantization.affine.quantize", false]], "quantize_dequantize() (in module aimet_torch.quantization.affine)": [[154, "aimet_torch.quantization.affine.quantize_dequantize", false]], "quantized_repr() (aimet_torch.quantization.dequantizedtensor method)": [[147, "aimet_torch.quantization.DequantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensor method)": [[148, "aimet_torch.quantization.QuantizedTensor.quantized_repr", false]], "quantized_repr() (aimet_torch.quantization.quantizedtensorbase method)": [[149, "aimet_torch.quantization.QuantizedTensorBase.quantized_repr", false]], "quantizedadaptiveavgpool1d (class in aimet_torch.nn)": [[20, "aimet_torch.nn.QuantizedAdaptiveAvgPool1d", false]], "quantizedadaptiveavgpool2d (class in aimet_torch.nn)": [[21, "aimet_torch.nn.QuantizedAdaptiveAvgPool2d", false]], "quantizedadaptiveavgpool3d (class in aimet_torch.nn)": [[22, "aimet_torch.nn.QuantizedAdaptiveAvgPool3d", false]], "quantizedadaptivemaxpool1d (class in aimet_torch.nn)": [[23, "aimet_torch.nn.QuantizedAdaptiveMaxPool1d", false]], "quantizedadaptivemaxpool2d (class in aimet_torch.nn)": [[24, "aimet_torch.nn.QuantizedAdaptiveMaxPool2d", false]], "quantizedadaptivemaxpool3d (class in aimet_torch.nn)": [[25, "aimet_torch.nn.QuantizedAdaptiveMaxPool3d", false]], "quantizedalphadropout (class in aimet_torch.nn)": [[26, "aimet_torch.nn.QuantizedAlphaDropout", false]], "quantizedavgpool1d (class in aimet_torch.nn)": [[27, "aimet_torch.nn.QuantizedAvgPool1d", false]], "quantizedavgpool2d (class in aimet_torch.nn)": [[28, "aimet_torch.nn.QuantizedAvgPool2d", false]], "quantizedavgpool3d (class in aimet_torch.nn)": [[29, "aimet_torch.nn.QuantizedAvgPool3d", false]], "quantizedbatchnorm1d (class in aimet_torch.nn)": [[32, "aimet_torch.nn.QuantizedBatchNorm1d", false]], "quantizedbatchnorm2d (class in aimet_torch.nn)": [[33, "aimet_torch.nn.QuantizedBatchNorm2d", false]], "quantizedbatchnorm3d (class in aimet_torch.nn)": [[34, "aimet_torch.nn.QuantizedBatchNorm3d", false]], "quantizedbceloss (class in aimet_torch.nn)": [[30, "aimet_torch.nn.QuantizedBCELoss", false]], "quantizedbcewithlogitsloss (class in aimet_torch.nn)": [[31, "aimet_torch.nn.QuantizedBCEWithLogitsLoss", false]], "quantizedbilinear (class in aimet_torch.nn)": [[35, "aimet_torch.nn.QuantizedBilinear", false]], "quantizedcelu (class in aimet_torch.nn)": [[36, "aimet_torch.nn.QuantizedCELU", false]], "quantizedchannelshuffle (class in aimet_torch.nn)": [[38, "aimet_torch.nn.QuantizedChannelShuffle", false]], "quantizedcircularpad1d (class in aimet_torch.nn)": [[39, "aimet_torch.nn.QuantizedCircularPad1d", false]], "quantizedcircularpad2d (class in aimet_torch.nn)": [[40, "aimet_torch.nn.QuantizedCircularPad2d", false]], "quantizedcircularpad3d (class in aimet_torch.nn)": [[41, "aimet_torch.nn.QuantizedCircularPad3d", false]], "quantizedconstantpad1d (class in aimet_torch.nn)": [[42, "aimet_torch.nn.QuantizedConstantPad1d", false]], "quantizedconstantpad2d (class in aimet_torch.nn)": [[43, "aimet_torch.nn.QuantizedConstantPad2d", false]], "quantizedconstantpad3d (class in aimet_torch.nn)": [[44, "aimet_torch.nn.QuantizedConstantPad3d", false]], "quantizedconv1d (class in aimet_torch.nn)": [[45, "aimet_torch.nn.QuantizedConv1d", false]], "quantizedconv2d (class in aimet_torch.nn)": [[46, "aimet_torch.nn.QuantizedConv2d", false]], "quantizedconv3d (class in aimet_torch.nn)": [[47, "aimet_torch.nn.QuantizedConv3d", false]], "quantizedconvtranspose1d (class in aimet_torch.nn)": [[48, "aimet_torch.nn.QuantizedConvTranspose1d", false]], "quantizedconvtranspose2d (class in aimet_torch.nn)": [[49, "aimet_torch.nn.QuantizedConvTranspose2d", false]], "quantizedconvtranspose3d (class in aimet_torch.nn)": [[50, "aimet_torch.nn.QuantizedConvTranspose3d", false]], "quantizedcosineembeddingloss (class in aimet_torch.nn)": [[51, "aimet_torch.nn.QuantizedCosineEmbeddingLoss", false]], "quantizedcosinesimilarity (class in aimet_torch.nn)": [[52, "aimet_torch.nn.QuantizedCosineSimilarity", false]], "quantizedcrossentropyloss (class in aimet_torch.nn)": [[53, "aimet_torch.nn.QuantizedCrossEntropyLoss", false]], "quantizedctcloss (class in aimet_torch.nn)": [[37, "aimet_torch.nn.QuantizedCTCLoss", false]], "quantizeddropout (class in aimet_torch.nn)": [[54, "aimet_torch.nn.QuantizedDropout", false]], "quantizeddropout1d (class in aimet_torch.nn)": [[55, "aimet_torch.nn.QuantizedDropout1d", false]], "quantizeddropout2d (class in aimet_torch.nn)": [[56, "aimet_torch.nn.QuantizedDropout2d", false]], "quantizeddropout3d (class in aimet_torch.nn)": [[57, "aimet_torch.nn.QuantizedDropout3d", false]], "quantizedelu (class in aimet_torch.nn)": [[58, "aimet_torch.nn.QuantizedELU", false]], "quantizedembedding (class in aimet_torch.nn)": [[59, "aimet_torch.nn.QuantizedEmbedding", false]], "quantizedembeddingbag (class in aimet_torch.nn)": [[60, "aimet_torch.nn.QuantizedEmbeddingBag", false]], "quantizedequantize (class in aimet_torch.quantization.affine)": [[151, "aimet_torch.quantization.affine.QuantizeDequantize", false]], "quantizedfeaturealphadropout (class in aimet_torch.nn)": [[61, "aimet_torch.nn.QuantizedFeatureAlphaDropout", false]], "quantizedflatten (class in aimet_torch.nn)": [[62, "aimet_torch.nn.QuantizedFlatten", false]], "quantizedfold (class in aimet_torch.nn)": [[63, "aimet_torch.nn.QuantizedFold", false]], "quantizedfractionalmaxpool2d (class in aimet_torch.nn)": [[64, "aimet_torch.nn.QuantizedFractionalMaxPool2d", false]], "quantizedfractionalmaxpool3d (class in aimet_torch.nn)": [[65, "aimet_torch.nn.QuantizedFractionalMaxPool3d", false]], "quantizedgaussiannllloss (class in aimet_torch.nn)": [[70, "aimet_torch.nn.QuantizedGaussianNLLLoss", false]], "quantizedgelu (class in aimet_torch.nn)": [[66, "aimet_torch.nn.QuantizedGELU", false]], "quantizedglu (class in aimet_torch.nn)": [[67, "aimet_torch.nn.QuantizedGLU", false]], "quantizedgroupnorm (class in aimet_torch.nn)": [[71, "aimet_torch.nn.QuantizedGroupNorm", false]], "quantizedgru (class in aimet_torch.nn)": [[68, "aimet_torch.nn.QuantizedGRU", false]], "quantizedgrucell (class in aimet_torch.nn)": [[69, "aimet_torch.nn.QuantizedGRUCell", false]], "quantizedhardshrink (class in aimet_torch.nn)": [[72, "aimet_torch.nn.QuantizedHardshrink", false]], "quantizedhardsigmoid (class in aimet_torch.nn)": [[73, "aimet_torch.nn.QuantizedHardsigmoid", false]], "quantizedhardswish (class in aimet_torch.nn)": [[74, "aimet_torch.nn.QuantizedHardswish", false]], "quantizedhardtanh (class in aimet_torch.nn)": [[75, "aimet_torch.nn.QuantizedHardtanh", false]], "quantizedhingeembeddingloss (class in aimet_torch.nn)": [[76, "aimet_torch.nn.QuantizedHingeEmbeddingLoss", false]], "quantizedhuberloss (class in aimet_torch.nn)": [[77, "aimet_torch.nn.QuantizedHuberLoss", false]], "quantizedinstancenorm1d (class in aimet_torch.nn)": [[78, "aimet_torch.nn.QuantizedInstanceNorm1d", false]], "quantizedinstancenorm2d (class in aimet_torch.nn)": [[79, "aimet_torch.nn.QuantizedInstanceNorm2d", false]], "quantizedinstancenorm3d (class in aimet_torch.nn)": [[80, "aimet_torch.nn.QuantizedInstanceNorm3d", false]], "quantizedkldivloss (class in aimet_torch.nn)": [[81, "aimet_torch.nn.QuantizedKLDivLoss", false]], "quantizedl1loss (class in aimet_torch.nn)": [[82, "aimet_torch.nn.QuantizedL1Loss", false]], "quantizedlayernorm (class in aimet_torch.nn)": [[87, "aimet_torch.nn.QuantizedLayerNorm", false]], "quantizedleakyrelu (class in aimet_torch.nn)": [[88, "aimet_torch.nn.QuantizedLeakyReLU", false]], "quantizedlinear (class in aimet_torch.nn)": [[89, "aimet_torch.nn.QuantizedLinear", false]], "quantizedlocalresponsenorm (class in aimet_torch.nn)": [[90, "aimet_torch.nn.QuantizedLocalResponseNorm", false]], "quantizedlogsigmoid (class in aimet_torch.nn)": [[91, "aimet_torch.nn.QuantizedLogSigmoid", false]], "quantizedlogsoftmax (class in aimet_torch.nn)": [[92, "aimet_torch.nn.QuantizedLogSoftmax", false]], "quantizedlppool1d (class in aimet_torch.nn)": [[83, "aimet_torch.nn.QuantizedLPPool1d", false]], "quantizedlppool2d (class in aimet_torch.nn)": [[84, "aimet_torch.nn.QuantizedLPPool2d", false]], "quantizedlstm (class in aimet_torch.nn)": [[85, "aimet_torch.nn.QuantizedLSTM", false]], "quantizedlstmcell (class in aimet_torch.nn)": [[86, "aimet_torch.nn.QuantizedLSTMCell", false]], "quantizedmarginrankingloss (class in aimet_torch.nn)": [[94, "aimet_torch.nn.QuantizedMarginRankingLoss", false]], "quantizedmaxpool1d (class in aimet_torch.nn)": [[95, "aimet_torch.nn.QuantizedMaxPool1d", false]], "quantizedmaxpool2d (class in aimet_torch.nn)": [[96, "aimet_torch.nn.QuantizedMaxPool2d", false]], "quantizedmaxpool3d (class in aimet_torch.nn)": [[97, "aimet_torch.nn.QuantizedMaxPool3d", false]], "quantizedmaxunpool1d (class in aimet_torch.nn)": [[98, "aimet_torch.nn.QuantizedMaxUnpool1d", false]], "quantizedmaxunpool2d (class in aimet_torch.nn)": [[99, "aimet_torch.nn.QuantizedMaxUnpool2d", false]], "quantizedmaxunpool3d (class in aimet_torch.nn)": [[100, "aimet_torch.nn.QuantizedMaxUnpool3d", false]], "quantizedmish (class in aimet_torch.nn)": [[101, "aimet_torch.nn.QuantizedMish", false]], "quantizedmseloss (class in aimet_torch.nn)": [[93, "aimet_torch.nn.QuantizedMSELoss", false]], "quantizedmultilabelmarginloss (class in aimet_torch.nn)": [[102, "aimet_torch.nn.QuantizedMultiLabelMarginLoss", false]], "quantizedmultilabelsoftmarginloss (class in aimet_torch.nn)": [[103, "aimet_torch.nn.QuantizedMultiLabelSoftMarginLoss", false]], "quantizedmultimarginloss (class in aimet_torch.nn)": [[104, "aimet_torch.nn.QuantizedMultiMarginLoss", false]], "quantizednllloss (class in aimet_torch.nn)": [[105, "aimet_torch.nn.QuantizedNLLLoss", false]], "quantizednllloss2d (class in aimet_torch.nn)": [[106, "aimet_torch.nn.QuantizedNLLLoss2d", false]], "quantizedpairwisedistance (class in aimet_torch.nn)": [[108, "aimet_torch.nn.QuantizedPairwiseDistance", false]], "quantizedpixelshuffle (class in aimet_torch.nn)": [[109, "aimet_torch.nn.QuantizedPixelShuffle", false]], "quantizedpixelunshuffle (class in aimet_torch.nn)": [[110, "aimet_torch.nn.QuantizedPixelUnshuffle", false]], "quantizedpoissonnllloss (class in aimet_torch.nn)": [[111, "aimet_torch.nn.QuantizedPoissonNLLLoss", false]], "quantizedprelu (class in aimet_torch.nn)": [[107, "aimet_torch.nn.QuantizedPReLU", false]], "quantizedreflectionpad1d (class in aimet_torch.nn)": [[117, "aimet_torch.nn.QuantizedReflectionPad1d", false]], "quantizedreflectionpad2d (class in aimet_torch.nn)": [[118, "aimet_torch.nn.QuantizedReflectionPad2d", false]], "quantizedreflectionpad3d (class in aimet_torch.nn)": [[119, "aimet_torch.nn.QuantizedReflectionPad3d", false]], "quantizedrelu (class in aimet_torch.nn)": [[115, "aimet_torch.nn.QuantizedReLU", false]], "quantizedrelu6 (class in aimet_torch.nn)": [[116, "aimet_torch.nn.QuantizedReLU6", false]], "quantizedreplicationpad1d (class in aimet_torch.nn)": [[120, "aimet_torch.nn.QuantizedReplicationPad1d", false]], "quantizedreplicationpad2d (class in aimet_torch.nn)": [[121, "aimet_torch.nn.QuantizedReplicationPad2d", false]], "quantizedreplicationpad3d (class in aimet_torch.nn)": [[122, "aimet_torch.nn.QuantizedReplicationPad3d", false]], "quantizedrnn (class in aimet_torch.nn)": [[112, "aimet_torch.nn.QuantizedRNN", false]], "quantizedrnncell (class in aimet_torch.nn)": [[113, "aimet_torch.nn.QuantizedRNNCell", false]], "quantizedrrelu (class in aimet_torch.nn)": [[114, "aimet_torch.nn.QuantizedRReLU", false]], "quantizedselu (class in aimet_torch.nn)": [[123, "aimet_torch.nn.QuantizedSELU", false]], "quantizedsigmoid (class in aimet_torch.nn)": [[125, "aimet_torch.nn.QuantizedSigmoid", false]], "quantizedsilu (class in aimet_torch.nn)": [[124, "aimet_torch.nn.QuantizedSiLU", false]], "quantizedsmoothl1loss (class in aimet_torch.nn)": [[126, "aimet_torch.nn.QuantizedSmoothL1Loss", false]], "quantizedsoftmarginloss (class in aimet_torch.nn)": [[127, "aimet_torch.nn.QuantizedSoftMarginLoss", false]], "quantizedsoftmax (class in aimet_torch.nn)": [[128, "aimet_torch.nn.QuantizedSoftmax", false]], "quantizedsoftmax2d (class in aimet_torch.nn)": [[129, "aimet_torch.nn.QuantizedSoftmax2d", false]], "quantizedsoftmin (class in aimet_torch.nn)": [[130, "aimet_torch.nn.QuantizedSoftmin", false]], "quantizedsoftplus (class in aimet_torch.nn)": [[131, "aimet_torch.nn.QuantizedSoftplus", false]], "quantizedsoftshrink (class in aimet_torch.nn)": [[132, "aimet_torch.nn.QuantizedSoftshrink", false]], "quantizedsoftsign (class in aimet_torch.nn)": [[133, "aimet_torch.nn.QuantizedSoftsign", false]], "quantizedtanh (class in aimet_torch.nn)": [[134, "aimet_torch.nn.QuantizedTanh", false]], "quantizedtanhshrink (class in aimet_torch.nn)": [[135, "aimet_torch.nn.QuantizedTanhshrink", false]], "quantizedtensor (class in aimet_torch.quantization)": [[148, "aimet_torch.quantization.QuantizedTensor", false]], "quantizedtensorbase (class in aimet_torch.quantization)": [[149, "aimet_torch.quantization.QuantizedTensorBase", false]], "quantizedthreshold (class in aimet_torch.nn)": [[136, "aimet_torch.nn.QuantizedThreshold", false]], "quantizedtripletmarginloss (class in aimet_torch.nn)": [[137, "aimet_torch.nn.QuantizedTripletMarginLoss", false]], "quantizedtripletmarginwithdistanceloss (class in aimet_torch.nn)": [[138, "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss", false]], "quantizedunflatten (class in aimet_torch.nn)": [[139, "aimet_torch.nn.QuantizedUnflatten", false]], "quantizedunfold (class in aimet_torch.nn)": [[140, "aimet_torch.nn.QuantizedUnfold", false]], "quantizedupsample (class in aimet_torch.nn)": [[141, "aimet_torch.nn.QuantizedUpsample", false]], "quantizedupsamplingbilinear2d (class in aimet_torch.nn)": [[142, "aimet_torch.nn.QuantizedUpsamplingBilinear2d", false]], "quantizedupsamplingnearest2d (class in aimet_torch.nn)": [[143, "aimet_torch.nn.QuantizedUpsamplingNearest2d", false]], "quantizedzeropad1d (class in aimet_torch.nn)": [[144, "aimet_torch.nn.QuantizedZeroPad1d", false]], "quantizedzeropad2d (class in aimet_torch.nn)": [[145, "aimet_torch.nn.QuantizedZeroPad2d", false]], "quantizedzeropad3d (class in aimet_torch.nn)": [[146, "aimet_torch.nn.QuantizedZeroPad3d", false]], "quantizer_parameters() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.quantizer_parameters", false]], "quantizer_state_dict() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.quantizer_state_dict", false]], "quantizergroup (class in aimet_onnx.amp.quantizer_groups)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup", false]], "quantizergroup (class in aimet_torch.amp.quantizer_groups)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup", false]], "quantizers() (aimet_torch.quantizationsimmodel method)": [[172, "aimet_torch.QuantizationSimModel.quantizers", false]], "quantscheme (class in aimet_onnx.common.defs)": [[10, "aimet_onnx.common.defs.QuantScheme", false]], "quantscheme (class in aimet_torch.common.defs)": [[172, "aimet_torch.common.defs.QuantScheme", false], [180, "aimet_torch.common.defs.QuantScheme", false]], "quantsim": [[183, "term-QuantSim", true]], "quic": [[183, "term-QUIC", true]], "reestimate_bn_stats() (in module aimet_torch.bn_reestimation)": [[15, "aimet_torch.bn_reestimation.reestimate_bn_stats", false], [192, "aimet_torch.bn_reestimation.reestimate_bn_stats", false]], "remove_activation_quantizers() (in module aimet_torch.utils)": [[175, "aimet_torch.utils.remove_activation_quantizers", false]], "remove_all_quantizers() (in module aimet_torch.utils)": [[175, "aimet_torch.utils.remove_all_quantizers", false]], "remove_input_quantizers() (in module aimet_torch.utils)": [[175, "aimet_torch.utils.remove_input_quantizers", false]], "remove_output_quantizers() (in module aimet_torch.utils)": [[175, "aimet_torch.utils.remove_output_quantizers", false]], "remove_param_quantizers() (in module aimet_torch.utils)": [[175, "aimet_torch.utils.remove_param_quantizers", false]], "save_checkpoint() (aimet_torch.v1.quantsim method)": [[180, "aimet_torch.v1.quantsim.save_checkpoint", false]], "seqmseparams (class in aimet_torch.seq_mse)": [[173, "aimet_torch.seq_mse.SeqMseParams", false], [200, "aimet_torch.seq_mse.SeqMseParams", false]], "seqmseparams (class in aimet_torch.v1.seq_mse)": [[181, "aimet_torch.v1.seq_mse.SeqMseParams", false]], "set_activation_quantizers_to_float() (in module aimet_torch.v2.quantsim.config_utils)": [[161, "aimet_torch.v2.quantsim.config_utils.set_activation_quantizers_to_float", false]], "set_backend() (in module aimet_torch.quantization)": [[157, "aimet_torch.quantization.set_backend", false]], "set_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[161, "aimet_torch.v2.quantsim.config_utils.set_blockwise_quantization_for_weights", false]], "set_default_kernel() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.set_default_kernel", false]], "set_extra_state() (aimet_torch.quantization.float.floatquantizedequantize method)": [[155, "aimet_torch.quantization.float.FloatQuantizeDequantize.set_extra_state", false]], "set_grouped_blockwise_quantization_for_weights() (in module aimet_torch.v2.quantsim.config_utils)": [[161, "aimet_torch.v2.quantsim.config_utils.set_grouped_blockwise_quantization_for_weights", false]], "set_kernel() (aimet_torch.nn.quantizationmixin method)": [[19, "aimet_torch.nn.QuantizationMixin.set_kernel", false]], "set_lpbq_for_params() (in module aimet_onnx.quantsim)": [[8, "aimet_onnx.quantsim.set_lpbq_for_params", false]], "set_model_input_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[165, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false], [222, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_input_precision", false]], "set_model_output_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[165, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false], [222, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_model_output_precision", false]], "set_precision() (aimet_torch.v2.mixed_precision.mixedprecisionconfigurator method)": [[165, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false], [222, "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator.set_precision", false]], "set_quantizers_to_candidate() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "set_quantizers_to_candidate() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup.set_quantizers_to_candidate", false]], "sqnr() (aimet_onnx.amp.mixed_precision_algo.evalcallbackfactory method)": [[2, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [219, "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "sqnr() (aimet_torch.amp.mixed_precision_algo.evalcallbackfactory method)": [[165, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [177, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false], [219, "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory.sqnr", false]], "target hardware accelerator": [[183, "term-Target-Hardware-Accelerator", true]], "target runtime": [[183, "term-Target-Runtime", true]], "tensorflow": [[183, "term-TensorFlow", true]], "to_list() (aimet_onnx.amp.quantizer_groups.quantizergroup method)": [[2, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false], [219, "aimet_onnx.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_list() (aimet_torch.amp.quantizer_groups.quantizergroup method)": [[165, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [177, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false], [219, "aimet_torch.amp.quantizer_groups.QuantizerGroup.to_list", false]], "to_onnx_qdq() (aimet_onnx.quantizationsimmodel method)": [[10, "aimet_onnx.QuantizationSimModel.to_onnx_qdq", false]], "torchscript": [[183, "term-TorchScript", true]], "torchscript (aimet_torch.layer_output_utils.namingscheme attribute)": [[160, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false], [205, "aimet_torch.layer_output_utils.NamingScheme.TORCHSCRIPT", false]], "validate_model() (aimet_torch.model_validator.model_validator.modelvalidator static method)": [[164, "aimet_torch.model_validator.model_validator.ModelValidator.validate_model", false]], "variant": [[183, "term-Variant", true]], "visualize_stats() (in module aimet_torch.v2.visualization_tools)": [[159, "aimet_torch.v2.visualization_tools.visualize_stats", false], [204, "aimet_torch.v2.visualization_tools.visualize_stats", false]], "weights": [[183, "term-Weights", true]], "wrap() (aimet_torch.nn.quantizationmixin class method)": [[19, "aimet_torch.nn.QuantizationMixin.wrap", false]]}, "objects": {"aimet_onnx": [[10, 0, 1, "", "QuantizationSimModel"], [200, 2, 1, "", "apply_seq_mse"], [10, 2, 1, "", "compute_encodings"]], "aimet_onnx.QuantizationSimModel": [[10, 1, 1, "", "compute_encodings"], [10, 1, 1, "", "export"], [10, 1, 1, "", "from_onnx_qdq"], [10, 1, 1, "", "to_onnx_qdq"]], "aimet_onnx.amp.mixed_precision_algo": [[219, 0, 1, "", "EvalCallbackFactory"]], "aimet_onnx.amp.mixed_precision_algo.EvalCallbackFactory": [[219, 1, 1, "", "sqnr"]], "aimet_onnx.amp.quantizer_groups": [[219, 0, 1, "", "QuantizerGroup"]], "aimet_onnx.amp.quantizer_groups.QuantizerGroup": [[219, 1, 1, "", "get_activation_quantizers"], [219, 1, 1, "", "get_active_quantizers"], [219, 1, 1, "", "get_candidate"], [219, 1, 1, "", "get_param_quantizers"], [219, 1, 1, "", "set_quantizers_to_candidate"], [219, 1, 1, "", "to_list"]], "aimet_onnx.batch_norm_fold": [[193, 2, 1, "", "fold_all_batch_norms_to_weight"]], "aimet_onnx.common.defs": [[219, 0, 1, "", "CallbackFunc"], [10, 0, 1, "", "QuantScheme"]], "aimet_onnx.common.defs.QuantScheme": [[10, 1, 1, "", "from_str"]], "aimet_onnx.cross_layer_equalization": [[194, 2, 1, "", "equalize_model"]], "aimet_onnx.layer_output_utils": [[205, 0, 1, "", "LayerOutputUtil"]], "aimet_onnx.layer_output_utils.LayerOutputUtil": [[205, 1, 1, "", "generate_layer_outputs"]], "aimet_onnx.mixed_precision": [[219, 2, 1, "", "choose_mixed_precision"]], "aimet_onnx.quant_analyzer": [[206, 0, 1, "", "QuantAnalyzer"]], "aimet_onnx.quant_analyzer.QuantAnalyzer": [[206, 1, 1, "", "analyze"], [206, 1, 1, "", "enable_per_layer_mse_loss"]], "aimet_onnx.quantsim": [[8, 2, 1, "", "set_lpbq_for_params"]], "aimet_torch": [[172, 0, 1, "", "QuantizationSimModel"]], "aimet_torch.QuantizationSimModel": [[172, 1, 1, "", "compute_encodings"], [172, 1, 1, "", "export"], [172, 1, 1, "", "fold_param_quantizers"], [172, 1, 1, "", "named_qmodules"], [172, 1, 1, "", "named_quantizer_parameters"], [172, 1, 1, "", "named_quantizers"], [172, 3, 1, "", "onnx"], [172, 1, 1, "", "qmodules"], [172, 1, 1, "", "quantizer_parameters"], [172, 1, 1, "", "quantizer_state_dict"], [172, 1, 1, "", "quantizers"]], "aimet_torch.amp.mixed_precision_algo": [[219, 0, 1, "", "EvalCallbackFactory"]], "aimet_torch.amp.mixed_precision_algo.EvalCallbackFactory": [[219, 1, 1, "", "sqnr"]], "aimet_torch.amp.quantizer_groups": [[219, 0, 1, "", "QuantizerGroup"]], "aimet_torch.amp.quantizer_groups.QuantizerGroup": [[219, 1, 1, "", "get_active_quantizers"], [219, 1, 1, "", "get_candidate"], [219, 1, 1, "", "get_input_quantizer_modules"], [219, 1, 1, "", "set_quantizers_to_candidate"], [219, 1, 1, "", "to_list"]], "aimet_torch.batch_norm_fold": [[193, 2, 1, "", "fold_all_batch_norms"]], "aimet_torch.bn_reestimation": [[192, 2, 1, "", "reestimate_bn_stats"]], "aimet_torch.common.defs": [[219, 0, 1, "", "CallbackFunc"], [180, 0, 1, "", "QuantScheme"]], "aimet_torch.common.defs.QuantScheme": [[180, 1, 1, "", "from_str"]], "aimet_torch.common.utils": [[206, 0, 1, "", "CallbackFunc"]], "aimet_torch.cross_layer_equalization": [[194, 2, 1, "", "equalize_model"]], "aimet_torch.experimental.adascale": [[190, 2, 1, "", "apply_adascale"]], "aimet_torch.experimental.omniquant": [[196, 2, 1, "", "apply_omniquant"]], "aimet_torch.experimental.spinquant": [[201, 2, 1, "", "apply_spinquant"]], "aimet_torch.layer_output_utils": [[205, 0, 1, "", "LayerOutputUtil"], [205, 0, 1, "", "NamingScheme"]], "aimet_torch.layer_output_utils.LayerOutputUtil": [[205, 1, 1, "", "generate_layer_outputs"]], "aimet_torch.layer_output_utils.NamingScheme": [[205, 4, 1, "", "ONNX"], [205, 4, 1, "", "PYTORCH"], [205, 4, 1, "", "TORCHSCRIPT"]], "aimet_torch.mixed_precision": [[219, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.model_preparer": [[163, 2, 1, "", "prepare_model"]], "aimet_torch.model_validator.model_validator": [[164, 0, 1, "", "ModelValidator"]], "aimet_torch.model_validator.model_validator.ModelValidator": [[164, 1, 1, "", "add_check"], [164, 1, 1, "", "validate_model"]], "aimet_torch.nn": [[19, 0, 1, "", "QuantizationMixin"], [20, 0, 1, "", "QuantizedAdaptiveAvgPool1d"], [21, 0, 1, "", "QuantizedAdaptiveAvgPool2d"], [22, 0, 1, "", "QuantizedAdaptiveAvgPool3d"], [23, 0, 1, "", "QuantizedAdaptiveMaxPool1d"], [24, 0, 1, "", "QuantizedAdaptiveMaxPool2d"], [25, 0, 1, "", "QuantizedAdaptiveMaxPool3d"], [26, 0, 1, "", "QuantizedAlphaDropout"], [27, 0, 1, "", "QuantizedAvgPool1d"], [28, 0, 1, "", "QuantizedAvgPool2d"], [29, 0, 1, "", "QuantizedAvgPool3d"], [30, 0, 1, "", "QuantizedBCELoss"], [31, 0, 1, "", "QuantizedBCEWithLogitsLoss"], [32, 0, 1, "", "QuantizedBatchNorm1d"], [33, 0, 1, "", "QuantizedBatchNorm2d"], [34, 0, 1, "", "QuantizedBatchNorm3d"], [35, 0, 1, "", "QuantizedBilinear"], [36, 0, 1, "", "QuantizedCELU"], [37, 0, 1, "", "QuantizedCTCLoss"], [38, 0, 1, "", "QuantizedChannelShuffle"], [39, 0, 1, "", "QuantizedCircularPad1d"], [40, 0, 1, "", "QuantizedCircularPad2d"], [41, 0, 1, "", "QuantizedCircularPad3d"], [42, 0, 1, "", "QuantizedConstantPad1d"], [43, 0, 1, "", "QuantizedConstantPad2d"], [44, 0, 1, "", "QuantizedConstantPad3d"], [45, 0, 1, "", "QuantizedConv1d"], [46, 0, 1, "", "QuantizedConv2d"], [47, 0, 1, "", "QuantizedConv3d"], [48, 0, 1, "", "QuantizedConvTranspose1d"], [49, 0, 1, "", "QuantizedConvTranspose2d"], [50, 0, 1, "", "QuantizedConvTranspose3d"], [51, 0, 1, "", "QuantizedCosineEmbeddingLoss"], [52, 0, 1, "", "QuantizedCosineSimilarity"], [53, 0, 1, "", "QuantizedCrossEntropyLoss"], [54, 0, 1, "", "QuantizedDropout"], [55, 0, 1, "", "QuantizedDropout1d"], [56, 0, 1, "", "QuantizedDropout2d"], [57, 0, 1, "", "QuantizedDropout3d"], [58, 0, 1, "", "QuantizedELU"], [59, 0, 1, "", "QuantizedEmbedding"], [60, 0, 1, "", "QuantizedEmbeddingBag"], [61, 0, 1, "", "QuantizedFeatureAlphaDropout"], [62, 0, 1, "", "QuantizedFlatten"], [63, 0, 1, "", "QuantizedFold"], [64, 0, 1, "", "QuantizedFractionalMaxPool2d"], [65, 0, 1, "", "QuantizedFractionalMaxPool3d"], [66, 0, 1, "", "QuantizedGELU"], [67, 0, 1, "", "QuantizedGLU"], [68, 0, 1, "", "QuantizedGRU"], [69, 0, 1, "", "QuantizedGRUCell"], [70, 0, 1, "", "QuantizedGaussianNLLLoss"], [71, 0, 1, "", "QuantizedGroupNorm"], [72, 0, 1, "", "QuantizedHardshrink"], [73, 0, 1, "", "QuantizedHardsigmoid"], [74, 0, 1, "", "QuantizedHardswish"], [75, 0, 1, "", "QuantizedHardtanh"], [76, 0, 1, "", "QuantizedHingeEmbeddingLoss"], [77, 0, 1, "", "QuantizedHuberLoss"], [78, 0, 1, "", "QuantizedInstanceNorm1d"], [79, 0, 1, "", "QuantizedInstanceNorm2d"], [80, 0, 1, "", "QuantizedInstanceNorm3d"], [81, 0, 1, "", "QuantizedKLDivLoss"], [82, 0, 1, "", "QuantizedL1Loss"], [83, 0, 1, "", "QuantizedLPPool1d"], [84, 0, 1, "", "QuantizedLPPool2d"], [85, 0, 1, "", "QuantizedLSTM"], [86, 0, 1, "", "QuantizedLSTMCell"], [87, 0, 1, "", "QuantizedLayerNorm"], [88, 0, 1, "", "QuantizedLeakyReLU"], [89, 0, 1, "", "QuantizedLinear"], [90, 0, 1, "", "QuantizedLocalResponseNorm"], [91, 0, 1, "", "QuantizedLogSigmoid"], [92, 0, 1, "", "QuantizedLogSoftmax"], [93, 0, 1, "", "QuantizedMSELoss"], [94, 0, 1, "", "QuantizedMarginRankingLoss"], [95, 0, 1, "", "QuantizedMaxPool1d"], [96, 0, 1, "", "QuantizedMaxPool2d"], [97, 0, 1, "", "QuantizedMaxPool3d"], [98, 0, 1, "", "QuantizedMaxUnpool1d"], [99, 0, 1, "", "QuantizedMaxUnpool2d"], [100, 0, 1, "", "QuantizedMaxUnpool3d"], [101, 0, 1, "", "QuantizedMish"], [102, 0, 1, "", "QuantizedMultiLabelMarginLoss"], [103, 0, 1, "", "QuantizedMultiLabelSoftMarginLoss"], [104, 0, 1, "", "QuantizedMultiMarginLoss"], [105, 0, 1, "", "QuantizedNLLLoss"], [106, 0, 1, "", "QuantizedNLLLoss2d"], [107, 0, 1, "", "QuantizedPReLU"], [108, 0, 1, "", "QuantizedPairwiseDistance"], [109, 0, 1, "", "QuantizedPixelShuffle"], [110, 0, 1, "", "QuantizedPixelUnshuffle"], [111, 0, 1, "", "QuantizedPoissonNLLLoss"], [112, 0, 1, "", "QuantizedRNN"], [113, 0, 1, "", "QuantizedRNNCell"], [114, 0, 1, "", "QuantizedRReLU"], [115, 0, 1, "", "QuantizedReLU"], [116, 0, 1, "", "QuantizedReLU6"], [117, 0, 1, "", "QuantizedReflectionPad1d"], [118, 0, 1, "", "QuantizedReflectionPad2d"], [119, 0, 1, "", "QuantizedReflectionPad3d"], [120, 0, 1, "", "QuantizedReplicationPad1d"], [121, 0, 1, "", "QuantizedReplicationPad2d"], [122, 0, 1, "", "QuantizedReplicationPad3d"], [123, 0, 1, "", "QuantizedSELU"], [124, 0, 1, "", "QuantizedSiLU"], [125, 0, 1, "", "QuantizedSigmoid"], [126, 0, 1, "", "QuantizedSmoothL1Loss"], [127, 0, 1, "", "QuantizedSoftMarginLoss"], [128, 0, 1, "", "QuantizedSoftmax"], [129, 0, 1, "", "QuantizedSoftmax2d"], [130, 0, 1, "", "QuantizedSoftmin"], [131, 0, 1, "", "QuantizedSoftplus"], [132, 0, 1, "", "QuantizedSoftshrink"], [133, 0, 1, "", "QuantizedSoftsign"], [134, 0, 1, "", "QuantizedTanh"], [135, 0, 1, "", "QuantizedTanhshrink"], [136, 0, 1, "", "QuantizedThreshold"], [137, 0, 1, "", "QuantizedTripletMarginLoss"], [138, 0, 1, "", "QuantizedTripletMarginWithDistanceLoss"], [139, 0, 1, "", "QuantizedUnflatten"], [140, 0, 1, "", "QuantizedUnfold"], [141, 0, 1, "", "QuantizedUpsample"], [142, 0, 1, "", "QuantizedUpsamplingBilinear2d"], [143, 0, 1, "", "QuantizedUpsamplingNearest2d"], [144, 0, 1, "", "QuantizedZeroPad1d"], [145, 0, 1, "", "QuantizedZeroPad2d"], [146, 0, 1, "", "QuantizedZeroPad3d"]], "aimet_torch.nn.QuantizationMixin": [[19, 1, 1, "", "compute_encodings"], [19, 1, 1, "", "forward"], [19, 1, 1, "", "from_module"], [19, 1, 1, "", "get_default_kernel"], [19, 1, 1, "", "get_kernel"], [19, 1, 1, "", "ignore"], [19, 1, 1, "", "ignore_unknown_modules"], [19, 1, 1, "", "implements"], [19, 4, 1, "", "input_quantizers"], [19, 4, 1, "", "output_quantizers"], [19, 4, 1, "", "param_quantizers"], [19, 1, 1, "", "set_default_kernel"], [19, 1, 1, "", "set_kernel"], [19, 1, 1, "", "wrap"]], "aimet_torch.nn.QuantizedCircularPad1d": [[39, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedCircularPad2d": [[40, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedCircularPad3d": [[41, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad1d": [[42, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad2d": [[43, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedConstantPad3d": [[44, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedHardsigmoid": [[73, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedHardswish": [[74, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedLinear": [[89, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad1d": [[117, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad2d": [[118, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReflectionPad3d": [[119, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad1d": [[120, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad2d": [[121, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedReplicationPad3d": [[122, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedThreshold": [[136, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedTripletMarginWithDistanceLoss": [[138, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedUnflatten": [[139, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad1d": [[144, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad2d": [[145, 1, 1, "", "forward"]], "aimet_torch.nn.QuantizedZeroPad3d": [[146, 1, 1, "", "forward"]], "aimet_torch.onnx": [[168, 2, 1, "", "export"]], "aimet_torch.quant_analyzer": [[206, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.quant_analyzer.QuantAnalyzer": [[206, 1, 1, "", "analyze"], [206, 1, 1, "", "check_model_sensitivity_to_quantization"], [206, 1, 1, "", "export_per_layer_encoding_min_max_range"], [206, 1, 1, "", "export_per_layer_mse_loss"], [206, 1, 1, "", "export_per_layer_stats_histogram"], [206, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [206, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.quantization": [[147, 0, 1, "", "DequantizedTensor"], [148, 0, 1, "", "QuantizedTensor"], [149, 0, 1, "", "QuantizedTensorBase"], [156, 2, 1, "", "get_backend"], [157, 2, 1, "", "set_backend"]], "aimet_torch.quantization.DequantizedTensor": [[147, 1, 1, "", "dequantize"], [147, 1, 1, "", "quantize"], [147, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensor": [[148, 1, 1, "", "dequantize"], [148, 1, 1, "", "quantize"], [148, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.QuantizedTensorBase": [[149, 1, 1, "", "clone"], [149, 1, 1, "", "dequantize"], [149, 1, 1, "", "detach"], [149, 1, 1, "", "new_empty"], [149, 1, 1, "", "quantize"], [149, 1, 1, "", "quantized_repr"]], "aimet_torch.quantization.affine": [[150, 0, 1, "", "Quantize"], [151, 0, 1, "", "QuantizeDequantize"], [152, 2, 1, "", "dequantize"], [153, 2, 1, "", "quantize"], [154, 2, 1, "", "quantize_dequantize"]], "aimet_torch.quantization.affine.Quantize": [[150, 1, 1, "", "forward"]], "aimet_torch.quantization.affine.QuantizeDequantize": [[151, 1, 1, "", "forward"]], "aimet_torch.quantization.float": [[155, 0, 1, "", "FloatQuantizeDequantize"]], "aimet_torch.quantization.float.FloatQuantizeDequantize": [[155, 3, 1, "", "bitwidth"], [155, 1, 1, "", "compute_encodings"], [155, 3, 1, "", "exponent_bits"], [155, 1, 1, "", "forward"], [155, 1, 1, "", "from_encodings"], [155, 1, 1, "", "get_encodings"], [155, 1, 1, "", "get_extra_state"], [155, 1, 1, "", "is_bfloat16"], [155, 1, 1, "", "is_float16"], [155, 1, 1, "", "load_state_dict"], [155, 3, 1, "", "mantissa_bits"], [155, 1, 1, "", "set_extra_state"]], "aimet_torch.quantsim": [[172, 0, 1, "", "QuantizationSimModelOnnxExporter"]], "aimet_torch.quantsim.QuantizationSimModelOnnxExporter": [[172, 1, 1, "", "export"]], "aimet_torch.seq_mse": [[200, 0, 1, "", "SeqMseParams"], [200, 2, 1, "", "apply_seq_mse"]], "aimet_torch.seq_mse.SeqMseParams": [[200, 1, 1, "", "forward_fn"], [200, 1, 1, "", "get_loss_fn"]], "aimet_torch.utils": [[175, 2, 1, "", "remove_activation_quantizers"], [175, 2, 1, "", "remove_all_quantizers"], [175, 2, 1, "", "remove_input_quantizers"], [175, 2, 1, "", "remove_output_quantizers"], [175, 2, 1, "", "remove_param_quantizers"]], "aimet_torch.v1.adaround.adaround_weight": [[176, 0, 1, "", "AdaroundParameters"]], "aimet_torch.v1.adaround.adaround_weight.Adaround": [[176, 2, 1, "", "apply_adaround"]], "aimet_torch.v1.mixed_precision": [[177, 2, 1, "", "choose_mixed_precision"]], "aimet_torch.v1.quant_analyzer": [[179, 0, 1, "", "QuantAnalyzer"]], "aimet_torch.v1.quant_analyzer.QuantAnalyzer": [[179, 1, 1, "", "analyze"], [179, 1, 1, "", "check_model_sensitivity_to_quantization"], [179, 1, 1, "", "export_per_layer_encoding_min_max_range"], [179, 1, 1, "", "export_per_layer_mse_loss"], [179, 1, 1, "", "export_per_layer_stats_histogram"], [179, 1, 1, "", "perform_per_layer_analysis_by_disabling_quant_wrappers"], [179, 1, 1, "", "perform_per_layer_analysis_by_enabling_quant_wrappers"]], "aimet_torch.v1.quantsim": [[180, 0, 1, "", "QuantizationSimModel"], [180, 1, 1, "", "load_checkpoint"], [180, 1, 1, "", "save_checkpoint"]], "aimet_torch.v1.quantsim.QuantizationSimModel": [[180, 1, 1, "", "compute_encodings"], [180, 1, 1, "", "export"]], "aimet_torch.v1.seq_mse": [[181, 0, 1, "", "SeqMseParams"], [181, 2, 1, "", "apply_seq_mse"]], "aimet_torch.v1.seq_mse.SeqMseParams": [[181, 1, 1, "", "forward_fn"], [181, 1, 1, "", "get_loss_fn"]], "aimet_torch.v2.mixed_precision": [[222, 0, 1, "", "MixedPrecisionConfigurator"]], "aimet_torch.v2.mixed_precision.MixedPrecisionConfigurator": [[222, 1, 1, "", "apply"], [222, 1, 1, "", "set_model_input_precision"], [222, 1, 1, "", "set_model_output_precision"], [222, 1, 1, "", "set_precision"]], "aimet_torch.v2.quantsim.config_utils": [[161, 2, 1, "", "set_activation_quantizers_to_float"], [161, 2, 1, "", "set_blockwise_quantization_for_weights"], [161, 2, 1, "", "set_grouped_blockwise_quantization_for_weights"]], "aimet_torch.v2.visualization_tools": [[204, 2, 1, "", "visualize_stats"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "function", "Python function"], "3": ["py", "property", "Python property"], "4": ["py", "attribute", "Python attribute"]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:function", "3": "py:property", "4": "py:attribute"}, "terms": {"": [2, 8, 9, 11, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 155, 161, 163, 165, 166, 170, 173, 177, 178, 179, 180, 181, 183, 186, 188, 189, 190, 191, 194, 196, 200, 201, 202, 203, 206, 207, 208, 209, 210, 211, 215, 216, 217, 218, 219, 221, 222, 223, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 255, 256, 258, 259], "0": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260], "00": [153, 154, 194, 229, 230, 231, 232, 233, 234, 241, 242, 243, 254], "000": 256, "0000": [147, 154, 168, 171], "0000e": [153, 154], "001": [192, 236, 244, 252], "0014807": 187, "00183112e": 193, "00215936e": 193, "0022e": 194, "0030": 171, "00317": 190, "0032": 171, "0034": 171, "0035": 171, "0036": [151, 171], "0037": 171, "0038": 171, "0039": [151, 171], "005": 157, "0059": 171, "0063": 171, "0064": 171, "0068": 171, "0069": 171, "0073": 171, "0074": 171, "0078": 171, "0086": 188, "01": [12, 153, 154, 176, 189, 191, 193, 194, 229, 230, 231, 232, 233, 234, 246, 254], "0115": 151, "0117": 171, "01392324": 188, "0142": 149, "0156": 171, "0158": 151, "01675645": 188, "0176": 151, "0182b7a": 202, "0195": 171, "01e7422": 202, "02": [153, 154, 193, 194, 229, 230, 231, 233, 234, 254], "0234": [171, 188], "0235": 191, "0244e": 194, "0252": 188, "026354755942277083": 187, "02635476": 187, "0271e": 194, "0273": 171, "0278355": 187, "02887694e": 194, "0293162": 187, "0295": 151, "03": [193, 194, 231, 232, 233, 254], "0303": 188, "0312": 171, "03513189": 188, "0352": 171, "03658897": 188, "0375e": 194, "03798249e": 193, "0386": 151, "0391": 171, "04": [185, 186, 187, 188, 230, 231, 234, 254], "04025269e": 193, "0406616e": 194, "0422": 188, "0424": 151, "0428": 192, "0430": 171, "0469": 171, "0471": 151, "05": [153, 154, 193, 194], "0500e": [153, 154], "0508": 171, "05270951": 187, "0541903": 187, "0549": 151, "0564": 151, "0597e": 194, "05f6810": 202, "06": [229, 230, 234, 252, 254], "0600": 188, "0639": 151, "0667": 154, "0680": 151, "0680e": 194, "07": [229, 233, 234, 252, 254], "074e85f": 202, "0784": 151, "07906426": 187, "08": [153, 154, 230, 231, 254], "080545": 187, "0819": 151, "0820258": 187, "0870": 188, "087e9b1": 202, "0882": 149, "0891": 155, "0897725": 188, "08c17b8": 202, "09": [229, 230, 254], "0901e": 194, "09111059e": 193, "0932e": 194, "0947": 155, "0950": 155, "0973e": 194, "0fe6701": 202, "0x7f127685a598": 164, "0x7f9dd9bd90d0": 164, "0x7ff5703eff28": 164, "0x7ff577373598": 164, "1": [0, 2, 10, 12, 13, 14, 18, 19, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 160, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 186, 187, 188, 194, 197, 198, 199, 207, 208, 210, 211, 212, 214, 215, 218, 256, 260], "10": [7, 10, 18, 19, 147, 150, 151, 153, 163, 166, 171, 172, 180, 185, 187, 188, 207, 208, 210, 211, 212, 214, 221, 223, 224, 230, 233, 234, 241, 242, 243, 247, 249, 250, 254, 260], "100": [2, 15, 157, 162, 177, 183, 187, 192, 194, 219, 221, 247, 252], "1000": [9, 149, 170, 179, 206, 221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "10000": [1, 189, 236, 237, 238, 239, 240, 244, 245], "1000e": [153, 154], "102": 171, "1024": [223, 230, 233, 254], "103": [150, 171], "104e7e8": 202, "10541902": 187, "106": 150, "1060": 151, "1068997": 187, "107": 171, "107b339": 202, "108": 171, "1080": 188, "109": 150, "10984787": 188, "10k": [1, 12, 176, 189], "11": [148, 149, 153, 171, 230, 232, 239, 243, 254, 260], "110": [150, 171], "1108": 188, "111": [150, 171], "1128": 151, "11446196": 188, "1176": 151, "118": 171, "11899511": 188, "119": 171, "12": [153, 171, 186, 187, 207, 229, 230, 233, 234, 254, 260], "120": 157, "12039044e": 194, "122": 150, "1228e": 194, "1232": 151, "1234": 252, "127": [148, 149, 171, 172, 175, 223, 224], "1279": 188, "128": [2, 14, 148, 149, 161, 165, 171, 172, 175, 177, 178, 187, 191, 200, 207, 218, 219, 223, 224, 229, 230, 232, 233, 234, 252, 254], "129": 150, "13": [153, 171, 229, 230, 232, 233, 234, 239, 254, 260], "1307": 151, "131": 150, "13137": [190, 196, 202], "1316e": 194, "13177378": 187, "1333": 154, "135a0af": 202, "1377e": 194, "1390b96": 202, "1398": 211, "1399": 155, "14": [153, 171, 229, 230, 232, 233, 239, 254, 260], "140": 157, "1406": 211, "141": 150, "143": 150, "144": 150, "145": 150, "1450607": 188, "146": 150, "1489e": 194, "1493fe1": 202, "14c8e81": 202, "15": [153, 154, 171, 192, 211, 232, 239, 247, 249, 250, 254, 260], "150": 150, "1500": [13, 190], "15000": 189, "1500e": [153, 154], "152": 150, "153": 148, "155": 150, "15717569e": 193, "15812853": 187, "15c8b9b": 202, "15e": [241, 243], "15k": [1, 12, 176, 189], "16": [2, 19, 150, 155, 165, 166, 169, 171, 172, 177, 180, 188, 190, 191, 194, 196, 198, 199, 201, 207, 219, 223, 224, 231, 232, 236, 239, 244, 252, 254, 255, 259, 260], "160": 157, "1609": 188, "162": 150, "16406": [201, 202], "1647": 151, "1676": 188, "16839484e": 194, "16966406e": 194, "17": [150, 171, 194, 229, 231, 232, 233, 234, 254, 260], "1706e": 194, "1709": 151, "172": 150, "1727": 155, "1741": [151, 155], "178": 150, "179": 150, "18": [171, 236, 237, 238, 239, 240, 260], "180": 157, "181": 150, "18136823e": 193, "18448329": 187, "186": 150, "18673885e": 193, "187": 150, "188": 150, "1889": 151, "18dfedc": 202, "19": [150, 168, 171, 232, 254, 260], "192": 150, "1921e": [153, 154], "194": 150, "1943": 211, "1945": 149, "1955": 211, "1977": 151, "19778645e": 193, "19e5a4": 202, "1b": [174, 196, 201], "1b99a39": 202, "1bf8b82": 202, "1e": [192, 193, 194, 224], "1e8eceb": 202, "1k": [189, 191, 192], "1m": [237, 238, 239, 240, 245, 248, 249, 250, 251], "1x1": 202, "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 194, 195, 197, 198, 199, 203, 204, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 218, 220, 226, 228, 231, 232, 233, 234, 253, 256, 257, 258, 260], "20": [11, 12, 149, 157, 173, 176, 181, 189, 192, 198, 199, 200, 229, 230, 231, 232, 233, 234, 247, 249, 250, 252, 254, 260], "200": [162, 246], "2000": [12, 148, 154, 176, 189, 245, 246, 252], "2000e": [153, 154], "2012": [221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "2014": 211, "2016": 211, "2017": 211, "2018e": 194, "2019": [238, 248], "2029": 188, "203": 150, "2048": [196, 201, 229, 232, 254], "205": 148, "2050e": 194, "207": 150, "20722957": 188, "21": [149, 168, 252, 260], "21066449e": 193, "21083805": 187, "2118": 151, "2123188": 187, "21250950e": 193, "2137995": 187, "216": 150, "218": 150, "2196": 151, "21cddb6": 202, "22": [171, 185, 186, 187, 188, 230, 232, 233, 234, 254, 260], "2205": 149, "2212": 151, "224": [172, 188, 189, 191, 192, 193, 194, 200, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "225": [189, 191, 200, 221, 223, 237, 246, 252], "22583652e": 193, "2259": 188, "226": 150, "2260": 149, "2265": 151, "22884297": 188, "229": [189, 191, 200, 221, 223, 237, 246, 252], "2298e": 194, "22b5c94": 202, "22cac5c": 202, "23": [148, 149, 260], "2306": 190, "2308": [190, 196, 202], "2353": 151, "2355": 151, "2363": 151, "237": 150, "23719281": 187, "23799022": 188, "238": 150, "24": [230, 254, 260], "240": 171, "2401543": 187, "2405": [201, 202], "241": 171, "242": 171, "24257803e": 193, "243": 171, "244": 171, "2443e": 194, "245": 171, "2458": 151, "246": 171, "24665177": 188, "247": 171, "248": 171, "249": 171, "2494": 188, "2494d90": 202, "25": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "250": 171, "2500e": [153, 154], "251": 171, "252": 171, "253": 171, "254": 171, "2546": 151, "255": [147, 150, 157, 171, 172, 175, 223, 224], "256": [9, 171, 189, 191, 200, 206, 221, 223, 231, 237, 244, 252, 254], "2568": 151, "26": [10, 148, 171, 232, 233, 254, 260], "26179108e": 193, "26354757": 187, "2650282": 187, "2667": 154, "27": [233, 254, 260], "27045077": 188, "273dd82": 202, "2771": 151, "28": [148, 149, 229, 230, 254, 260], "28065038": 188, "28238320e": 194, "283ecc1": 202, "28990233": 187, "28a7382": 202, "28f89a7": 202, "29": [171, 230, 233, 234, 254, 260], "291383": 187, "2921": 149, "2930528e": 194, "2998e": 194, "2b": 209, "2b7b548": 202, "2bc8c94": 202, "2c64eac": 202, "2c88364": 202, "2c8ae88": 202, "2d": [207, 208, 211, 215], "2d4e0eb": 202, "2ed8305": 202, "2f": 252, "2f05175": 202, "2gb": [168, 202], "2x": 202, "3": [2, 14, 18, 147, 149, 153, 154, 160, 162, 163, 164, 165, 169, 171, 172, 173, 174, 175, 177, 178, 181, 185, 187, 188, 194, 198, 207, 208, 209, 212, 214, 215, 222, 232, 234, 260], "30": [150, 171, 232, 254, 260], "300": 244, "3000": 148, "3038": 151, "31": [9, 12, 170, 172, 176, 179, 180, 189, 206, 223, 224, 229, 254, 260], "31080866e": 193, "312": 148, "3136828": 202, "3137": 151, "31625706": 187, "3178": 188, "31ca7fd": 202, "32": [150, 171, 172, 180, 183, 189, 190, 191, 192, 193, 194, 200, 216, 221, 223, 224, 227, 231, 232, 234, 236, 237, 238, 241, 242, 243, 244, 245, 246, 248, 249, 250, 252, 254, 260], "3209": 188, "32141271e": 193, "3216": 151, "3258": 188, "33": [150, 232, 233, 241, 242, 243, 254, 260], "3333": 154, "339a225": 202, "34": [229, 232, 234, 254, 260], "34215236e": 194, "34261182": 187, "3434e": 194, "3435e": 194, "3451": 151, "3467390e": 194, "34694423e": 194, "347054": 187, "3470540046691895": 187, "3479e": 194, "35": [230, 233, 234, 254, 260], "350m": [198, 199], "35107604": 188, "35139937e": 194, "35536635": 188, "35602ea": 202, "3576329": 188, "3587": 188, "35ad990": 202, "36": [150, 230, 231, 234, 254], "3603": 188, "3657e": 194, "36678016": 188, "3687": 188, "36896658": 187, "37": [149, 150, 171, 231, 232, 233, 254], "3706": 188, "3724": 188, "3734087606114667": 187, "374e8db": 202, "3750526": 202, "37757687e": 194, "3792e": 194, "38": [148, 149, 211, 229, 254], "3851556": 188, "39": [147, 232, 234, 254], "3938e": 194, "3992": 151, "3a8659b": 202, "3adcbe": 202, "3b8e0f0": 202, "3c92bb7": 202, "3d": 207, "3d4725f": 202, "3d5e0dd": 202, "3rd": 10, "4": [2, 8, 9, 12, 14, 148, 149, 153, 154, 161, 162, 163, 169, 170, 171, 172, 176, 178, 179, 180, 188, 193, 194, 198, 199, 207, 210, 215, 216, 218, 219, 224, 240, 241, 242, 243, 246, 251, 260], "40": [148, 149, 157, 196, 201], "4000": [147, 154], "406": [189, 191, 200, 221, 223, 237, 246, 252], "4082": 188, "4094e": 194, "4096": 254, "40gb": 254, "41": [147, 229, 230, 231, 233, 234, 254], "41059163e": 194, "4130": 188, "4132449": 188, "414cdde": 202, "4157": 151, "4186": 188, "42": [171, 229, 232, 233, 234, 254], "42083430e": 193, "4216761": 187, "4231569": 187, "4236": 188, "4246376": 187, "42477691e": 193, "43": [171, 229, 231, 232, 233, 254], "43178225": 188, "43477735": 188, "434ac6b": 202, "4392": 151, "44": [229, 230, 232, 254], "4404": 188, "44632760e": 194, "4475": 151, "44803086": 187, "4495116": 187, "44993666e": 194, "45": [231, 254], "4503196": 188, "45040053": 188, "4549": 151, "455": [185, 187, 188], "456": [189, 191, 200, 221, 223, 237, 246, 252], "4578e": 194, "4585028e": 194, "4599525": 188, "45c2a65": 202, "46": [229, 230, 232, 254], "46642041e": 194, "4667": 154, "46723792": 188, "4677236": 188, "4686e": 194, "4694": 151, "47": [234, 254], "4706": 151, "47438562": 187, "4758663": 187, "4784": 151, "47f574d": 202, "47fae94": 202, "48": [171, 229, 231, 234, 254], "485": [189, 191, 200, 221, 223, 237, 246, 252], "4863": 151, "4881": 188, "489f7df": 202, "49": [150, 171, 230, 233, 254], "4933": 188, "4943e": 194, "495567f": 202, "499df9f": 202, "4ad0703": 202, "4b94ca9": 202, "4d": 207, "4ddbd66": 202, "4f": [191, 221, 223], "4febdd4": 202, "5": [149, 150, 151, 153, 154, 155, 162, 166, 169, 171, 174, 185, 187, 188, 192, 193, 194, 207, 209, 224, 238, 240, 241, 242, 243, 245, 248, 249, 250, 252, 260], "50": [209, 231, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254], "500": [206, 223, 237, 238, 239, 240, 245, 246, 248, 249, 250, 251, 256], "5000": 246, "50000": [189, 237], "5000e": [153, 154], "5006": 213, "50074035": 187, "501eebd": 202, "5022211": 187, "50366503": 188, "5091695": 188, "50f35dd": 202, "51": [147, 230, 231, 254], "512": [198, 199, 234, 254], "5176e": 194, "5181860": 202, "51876003e": 193, "51f8990": 202, "52": [147, 229, 233, 254], "5203": 188, "521": 147, "5220": 151, "5255": 151, "52709514": 187, "52974629e": 193, "53": [229, 231, 234, 254], "5315115": 188, "5317543": 188, "5333": [151, 154], "536": 254, "54": [171, 232, 233, 252, 254], "5430": 188, "55": [150, 171, 230, 254], "550c029": 202, "552ad83": 202, "55344987": 187, "5540": 151, "5549306": 187, "56632766e": 193, "56810045": 188, "5695": 151, "57": [148, 149, 171, 230, 233, 254], "57021021e": 194, "57980466": 187, "58": [230, 231, 254], "5803": 188, "5825": 188, "5856506e": 194, "5876": 192, "5897": 151, "58ce71d": 202, "59": [150, 171, 230, 254], "59350af": 202, "59643137e": 193, "5985": 188, "5996e": 194, "59aac3": 202, "59e0125": 202, "5a419f3": 202, "5aac9c5": 202, "5ad7ea6": 202, "5e": [242, 243, 247, 249, 250], "5e23ceb": 202, "6": [10, 149, 153, 154, 171, 172, 193, 194, 207, 219, 223, 229, 231, 243, 245, 248, 249, 250, 260], "60": [157, 171, 230, 254], "6000": [147, 148, 149, 154], "6014": 188, "6039": 151, "6054": 151, "6061594": 187, "6079": 151, "60808927": 188, "61": [171, 231, 254], "61087702e": 193, "6169e": 194, "6177": 151, "6196": 151, "62": [171, 231, 233, 254], "6208e": 194, "6213797e": 194, "6216ca0": 202, "62274104": 188, "6247": 151, "624ba30": 202, "62f5879": 202, "63": [10, 150, 231, 254], "63172388e": 193, "6325141": 187, "64": [8, 9, 150, 161, 171, 172, 185, 187, 188, 190, 191, 206, 207, 218, 223, 224, 231, 254], "6431": 151, "6463e": 194, "65": [209, 230, 232, 233, 234, 254], "6529": 168, "654f4b1": 202, "6561f0e": 202, "6588689": 187, "658ec3c": 202, "65c4b3b": 202, "66": [150, 209, 234, 241, 242, 243, 254], "6603496": 187, "6618e": 194, "6636515": 202, "6667": 154, "6676a6c": 202, "6695": 151, "66ccb45": 202, "67": [229, 254], "6761a19": 202, "68": [147, 229, 231, 232, 254], "68016": 224, "6836": 221, "68522364": 187, "6865": 221, "6867044": 187, "6885": 221, "69": [230, 254], "6998855": 188, "69f96ff": 202, "6a37239": 202, "6c92a97": 202, "6c9f584": 202, "6ca06d6": 202, "6d1664c": 202, "6d3aa97": 202, "6f670a4": 202, "6fe56b0": 202, "7": [149, 153, 154, 171, 172, 193, 194, 215, 223, 224, 233, 242, 243, 245, 247, 248, 249, 250, 254, 260], "70": [171, 234, 254], "70029c5": 202, "70130579e": 194, "70838": 224, "71": [149, 234, 254], "7115784": 187, "7119e": 194, "7137849": 202, "7159e": 194, "7164": 191, "7173": 223, "72246db": 202, "72468403e": 194, "73242160e": 193, "7333": 154, "7336ead": 202, "7364b37": 202, "73793316": 187, "74": [150, 230, 254], "74478185e": 193, "75": 209, "754d030": 202, "755c54a": 202, "76": 171, "76428795": 187, "77": [171, 231, 234, 254], "77213307e": 193, "7741": 151, "7765": 151, "78": [233, 254], "7894": 151, "79": [234, 252, 254], "7932": 151, "7978e": 194, "7b3cc4c": 202, "7d4659d": 202, "7d63e66": 202, "7e5342b": 202, "8": [1, 2, 9, 12, 14, 19, 147, 148, 149, 150, 151, 153, 154, 161, 162, 163, 165, 166, 170, 171, 172, 176, 177, 178, 179, 183, 187, 188, 189, 191, 193, 194, 200, 206, 212, 214, 215, 218, 219, 222, 223, 224, 236, 237, 238, 239, 240, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 260], "80": [150, 157], "800": [167, 196], "8000": [151, 154], "80053532e": 193, "8009871": 202, "8078": 151, "80cd141": 202, "80fb4fe": 202, "81": [233, 252, 254], "81699747": 187, "81760912e": 193, "81884710e": 193, "82": [229, 232, 254], "8229": 151, "83": [150, 171], "832ea91": 202, "8347e": 194, "83640555e": 194, "8365e": 194, "836ab1": 202, "84": [171, 229, 254], "8433522": 187, "84edcf5": 202, "85": [231, 234, 254], "8560e13": 202, "86": [150, 233, 254], "8667": 154, "86945379e": 193, "8706": 151, "8711877": 187, "87656835e": 193, "8789e": 194, "8796": 151, "88": [230, 254], "8836": 151, "88706ef": 202, "8874173": 202, "89": [150, 229, 231, 232, 254], "89074164e": 194, "89348674e": 193, "8945e": 194, "8998": 155, "8a999a1": 202, "8bit": 202, "9": [147, 153, 154, 163, 171, 192, 193, 194, 223, 239, 241, 242, 243, 244, 245, 249, 250, 251, 260], "90": [230, 231, 254], "9086e": 194, "91": 171, "91109af": 202, "911af75": 202, "9157": 151, "9176": 151, "92": [171, 232, 254], "92f63f5": 202, "93": 171, "9333": 154, "93787616e": 193, "9395e32": 202, "94": [150, 171, 252], "9487": 151, "94877124": 187, "9490": 151, "95": [171, 252], "95260113e": 193, "9570": 171, "9585e": 194, "96": [171, 190], "9609": 171, "96155685e": 193, "9648": 171, "9688": 171, "97": [147, 233, 254], "9700": 151, "9727": 171, "9766": 171, "9805": 171, "9844": 171, "9883": 171, "99": [230, 254], "99160d2": 202, "9922": 171, "9961": [151, 171], "9972c1b": 202, "9999": 168, "9a2a407": 202, "9b8c655": 202, "A": [2, 3, 9, 10, 12, 14, 16, 17, 18, 161, 165, 169, 170, 172, 174, 176, 177, 178, 179, 180, 182, 183, 189, 191, 193, 194, 198, 200, 201, 206, 207, 208, 210, 211, 212, 214, 216, 218, 219, 222, 223, 224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 258], "And": [236, 244, 258], "As": [19, 191, 201, 206, 207, 209, 210, 226, 236, 238, 241, 242, 243, 244, 247, 248, 249, 250], "At": [7, 201, 209, 213, 221, 255], "But": [163, 211, 236, 239, 244, 247, 251], "By": [10, 18, 166, 169, 172, 189, 208, 211, 212, 214, 223, 224, 244, 245, 247, 248, 249, 250, 252, 256], "For": [9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 162, 163, 164, 166, 168, 169, 170, 171, 172, 179, 184, 185, 187, 188, 191, 194, 201, 202, 206, 207, 208, 209, 210, 211, 216, 219, 222, 223, 224, 225, 226, 235, 236, 239, 244, 247, 251, 252, 254, 255, 256, 258, 259], "If": [1, 2, 8, 9, 10, 12, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 152, 153, 154, 155, 162, 163, 164, 165, 166, 168, 170, 171, 172, 174, 176, 177, 178, 179, 180, 189, 190, 191, 192, 196, 198, 200, 201, 202, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 259], "In": [19, 149, 162, 163, 164, 166, 172, 177, 180, 188, 191, 192, 193, 198, 199, 201, 207, 209, 210, 211, 213, 215, 218, 219, 220, 221, 222, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 258], "It": [1, 2, 6, 9, 10, 162, 163, 172, 180, 189, 190, 191, 196, 200, 202, 203, 205, 206, 208, 210, 215, 216, 219, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251], "Its": 215, "NOT": [14, 178, 191], "No": [164, 193, 254, 258], "Not": [9, 170, 179, 190, 192, 196, 206, 207, 210, 222, 224, 235, 236, 239, 247, 251], "ON": 186, "OR": 162, "Of": [241, 242, 243, 249, 250], "On": [186, 229, 230, 231, 232, 233, 234, 236, 244, 254], "One": [206, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 256], "Or": [163, 180, 211], "Such": 163, "That": [215, 237, 238, 240, 245, 248, 249, 250], "The": [1, 2, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 153, 154, 155, 159, 160, 162, 163, 164, 165, 166, 168, 169, 171, 172, 173, 174, 176, 177, 178, 180, 181, 183, 184, 187, 189, 190, 191, 192, 194, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259], "Then": [9, 170, 179, 194, 206, 211, 249, 250, 255], "There": [160, 162, 164, 180, 191, 205, 206, 224, 236, 237, 239, 244, 245, 247, 248, 249, 250, 251, 258, 259], "These": [14, 162, 164, 166, 178, 180, 189, 191, 196, 198, 199, 206, 208, 209, 225, 227, 236, 238, 244, 246, 248, 255, 256], "To": [2, 19, 158, 162, 165, 166, 169, 177, 184, 187, 188, 189, 190, 192, 193, 194, 196, 200, 201, 206, 207, 210, 213, 218, 219, 222, 223, 225, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 258, 259], "With": [147, 185, 187, 224, 229, 230, 231, 232, 233, 234, 239, 251, 252, 254], "_": [9, 10, 19, 150, 151, 154, 155, 159, 166, 170, 171, 172, 179, 184, 187, 189, 190, 193, 194, 198, 200, 202, 204, 205, 206, 207, 216, 218, 219, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259], "__getitem__": 246, "__init__": [10, 14, 163, 166, 178, 191, 196, 201, 202, 226, 246], "__iter__": [191, 196, 201], "__len__": [9, 196, 201, 206, 246], "__name__": 156, "__next__": [191, 201], "__quant_init__": [19, 166], "__torch_function__": 163, "__version__": 188, "_batch_index": 191, "_contextmanag": 157, "_create_sampled_data_load": 246, "_data": 191, "_dataset": 246, "_default_forward_fn": [14, 178, 191], "_default_r1_fusion_func": 201, "_default_rmsnorm_linear_pairs_func": 201, "_encodingmismatchinfo": 223, "_from_qnn_encoding_dict": 202, "_infer_activation_dtyp": 202, "_int": 202, "_is_encoding_frozen": 162, "_max": 155, "_module_to_wrap": 162, "_not_specifi": [172, 223, 224], "_q": 202, "_quantizationbackendprotocol": 156, "_quantizationsimmodelinterfac": [170, 177, 179, 180, 206], "_quantschemepair": [14, 178, 191], "_remove_input_quant": 162, "_remove_output_quant": 162, "_remove_param_quant": 162, "_step": [153, 154], "_tie_quant": 202, "a1c197d": 202, "a29f44f": 202, "a2adae2": 202, "a8ac6f4": 202, "a8f32fc": 202, "a967b8f": 202, "a97354f": 202, "ab": [190, 196], "ab63866": 202, "abe8782": 202, "abil": [202, 259], "abl": [9, 14, 147, 148, 149, 163, 164, 178, 191, 206, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 255], "about": [147, 182, 184, 194, 216, 218, 237, 238, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 259], "abov": [163, 165, 207, 211, 213, 215, 222, 236, 239, 241, 242, 243, 244, 251, 255, 256], "absolute_path_to_workspac": 235, "absorpt": [238, 248], "abstract": [19, 149, 166], "ac05d10": 202, "acc_top1": 191, "acceler": [172, 183, 187, 188, 202, 211, 221, 223, 240, 249, 250], "accept": [173, 200, 202, 210, 219, 223, 227, 254, 255, 258, 259], "access": [162, 201, 202, 244, 245, 248, 249, 250, 252], "accord": [10, 160, 172, 205, 223, 224], "accordingli": 202, "account": [13, 190, 202, 210, 227], "accumul": [209, 211, 212, 214, 217], "accur": [193, 252, 254], "accuraci": [2, 11, 12, 14, 18, 169, 176, 177, 178, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 217, 219, 220, 221, 223, 224, 227, 228, 239, 246, 247, 251, 254, 256], "achiev": [171, 183, 185, 209, 219, 220, 222, 237, 245, 252, 255, 258, 259], "acronym": 184, "across": [163, 183, 187, 194, 195, 202, 206, 254, 255, 256], "act": [2, 9, 165, 170, 177, 179, 206, 219, 236], "action": 215, "activ": [2, 9, 10, 13, 159, 161, 163, 165, 166, 168, 169, 170, 172, 173, 177, 179, 181, 183, 185, 186, 189, 190, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 207, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 252, 254, 255, 256, 258, 259], "activation_bitwidth": 216, "activation_encod": 216, "activation_quant": [2, 219], "activation_typ": [10, 188, 189, 200, 221, 223, 236, 237, 238, 240], "activations_pdf": [206, 239, 251], "actual": [203, 236, 244, 247], "ad": [164, 169, 197, 201, 202, 216, 225, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250], "ada": 237, "ada_model": 245, "adamw": [198, 199, 252], "adapt": [12, 15, 169, 173, 176, 181, 183, 191, 192, 196, 197, 198, 199, 200, 202, 206, 235, 247, 254, 255], "adapter1": 169, "adaptiveavgpool1d": 20, "adaptiveavgpool2d": 21, "adaptiveavgpool3d": 22, "adaptivemaxpool1d": 23, "adaptivemaxpool2d": 24, "adaptivemaxpool3d": 25, "adaptiveround": 202, "adaptor": [167, 196], "adaround": [0, 1, 14, 158, 162, 178, 183, 189, 191, 202, 206, 227, 235, 246, 255], "adaround_data_load": [191, 246], "adaround_dataset_s": [191, 246], "adaround_param": [14, 178, 191, 246], "adaround_weight": [12, 162, 176, 189, 191, 245, 246], "adarounded_model": 189, "adaroundparamet": [12, 14, 176, 178, 189, 191, 245, 246], "adascal": [0, 158, 202, 229, 230, 231, 232, 233, 234, 254], "adascale_model_config_dict": 190, "adascale_optim": 190, "adascalemodelconfig": 190, "add": [163, 164, 166, 169, 180, 202, 215, 225, 226, 239, 240, 241, 242, 243, 249, 250, 256, 258, 259], "add_adapt": 169, "add_check": 164, "add_lora_to_r": 199, "addit": [19, 171, 172, 180, 192, 202, 216, 218, 223, 224, 225, 236, 239, 244, 247, 251, 253, 254, 255], "addition": 254, "address": [194, 202, 213, 227, 254], "adequ": 258, "adher": 259, "adjac": [183, 193, 194, 195, 201, 225, 236, 237, 238, 240, 244, 245, 248, 249, 250], "adjust": [159, 195, 198, 199, 202, 204, 208, 209, 218, 224, 227, 255], "admin": 235, "advanc": [171, 183, 185, 202, 228], "advantag": [224, 255], "ae02aa8": 202, "ae1abd1": 202, "ae7d5ef": 202, "ae981f7": 202, "af5a82d": 202, "affect": [12, 176, 183, 189, 218, 225, 237, 259], "affin": [8, 19, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 161, 166, 193, 194, 198, 207, 218, 252], "affine_q": 162, "affine_qdq": 162, "affine_quant": 162, "affinequant": 162, "affinequantizerbas": 252, "afford": 254, "after": [1, 9, 155, 163, 164, 166, 170, 171, 179, 180, 183, 189, 191, 192, 193, 194, 201, 202, 206, 209, 211, 213, 221, 223, 224, 227, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 255], "afterward": [13, 190], "again": [238, 241, 242, 243, 248, 259], "against": [9, 170, 179, 202, 206, 208, 247, 249, 250, 254], "aggress": 183, "agre": 259, "ahead": 228, "ai": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 254, 255, 256, 257, 258, 259, 260], "aim": [219, 241, 242, 243, 247, 255, 259], "aimet": [5, 10, 18, 19, 158, 162, 163, 164, 166, 169, 171, 183, 187, 189, 190, 191, 192, 193, 194, 195, 196, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 214, 216, 217, 218, 219, 220, 221, 222, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 259], "aimet_common": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260], "aimet_exported_model": 253, "aimet_exported_model_path": 253, "aimet_omniquant_artifact": [167, 196], "aimet_onnx": [0, 187, 188, 189, 193, 194, 200, 202, 205, 206, 216, 218, 219, 221, 223, 236, 237, 238, 239, 240], "aimet_repo_path": 239, "aimet_tf": 187, "aimet_torch": [0, 10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 212, 214, 216, 218, 219, 222, 223, 224, 236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "algo": 177, "algorithm": [2, 18, 177, 208, 209, 210, 211, 212, 214, 215, 216, 227, 255], "alia": [10, 172, 180, 202, 223, 224], "aliasbackward0": [147, 148, 149, 150, 151, 168, 171, 188], "align": [202, 223, 225], "all": [0, 1, 2, 3, 9, 10, 16, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 158, 159, 161, 163, 164, 165, 166, 170, 171, 172, 173, 174, 175, 177, 179, 180, 181, 186, 187, 188, 189, 190, 193, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 210, 211, 216, 218, 219, 221, 222, 223, 224, 225, 227, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 252, 254, 255, 256, 258], "all_q_modul": 162, "all_quant_wrapp": 162, "allow": [2, 14, 18, 147, 148, 149, 161, 163, 177, 178, 191, 202, 203, 207, 208, 211, 212, 213, 214, 216, 218, 219, 226, 227, 236, 244, 246, 254, 255], "allow_custom_downsample_op": [18, 208, 241, 243], "allow_overwrit": [162, 189, 199, 223], "allowed_accuracy_drop": [2, 14, 177, 178, 191, 219, 236, 244, 246], "alon": [169, 256], "along": [148, 169, 171, 172, 219, 254, 255, 256], "alpha": [194, 219], "alphadropout": 26, "alreadi": [17, 19, 166, 171, 172, 180, 186, 194, 198, 201, 202, 219, 221, 223, 224, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255], "also": [12, 149, 163, 171, 172, 176, 177, 178, 179, 180, 181, 186, 187, 189, 190, 193, 196, 202, 203, 206, 207, 208, 211, 215, 216, 219, 221, 222, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 253, 254, 255, 258], "alter": [189, 245], "altern": [9, 170, 179, 206, 207, 218, 236, 241, 242, 243, 244], "alwai": [198, 199, 202, 210, 219], "am": [9, 170, 179, 206], "among": [12, 14, 176, 178, 189, 191, 202, 258], "amount": [200, 239, 251], "amp": [0, 2, 14, 158, 165, 177, 178, 191, 202, 219, 220, 235], "amp_search_algo": [2, 177, 219, 236, 244], "ampsearchalgo": [2, 177, 219, 236, 244], "an": [8, 9, 10, 14, 18, 19, 147, 148, 149, 155, 159, 161, 162, 163, 164, 166, 171, 172, 178, 180, 183, 186, 189, 191, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 256, 257, 258, 259], "analys": [206, 239, 251], "analysi": [2, 9, 18, 170, 177, 179, 202, 208, 211, 212, 214, 236, 241, 242, 243, 244], "analyt": 213, "analyz": [9, 18, 150, 151, 155, 166, 170, 179, 187, 191, 195, 202, 208, 212, 213, 214, 222, 235, 246], "analyze_per_layer_sensit": [7, 202, 221], "anchor": [137, 138], "andrea": 211, "andrei": 211, "andrew": 211, "ani": [2, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 155, 163, 164, 165, 170, 172, 176, 177, 178, 179, 180, 187, 189, 190, 191, 192, 198, 202, 206, 207, 208, 212, 214, 218, 219, 222, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 258], "anneal": [12, 176, 189], "anoth": [169, 172, 180, 183, 214, 215, 216, 219, 223, 224, 225], "anyth": [2, 177, 219, 236, 239, 244, 251], "api": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 159, 160, 161, 162, 165, 167, 168, 170, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 198, 199, 202, 203, 213, 216, 226, 235, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 253, 254], "app": 182, "appear": [18, 163, 164, 208, 212, 214], "appli": [1, 2, 8, 9, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 152, 153, 154, 161, 164, 165, 166, 173, 174, 178, 181, 183, 184, 187, 189, 190, 191, 193, 194, 195, 196, 198, 200, 201, 202, 206, 207, 211, 213, 217, 219, 224, 225, 227, 235, 236, 241, 242, 243, 244, 246, 247, 252, 254, 255, 256, 258, 259], "applic": [2, 177, 219, 236, 237, 244, 245, 248, 252, 253, 259], "apply_adaround": [0, 5, 12, 176, 189, 202, 237, 245], "apply_adascal": [13, 190], "apply_omniqu": [167, 196], "apply_seq_ms": [0, 5, 162, 173, 181, 200, 202], "apply_spinqu": [174, 201], "approach": [165, 198, 200, 202, 222, 224, 255, 256], "appropri": [12, 166, 172, 176, 177, 180, 187, 189, 198, 199, 210, 223, 224, 227, 236, 239, 244, 247, 251, 255, 256], "approx": 256, "approxim": [9, 200, 206, 209, 256, 259], "ar": [2, 7, 9, 10, 12, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 155, 160, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 185, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258, 259], "arang": [153, 154, 157, 163, 171], "arbitrari": 207, "architectur": [174, 183, 201, 202, 254], "area": [206, 217], "aren": 186, "arg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 161, 165, 166, 168, 172, 173, 180, 186, 200, 202, 207, 218, 219, 222, 223, 224], "argmax": [189, 191, 200, 221, 223, 224, 237], "argument": [2, 10, 12, 13, 14, 19, 155, 161, 163, 164, 165, 167, 168, 170, 172, 173, 176, 177, 178, 179, 180, 181, 186, 189, 190, 191, 196, 200, 202, 206, 207, 216, 218, 219, 223, 224, 239, 251, 253], "arm64": 202, "around": [185, 202], "arrai": [161, 188, 207, 216, 218], "arrang": 213, "art": [182, 185, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251], "artifact": [10, 165, 169, 202, 220, 222, 223, 253, 254, 258], "arxiv": [190, 196, 201, 202], "asic": 183, "ask": [19, 236, 239, 244, 247, 251], "assert": [162, 171, 172, 223, 244], "assess": 210, "assign": [19, 150, 151, 155, 166, 202, 218, 220, 255], "associ": [2, 14, 19, 149, 164, 165, 166, 171, 172, 177, 178, 191, 219, 239, 251, 253], "assum": [14, 161, 178, 191, 207, 210, 218], "astral": 186, "astyp": 239, "asym": [173, 181, 200], "asymmetr": [150, 151, 171, 225, 256], "attent": [174, 201], "attention_mask": [196, 198, 199, 201], "attribut": [19, 162, 163, 165, 166, 171, 177, 201, 202, 206, 219], "attributeerror": 202, "augment": 213, "auto": [18, 208, 212, 214, 216, 236, 241, 242, 243, 244], "auto_param": [241, 242, 243], "auto_qu": [191, 246], "autoconfig": [196, 198, 199, 201], "autograd": [147, 148, 149], "autom": [163, 183, 226, 244, 245, 249, 250, 251], "automat": [14, 18, 165, 178, 183, 202, 206, 207, 208, 209, 211, 212, 214, 217, 218, 221, 235, 246], "automodelforcausallm": [174, 198, 199, 201], "automodeparam": [18, 208, 212, 214, 241, 242, 243], "autoqu": [0, 14, 158, 178, 183, 191, 202, 235], "autoquantwithautomixedprecis": [14, 178, 191], "autotoken": [196, 198, 199, 201], "avail": [163, 173, 176, 177, 178, 179, 180, 181, 186, 187, 200, 202, 206, 221, 225, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257], "avgpool1d": 27, "avgpool2d": [28, 222], "avgpool3d": 29, "avoid": [9, 170, 179, 180, 183, 202, 206, 209, 226, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 254], "awai": 224, "awar": [19, 183, 185, 192, 195, 211, 227, 235, 254, 256, 259], "axi": [171, 189, 191, 200, 202, 206, 216, 221, 223, 237, 239, 251, 256], "b": [19, 150, 151, 152, 153, 154, 171, 207], "b1415bd": 202, "b1dafe6": 202, "b2350b2": 202, "b47a97": 202, "b5521f3": 202, "b55b058": 202, "b58b00b": 202, "b73bb71": 202, "b79611c": 202, "b924107": 202, "b_": [150, 151, 152, 153, 154], "b_0": [150, 151, 152, 153, 154], "b_1": [150, 151, 152, 153, 154, 207], "b_2": 207, "b_d": [150, 151, 152, 153, 154], "b_n": 207, "ba10947": 202, "back": [2, 147, 148, 165, 170, 177, 179, 202, 206, 219, 225, 256], "backend": [10, 156, 157, 172, 180, 202, 223, 224, 253, 258], "backpropag": [147, 148, 149], "backslash": 202, "backward": [0, 158, 192, 193, 198, 199, 202, 223, 224, 252], "balanc": [183, 194, 211, 217, 221, 255, 259], "bandwidth": 209, "bar": 202, "base": [2, 7, 9, 10, 19, 149, 150, 151, 153, 154, 155, 162, 166, 169, 170, 171, 174, 177, 179, 187, 190, 196, 197, 199, 200, 201, 202, 206, 207, 208, 209, 210, 213, 219, 220, 223, 236, 244, 245, 246, 247, 252, 253, 254, 255, 258], "baselin": [2, 177, 210, 211, 219, 224, 246, 252, 254], "basi": 202, "basic": [202, 224, 236, 244, 247, 253, 255], "batch": [2, 9, 12, 14, 15, 155, 165, 173, 176, 177, 178, 181, 183, 189, 190, 191, 194, 196, 198, 199, 200, 201, 202, 206, 219, 223, 224, 239, 241, 242, 243, 247, 252, 254], "batch_cntr": [191, 236, 238, 239, 240, 245, 247, 248, 249, 250, 251], "batch_id": [196, 198, 199], "batch_idx": 200, "batch_norm": [3, 16, 193], "batch_norm_fold": [0, 5, 158, 192, 193, 224, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250], "batch_siz": [12, 176, 189, 190, 191, 192, 193, 196, 198, 199, 200, 201, 221, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "batchnorm": [14, 15, 178, 191, 192, 193, 194, 202, 215, 235, 236, 237, 238, 240, 244, 245, 248, 249, 250], "batchnorm1d": [16, 32, 193], "batchnorm2d": [16, 33, 193, 194], "batchnorm3d": 34, "batchnrom": 247, "bb93c76": 202, "bc": [238, 248], "bceloss": 30, "bcewithlogitsloss": 31, "becaus": [163, 207, 219, 247, 254, 258], "becom": [202, 220], "been": [0, 19, 147, 158, 159, 164, 183, 188, 198, 202, 204, 215, 216, 246, 252, 253, 255, 256], "befor": [1, 2, 9, 10, 13, 19, 159, 162, 166, 171, 189, 190, 191, 192, 193, 194, 197, 198, 199, 201, 202, 204, 206, 211, 219, 221, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 254, 256, 258], "begin": [150, 151, 152, 153, 154, 163, 164, 184], "behav": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 166, 202, 227], "behavior": [162, 163, 166, 193, 202, 225, 236, 244, 245, 248, 249, 250, 256, 258], "behind": 258, "being": [18, 162, 163, 164, 170, 179, 192, 202, 206, 207, 208, 212, 214, 216, 226], "below": [19, 150, 151, 153, 154, 162, 165, 166, 169, 171, 186, 187, 188, 198, 199, 201, 205, 207, 208, 210, 211, 213, 216, 221, 222, 223, 225, 226, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256], "benefici": [236, 239, 244, 251], "benefit": [200, 216, 218, 224, 255, 256], "bert": 227, "bespok": 198, "best": [14, 165, 178, 185, 187, 191, 195, 209, 211, 217, 222, 228, 246, 256], "beta": [12, 176, 189, 190, 202], "better": [191, 211, 219, 223, 224, 237, 241, 242, 243, 245, 247, 248, 249, 250, 254, 258], "between": [2, 9, 12, 18, 160, 162, 165, 166, 170, 176, 177, 179, 183, 189, 194, 195, 200, 203, 205, 206, 207, 208, 212, 214, 219, 221, 223, 225, 236, 244, 254, 255, 256, 259], "bfba557": 202, "bfloat16": [155, 202], "bia": [10, 19, 155, 162, 163, 166, 168, 169, 172, 175, 193, 194, 201, 202, 208, 223, 224, 225, 238, 248], "bias": [193, 236, 238, 244, 248], "bilinear": 35, "billion": [258, 259], "bin": [186, 253], "binari": [2, 177, 219, 236, 244, 253], "binary_fil": 253, "binary_file_nam": 253, "bit": [1, 2, 9, 12, 155, 161, 170, 171, 176, 177, 179, 183, 185, 187, 188, 189, 198, 202, 206, 207, 216, 217, 218, 219, 220, 224, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 256, 258, 259], "bitop": [2, 177, 219], "bitwidth": [1, 2, 8, 9, 12, 14, 147, 148, 149, 150, 151, 153, 154, 155, 161, 162, 165, 166, 170, 171, 172, 176, 177, 178, 179, 180, 183, 187, 189, 191, 194, 198, 202, 206, 207, 216, 218, 219, 223, 224, 236, 244, 245, 256, 258], "biwidth": 245, "bkd": [13, 190, 195, 196, 254], "blankevoort": 211, "block": [8, 14, 150, 151, 152, 153, 154, 155, 161, 162, 167, 178, 190, 191, 196, 198, 199, 202, 207, 216, 217, 218, 254, 256], "block_group": [161, 218], "block_siz": [8, 150, 151, 152, 153, 154, 155, 161, 171, 202, 207, 216, 218, 254], "blockwis": [8, 161, 171, 190, 195, 196, 202, 216, 254, 256], "bn": [3, 15, 16, 183, 192, 193, 195, 202, 206, 236, 237, 238, 240, 244, 245, 248, 249, 250], "bn_conv": 202, "bn_reestim": [0, 158, 192, 247], "bnf": [192, 193, 194], "bokeh": 219, "bool": [2, 8, 10, 14, 18, 150, 151, 153, 154, 155, 161, 163, 164, 165, 168, 172, 177, 178, 180, 191, 207, 208, 212, 214, 216, 218, 219, 222, 223, 224, 239, 241, 242, 243, 245, 247, 248, 249, 250, 251], "boolean": [19, 165, 201, 202, 222], "both": [153, 154, 162, 163, 166, 170, 179, 185, 187, 188, 193, 197, 206, 211, 212, 213, 215, 217, 219, 223, 224, 225, 227, 229, 230, 231, 232, 233, 234, 236, 243, 244, 249, 250, 252, 253, 254, 255, 256, 259], "bottleneck": 227, "box": [202, 228], "bq": [161, 202, 207, 218], "branch": [163, 225, 235], "break": [191, 196, 198, 199, 200, 202, 223, 236, 238, 239, 240, 245, 247, 248, 249, 250, 251, 252], "bridg": 220, "brief": 217, "british": 211, "broad": 255, "broken": [197, 216], "browser": 235, "bruteforc": [236, 244], "buffer": 155, "bug": [168, 202, 216], "bugfix": 202, "build": [10, 162, 202, 223], "build_sess": 202, "buildx": 186, "built": [2, 19, 165, 166, 177, 186, 187, 202, 219], "bw": [2, 14, 161, 165, 172, 177, 178, 180, 191, 202, 216, 218, 219, 223, 224, 226], "bw_output": 226, "c": [171, 186, 209], "c014961": 202, "c0bdb46": 202, "c18fd05": 202, "c96894f": 202, "c_": 171, "cach": [2, 14, 167, 177, 178, 191, 196, 202, 219, 229, 230, 231, 234, 236, 244, 254], "cache_id": [14, 178, 191], "calcul": [2, 14, 166, 177, 178, 180, 191, 197, 198, 202, 206, 210, 219, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 256], "calculate_and_fuse_encodings_into_weight": 198, "calibr": [9, 10, 150, 151, 155, 159, 166, 169, 170, 172, 179, 180, 183, 188, 198, 202, 203, 204, 217, 218, 220, 221, 224, 227, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 254, 256, 258], "calibration_callback": [198, 199], "calibration_data": 188, "calibration_data_load": [166, 221, 223], "calibration_dataset": [221, 223], "calibration_dataset_s": [191, 246], "calibration_wrapp": [196, 201], "call": [2, 9, 13, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 149, 155, 163, 165, 166, 170, 171, 172, 177, 178, 179, 190, 191, 202, 206, 207, 212, 214, 218, 219, 221, 222, 223, 237, 238, 239, 240, 241, 242, 243, 245, 248, 249, 250, 251, 253, 254, 258, 259], "call_funct": 163, "callabl": [2, 9, 10, 12, 13, 14, 15, 18, 19, 161, 163, 164, 165, 167, 170, 172, 173, 176, 177, 178, 179, 181, 189, 190, 191, 192, 196, 200, 206, 207, 208, 212, 214, 218, 219, 223, 224], "callal": [2, 177, 219], "callback": [2, 9, 18, 165, 170, 173, 177, 179, 180, 191, 198, 200, 208, 212, 214, 218, 219, 224, 239, 241, 242, 243, 251, 256], "callbackfunc": [2, 165, 170, 177, 179, 206, 219, 236, 239, 244, 251], "callbak": [236, 244], "can": [2, 6, 9, 10, 11, 12, 16, 17, 147, 149, 150, 151, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 176, 177, 179, 180, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 205, 206, 207, 209, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259], "candid": [2, 11, 14, 18, 165, 173, 177, 178, 181, 190, 191, 196, 200, 202, 208, 210, 211, 212, 214, 219, 236, 244, 255], "cannot": [150, 151, 163, 164, 236, 237, 238, 240, 244, 245, 248, 249, 250, 254], "capabl": [166, 185, 187, 188, 213], "captur": [6, 160, 163, 172, 203, 205, 223, 224, 237, 238, 239, 240, 245, 248, 249, 250, 251], "card": [185, 187, 188], "care": 211, "carefulli": 183, "carri": [147, 148, 149], "case": [153, 154, 161, 162, 163, 164, 166, 188, 197, 201, 202, 207, 210, 218, 222, 225, 226, 228, 237, 239, 251, 259], "cast": [155, 202], "cat": 171, "catch": 202, "caus": [202, 211, 219, 221, 225, 227, 241, 242, 243], "cbe67a": 202, "cd": [186, 235], "cdot": [150, 151, 152, 153, 154], "ce1ea63": 202, "ce68e75": 202, "ceil": [12, 176, 189, 191, 221], "cell": [202, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "celu": 36, "center": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "centercrop": [189, 191, 200, 221, 223, 237, 244, 246, 252], "certain": [163, 170, 179, 202, 206, 211, 216, 219, 222, 223, 225, 226, 244, 245, 248, 249, 250, 251, 254], "chain": [196, 198, 199, 201], "challeng": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254], "chang": [2, 13, 149, 159, 162, 163, 169, 171, 172, 177, 189, 190, 192, 193, 194, 196, 200, 201, 202, 204, 206, 211, 215, 216, 219, 220, 221, 222, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 258], "channel": [8, 9, 18, 161, 166, 170, 179, 183, 192, 193, 194, 195, 202, 206, 207, 209, 210, 212, 214, 215, 216, 217, 218, 225, 227, 235, 236, 237, 238, 239, 240, 242, 244, 245, 247, 248, 249, 250, 251, 254, 256], "channel_index": [9, 170, 179, 206], "channel_index_0": [239, 251], "channel_index_1": [239, 251], "channel_index_n": [239, 251], "channel_prun": [18, 208, 212, 214, 241, 243], "channelpruningparamet": [18, 208, 212, 214, 241, 243], "channelshuffl": 38, "characterist": [236, 239, 244, 247, 251], "chart": 227, "check": [14, 163, 164, 171, 178, 184, 188, 191, 202, 203, 226, 236, 239, 244, 247, 251], "check_model_sensitivity_to_quant": [9, 170, 179, 206], "checker": [164, 202], "checkpoint": [173, 180, 181, 200, 254], "checkpoints_config": [173, 181, 200], "chipset": 258, "choic": [157, 187, 211, 216, 239, 251, 254, 256], "choos": [180, 183, 189, 190, 196, 200, 202, 205, 208, 209, 211, 219, 225, 236, 241, 242, 243, 244, 254], "choose_mixed_precis": [2, 165, 177, 219, 236, 244], "chose": 169, "chosen": [235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251], "chunk": 256, "ci": 186, "cin": 171, "circularpad1d": 39, "circularpad2d": 40, "circularpad3d": 41, "cl": [19, 202, 238, 248], "clamp": [150, 151, 153, 154, 155, 256], "class": [2, 6, 9, 10, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 155, 160, 162, 163, 164, 165, 166, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 189, 191, 196, 198, 199, 200, 201, 205, 206, 208, 212, 214, 219, 222, 223, 224, 225, 226, 228, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254], "classif": [183, 189, 211, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "classmethod": [10, 19, 155, 166, 172, 180, 223], "cle": [4, 17, 191, 194, 202, 206, 227, 235], "clean": 202, "clean_start": [2, 177, 219, 236, 244], "clear": 202, "clearli": 202, "clip": [190, 193, 196, 256], "clone": [149, 174, 201, 235], "close": [209, 256], "closer": [237, 245, 254], "cloud": [183, 258], "cmake": 186, "cmake_arg": 186, "cnn": 183, "cnt": 202, "coars": 194, "code": [187, 189, 191, 194, 201, 202, 205, 207, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 254], "codebas": 187, "collate_fn": [196, 198, 199, 201, 239], "collect": [1, 9, 11, 14, 159, 170, 178, 179, 182, 183, 189, 191, 200, 203, 204, 206, 210, 223, 236, 237, 238, 240, 244, 245, 248, 249, 250], "color": [202, 215], "column": 210, "com": [168, 186, 187, 235], "combin": [2, 163, 177, 183, 191, 202, 207, 209, 211, 219, 236, 244, 254], "come": [211, 220, 224, 255], "command": [186, 187, 213, 235, 253], "common": [2, 10, 18, 162, 165, 170, 171, 172, 177, 179, 180, 188, 189, 190, 191, 192, 196, 201, 202, 206, 208, 212, 214, 218, 219, 221, 222, 223, 227, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "commonli": [12, 171, 176, 183, 189, 223], "commun": 254, "comp": [18, 208, 212, 214], "comp_stat": [241, 242], "compar": [2, 163, 177, 183, 190, 196, 206, 207, 219, 220, 224, 236, 239, 244, 247, 251, 252, 254, 255, 258], "comparison": [203, 205, 238, 240, 245, 248, 249, 250], "compat": [0, 158, 161, 185, 187, 188, 202, 207, 218, 223, 226, 237, 254], "compil": [10, 185, 202, 223, 259], "complet": [2, 177, 219, 227, 236, 241, 242, 243, 244, 253, 255], "complex": [180, 222], "compli": [244, 245, 249, 250, 251], "complic": 202, "compon": [162, 171, 196, 202, 221, 223], "compos": [165, 189, 191, 200, 221, 222, 223, 237, 244, 246, 252], "comprehens": 255, "compress": [0, 8, 158, 183, 184, 202, 215, 216, 218, 235], "compress_model": [18, 208, 212, 213, 214, 241, 242, 243], "compress_schem": [18, 208, 212, 214, 241, 242, 243], "compressed_bw": 216, "compressed_model": [241, 242], "compressionschem": [18, 208, 212, 214, 241, 242, 243], "compressionstat": [18, 208, 212, 214], "compressor": [18, 208, 212, 214], "compris": [196, 210, 236, 244], "compromis": [241, 242, 243], "compuat": [14, 178, 191], "comput": [2, 9, 10, 13, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 169, 170, 172, 173, 177, 178, 179, 180, 181, 183, 184, 185, 187, 188, 189, 190, 191, 193, 194, 196, 198, 199, 200, 201, 202, 206, 208, 211, 212, 213, 214, 215, 216, 218, 219, 237, 238, 239, 240, 241, 242, 243, 245, 248, 249, 250, 251, 252, 254, 256, 258, 259], "computation": 213, "compute_encod": [10, 13, 19, 148, 149, 150, 151, 155, 159, 162, 165, 166, 171, 172, 173, 180, 181, 188, 189, 190, 196, 198, 199, 200, 201, 202, 204, 221, 222, 223, 224, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 252], "computeencod": 187, "concat": 202, "concatenated_exampl": [196, 198, 199, 201], "concept": [162, 207], "concis": 216, "conclus": 255, "concret": 163, "concrete_arg": [14, 163, 178, 191], "concurr": 169, "conda": 186, "condit": [163, 164, 192], "confer": 211, "config": [2, 9, 165, 169, 170, 172, 173, 177, 179, 180, 181, 192, 196, 198, 199, 200, 201, 202, 206, 219, 222, 223, 224, 225, 236, 244, 252], "config_fil": [9, 10, 14, 170, 172, 178, 179, 180, 191, 192, 206, 221, 222, 223, 224, 239, 251, 252], "config_util": [0, 158, 207, 218], "configur": [9, 10, 12, 14, 18, 155, 165, 170, 171, 172, 176, 178, 179, 180, 189, 191, 198, 199, 200, 202, 206, 207, 208, 212, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 252, 255], "conflict": [165, 202, 222], "conform": [223, 259], "confus": 254, "conjunct": 256, "connect": [183, 208, 211, 214], "connectedgraph": [164, 202], "consecut": [194, 219, 238, 248], "consid": [13, 190, 212, 214, 219, 227, 241, 243, 259], "consist": [160, 161, 162, 172, 201, 202, 205, 207, 210, 218, 236, 244, 256], "consol": 203, "consolid": 254, "constant": [163, 202, 210, 240, 249, 250], "constantpad2d": [42, 43], "constantpad3d": 44, "constrain": [183, 207, 252], "constraint": [186, 202, 216], "constrast": 224, "construct": [10, 202], "constructor": [6, 160, 163, 205, 226], "consum": [191, 202, 211, 216, 220, 246, 254], "consumpt": 255, "contain": [1, 2, 9, 10, 18, 19, 147, 155, 159, 163, 164, 165, 166, 170, 177, 179, 183, 188, 189, 194, 204, 206, 208, 212, 214, 216, 219, 223, 225, 237, 238, 239, 240, 241, 242, 243, 245, 246, 248, 249, 250, 251, 253, 255, 256, 258], "content": 183, "context": [10, 19, 166, 202, 253, 254, 259], "contigu": [190, 196, 198, 199, 202], "continu": [2, 164, 177, 180, 198, 202, 218, 219, 227, 236, 244, 254, 255], "contrast": 162, "contribut": [187, 206, 227], "control": [14, 163, 166, 171, 178, 191, 202, 256], "conv": [3, 8, 12, 16, 161, 163, 176, 189, 193, 194, 202, 207, 212, 214, 215, 218, 222, 225, 238, 248], "conv1": [163, 172, 194, 218, 223, 224, 226, 241, 242, 243], "conv1d": [16, 45, 193, 202], "conv2": [161, 194, 207, 218, 226], "conv2d": [16, 19, 46, 161, 162, 163, 166, 172, 193, 194, 202, 207, 208, 211, 215, 218, 223, 224], "conv2dnormactiv": [193, 194], "conv3d": [47, 202], "conv3dtranspos": 202, "conv_1": 207, "conv_weight": 193, "conv_weight_arrai": 193, "conv_weight_nam": 193, "conveni": 258, "convent": [202, 225], "converg": [183, 202, 224, 255], "convers": [202, 223, 226, 255], "convert": [2, 147, 162, 163, 165, 168, 172, 177, 183, 189, 191, 193, 194, 221, 223, 224, 244, 253, 255, 259], "convolut": [161, 183, 192, 193, 194, 195, 207, 208, 209, 211, 212, 214, 215, 218, 219, 227, 236, 237, 238, 240, 241, 243, 244, 245, 248, 249, 250], "convtranspos": 202, "convtranspose1d": [48, 202], "convtranspose2d": [16, 49, 193], "convtranspose3d": 50, "copi": [10, 149, 155, 180, 196, 198, 199, 201, 221, 235, 237, 244, 245, 248, 249, 250, 256], "copy_": 162, "correct": [14, 178, 183, 191, 222, 223, 224, 238, 239, 246, 248, 251, 254, 259], "correct_predict": [189, 200, 221, 223, 237], "correctli": 202, "correl": [219, 236, 244], "correspond": [2, 3, 6, 9, 12, 16, 155, 160, 164, 165, 166, 170, 172, 176, 177, 179, 180, 181, 187, 189, 193, 195, 205, 206, 207, 208, 215, 219, 223, 224, 246, 255, 256], "cosine_similar": 168, "cosineembeddingloss": 51, "cosinesimilar": 52, "cost": [18, 208, 210, 211, 212, 214, 217, 220, 224, 241, 243, 256], "cost_metr": [18, 208, 212, 214, 241, 242, 243], "costmetr": [18, 208, 212, 214, 241, 242, 243], "could": [162, 180, 215, 216, 223, 226, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 255], "counter": 247, "counterpart": [166, 255], "cours": [241, 242, 243, 249, 250], "cout": 171, "cover": [236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 256], "coverag": 254, "cp": [208, 209, 210, 215], "cp310": 187, "cp_comp_stat": 243, "cpu": [17, 163, 172, 180, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 200, 201, 202, 222, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "cpuexecutionprovid": [10, 189, 200, 202, 221, 223, 236, 237, 238, 240], "creat": [2, 9, 10, 13, 14, 18, 19, 155, 159, 163, 165, 166, 169, 171, 177, 178, 180, 183, 185, 188, 189, 190, 191, 192, 196, 198, 200, 201, 202, 204, 208, 211, 212, 214, 218, 220, 222, 235, 239, 241, 242, 243, 246, 251, 253, 254, 256, 258, 259], "create_quantsim_and_encod": [9, 206], "creation": 202, "cross": [4, 14, 17, 178, 191, 206, 227, 235], "cross_entropi": 252, "cross_layer_equ": [0, 5, 158, 194, 238, 248], "crossentropyloss": [53, 192, 198, 199, 224], "crosslayerequ": 202, "ctcloss": 37, "ctivations_pdf": [9, 170, 179, 206], "cu121": 187, "cubla": 202, "cuda": [6, 185, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 205, 221, 222, 223, 224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254], "cudaexecutionprovid": [189, 200, 221, 223, 236, 237, 238, 240], "cudnn": [185, 186, 187, 188], "cudnn_conv_algo_search": [236, 237, 238, 240], "cumul": 246, "curat": 184, "curl": 186, "current": [13, 18, 19, 149, 155, 164, 165, 177, 190, 196, 201, 202, 208, 212, 214, 219, 244, 251, 254, 260], "current_batch": 201, "curv": [2, 177, 210, 219, 236, 244], "custom": [10, 19, 163, 166, 183, 196, 201, 202, 216, 223, 224, 227, 236, 246, 256], "custom_function_not_to_be_trac": 163, "customdataload": 191, "customdataset": 190, "custommodel": 163, "custommodul": 163, "customsilu": 202, "cycl": 258, "d": [150, 151, 152, 153, 154, 196, 201, 216], "d33e98c": 202, "d57a934": 202, "d99b6c4": 202, "dangl": 223, "dark": [236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251], "data": [2, 9, 12, 13, 14, 15, 18, 147, 148, 149, 159, 162, 163, 165, 167, 170, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 189, 190, 191, 192, 195, 196, 198, 199, 200, 201, 202, 204, 206, 208, 212, 214, 217, 219, 221, 223, 224, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 256], "data_load": [2, 12, 13, 14, 18, 159, 165, 173, 176, 177, 178, 181, 189, 190, 191, 192, 196, 200, 204, 208, 219, 221, 223, 224, 236, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "data_set": [13, 190], "data_typ": [14, 162, 172, 178, 180, 191, 202, 223, 224], "dataload": [2, 12, 13, 14, 15, 165, 167, 176, 177, 178, 181, 189, 190, 191, 192, 196, 198, 199, 200, 201, 206, 219, 221, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254], "dataloader_wrapper_len": 196, "dataset": [2, 9, 10, 13, 14, 15, 170, 172, 177, 178, 179, 188, 189, 190, 191, 192, 198, 199, 200, 206, 219, 221, 223, 224, 235, 254, 256, 258], "dataset_dir": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "dataset_path": 223, "dataset_root": [189, 200], "datasetfold": 252, "datatyp": [14, 178, 191, 216], "db99447": 202, "dc34fa4": 202, "dc8d978": 202, "de": [171, 256], "debug": [162, 202, 203, 205, 216, 228], "decai": 211, "decid": [173, 181, 190, 196, 200, 213, 219, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251], "decim": [241, 242, 243], "decis": 259, "declar": 19, "decod": 254, "decompos": [211, 212, 214], "decomposit": [211, 212, 214, 242, 243], "decompress": [161, 218], "decompressed_bw": [161, 218], "decor": 19, "decreas": [11, 183, 200], "dedic": 183, "deem": [219, 258], "deep": [183, 184, 185, 193, 211, 252], "deepcopi": [221, 237], "deeper": 235, "deepseek": 196, "deepspe": 202, "def": [2, 10, 18, 19, 162, 163, 165, 166, 172, 177, 180, 188, 189, 190, 191, 196, 198, 199, 200, 201, 208, 212, 214, 219, 221, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "default": [2, 9, 10, 12, 14, 18, 19, 149, 150, 151, 153, 154, 155, 159, 163, 165, 166, 168, 169, 170, 172, 173, 176, 177, 178, 179, 180, 181, 183, 188, 189, 191, 193, 194, 200, 202, 204, 206, 208, 210, 211, 212, 213, 214, 219, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256], "default_activation_bw": [9, 206, 239], "default_beta_rang": [12, 176, 189], "default_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "default_config_fil": [12, 176, 189], "default_data_col": [196, 198, 199, 201], "default_data_typ": [172, 180, 223, 224], "default_forward_fn": [173, 181, 190, 200], "default_new": 169, "default_num_iter": [12, 176, 189, 245, 246], "default_output_bw": [170, 172, 179, 180, 188, 190, 196, 198, 199, 200, 201, 206, 222, 223, 224, 236, 238, 244, 245, 247, 248, 249, 250, 251, 252], "default_param_bw": [9, 12, 170, 172, 176, 179, 180, 188, 189, 190, 196, 198, 199, 200, 201, 206, 222, 223, 224, 236, 238, 239, 244, 245, 247, 248, 249, 250, 251, 252], "default_quant_schem": [12, 176, 189, 245], "default_reg_param": [12, 176, 189], "default_warm_start": [12, 176, 189], "defer": 185, "defin": [2, 19, 163, 164, 166, 171, 177, 190, 196, 201, 202, 206, 216, 219, 223, 225, 226, 239, 241, 242, 243, 245, 248, 249, 250, 251, 252, 255, 256], "definit": [2, 18, 19, 160, 162, 163, 165, 166, 169, 177, 180, 205, 208, 212, 214, 219, 223, 226, 236, 239, 244, 245, 248, 249, 250, 251, 256], "degrad": [221, 237, 238, 240, 245, 248, 249, 250, 255, 258, 259], "degre": [212, 214], "deleg": 171, "delet": [2, 177, 202, 206, 219, 236, 244], "deliber": [236, 239, 244, 251], "deliv": 254, "delta": [9, 170, 179, 187, 202, 206, 256], "demand": 183, "demonstr": [239, 251, 252, 254], "denable_cuda": 186, "denable_onnx": 186, "denable_test": 186, "denable_torch": 186, "denot": [201, 215, 216], "dens": 202, "depend": [147, 148, 149, 163, 202, 209, 225, 228, 235, 255], "deploi": [182, 254, 256, 257, 258], "deploy": [182, 183, 184, 185, 223, 252, 253, 254, 255, 257, 259], "deprec": [172, 180, 202, 223, 224], "depth": [192, 202, 227], "depthwis": 202, "depthwiseconv": 202, "dequant": [147, 148, 149, 151, 154, 155, 171, 183, 187, 202, 258, 259], "dequantizedtensor": [148, 149, 151, 155, 171, 172, 188], "dequantizelinear": [10, 168, 172, 202, 255], "deriv": [150, 151, 153, 154, 171, 189, 244, 256], "descend": 155, "describ": [155, 162, 186, 188, 201, 207, 211, 213, 216, 219, 225, 227, 256, 257, 258, 259], "descript": [166, 216, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256], "design": [10, 164, 183, 189, 235, 236, 239, 244, 251, 254, 255], "desir": [9, 149, 170, 171, 172, 179, 180, 206, 209, 211, 219, 220, 223, 224, 227, 236, 241, 242, 243, 244, 253, 255, 259], "detach": [149, 168, 174, 201], "detail": [14, 163, 172, 178, 184, 191, 208, 210, 213, 216, 222, 227, 228, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 256, 259], "detect": [202, 211, 255], "determin": [9, 161, 166, 170, 179, 180, 183, 189, 190, 191, 195, 196, 198, 199, 200, 201, 206, 207, 216, 218, 220, 221, 223, 236, 244, 246, 255], "determinist": 163, "dev": 186, "develop": [159, 182, 183, 185, 187, 204, 258], "devic": [6, 12, 14, 149, 163, 172, 174, 176, 177, 178, 180, 182, 183, 184, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 203, 205, 222, 223, 224, 228, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259], "df8b875": 202, "diable_missing_quant": 223, "diagnost": 227, "diagram": [198, 199], "dict": [1, 2, 7, 9, 10, 11, 13, 14, 18, 155, 160, 163, 165, 166, 170, 172, 177, 178, 179, 180, 189, 190, 191, 200, 205, 206, 208, 212, 214, 216, 219, 221, 222, 223, 226, 237], "dictat": 258, "dictionari": [7, 9, 18, 170, 172, 179, 180, 201, 206, 208, 210, 211, 212, 214, 221, 223, 224, 225], "didn": 247, "diff": 202, "differ": [18, 162, 163, 169, 183, 195, 197, 200, 207, 208, 210, 211, 212, 214, 216, 218, 219, 220, 221, 222, 223, 227, 236, 239, 241, 242, 243, 244, 249, 250, 253, 255, 256, 259], "dilat": 202, "dim": [161, 171, 207, 218, 244], "dimens": [8, 161, 166, 207, 212, 214, 216, 218, 227, 256], "dir": [10, 223], "dir_path": [6, 160, 205], "direct": [1, 184, 189, 193, 202, 205, 207, 216, 218, 219, 221, 254, 255, 256, 258, 259], "directli": [9, 185, 188, 198, 202, 206, 222, 223, 247, 254, 256, 258], "directori": [2, 6, 9, 10, 14, 18, 160, 170, 177, 178, 179, 186, 189, 191, 193, 194, 200, 205, 206, 208, 212, 214, 219, 223, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253], "disabl": [2, 9, 162, 166, 169, 170, 172, 173, 179, 180, 181, 193, 200, 201, 202, 206, 210, 211, 219, 223, 224, 225, 241, 243, 256], "disable_missing_quant": 223, "discrep": 202, "discret": 254, "discuss": [241, 242, 243], "disk": [6, 160, 202, 205], "dispatch": 202, "displai": [204, 213, 235], "display_comp_ratio_plot": 213, "display_eval_scor": 213, "dist": 186, "distict": 202, "distil": [190, 195, 196, 254], "distinct": 164, "distort": 256, "distribut": [18, 183, 202, 208, 212, 214, 227, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 256], "divbackward0": 171, "dive": 235, "diverg": 183, "divid": [207, 209], "divis": [8, 171, 183, 207, 218], "dlc": 253, "dlc_path": 253, "dlcompress": 202, "dlequal": 202, "dlf": 183, "do": [10, 12, 163, 164, 169, 172, 176, 187, 189, 193, 196, 198, 201, 206, 211, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 256, 258, 259], "do_constant_fold": [193, 236, 238, 240], "do_not_trace_m": 163, "doc": [163, 169, 186, 202, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "dockerfil": 186, "docstr": 207, "document": [169, 187, 193, 202, 205, 207, 213, 216, 218, 219, 221, 235, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258, 259], "doe": [19, 163, 177, 190, 196, 201, 202, 204, 208, 210, 224, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 258, 259], "doesn": [19, 223, 236, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 258], "doesnt": [165, 222], "don": [19, 163, 186, 198, 199, 206, 239, 251], "done": [150, 151, 171, 202, 211, 223, 225, 247], "doubl": 254, "down": [197, 216], "down_proj": 201, "download": [184, 187, 221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "downsampl": [241, 243], "downstream": [202, 215, 216, 223, 255], "dq": 10, "dq_output": 19, "drastic": [210, 254, 259], "draw": 255, "drawback": 256, "drift": 224, "driver": [185, 187, 188, 254], "drop": [2, 14, 166, 177, 178, 191, 202, 206, 209, 211, 217, 219, 221, 224, 227, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 256], "dropout": 54, "dropout1d": 55, "dropout2d": 56, "dropout3d": 57, "dtype": [147, 148, 149, 155, 161, 162, 163, 165, 188, 196, 198, 199, 201, 202, 207, 216, 222], "due": [164, 168, 202, 207, 224, 254, 255], "dummi": [9, 12, 14, 16, 17, 160, 164, 170, 172, 176, 177, 178, 179, 180, 189, 191, 193, 194, 198, 199, 205, 206, 223, 224, 236, 239, 244, 251], "dummy_attention_mask": [198, 199], "dummy_input": [9, 10, 12, 13, 14, 16, 17, 159, 160, 169, 170, 172, 176, 177, 178, 179, 180, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 204, 205, 206, 221, 222, 223, 224, 226, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 252], "dummy_input_id": [198, 199], "dummy_model": [190, 196], "dummymodel": [13, 190], "dump": [196, 202], "duplic": [149, 163, 202], "dure": [1, 2, 10, 11, 12, 13, 14, 18, 19, 155, 159, 166, 176, 177, 178, 183, 184, 189, 190, 191, 200, 202, 203, 204, 208, 211, 212, 213, 214, 219, 221, 223, 224, 225, 239, 240, 241, 242, 243, 245, 247, 249, 250, 251, 252, 254, 255, 256, 259], "dynam": [155, 163, 183, 194, 202, 224, 254, 256], "dynamic_ax": [189, 193, 200, 221, 223, 236, 237, 238, 239, 240], "dynamo": [168, 172, 188, 189, 193, 194, 200, 202, 221, 223, 236, 237, 238, 239, 240], "e": [9, 10, 167, 172, 174, 180, 194, 195, 196, 201, 206, 216, 221, 223, 224, 236, 237, 239, 244, 247, 251, 254, 255], "e026fd1": 202, "e250abd": 202, "e49660c": 202, "e4c49eb": 202, "e78dbec": 202, "e7d10c7": 202, "e8cb098": 202, "ea4af6a": 202, "each": [1, 2, 9, 10, 11, 12, 19, 159, 164, 166, 167, 170, 171, 173, 176, 177, 179, 183, 189, 190, 193, 194, 196, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 216, 219, 223, 225, 227, 228, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256], "earli": [2, 177, 219], "easi": [202, 222, 258], "easier": 162, "easili": [171, 254], "eb1ac5c": 202, "ec22d86": 202, "echo": 186, "ecosystem": [182, 254], "ed": 189, "edg": [183, 184, 258], "edit": [202, 216, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "ee949a2": 202, "effect": [12, 15, 166, 176, 183, 189, 192, 195, 201, 206, 222, 240, 249, 250, 252, 258, 259], "effici": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 247, 252, 253, 254, 255, 256, 257, 258, 259, 260], "efficientnetb4": 202, "effort": [14, 165, 178, 191, 222, 246, 255, 258, 259], "eights_pdf": [9, 170, 179, 206], "either": [7, 18, 161, 165, 171, 201, 207, 208, 212, 214, 215, 218, 219, 221, 222, 226, 236, 241, 242, 243, 244, 254, 255], "element": 216, "elementwis": [163, 166, 202], "elev": 255, "elimin": [183, 193, 206, 256], "els": [2, 163, 164, 177, 188, 189, 190, 191, 192, 193, 194, 196, 200, 201, 219, 222, 223, 224, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 252, 255], "elu": 58, "embed": [59, 163, 168, 172, 174, 180, 201, 202, 211, 223, 224, 227], "embed_token": [174, 201, 202], "embeddingbag": 60, "embodi": 183, "empir": [246, 255], "emploi": [183, 184], "empti": [202, 225], "emul": 256, "enabl": [2, 8, 9, 14, 18, 155, 162, 165, 169, 170, 173, 177, 178, 179, 181, 183, 185, 191, 192, 193, 195, 200, 201, 202, 205, 207, 208, 212, 213, 214, 218, 219, 220, 221, 222, 225, 241, 243, 244, 250, 253, 254, 256, 258], "enable_convert_op_reduct": [165, 177, 219, 244], "enable_onnx_check": [172, 180, 223, 224], "enable_per_layer_mse_loss": [9, 206, 239, 251], "enbl": 218, "enc": 19, "enc_typ": 216, "encapsul": [2, 165, 170, 177, 179, 206, 219], "encaptur": 169, "encod": [1, 2, 9, 10, 11, 12, 13, 14, 19, 147, 148, 149, 150, 151, 155, 162, 165, 168, 169, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 183, 187, 189, 190, 191, 195, 196, 197, 198, 199, 200, 201, 202, 203, 218, 219, 222, 224, 237, 238, 240, 245, 246, 248, 249, 250, 252, 253, 254, 255, 258], "encoding_analyz": [150, 151, 155], "encoding_path": [191, 245, 246], "encoding_vers": [10, 172, 223], "encodinganalyz": [150, 151, 155, 202], "encodinganalyzerforpython": 187, "encodingbas": [147, 148, 155], "encodingmismatchinfo": 223, "encodingtyp": 216, "encount": [19, 202, 254], "encourag": [163, 226], "end": [13, 150, 151, 152, 153, 154, 163, 164, 184, 188, 190, 211, 223, 235, 241, 242, 243, 247, 249, 250, 254, 255, 258], "end_beta": [12, 176, 189], "end_idx": 191, "enforc": 155, "engin": [184, 193, 202, 205, 207, 216, 218, 219, 221, 254, 255, 256, 258, 259], "english": 254, "enhanc": [9, 169, 170, 179, 202, 206, 223, 239, 251, 254, 256], "enough": [239, 241, 242, 243, 254, 259], "enpu": 202, "enpu_v6": [10, 172, 180, 223, 224], "ensur": [166, 188, 192, 198, 199, 202, 210, 219, 227, 252, 254, 259], "enter": [19, 166, 191], "entir": [9, 170, 171, 179, 206, 211, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256], "entri": [14, 172, 178, 180, 191, 216, 223, 224, 225], "enum": [2, 10, 12, 18, 160, 172, 176, 177, 180, 189, 205, 208, 212, 214, 216, 219, 223, 236, 244, 247], "enumer": [18, 160, 196, 198, 199, 200, 205, 208, 212, 214, 221, 223, 252], "environ": [183, 187, 235, 236, 244, 251, 252], "ep": [193, 194, 202], "epoch": [192, 198, 199, 211, 224, 237, 238, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250], "epsilon": 202, "equal": [4, 14, 17, 148, 149, 161, 171, 172, 178, 191, 202, 206, 207, 209, 210, 218, 227, 235, 256], "equalize_model": [4, 17, 194, 238, 248], "equat": [150, 151, 152, 153, 154, 207, 256], "equival": [14, 19, 153, 154, 155, 161, 165, 172, 177, 178, 180, 190, 191, 196, 201, 207, 218, 219, 223, 224, 226, 236, 244, 245, 248, 249, 250, 251, 253], "error": [1, 8, 14, 19, 163, 171, 174, 178, 183, 189, 191, 194, 201, 202, 203, 218, 222, 223, 227, 254, 256], "especi": [168, 183, 198, 224, 227, 236, 244, 258], "essenti": [236, 244, 247, 254], "establish": [252, 254], "estim": [202, 235, 256, 258], "esults_dir": [9, 170, 179, 206], "etc": [13, 183, 190, 202, 209, 216, 219, 236, 244, 254], "eval": [2, 9, 13, 14, 18, 163, 165, 170, 173, 177, 178, 179, 181, 188, 189, 190, 191, 192, 193, 194, 200, 202, 206, 208, 210, 211, 212, 213, 214, 219, 222, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254], "eval_callback": [9, 14, 18, 170, 178, 179, 191, 206, 208, 212, 214, 239, 241, 242, 243, 246, 251], "eval_callback_factori": [236, 244], "eval_callback_for_phase1": [2, 177, 219, 236, 244], "eval_callback_for_phase2": [2, 177, 219, 236, 244], "eval_data_load": [221, 223], "eval_dataset": [221, 223], "eval_dataset_s": [191, 246], "eval_fn": 221, "eval_iter": [18, 208, 212, 214, 241, 242, 243], "eval_scor": [9, 18, 170, 179, 206, 208, 212, 214], "evalcallbackfactori": [2, 165, 177, 219, 236, 244], "evalu": [2, 9, 14, 18, 165, 170, 173, 177, 178, 179, 188, 189, 190, 191, 196, 200, 201, 202, 208, 210, 211, 212, 213, 214, 218, 219, 224, 229, 230, 231, 232, 233, 234, 235, 246, 255, 256, 259], "evaluate_accuraci": 244, "even": [19, 166, 171, 219, 222, 224, 236, 244, 254], "evenli": [8, 207, 218], "eventu": 222, "everi": [6, 9, 12, 160, 170, 171, 176, 179, 189, 190, 196, 203, 205, 206, 210, 211, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 247, 249, 250, 251, 256], "evid": 255, "ex": 169, "exactli": [13, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 166, 190, 239, 251, 256], "examin": 163, "exampl": [8, 10, 13, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 161, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 181, 183, 186, 188, 189, 190, 191, 193, 194, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 209, 215, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 246, 252, 254, 255, 256, 259], "exce": [159, 204], "exceed": [159, 204], "except": [155, 181, 193, 194, 202, 222, 229, 230, 231, 232, 233, 234, 236, 238, 239, 240, 251, 254], "exchang": 183, "exclud": [8, 11, 14, 19, 163, 164, 166, 170, 173, 178, 179, 181, 191, 200, 202, 206, 216, 218], "excluded_lay": 216, "exclus": [155, 161, 207], "execut": [2, 10, 14, 163, 172, 177, 178, 180, 191, 193, 202, 213, 219, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255], "exercis": [236, 239, 244, 247, 251], "exhibit": 255, "exist": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 166, 172, 180, 207, 218, 223, 224, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 258], "exist_ok": [236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "exit": [2, 14, 19, 166, 177, 178, 191, 219], "expand": 202, "expand_dim": 191, "expans": 211, "expect": [2, 9, 12, 14, 18, 155, 159, 163, 165, 167, 170, 172, 173, 176, 177, 178, 179, 180, 181, 189, 191, 196, 200, 202, 204, 206, 208, 211, 212, 214, 219, 222, 223, 224, 236, 241, 242, 243, 244, 247, 252, 254, 255], "experi": [18, 161, 185, 187, 208, 211, 212, 214, 218, 237, 241, 242, 243, 245, 248, 249, 250, 255, 259], "experiment": [0, 158, 185, 190, 196, 201, 202, 207, 211], "expert": 246, "explain": [202, 207, 208, 211, 236, 239, 244, 247, 251], "explan": [184, 237, 238, 240, 245, 248, 249, 250], "explicit": 202, "explicitli": [14, 178, 191, 215, 254], "explor": [219, 255], "expon": [155, 161, 207], "exponent_bit": [155, 161, 207], "export": [9, 10, 13, 14, 160, 162, 165, 169, 170, 172, 173, 178, 179, 180, 181, 185, 186, 188, 189, 190, 191, 192, 193, 194, 196, 200, 202, 203, 205, 206, 211, 216, 218, 219, 220, 221, 222, 224, 226, 235, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 253, 254, 255], "export_int32_bia": [10, 168, 172, 202, 223], "export_model": [10, 169, 172, 180, 223, 224], "export_param": [236, 238, 240], "export_per_layer_encoding_min_max_rang": [9, 170, 179, 206], "export_per_layer_mse_loss": [9, 170, 179, 206], "export_per_layer_stats_histogram": [9, 170, 179, 206], "export_to_torchscript": [172, 180, 223, 224], "expos": [163, 201, 202, 206], "express": [18, 198, 199, 208, 212, 214], "extend": [162, 202], "extens": [166, 235, 253], "extent": 255, "extern": 202, "extra": [155, 166, 186, 202], "extract": [183, 223, 238, 239, 240, 245, 248, 249, 250, 251], "extractor": 202, "extrem": [236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 256], "f": [163, 168, 171, 172, 186, 187, 188, 189, 191, 193, 194, 200, 221, 223, 224, 237, 246, 252], "f0": 209, "f0bc6c9": 202, "f333188": 202, "f39c0bf": 202, "f547a49": 202, "f7e700f": 202, "f94f3e2": 202, "f961ed4": 202, "f9d0d6c": 202, "facebook": [183, 198, 199], "facilit": 184, "fact": 198, "factor": [194, 202, 209, 211, 224, 241, 242, 243, 247, 249, 250], "factori": [2, 165, 177, 219], "fail": [163, 164, 165, 172, 191, 193, 194, 202, 222, 223, 224, 236, 238, 240], "failur": 202, "fair": 183, "fairli": [239, 251], "fake": [151, 154, 155, 165, 177, 202, 219, 221, 223, 236, 237, 238, 240, 244, 245, 246, 247, 248, 249, 250, 252], "fakequ": [172, 180, 223, 224], "fall": [210, 225, 241, 242, 243, 259], "fallback": [202, 253], "fals": [2, 8, 10, 14, 18, 19, 147, 149, 150, 151, 153, 154, 155, 161, 162, 163, 164, 165, 166, 168, 169, 171, 172, 175, 177, 178, 180, 187, 188, 189, 191, 193, 194, 196, 198, 199, 200, 201, 207, 208, 212, 214, 216, 218, 219, 221, 222, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252], "famili": [183, 202, 222], "familiar": 235, "far": 247, "farther": [237, 245], "fast": 186, "faster": [202, 219, 236, 244, 246, 255], "fastest": 185, "fc": [211, 214], "fd7e40d": 202, "fe66376": 202, "fea395f": 202, "feasibl": 254, "featur": [8, 13, 159, 162, 163, 164, 183, 184, 190, 191, 193, 194, 196, 201, 202, 204, 211, 213, 218, 219, 222, 226, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 256], "featurealphadropout": 61, "feed": 256, "feel": [222, 247], "feez": [173, 181, 200], "fefd504": 202, "few": [192, 209, 221, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252], "fewer": [211, 258], "ff7a284": 202, "field": [2, 155, 177, 216, 219, 236, 244, 254], "figur": [189, 191, 208, 210, 212, 213, 214, 215, 227, 256], "file": [2, 9, 10, 12, 14, 18, 165, 170, 172, 173, 176, 177, 178, 179, 180, 181, 186, 187, 189, 191, 192, 200, 202, 206, 208, 212, 214, 216, 219, 222, 223, 224, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 256, 258], "file_path": [180, 188, 189, 193, 194, 200, 221, 223], "filenam": [9, 10, 12, 172, 176, 180, 189, 190, 192, 196, 206, 223, 224, 236, 237, 238, 240], "filename_prefix": [10, 12, 172, 176, 180, 189, 190, 192, 196, 200, 223, 224, 236, 237, 244, 245, 246, 247, 248, 249, 250], "filename_prefix_encod": [169, 172, 180, 223, 224], "fill": [14, 172, 178, 180, 191, 223, 224], "filter": 183, "final": [18, 164, 181, 198, 199, 201, 206, 207, 208, 209, 210, 212, 214, 227, 236, 239, 244, 247, 251, 258], "find": [2, 164, 165, 173, 177, 180, 181, 185, 186, 190, 196, 200, 206, 210, 220, 222, 224, 235, 236, 237, 241, 242, 243, 244, 245, 246, 248, 249, 250, 254, 256, 259], "fine": [18, 169, 180, 184, 208, 209, 212, 214, 224, 237, 238, 240, 245, 246, 248, 249, 250, 252, 254, 255, 258, 259], "finer": [171, 207, 254], "finess": 259, "finetun": [237, 238, 241, 242, 243, 245, 247, 249, 250], "finetuned_accuraci": [247, 249, 250], "finetuned_accuracy_bn_reestim": 247, "finetuned_model": [241, 242], "finish": [249, 250], "finit": 155, "first": [12, 163, 166, 169, 171, 173, 176, 181, 189, 200, 202, 211, 227, 239, 241, 242, 243, 247, 251, 254, 258], "fit": [18, 208, 210, 212, 214, 247, 258], "five": [238, 248], "fix": [155, 164, 202, 216, 219, 224, 236, 237, 238, 240, 256], "flag": [2, 14, 159, 162, 163, 164, 165, 172, 177, 178, 180, 186, 191, 202, 204, 219, 222, 223, 224, 236, 244], "flatten": [62, 198, 199, 216, 254], "flexibl": [236, 244, 254], "flexround": 190, "flip": [7, 221, 255], "flip_layers_to_higher_precis": [7, 221], "float": [2, 7, 9, 10, 12, 14, 18, 19, 70, 147, 148, 155, 161, 165, 168, 170, 172, 176, 177, 178, 179, 180, 183, 189, 191, 193, 194, 201, 202, 206, 207, 208, 212, 214, 216, 219, 221, 223, 224, 227, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 256, 258, 259], "float16": [7, 10, 155, 161, 162, 202, 207, 221, 223, 255], "float32": [10, 188, 223, 239], "float4": 202, "float8": 202, "float8_e4m3fnuz": 155, "float_fallback": 253, "floatencod": [155, 202], "floatquant": 162, "floatquantizedequant": 162, "flow": [14, 163, 178, 191, 202, 227, 256], "fly": [10, 168, 172, 223], "focu": 247, "fold": [3, 14, 16, 63, 172, 178, 183, 191, 192, 194, 201, 202, 206, 237, 255], "fold_all_batch_norm": [16, 193, 224, 244, 245, 248, 249, 250], "fold_all_batch_norms_to_scal": [192, 247], "fold_all_batch_norms_to_weight": [3, 193, 202, 236, 237, 238, 240], "fold_param_quant": 172, "folder": [206, 239, 251], "follow": [0, 5, 6, 8, 9, 10, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 158, 160, 162, 163, 164, 165, 166, 168, 169, 170, 172, 179, 180, 186, 187, 188, 189, 191, 192, 193, 194, 200, 202, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259], "footprint": [183, 184], "forall_": [150, 151, 152, 153, 154], "forc": [168, 172, 194, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251], "force_activation_a": [168, 172], "form": [239, 251], "formal": 207, "format": [9, 10, 12, 149, 167, 170, 172, 176, 179, 180, 183, 184, 185, 189, 191, 196, 202, 206, 223, 224, 237, 253, 255, 256, 258], "former": 226, "forward": [2, 9, 10, 12, 13, 14, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 155, 163, 164, 165, 166, 167, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 189, 190, 191, 192, 193, 196, 198, 199, 200, 201, 202, 206, 219, 223, 224, 226, 227, 236, 239, 240, 244, 245, 247, 248, 249, 250, 251, 259], "forward_callback": 239, "forward_fn": [2, 12, 13, 14, 15, 165, 167, 173, 176, 177, 178, 181, 189, 190, 191, 192, 196, 200, 219, 236, 244, 247], "forward_one_batch": [236, 244], "forward_pass": [188, 189, 190, 198, 199, 200, 219], "forward_pass_arg": 223, "forward_pass_callback": [2, 9, 10, 170, 172, 177, 179, 180, 206, 219, 223, 224, 236, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "forward_pass_callback_arg": [10, 172, 180, 223, 224, 244, 245, 247, 248, 249, 250], "found": [162, 255, 256], "four": [191, 219, 256], "fp": [195, 201, 224, 252, 254], "fp16": [165, 202, 221, 222, 255, 258], "fp32": [2, 6, 9, 160, 165, 170, 173, 177, 179, 181, 183, 190, 196, 200, 202, 205, 206, 216, 219, 221, 229, 230, 231, 232, 233, 234, 239, 251, 253, 254, 256, 258, 259], "fp32_output": [2, 219], "fp_accuraci": 221, "fp_input": 221, "fp_qdq": 162, "fp_quantiz": 162, "fp_session": 221, "fpt": 202, "fptquant": 202, "frac": [150, 151, 152, 153, 154, 155, 171, 194, 256], "fraction": [219, 259], "fractionalmaxpool2d": 64, "fractionalmaxpool3d": 65, "framework": [183, 184, 187, 188, 190, 193, 196, 201, 205, 223, 225, 235, 253, 256, 259], "free": [222, 238, 247, 248], "freez": [162, 190, 196, 200, 202, 245], "freeze_encod": 162, "friendli": [169, 191, 194, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 258], "from": [2, 7, 8, 10, 11, 12, 13, 14, 15, 19, 148, 149, 150, 151, 155, 158, 161, 165, 166, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 183, 184, 185, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 205, 206, 207, 208, 209, 210, 211, 215, 216, 218, 219, 221, 222, 223, 224, 225, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259], "from_encod": 155, "from_modul": 19, "from_numpi": 168, "from_onnx_qdq": 10, "from_pretrain": [174, 196, 198, 199, 201], "from_str": [10, 172, 180, 223], "front": [2, 14, 177, 178, 191, 236, 244], "frozen": [1, 189, 198, 199, 223, 237], "full": [13, 183, 190, 220, 226, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 254], "fulli": [0, 10, 158, 202, 211, 214, 216, 223], "func": [2, 165, 170, 177, 179, 206, 219], "func_callback_arg": [2, 165, 170, 177, 179, 206, 219, 236, 244], "function": [0, 2, 5, 8, 9, 10, 12, 13, 14, 15, 18, 19, 147, 148, 149, 153, 154, 155, 158, 159, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 183, 189, 190, 191, 192, 194, 196, 198, 199, 200, 201, 202, 204, 206, 207, 208, 210, 211, 212, 213, 214, 218, 219, 223, 224, 226, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 253, 254, 256, 259], "function_nam": [241, 242, 243], "functional_op": 164, "fundament": 162, "furhter": 246, "furo": 202, "further": [147, 150, 151, 152, 153, 154, 163, 171, 208, 211, 224, 228, 243, 255, 256, 259], "fuse": [174, 198, 201, 225, 256], "fuse_bn_into_conv": 193, "fusion": [183, 201, 202], "futur": [159, 201, 204], "fx": [14, 163, 178, 191], "g": [167, 174, 180, 194, 196, 201, 216, 221, 236, 237, 239, 244, 247, 251, 254, 255], "gain": [208, 209, 211, 237, 241, 242, 243, 245, 248, 249, 250, 255, 259], "gamma": 190, "gap": 220, "gate_proj": 201, "gaussiannllloss": 70, "gave": 191, "gb": [229, 230, 231, 232, 233, 234, 254], "gelu": 66, "gemm": [8, 202, 218, 225], "gemma3": 202, "genai": 202, "genaitest": [202, 254], "gener": [6, 10, 19, 150, 151, 152, 153, 154, 160, 168, 171, 172, 177, 183, 186, 189, 190, 192, 193, 194, 196, 198, 199, 200, 201, 202, 206, 207, 211, 216, 218, 219, 221, 222, 223, 224, 225, 227, 235, 236, 244, 246, 253, 254, 255, 256, 258], "generate_calibration_callback": [198, 199], "generate_layer_output": [6, 160, 205], "get": [2, 18, 155, 156, 163, 165, 169, 177, 184, 186, 189, 199, 200, 202, 211, 212, 214, 217, 219, 228, 235, 236, 237, 238, 240, 255], "get_activation_quant": [2, 219], "get_active_quant": [2, 165, 177, 219], "get_available_provid": [189, 200, 236, 237, 238, 240], "get_calibration_and_eval_data_load": [221, 223], "get_candid": [2, 165, 177, 219], "get_default_kernel": 19, "get_devic": 246, "get_encod": 155, "get_extra_st": 155, "get_input": [189, 221, 236, 237, 238, 239, 240], "get_input_quantizer_modul": [165, 177, 219], "get_kernel": 19, "get_loss_fn": [173, 181, 200], "get_offset": 171, "get_param_quant": [2, 219], "get_path_for_per_channel_config": [188, 192, 222, 223], "get_peft_model": 169, "get_quant_scheme_candid": [14, 178, 191], "get_scal": [149, 171], "get_swap_module_params_on_convers": 155, "get_unlabeled_dataload": 239, "get_val_dataload": [236, 238, 239, 240, 241, 243, 244, 245, 247, 248, 249, 250, 251], "getenv": 252, "git": 235, "github": [168, 187, 202, 235], "give": [202, 206, 211, 217, 239, 241, 242, 243, 251, 255, 257], "given": [2, 4, 7, 12, 14, 15, 17, 18, 19, 155, 159, 165, 166, 172, 173, 176, 177, 178, 180, 181, 189, 191, 192, 194, 200, 201, 204, 208, 210, 211, 212, 214, 217, 219, 221, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255, 259], "global": [156, 157, 183, 200, 227], "glu": 67, "go": [202, 236, 239, 244, 247, 251, 254], "goal": [14, 178, 191, 224, 236, 244, 258], "good": [169, 211, 241, 242, 243, 247, 249, 250, 254, 255, 259], "googl": 183, "got": [3, 16, 163, 193], "gpu": [183, 185, 186, 187, 188, 202, 223, 239, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 254], "grad_fn": [147, 148, 149, 150, 151, 168, 171, 188], "gradient": [147, 148, 149, 198, 199, 202, 223], "grant": 235, "granular": [18, 202, 207, 208, 211, 212, 214, 227, 241, 242, 243, 254], "graph": [10, 12, 14, 149, 163, 168, 172, 176, 178, 180, 189, 191, 193, 194, 200, 202, 213, 216, 219, 221, 222, 223, 224, 236, 237, 238, 240, 241, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258], "graph_optimization_level": 168, "graphmodul": 163, "graphoptimizationlevel": 168, "greater": [18, 208, 209, 210, 212, 214, 255], "greatest": 221, "greedi": [18, 208, 211, 212, 213, 214], "greedili": [236, 244], "greedy_param": [241, 242, 243], "greedy_select_param": [18, 208, 212, 241, 242, 243], "greedymixedprecisionalgo": [165, 177, 219, 244], "greedyselectionparamet": [18, 208, 212, 214, 241, 242, 243], "green": 215, "grid": [171, 173, 181, 194, 200, 218, 256], "group": [2, 8, 161, 165, 177, 194, 202, 218, 225, 236, 244, 256], "groupnorm": 71, "gru": [68, 202], "grucel": 69, "guarante": 202, "guid": [0, 158, 184, 186, 197, 202, 223, 228, 235, 236, 244, 245, 246, 248, 249, 250, 251, 254, 255, 258], "guidebook": 211, "guidelin": [163, 223, 244, 245, 248, 249, 250, 251], "gz": [221, 223, 239], "h": [214, 215, 235], "ha": [0, 9, 12, 14, 19, 147, 158, 162, 163, 164, 172, 176, 177, 178, 180, 183, 185, 186, 189, 190, 191, 196, 197, 202, 206, 207, 209, 210, 213, 215, 218, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256, 258], "had": 226, "hadamard": [174, 201], "half": 209, "hand": [236, 244], "handl": [14, 15, 178, 187, 188, 191, 192, 202, 207, 256], "hard": 163, "hardshrink": 72, "hardsigmoid": 73, "hardswish": 74, "hardtanh": 75, "hardwar": [10, 172, 180, 183, 207, 223, 224, 252, 254, 256, 258], "hat": 256, "have": [9, 19, 159, 160, 161, 162, 163, 164, 167, 170, 171, 174, 179, 183, 185, 186, 188, 194, 196, 198, 200, 201, 202, 204, 205, 206, 207, 211, 216, 218, 221, 223, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 259], "hba": [238, 248], "he": 211, "head": [229, 230, 231, 232, 233, 234, 254], "heavi": [159, 204, 213, 255], "height": [193, 194, 212, 214, 215, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "held": [19, 166, 249, 250], "help": [162, 164, 165, 172, 180, 182, 195, 201, 206, 207, 210, 211, 219, 220, 222, 223, 224, 225, 227, 235, 238, 239, 248, 251, 252, 258, 259], "helper": [165, 171, 172, 177, 198, 201, 219, 247], "hen": [14, 178, 191], "here": [12, 162, 169, 171, 176, 185, 186, 189, 194, 198, 199, 221, 223, 225, 236, 238, 239, 240, 241, 243, 244, 245, 247, 248, 249, 250, 251, 258, 259], "heterogen": 258, "heurist": 246, "hexagon": 252, "hh": [229, 230, 231, 232, 233, 234, 254], "hidden": 202, "hide": 222, "high": [2, 4, 17, 162, 177, 183, 184, 194, 195, 197, 202, 209, 210, 219, 224, 237, 238, 240, 241, 242, 243, 245, 248, 249, 250, 252, 254, 255], "higher": [7, 12, 155, 176, 177, 189, 202, 210, 218, 219, 220, 221, 222, 224, 227, 236, 241, 242, 243, 244, 255, 256, 258], "highest": [14, 178, 191, 210, 221], "highli": [168, 255], "highlight": [213, 252], "hingeembeddingloss": 76, "histogram": [9, 170, 179, 202, 203, 239, 251, 256], "historgram": [9, 206, 239], "hold": [147, 148, 149, 166, 192, 225, 227], "hood": [162, 254], "hook": 256, "hope": [236, 244, 247], "host": [187, 202, 213], "hotspot": [9, 170, 179, 206, 217], "hover": 202, "how": [2, 164, 166, 169, 171, 172, 177, 183, 184, 186, 188, 189, 197, 201, 202, 206, 207, 211, 212, 214, 216, 217, 218, 219, 222, 223, 224, 227, 228, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 257, 259], "howev": [162, 187, 189, 197, 202, 207, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 255, 256, 258, 259], "html": [9, 159, 163, 170, 179, 186, 202, 203, 204, 206, 235, 239, 251], "htp": [202, 252, 253], "htp_v66": [10, 172, 180, 223, 224], "htp_v68": [10, 172, 180, 223, 224], "htp_v69": [10, 172, 180, 223, 224], "htp_v73": [10, 172, 180, 223, 224], "htp_v75": [10, 172, 180, 223, 224, 239], "htp_v79": [10, 172, 180, 223, 224], "htp_v81": [10, 172, 180, 223, 224, 252], "http": [163, 168, 186, 187, 190, 196, 201, 202, 213, 221, 223, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "hub": [182, 184, 193, 202, 205, 207, 216, 218, 219, 221, 254, 255, 256, 258, 259], "huberloss": 77, "huggingfac": [169, 174, 198, 199, 201, 202, 254], "hx": [68, 69, 85, 86, 112, 113], "hxwx5": 215, "hxwx8": 215, "hyper": [192, 237, 241, 242, 243, 245, 247, 248, 249, 250, 258], "hyperparamet": [224, 229, 230, 231, 232, 233, 234, 254, 255], "i": [0, 1, 2, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18, 19, 150, 151, 152, 153, 154, 155, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 170, 171, 172, 176, 177, 178, 179, 180, 181, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259], "i_": [150, 151, 152, 153, 154], "i_0": [150, 151, 152, 153, 154], "i_d": [150, 151, 152, 153, 154], "iccv": [211, 238, 248], "id": [6, 14, 178, 191, 198, 199, 205, 213, 254], "idea": 194, "ideal": [202, 239, 251], "idempot": 149, "ident": [193, 226], "identifi": [161, 164, 201, 202, 203, 206, 207, 208, 215, 217, 218, 219, 221, 227, 235, 255, 256, 258], "ieee": [155, 211], "ignor": [2, 18, 19, 155, 163, 166, 177, 202, 208, 212, 214, 219, 223, 236, 238, 239, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 259], "ignore_quant_ops_list": [12, 176, 189], "ignore_unknown_modul": [19, 166], "illustr": [189, 208, 210, 212, 213, 214, 215, 241, 242, 243, 256, 258], "ilsvrc": [191, 221, 223, 239], "ilsvrc2012": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "ilsvrc2012_devkit_t12": [221, 223, 239], "ilsvrc2012_img_v": [221, 223, 239], "imag": [9, 170, 179, 183, 186, 189, 191, 192, 200, 202, 206, 219, 221, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "image1": 252, "image2": 252, "image_bw": 226, "image_net_config": [236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "image_net_data_load": [236, 238, 239, 240, 241, 243, 245, 247, 248, 249, 250, 251], "image_net_evalu": [236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "image_net_train": [241, 242, 243, 247, 249, 250], "image_rgb": 226, "image_s": [236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "imagefold": [244, 246, 252], "imagenet": [188, 189, 191, 192, 193, 194, 200, 221, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "imagenet1k_v1": 252, "imagenet_data": [189, 200, 237], "imagenet_dataset": [221, 223, 239, 246, 252], "imagenet_dir": 252, "imagenetdataload": [236, 238, 239, 240, 241, 243, 245, 247, 248, 249, 250, 251], "imagenetdatapipelin": [236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "imagenetevalu": [236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "imagenettrain": [241, 242, 243, 247, 249, 250], "images_dir": 247, "images_mean": 244, "images_std": 244, "imbal": 194, "immedi": [238, 246, 248], "impact": [183, 210, 221, 224, 227, 228, 236, 244, 252, 254, 255], "implement": [6, 19, 160, 166, 171, 191, 198, 199, 201, 202, 205, 227, 235, 236, 244, 254, 258, 259], "impli": [236, 244], "import": [12, 19, 147, 148, 149, 150, 151, 153, 154, 155, 158, 161, 162, 163, 164, 166, 168, 169, 171, 172, 174, 176, 180, 183, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 207, 208, 218, 219, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256], "impos": 207, "improp": 202, "improv": [184, 189, 190, 193, 195, 196, 201, 202, 207, 209, 211, 219, 220, 221, 223, 227, 237, 238, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 252, 254, 255, 257, 259], "in1": 222, "in2": 222, "in_channel": [161, 207, 218], "in_eval_mod": 246, "in_featur": [19, 162, 166, 172], "in_plac": [172, 180, 196, 198, 199, 201, 223, 224, 252], "inc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "includ": [2, 8, 14, 159, 177, 178, 183, 187, 191, 193, 194, 201, 202, 204, 206, 210, 213, 216, 218, 219, 224, 225, 254, 255, 256, 259], "incompat": [8, 202, 218], "inconsist": 202, "incorrect": [165, 202, 222], "incorrectli": [202, 259], "increas": [18, 195, 208, 210, 212, 214, 221, 224, 236, 244, 255, 256], "increment": 259, "incur": [206, 219], "independ": [163, 183, 227], "index": [9, 166, 170, 179, 186, 202, 206, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "index_0": [239, 251], "index_1": [239, 251], "index_n": [239, 251], "indexerror": 202, "indic": [18, 98, 99, 100, 165, 166, 171, 172, 202, 208, 209, 212, 214, 215, 216, 222, 223, 224, 244, 246, 252], "indirect": [219, 236, 244], "individu": [9, 170, 179, 184, 200, 206, 210, 217, 225], "induc": 256, "inf": 155, "infer": [10, 12, 14, 172, 176, 178, 183, 188, 189, 190, 191, 193, 196, 200, 202, 207, 209, 219, 221, 222, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 259], "inferencesess": [2, 9, 10, 168, 202, 206, 219, 221, 223, 236, 237, 238, 240], "influenc": 256, "info": [10, 164, 202], "inform": [2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 162, 164, 168, 177, 182, 184, 202, 216, 219, 223, 236, 244, 247, 252], "inherit": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 166, 202], "init": [193, 194, 201], "initi": [6, 7, 10, 150, 151, 155, 166, 171, 172, 180, 193, 194, 205, 221, 223, 236, 238, 239, 240, 244, 245, 249, 250, 251], "initial_accuraci": [191, 246], "initializd": 166, "inner": 227, "innov": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "inp_symmetri": [173, 181, 200], "inplac": [193, 194], "input": [1, 2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 152, 153, 154, 155, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 183, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 204, 206, 207, 208, 210, 211, 212, 214, 215, 216, 218, 219, 221, 223, 224, 225, 226, 236, 237, 238, 239, 240, 244, 247, 251, 253, 256], "input1": [35, 51, 94], "input2": [35, 51, 94], "input_": [150, 151, 152, 153, 154], "input_channel": [161, 207, 218], "input_data": [191, 223, 236, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "input_dlc": 253, "input_id": [196, 198, 199, 201], "input_inst": [6, 160, 205], "input_length": 37, "input_list": 253, "input_nam": [168, 172, 180, 188, 189, 193, 200, 221, 223, 224, 236, 237, 238, 239, 240], "input_network": 253, "input_q": 171, "input_qdq": 171, "input_qtzr": 19, "input_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 162, 165, 166, 172, 175, 177, 219, 223, 224], "input_shap": [16, 17, 18, 189, 193, 194, 200, 208, 212, 214, 221, 222, 223, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250], "inputs_batch": [223, 236, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "insert": [163, 195, 201, 202, 219, 221, 223, 236, 237, 238, 239, 240, 241, 243, 244, 245, 247, 248, 249, 250, 254, 256, 258], "insid": [13, 19, 163, 166, 186, 190, 202, 222], "insight": [213, 227, 255], "inspect": 211, "instabl": 247, "instal": [183, 184, 202, 213, 235, 246, 248, 249, 250, 258], "instanc": [1, 6, 7, 11, 19, 160, 163, 164, 180, 189, 200, 205, 213, 221, 224, 255, 259], "instancenorm": 202, "instancenorm1d": 78, "instancenorm2d": 79, "instancenorm3d": 80, "instanti": [169, 171, 198, 199, 201, 202, 207, 213, 216, 219, 225, 226, 236, 239, 244, 247, 251], "instead": [163, 164, 172, 180, 202, 207, 215, 216, 221, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "instruct": [174, 184, 186, 187, 196, 201, 202, 207, 235, 253, 259], "instrument": 254, "int": [1, 2, 6, 8, 9, 11, 12, 13, 14, 15, 18, 48, 49, 50, 98, 99, 100, 150, 151, 152, 153, 154, 155, 161, 162, 165, 167, 170, 172, 173, 176, 177, 178, 179, 180, 181, 189, 190, 191, 192, 196, 198, 199, 200, 202, 205, 206, 207, 208, 212, 214, 216, 218, 219, 221, 223, 224, 236, 241, 242, 243, 244, 246, 252, 256], "int16": [7, 10, 165, 188, 191, 202, 219, 221, 222, 223, 224, 229, 230, 231, 232, 233, 234, 236, 244, 254, 255, 256, 258, 259], "int2": [10, 223], "int32": [10, 168, 172, 198, 199, 202, 216, 223], "int4": [10, 165, 189, 191, 200, 202, 222, 223, 224, 229, 230, 231, 232, 233, 234, 254, 258], "int8": [10, 148, 149, 165, 183, 188, 189, 191, 200, 219, 221, 222, 223, 224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 240, 244, 245, 248, 249, 250, 254, 255, 256, 258, 259], "int_multipli": 19, "integ": [153, 154, 161, 168, 171, 172, 180, 183, 189, 198, 202, 206, 207, 216, 218, 219, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 255, 258, 259], "integr": [14, 169, 178, 190, 191], "intel": [185, 187, 188], "intellig": 211, "intend": [183, 216, 222], "intens": 224, "inter": 203, "interact": [159, 162, 183], "intercept": 256, "interdepend": 219, "interest": [9, 170, 179, 206], "interfac": [202, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "intermedi": [2, 6, 14, 160, 167, 172, 177, 178, 180, 183, 191, 196, 202, 203, 205, 219, 223, 224, 236, 244], "intern": [14, 162, 178, 180, 191, 201, 211, 225, 245], "interpol": [202, 210, 236, 244], "interpret": 223, "intersect": 255, "introduc": [166, 190, 194, 195, 196, 202, 225, 254, 256], "invalid": [163, 207], "invoc": [241, 242, 243], "invok": [9, 10, 166, 170, 172, 179, 180, 206, 211, 213, 223, 224], "involv": [2, 162, 177, 192, 197, 198, 199, 219, 227, 236, 244, 254, 259], "io": [165, 222], "ip": 235, "ipynb": 235, "ir": 202, "irrespect": [13, 190], "is_avail": [188, 189, 190, 191, 192, 193, 194, 196, 200, 201, 222, 223, 224, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "is_bfloat16": 155, "is_float16": 155, "is_initi": [19, 150, 151, 166, 171], "is_input_quant": 225, "is_leaf_modul": 163, "is_output_quant": 225, "is_quant": 225, "is_sym": 216, "is_symmetr": [187, 216, 225], "is_train": [236, 238, 239, 240, 241, 243, 245, 247, 248, 249, 250, 251], "is_unsigned_symmetr": 162, "isinst": [161, 198, 199, 207, 218, 252], "islic": [189, 200, 221, 224, 237], "isntal": 186, "isol": [186, 202, 256], "issu": [164, 168, 192, 202, 203, 205, 213, 222, 227, 254, 259], "item": [196, 198, 199, 201, 223, 224, 239, 251, 253], "iter": [9, 10, 12, 13, 14, 18, 167, 176, 178, 185, 189, 190, 191, 196, 201, 202, 206, 208, 212, 214, 223, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 254, 255], "itertool": [189, 198, 199, 200, 221, 224, 237], "its": [9, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 155, 165, 166, 183, 184, 190, 196, 198, 199, 206, 207, 208, 215, 219, 222, 224, 225, 235, 236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 254, 256, 259], "itself": [211, 212, 239, 247, 251], "j_": [150, 151, 152, 153, 154], "j_0": [150, 151, 152, 153, 154], "j_d": [150, 151, 152, 153, 154], "jaderberg": 211, "jan": 211, "jenkin": 186, "jian": 211, "jianhua": 211, "jit": [198, 199, 226], "job": [241, 242, 243, 247, 249, 250, 253], "join": [188, 189, 193, 194, 200, 221, 223, 244, 245, 246, 252], "jointli": [249, 250], "jpeg": 252, "json": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 239, 251, 253, 254, 255, 256, 257, 258, 259, 260], "jupyt": [184, 202, 235], "just": [202, 215, 222, 236, 239, 244, 249, 250, 251, 256], "k": [171, 196, 198, 199, 201, 214, 246], "k_proj": 201, "kaim": 211, "kd": 202, "keep": [194, 225, 240, 254, 255, 259], "kei": [155, 165, 188, 196, 198, 199, 201, 216, 222, 237, 238, 240, 241, 242, 243, 245, 248, 249, 250, 254, 255], "kept": [223, 227, 254], "kera": 202, "kernel": [19, 202, 208, 212, 214, 218], "kernel_s": [163, 172, 175, 193, 194, 223, 224], "keyword": [19, 164], "kl": 183, "kldivloss": 81, "know": [19, 236, 244, 247], "knowledg": [190, 195, 196, 216, 254], "known": [164, 168, 201, 202, 207, 256], "kpi": 254, "kullback": 183, "kuzmin": 211, "kv": [202, 229, 230, 231, 234, 254], "kwarg": [10, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 164, 165, 166, 168, 172, 173, 180, 200, 202, 219, 223, 224], "l1": [173, 181, 200, 222], "l1loss": 82, "l2": 222, "lab": [183, 258], "label": [189, 191, 192, 196, 198, 199, 200, 201, 206, 221, 223, 224, 236, 237, 244, 246, 252], "labeled_data": 191, "labeled_data_load": 191, "lambda": [14, 161, 165, 167, 177, 178, 191, 196, 202, 207, 218, 219, 239], "languag": [174, 201, 254], "laptop": [183, 184], "larg": [168, 169, 194, 195, 197, 202, 209, 211, 212, 214, 224, 254, 259], "larger": [212, 214, 255], "last": [202, 222], "latenc": [185, 202, 209, 219, 220, 221, 255, 259], "later": [180, 185, 187, 188, 201, 202, 221, 223, 236, 239, 247, 251, 252], "latest": [165, 185, 187, 188, 202, 222], "launch": 235, "layer": [1, 3, 4, 6, 7, 8, 9, 11, 12, 14, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 159, 160, 161, 164, 166, 169, 170, 172, 173, 174, 176, 178, 179, 180, 181, 183, 189, 190, 191, 192, 193, 196, 198, 199, 200, 201, 202, 204, 207, 208, 209, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 228, 235, 237, 241, 242, 243, 254, 255, 256, 258, 259], "layer1": [239, 251], "layer2": [239, 251], "layer_nam": [7, 9, 167, 170, 179, 196, 206, 221], "layer_output_util": [0, 5, 158, 205], "layer_sensitivity_dict": [7, 221], "layern": [239, 251], "layernorm": [87, 202], "layeroutpututil": [6, 160, 205], "layers_to_exclud": 164, "layout": [149, 163], "lazili": 202, "lazyextractor": 202, "lceil": [150, 151, 153, 154, 155, 256], "lead": [9, 11, 200, 202, 206, 223, 227], "leaf": [163, 165, 190, 196, 202], "leakyrelu": [88, 202], "learn": [162, 180, 182, 183, 184, 185, 193, 194, 197, 198, 199, 202, 211, 224, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 252, 255, 259], "learnabl": [150, 151, 190, 195, 196, 202, 224, 254], "learnedgrid": 202, "learnedgridquant": 162, "learning_r": [241, 242, 243, 247, 249, 250], "learning_rate_schedul": [241, 242, 243, 247, 249, 250], "learnt": 196, "least": [177, 221, 237, 245], "leav": 199, "left": [150, 151, 152, 153, 154, 155, 171, 197, 207, 210, 215, 219, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 256], "leftarrow": 194, "legaci": 202, "leibler": 183, "len": [10, 12, 163, 176, 189, 191, 196, 198, 199, 201, 221, 224, 244, 246], "length": [161, 166, 207, 216, 218, 254], "leq": [150, 151, 152, 153, 154], "less": [183, 208, 210, 220, 225, 244, 256, 258], "lesser": [236, 244], "let": [163, 194, 196, 221, 223, 236, 244, 258], "level": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 161, 162, 163, 165, 167, 170, 171, 173, 174, 176, 177, 178, 179, 180, 181, 184, 189, 190, 191, 192, 193, 194, 196, 197, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 212, 213, 214, 218, 219, 220, 221, 222, 223, 224, 227, 237, 240, 241, 242, 243, 245, 249, 250, 254, 256], "leverag": 255, "lfloor": [150, 151, 152, 153, 154, 155], "libpymo": [162, 187, 202], "libpython": 202, "libqnnhtp": 253, "libqnnmodeldlc": 253, "librari": [10, 169, 183, 223, 253, 254], "lie": 218, "light": [172, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251], "lightweight": 197, "like": [162, 184, 202, 205, 206, 211, 216, 217, 218, 219, 221, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 256, 258], "limit": [196, 201, 202, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 254], "limitedbatchdataload": [196, 201], "line": [223, 253], "linear": [3, 12, 16, 19, 89, 161, 162, 164, 166, 169, 174, 176, 189, 192, 193, 194, 195, 201, 202, 207, 208, 218, 254], "linear1": [161, 207, 218], "linear10": 218, "linear_1": 207, "link": [202, 235], "linux": [186, 254], "list": [1, 2, 3, 6, 10, 11, 12, 14, 16, 17, 18, 48, 49, 50, 98, 99, 100, 153, 154, 155, 160, 161, 163, 164, 165, 166, 170, 173, 176, 177, 178, 179, 181, 189, 191, 193, 194, 196, 198, 199, 200, 201, 205, 206, 207, 208, 210, 212, 214, 216, 218, 222, 223, 225, 236, 237, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 259], "list_of_module_comp_ratio_pair": [18, 208, 212, 214], "listen": 213, "lite": [184, 193, 202, 205, 207, 216, 218, 219, 253, 254, 256, 258, 259], "lite_mp": [0, 5, 221], "litemp": [202, 255], "liter": [165, 222], "littl": [162, 211, 237, 241, 242, 243, 245, 248, 249, 250, 259], "ll": [187, 235], "llama": [174, 196, 201, 202], "llama3": 196, "llamadecoderlay": [190, 196], "llamaforcausallm": [174, 196, 201], "llamamodel": [190, 196], "llm": [202, 228, 258], "llm_configur": 202, "lm": [229, 230, 231, 232, 233, 234, 254], "lm_head": [8, 174, 201, 202, 218], "load": [10, 13, 180, 183, 184, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 211, 219, 221, 223, 236, 237, 238, 240, 252, 253, 254, 258], "load_adapt": [198, 199], "load_checkpoint": 180, "load_dataset": [189, 191, 192, 196, 198, 199, 201], "load_encod": [189, 202, 223], "load_encodings_to_sim": [202, 223], "load_model": [188, 189, 193, 194, 200, 221, 223, 236, 237, 238, 239, 240], "load_state_dict": [155, 172], "loader": [2, 9, 12, 14, 15, 165, 173, 176, 177, 178, 181, 189, 191, 192, 200, 206, 219, 221, 223, 224, 237, 238, 239, 240, 241, 243, 245, 247, 248, 249, 250, 251], "local": [200, 202, 213, 221, 223, 235, 239, 252], "localresponsenorm": 90, "locat": [201, 221, 223, 236, 239, 244, 247, 251, 259], "log": [164, 165, 206, 222], "log_2": 155, "log_fil": [165, 222], "log_input": 111, "log_prob": 37, "logger": 164, "logic": [2, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 166, 177, 202, 219, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251], "logit": [198, 199, 224, 244, 246, 252], "logsigmoid": 91, "logsoftmax": 92, "long": [196, 198, 201, 254], "longer": [162, 172, 207, 216, 223, 224, 241, 242, 243, 254, 258], "look": [184, 216, 218, 236, 239, 244, 251, 253], "loop": [163, 224, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "lora": [167, 169, 183, 196, 202], "lora_a": [198, 199], "lora_a_lay": 199, "lora_add_lay": 199, "lora_alpha": 169, "lora_b": [198, 199], "lora_b_lay": 199, "lora_config": 169, "lora_dropout": 169, "lora_mul_lay": 199, "loraconfig": 169, "loralay": [198, 199], "lose": 215, "loss": [9, 11, 12, 147, 170, 173, 176, 179, 181, 184, 189, 192, 198, 199, 200, 202, 211, 220, 224, 237, 238, 240, 245, 248, 249, 250, 252, 256], "loss_fn": [173, 181, 192, 198, 199, 200, 224], "lost": [252, 256, 259], "lot": 255, "low": [8, 162, 171, 183, 195, 197, 202, 207, 211, 216, 223, 241, 243, 254], "lower": [11, 12, 176, 183, 189, 194, 200, 210, 218, 219, 220, 224, 227, 236, 244, 255, 258], "lowest": [208, 255], "lpai": 202, "lpbq": [8, 161, 202, 207, 216, 229, 230, 231, 232, 233, 234, 254], "lpbq_seqms": 254, "lpbqencod": 202, "lppool1d": 83, "lppool2d": 84, "lr": [192, 202, 224, 252], "lssf": 186, "lstm": [85, 202], "lstmcell": 86, "lsvrc": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "lt": 187, "lwc": 196, "m": [186, 187, 214, 235, 254], "mac": [18, 208, 209, 211, 212, 214, 217, 219, 236, 241, 242, 243, 244], "machin": [182, 183, 197, 202, 211, 254, 259], "maco": 186, "made": [163, 165, 183, 202, 222, 244, 245, 248, 249, 250, 252], "magnitud": 208, "mai": [2, 11, 14, 19, 147, 148, 149, 159, 162, 163, 166, 177, 178, 189, 191, 194, 200, 201, 202, 204, 206, 207, 211, 216, 219, 221, 223, 224, 236, 237, 238, 239, 244, 247, 248, 251, 255, 259], "main": [192, 224, 225], "maintain": [183, 191, 202, 210, 211, 245, 254, 255], "major": [211, 216], "make": [165, 166, 169, 183, 194, 202, 219, 222, 226, 247, 254, 258, 259], "make_dummy_input": 188, "make_psnr_eval_fn": 221, "makedir": [236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "manag": [10, 187, 202, 258], "mani": [163, 202, 209, 236, 239, 241, 242, 243, 244, 251], "manner": [14, 178, 191, 194], "mantissa": [155, 161, 207], "mantissa_bit": [155, 161, 207], "manual": [18, 150, 151, 162, 165, 187, 191, 202, 208, 211, 212, 214, 216, 241, 242, 243, 255], "manual_se": [244, 252], "manualmodeparam": [18, 208, 212, 214], "manylinux_2_34_x86_64": 187, "map": [19, 148, 153, 154, 164, 166, 183, 196, 198, 199, 201, 202, 206, 210, 216, 223, 225, 256], "marginrankingloss": 94, "mark": 201, "marku": 211, "mask": 19, "maskedadd": 19, "match": [155, 172, 180, 202, 206, 207, 208, 211, 215, 223, 224, 225, 227], "math": [191, 221, 259], "mathemat": [171, 183, 194, 201, 226, 236, 244], "matmul": [8, 202, 218], "matmul_8": 164, "matric": [174, 198, 201], "matrix": 255, "matter": [19, 239, 251, 254], "max": [9, 150, 151, 155, 159, 162, 170, 179, 187, 200, 202, 203, 204, 211, 216, 223, 240, 253, 258], "max_batch": 221, "max_epoch": [241, 242, 243, 247, 249, 250], "max_iter": [196, 198, 199], "maximum": [2, 14, 153, 154, 155, 177, 178, 191, 202, 219, 236, 239, 244, 251, 256], "maxpool1d": 95, "maxpool2d": 96, "maxpool3d": 97, "maxunpool1d": 98, "maxunpool2d": 99, "maxunpool3d": 100, "mdoel": 192, "mean": [15, 166, 189, 191, 192, 200, 203, 221, 223, 225, 236, 237, 239, 241, 242, 243, 244, 246, 247, 248, 251, 252, 254, 255, 256], "measur": [2, 9, 18, 170, 177, 179, 183, 206, 208, 212, 214, 219, 221, 223, 236, 241, 242, 243, 244, 254, 255, 259], "mechan": [163, 171, 189, 202], "meet": [14, 177, 178, 191, 209, 210, 219, 236, 244, 254, 255, 259], "member": 225, "memori": [18, 149, 169, 183, 184, 202, 208, 209, 211, 212, 214, 215, 217, 220, 224, 241, 242, 243, 254, 255, 259], "memory_format": 149, "merg": [174, 183, 201], "met": [2, 177, 219, 236, 244], "meta": [174, 196, 201], "metadata": [167, 196], "metapackag": 235, "method": [6, 10, 13, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 160, 162, 163, 165, 166, 172, 174, 177, 178, 180, 183, 187, 190, 191, 194, 195, 198, 201, 205, 210, 211, 219, 220, 222, 223, 224, 227, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 255, 256], "methodologi": 259, "metric": [18, 189, 202, 206, 208, 212, 214, 219, 236, 238, 239, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 254, 255, 256, 259], "mha": 202, "middl": 259, "might": [2, 177, 194, 202, 206, 216, 219, 227, 236, 237, 239, 241, 242, 243, 244, 245, 248, 249, 250, 251, 255, 259], "migrat": [0, 158, 202, 223], "mimic": 258, "mimick": 162, "min": [9, 150, 151, 155, 159, 162, 170, 179, 187, 201, 202, 203, 204, 216, 240, 253, 258], "min_max": [10, 189, 202, 221, 223, 236, 237, 240], "min_max_rang": [206, 239, 251], "minim": [173, 181, 183, 184, 194, 195, 200, 217, 219, 220, 236, 240, 244, 249, 250, 254, 255, 256], "minima": 200, "minimum": [153, 154, 163, 190, 196, 200, 202, 239, 251, 254, 256], "minor": [202, 216, 224], "miou": [236, 244, 255], "mish": 101, "misidentif": 202, "mismatch": [171, 203, 205, 223], "miss": [155, 164, 202, 223, 253], "missing_kei": 155, "mistral": [201, 202], "mistralforcausallm": 201, "mitig": 254, "mix": [2, 14, 165, 177, 178, 185, 191, 202, 235, 254, 259], "mixed_precis": [0, 5, 158, 219, 222, 236, 244], "mixed_precision_algo": [2, 165, 177, 219, 236, 244], "mixedprecisionconfigur": [165, 222], "mixin": [19, 166], "mkdir": 235, "ml": [184, 211, 240, 249, 250], "mlp": [174, 201], "mm": [229, 230, 231, 232, 233, 234, 254], "mmlu": [229, 230, 231, 232, 233, 234, 254], "mmp": [202, 220], "mmp_log": [165, 222], "mnt": [186, 239], "mobil": [183, 184], "mobilenet": [183, 189, 192], "mobilenet_v2": [188, 189, 191, 192, 193, 194, 200, 221, 222, 223, 224], "mobilenet_v2_weight": [189, 193, 194, 200, 221, 223], "mobilenetv2": [189, 191, 193, 194, 221, 223], "mode": [18, 172, 178, 180, 202, 208, 212, 214, 223, 224, 225, 241, 242, 243], "model": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 252, 253, 254, 256, 257, 258, 260], "model_config": [196, 201], "model_id": [196, 198, 199, 201], "model_input": 164, "model_or_pipelin": 189, "model_prepar": [0, 158, 202, 244, 245, 247, 249, 250, 251], "model_prepare_requir": [14, 178, 191], "model_qdq": 10, "model_transform": 163, "model_valid": [0, 158], "modelcompressor": [18, 208, 212, 214, 241, 242, 243], "modeling_llama": [196, 201], "modeling_opt": [198, 199], "modelprepar": [163, 244, 245, 249, 250, 251], "modelproto": [3, 4, 6, 9, 10, 193, 194, 205, 206, 223], "modelvalid": 164, "modelwithconsecutivelinearblock": 190, "modelwithlinear": 190, "modelwithnontorchfunct": 163, "modif": [244, 245, 248, 249, 250], "modifi": [13, 163, 172, 174, 180, 189, 190, 193, 194, 198, 199, 200, 201, 202, 215, 221, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 256, 258, 259], "modul": [2, 12, 13, 14, 15, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 159, 160, 161, 163, 164, 165, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 212, 214, 215, 218, 219, 222, 223, 224, 226, 236, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259], "module_cl": [19, 166], "module_classes_to_exclud": 163, "module_nam": [198, 199], "module_to_exclud": 163, "modulecompratiopair": [18, 208, 212, 214], "moduledict": [19, 162, 166, 172, 175, 223, 224], "modulelist": [19, 162, 166, 172, 175, 223, 224], "modules_to_exclud": [14, 163, 173, 178, 181, 191, 200], "modules_to_ignor": [18, 170, 179, 206, 208, 212, 214, 241, 242, 243], "momentum": [192, 193, 194], "monitor": 206, "monoton": [18, 208, 210, 212, 214], "more": [12, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 162, 164, 166, 168, 169, 172, 176, 177, 180, 183, 189, 194, 202, 206, 208, 210, 211, 212, 213, 214, 219, 220, 222, 224, 225, 227, 236, 239, 244, 247, 251, 252, 253, 254, 255, 256, 258, 259], "most": [171, 183, 187, 191, 220, 221, 252, 255, 256], "move": [219, 235], "movement": 202, "mp": [165, 220, 221, 222], "mp_configur": 222, "mse": [9, 11, 162, 170, 173, 179, 181, 202, 206, 224, 254, 256], "mseloss": 93, "much": [215, 241, 242, 243, 254, 259], "mul_scal": 199, "multi": 202, "multigpu": 202, "multiheadattent": 202, "multilabelmarginloss": 102, "multilabelsoftmarginloss": 103, "multimarginloss": 104, "multipl": [14, 16, 17, 18, 149, 160, 163, 164, 165, 166, 169, 172, 178, 180, 191, 193, 194, 202, 205, 207, 208, 211, 212, 214, 218, 222, 223, 224, 239, 251, 255, 256], "multipli": [166, 209, 211, 212, 214, 217, 236, 244], "must": [149, 155, 161, 164, 166, 174, 189, 190, 191, 192, 196, 200, 201, 206, 207, 209, 215, 218, 222, 223, 225, 237, 238, 239, 240, 241, 242, 243, 245, 248, 249, 250, 251, 258], "mutual": [155, 161, 207], "mymodel": 201, "mymodul": [19, 166], "n": [9, 170, 171, 179, 206, 207, 214, 224, 229, 230, 231, 232, 233, 234, 252, 254], "n01440764": 252, "n01443537": 252, "n_imag": 252, "n_iter": 252, "nagel": 211, "name": [1, 2, 8, 9, 11, 19, 157, 159, 160, 165, 166, 169, 170, 172, 177, 179, 180, 189, 193, 194, 196, 198, 199, 200, 201, 202, 204, 205, 206, 213, 216, 218, 219, 221, 222, 223, 224, 236, 237, 238, 239, 240, 251, 253, 256], "name_": [9, 170, 179, 206, 239, 251], "name_to_quantizer_dict": [2, 165, 177, 219], "named_modul": [198, 199], "named_paramet": 198, "named_qmodul": 172, "named_quant": 172, "named_quantizer_paramet": 172, "namedtupl": 155, "namespac": [158, 162, 176, 177, 178, 179, 180, 181], "naming_schem": [160, 205], "namingschem": [160, 205], "nan": 202, "nativ": [166, 202], "navig": 235, "na\u00efv": 195, "nconv": 193, "ndarrai": [1, 2, 6, 9, 10, 11, 189, 191, 200, 205, 206, 219, 223, 237], "nearest": [178, 180, 189, 237, 245], "necessari": [147, 148, 149, 172, 186, 201, 213, 223, 224, 235, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 259], "necessarili": [236, 244], "need": [9, 10, 12, 13, 18, 160, 165, 170, 176, 179, 183, 187, 189, 190, 192, 193, 194, 196, 200, 201, 202, 205, 206, 208, 212, 214, 216, 219, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259], "neg": [137, 138, 153, 154, 202, 224, 236, 244], "negat": 245, "nest": 202, "net": [221, 223, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253], "network": [166, 183, 202, 209, 210, 211, 213, 224, 227, 246, 254, 256], "neural": [183, 209, 211, 224, 227, 236, 244, 246, 255, 256], "neuron": 183, "new": [149, 150, 151, 162, 163, 172, 180, 184, 197, 198, 202, 207, 219, 222, 223, 224, 226, 244, 251], "new_empti": 149, "next": [194, 201, 202, 221, 223, 227, 236, 238, 244, 247, 252, 256], "next_conv_weight": 194, "night": [237, 238, 239, 240, 245, 248, 249, 250, 251], "nllloss": 105, "nllloss2d": 106, "nmodel": [193, 194], "nn": [0, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 158, 159, 161, 162, 163, 164, 165, 168, 172, 174, 189, 190, 191, 192, 198, 199, 200, 201, 202, 204, 207, 218, 222, 223, 224, 226, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 259], "nncf": 183, "nnext": 194, "no_grad": [172, 188, 189, 190, 198, 199, 200, 202, 223, 224, 244, 245, 246, 248, 249, 250, 251, 252], "node": [1, 8, 10, 11, 14, 163, 172, 177, 178, 180, 183, 189, 191, 193, 194, 200, 202, 218, 223, 224, 226, 236, 237, 238, 240, 244, 245, 248, 249, 250, 255, 258, 259], "node_names_to_optim": [1, 189], "nodes_to_exclud": [8, 11, 200, 218], "nodes_to_includ": [1, 8, 189, 218], "noffset": 171, "nois": [194, 206, 217, 221, 224, 225, 258], "noisi": 247, "non": [10, 163, 165, 169, 173, 181, 190, 196, 200, 202, 213, 223, 246, 253, 254, 256, 258], "none": [1, 2, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 48, 49, 50, 60, 68, 69, 73, 74, 85, 86, 89, 98, 99, 100, 112, 113, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 152, 153, 154, 155, 159, 160, 161, 162, 163, 165, 166, 168, 169, 170, 171, 172, 175, 176, 177, 178, 179, 180, 181, 187, 188, 189, 190, 191, 192, 193, 194, 200, 202, 204, 205, 206, 207, 208, 212, 213, 214, 218, 219, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "norm": [194, 202, 206], "normal": [174, 183, 189, 191, 193, 200, 201, 202, 206, 221, 223, 237, 246, 252], "notabl": 216, "note": [2, 9, 13, 14, 162, 170, 171, 172, 173, 177, 178, 179, 180, 181, 187, 190, 191, 194, 200, 206, 207, 213, 219, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 255], "note1": [236, 239, 244, 247, 251], "note2": [236, 239, 244, 247, 251], "notebook": [202, 228, 237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250, 252], "noth": [18, 208, 212, 214, 259], "notic": [159, 202, 204], "notimplementederror": 19, "now": [163, 164, 171, 172, 188, 189, 193, 194, 202, 216, 223, 224, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 254], "np": [1, 2, 10, 11, 187, 189, 191, 200, 219, 221, 223, 237, 239, 252], "nprev": 194, "npu": 252, "nscale": 171, "nullptr": 202, "num": [153, 154, 254], "num_batch": [9, 12, 15, 173, 176, 181, 189, 190, 191, 192, 200, 201, 206, 223, 229, 230, 231, 232, 233, 234, 237, 238, 239, 245, 246, 247, 248, 251, 254], "num_calibration_sampl": [189, 200, 221, 223, 237], "num_candid": [11, 173, 181, 200], "num_channel": 216, "num_comp_ratio_candid": [18, 208, 212, 214, 241, 242, 243], "num_epoch": [192, 202, 224], "num_eval_sampl": [189, 237], "num_iter": [1, 13, 167, 189, 190, 196, 202, 229, 230, 231, 232, 233, 234, 254], "num_of_sampl": 191, "num_reconstruction_sampl": [18, 208, 241, 243], "num_sampl": [2, 165, 177, 190, 219, 221, 244, 246], "num_samples_for_phase_1": [14, 178, 191], "num_samples_for_phase_2": [14, 178, 191], "num_step": [153, 154], "num_val_samples_per_class": 239, "num_work": [189, 191, 192, 200, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "number": [1, 2, 9, 11, 12, 13, 14, 15, 18, 19, 153, 154, 155, 161, 163, 165, 166, 167, 173, 176, 177, 178, 180, 181, 183, 189, 190, 191, 192, 196, 197, 200, 201, 202, 206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 218, 219, 221, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 254, 256, 258, 259], "numer": [160, 202, 205, 255], "numpi": [168, 187, 188, 189, 191, 200, 202, 221, 223, 236, 237, 238, 239, 240, 252], "numpy_help": [193, 194], "nvidia": [183, 185, 187, 188, 223], "o": [187, 188, 189, 193, 194, 200, 202, 221, 223, 236, 237, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252], "o_proj": 201, "object": [1, 2, 7, 9, 10, 13, 14, 19, 147, 148, 149, 155, 159, 161, 164, 165, 166, 167, 169, 170, 171, 172, 173, 177, 178, 179, 180, 181, 189, 190, 191, 196, 200, 201, 202, 203, 204, 206, 207, 218, 219, 220, 221, 222, 223, 224, 236, 239, 244, 245, 246, 248, 249, 250, 251, 254, 255, 256, 258, 259], "observ": [10, 19, 155, 166, 172, 202, 206, 211, 221, 223, 224, 247, 252, 255, 256], "obtain": [6, 160, 206, 209, 216, 253], "occur": [159, 201, 204, 216, 219], "occurr": [9, 170, 179, 206, 208], "oct": 211, "off": [12, 176, 186, 189, 192, 219, 223, 236, 244, 254, 259], "offer": [9, 170, 179, 191, 200, 203, 206, 207, 220, 246, 254, 255], "offset": [9, 19, 60, 150, 151, 152, 153, 154, 170, 171, 179, 187, 202, 206, 216, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 253, 256, 258], "offset_": [150, 151, 152, 153, 154], "often": [189, 191, 193, 194, 211, 224, 255], "old_weight": [174, 201], "older": [187, 222], "omit": [210, 225], "omniqu": [0, 158, 190, 202], "onc": [19, 149, 164, 171, 202, 208, 211, 222, 226, 240, 241, 242, 243, 248, 249, 250, 253, 254, 258, 259], "one": [10, 12, 14, 16, 163, 164, 165, 171, 172, 176, 177, 178, 180, 187, 189, 190, 191, 193, 196, 198, 199, 202, 207, 208, 211, 212, 213, 214, 216, 218, 219, 221, 222, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 258], "ones": [196, 201, 255, 256], "ones_lik": [150, 151], "onli": [2, 8, 9, 10, 147, 148, 149, 153, 154, 155, 159, 161, 162, 163, 165, 166, 170, 171, 172, 177, 179, 180, 185, 186, 187, 189, 193, 197, 198, 199, 200, 201, 202, 204, 206, 207, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 254, 255, 256, 259], "onnx": [0, 1, 2, 3, 5, 6, 8, 9, 10, 14, 158, 160, 165, 172, 178, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 200, 202, 204, 205, 206, 207, 216, 218, 219, 221, 222, 223, 224, 225, 226, 229, 230, 231, 232, 233, 234, 235, 239, 253, 254, 255, 256, 258, 259], "onnx_data": [189, 237], "onnx_data_gener": 223, "onnx_encoding_path": 223, "onnx_export_arg": [14, 160, 172, 178, 180, 191, 205, 223, 224], "onnx_file_nam": 226, "onnx_lpbq": 254, "onnx_model": [188, 221, 239], "onnx_model_path": 239, "onnx_output": 168, "onnx_pcq": 254, "onnx_qdq": 10, "onnxexportapiarg": [14, 160, 172, 178, 180, 191, 205, 223, 224], "onnxmodel": [9, 206], "onnxruntim": [10, 168, 186, 187, 188, 189, 200, 202, 221, 223, 236, 237, 238, 240], "onnxsim": [189, 193, 194, 221, 223, 236, 237, 238, 240], "onto": 258, "op": [2, 10, 12, 14, 164, 172, 176, 177, 178, 180, 189, 191, 201, 202, 219, 223, 224, 225, 236, 237, 238, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 253, 256], "op_typ": [2, 8, 10, 177, 218, 219, 225, 236, 244], "op_type_map": 164, "open": 183, "opencv": 202, "oper": [8, 19, 156, 157, 162, 163, 164, 166, 183, 188, 190, 194, 195, 196, 200, 201, 202, 205, 218, 219, 221, 223, 225, 226, 227, 236, 240, 244, 249, 250, 254, 256, 258, 259], "oppos": [238, 248], "opset": 202, "opset_vers": [168, 172, 180, 223, 224], "opt": [198, 199], "optim": [1, 2, 11, 12, 13, 14, 18, 155, 167, 173, 176, 177, 178, 181, 182, 183, 184, 185, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 208, 211, 213, 217, 219, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 256, 258, 259], "optimized_accuraci": [191, 246], "optimum": 224, "option": [2, 9, 10, 11, 12, 13, 14, 15, 18, 19, 150, 151, 152, 153, 154, 155, 161, 163, 165, 168, 169, 170, 172, 173, 176, 177, 178, 179, 180, 181, 187, 188, 189, 190, 191, 192, 200, 202, 206, 207, 208, 212, 214, 219, 221, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 255, 256], "optlearnedpositionalembed": [198, 199], "optmiz": [201, 236, 244], "orang": 215, "order": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 166, 202, 203, 208, 222, 225, 226, 237, 239, 241, 242, 243, 245, 247, 249, 250, 251, 253, 258], "org": [163, 190, 196, 201, 202, 221, 223, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "organ": [183, 252, 257], "origin": [18, 19, 162, 163, 166, 181, 183, 195, 200, 201, 202, 205, 206, 208, 209, 210, 212, 213, 214, 221, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 254, 255, 256, 259], "ort": [10, 168, 189, 200, 202, 223, 236, 237, 238, 240], "ort_disable_al": 168, "orthogon": [174, 201], "oscil": 192, "other": [10, 149, 161, 162, 163, 165, 184, 186, 187, 188, 190, 196, 197, 198, 199, 201, 202, 207, 210, 218, 222, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 257], "otherwis": [150, 151, 153, 154, 161, 164, 172, 180, 194, 207, 216, 218, 223, 224, 227, 237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250, 259], "our": [169, 247, 255], "out": [9, 10, 150, 151, 152, 153, 154, 155, 163, 170, 171, 172, 179, 180, 187, 188, 202, 206, 223, 224, 228, 254], "out1": 222, "out2": 222, "out3": 222, "out_": [150, 151, 152, 153, 154], "out_channel": [161, 207, 218], "out_featur": [19, 162, 166, 172], "outlier": [195, 201, 206, 254, 256], "outlin": [187, 197, 209, 255], "output": [2, 6, 7, 8, 9, 11, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 153, 155, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171, 172, 173, 175, 177, 178, 179, 180, 181, 183, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 200, 202, 206, 207, 208, 211, 212, 214, 215, 216, 218, 219, 221, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 254, 256], "output_bw": [14, 178, 191], "output_dir": 253, "output_dir_path": 253, "output_dlc": 253, "output_encod": 19, "output_nam": [168, 172, 180, 189, 193, 200, 221, 223, 224, 236, 237, 238, 239, 240], "output_path": [167, 196, 253], "output_qtzr": 19, "output_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 162, 165, 166, 172, 175, 177, 219, 223, 224], "output_s": [48, 49, 50, 98, 99, 100], "outsid": [201, 225], "over": [9, 14, 153, 154, 166, 171, 178, 191, 195, 202, 206, 210, 211, 218, 219, 254, 255, 256], "overal": [194, 209, 210, 219, 227], "overfit": 200, "overflow": 202, "overhead": [241, 243, 244, 256], "overlin": [151, 154], "overload": [8, 10, 153, 154, 172, 218, 223, 224], "overrid": [14, 149, 163, 172, 178, 180, 191, 222, 223, 224, 244, 245, 248, 249, 250], "overridden": [19, 166, 225], "override_precis": [7, 221], "overtax": 259, "overview": [162, 217, 255, 257], "overwri": 223, "overwriiten": 223, "overwrit": 252, "overwritten": [162, 199, 223], "own": [191, 201, 258], "p": [221, 223, 236, 239, 244], "p1": 222, "p2": 222, "pack": 228, "packag": [0, 158, 185, 202, 219, 235, 254], "pad": [163, 172, 193, 194, 202, 223, 224], "page": [184, 186, 187, 188, 209, 256, 259], "pair": [3, 16, 18, 193, 201, 208, 212, 214], "pairwis": 196, "pairwisedist": 108, "paper": 201, "param": [2, 12, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 164, 165, 166, 170, 172, 173, 176, 177, 179, 181, 189, 198, 200, 202, 206, 207, 208, 212, 214, 216, 219, 222, 236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 258], "param_bitwidth": 216, "param_bw": [14, 178, 191], "param_bw_override_list": [12, 176, 189], "param_encod": [162, 216], "param_nam": [9, 170, 179, 206], "param_name_": [9, 170, 179, 206, 239, 251], "param_quant": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 162, 166, 172, 175, 207, 223, 224], "param_typ": [10, 188, 189, 200, 221, 223, 236, 237, 238, 240], "paramet": [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 211, 212, 214, 216, 217, 218, 219, 220, 221, 222, 223, 225, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 258, 259], "parameter": [10, 223], "parameter_quant": [2, 165, 177, 219], "parent": [19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 166, 202, 239], "pareto": [2, 14, 177, 178, 191, 236, 244], "pareto_front": 191, "pareto_front_list": [236, 244], "pars": [12, 172, 176, 180, 189, 223, 224], "part": [9, 170, 179, 206, 211, 223, 258], "parti": 10, "partial": [10, 14, 163, 178, 191, 202, 223], "particular": [161, 164, 207, 218, 219, 225, 236, 244], "pass": [2, 9, 10, 12, 13, 14, 15, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 155, 160, 162, 163, 164, 165, 166, 170, 172, 173, 176, 177, 178, 179, 180, 181, 183, 189, 190, 191, 192, 198, 199, 200, 202, 205, 206, 207, 213, 219, 221, 222, 223, 224, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 255, 256, 259], "pass_calibration_data": [223, 224, 236, 238, 240, 244, 245, 247, 248, 249, 250, 251], "past": [202, 235], "patch": 216, "path": [2, 9, 10, 12, 14, 18, 159, 163, 165, 167, 170, 172, 176, 177, 178, 179, 180, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 204, 206, 208, 212, 214, 219, 221, 222, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254], "path_to_aimet_repo": 239, "path_to_imagenet": [221, 223, 224], "pathlik": 223, "pattern": [202, 211], "pbar": 252, "pcq": [9, 170, 179, 206, 229, 230, 231, 232, 233, 234, 254], "pcq_spinquant_adascal": 254, "pdf": [9, 170, 179, 201, 202, 206], "peak": [202, 221, 254], "peft": [0, 158, 167, 196, 198, 199, 202], "peft_model": 169, "peft_model_id": [198, 199], "peftmixedmodel": 169, "penalti": 219, "pend": [13, 190], "pendyam": 211, "per": [8, 9, 18, 155, 160, 161, 166, 170, 179, 183, 192, 193, 195, 202, 203, 205, 207, 208, 212, 213, 214, 216, 217, 218, 219, 221, 225, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 254, 255, 256], "per_block": 216, "per_block_int_scal": 216, "per_channel": 216, "per_channel_quant": [166, 216, 225], "per_layer_mse_loss": [206, 239, 251], "per_layer_quant_dis": [206, 239, 251], "per_layer_quant_en": [206, 239, 251], "per_sample_weight": 60, "per_tensor": 216, "percent_to_flip": [7, 221], "percentag": [7, 183, 202, 220, 221, 237, 238, 239, 240, 245, 248, 249, 250, 251, 255], "perform": [2, 3, 4, 9, 12, 14, 15, 17, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 150, 151, 155, 166, 169, 170, 173, 176, 177, 178, 179, 181, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 206, 208, 209, 210, 211, 216, 217, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 251, 252, 256, 257, 258, 259], "perform_per_layer_analysis_by_disabling_quant": [9, 206], "perform_per_layer_analysis_by_disabling_quant_wrapp": [170, 179, 206], "perform_per_layer_analysis_by_enabling_quant": [9, 206], "perform_per_layer_analysis_by_enabling_quant_wrapp": [170, 179, 206], "perhap": [236, 239, 244, 247, 251], "period": [12, 176, 189], "perman": [157, 202], "perplex": 254, "persist": 155, "person": 183, "perspect": [190, 196, 236, 244], "pg": 202, "phase": [2, 14, 165, 177, 178, 191, 211, 236, 244], "phase1": [2, 177, 219, 236, 244], "phase1_optim": [2, 177, 219, 236, 244], "phase2": 177, "phase2_revers": 177, "phi": 202, "phi3": [174, 201], "phone": [183, 184], "php": [235, 236, 239, 244, 247, 251], "pick": [2, 177, 209, 210, 219, 236, 244], "pickl": [18, 208, 212, 214], "pictur": [237, 238, 239, 240, 245, 248, 249, 250, 251], "piec": 163, "pin": [186, 202], "pin_memori": [149, 163, 244], "pink": 215, "pinpoint": 206, "pip": [187, 188, 202, 213, 235], "pipelin": [14, 178, 191, 202, 227, 256, 258], "pitr": 211, "pixelshuffl": 109, "pixelunshuffl": 110, "place": [2, 4, 12, 13, 17, 172, 174, 176, 177, 180, 189, 190, 194, 201, 202, 219, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "place_model": [198, 199], "placehold": [198, 199], "placement": [201, 202, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 254], "plan": [187, 202, 254, 258], "platform": [182, 183, 186, 187, 206, 237, 238, 240, 245, 248, 249, 250], "pleas": [162, 164, 169, 172, 180, 187, 202, 223, 224, 236, 239, 240, 244, 247, 251, 252, 253, 254], "plot": [2, 159, 177, 202, 204, 206, 219, 236, 239, 244, 251], "pmatrix": [150, 151, 152, 153, 154], "point": [2, 9, 19, 147, 148, 155, 161, 162, 168, 170, 177, 179, 180, 183, 193, 194, 195, 201, 202, 206, 207, 216, 219, 221, 223, 224, 227, 236, 238, 241, 242, 243, 244, 245, 247, 248, 249, 250, 252, 254, 255, 256, 258, 259], "pointer": [239, 251], "poissonnllloss": 111, "polish": 202, "poor": 255, "popul": 216, "popular": 197, "port": [202, 213], "portabl": 183, "portion": 194, "posit": [137, 138, 153, 154, 202], "possibl": [2, 3, 14, 164, 172, 177, 178, 180, 191, 193, 206, 210, 219, 223, 224, 226, 227, 236, 244, 246, 259], "post": [14, 178, 183, 185, 188, 191, 194, 202, 206, 211, 216, 218, 224, 237, 238, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 254, 256, 259], "post_training_tf": [9, 12, 159, 170, 176, 179, 189, 202, 204, 206, 216, 236, 244, 247, 248], "post_training_tf_enhanc": [9, 12, 14, 170, 172, 176, 178, 179, 189, 191, 206, 216, 223, 224, 236, 238, 239, 244, 245, 247, 248, 249, 251], "potenti": [213, 247], "power": [8, 183, 202, 207, 216, 220, 227, 254, 255], "pp": 211, "ppl": [229, 230, 231, 232, 233, 234, 254], "practic": [207, 211, 219, 236, 239, 241, 242, 243, 244, 246, 251], "pre": [187, 190, 193, 196, 200, 201, 202, 203, 205, 206, 208, 211, 217, 221, 223, 235, 237], "preced": [192, 195, 215, 222], "precis": [2, 7, 14, 150, 151, 152, 153, 154, 165, 177, 178, 183, 185, 191, 202, 216, 224, 227, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 240, 245, 247, 248, 249, 250, 251, 254], "precomput": [198, 223], "precursor": 193, "pred": 244, "pred_label": [189, 191, 200, 221, 223, 237], "pred_prob": [189, 191, 221, 223, 237], "predefin": 246, "predict": [183, 202, 223], "prefer": [162, 187, 235, 258], "prefix": [12, 166, 172, 176, 180, 189, 223, 224], "prelu": [107, 202], "prepar": [13, 14, 163, 169, 178, 190, 191, 193, 201, 202, 226, 254, 257, 259], "prepare_model": [163, 244, 245, 247, 249, 250, 251], "prepared_model": 163, "prepend": [166, 235], "preprocess": [189, 191, 200, 237, 252], "prequantize_const": [10, 168], "prerequisit": 213, "presenc": 224, "present": [7, 155, 160, 162, 164, 169, 205, 216, 221, 247, 254], "preserv": [155, 163, 189, 202, 219, 221], "preserve_format": 149, "pretrain": [189, 191, 192, 193, 194, 200, 222, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 254, 256], "prev": 194, "prev_conv_weight": 194, "prevent": [162, 163, 199, 202, 208], "previou": [2, 18, 171, 177, 183, 208, 209, 212, 214, 219, 227, 236, 244, 253, 254, 255], "primarili": 254, "print": [19, 153, 154, 162, 163, 164, 166, 171, 172, 175, 187, 188, 189, 191, 193, 194, 200, 206, 221, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252], "prior": [2, 177, 202, 219, 236, 244, 245, 248, 249, 250], "priorit": 254, "prioriti": [202, 254], "privileg": 235, "probabl": [183, 202], "problem": [163, 202, 227, 259], "problemat": [163, 227], "proce": [14, 178, 191, 193, 223, 255, 259], "procedur": [202, 210, 213, 238, 241, 243, 248], "proceed": [193, 194, 236, 238, 240], "process": [183, 191, 202, 208, 210, 211, 219, 222, 223, 236, 237, 238, 240, 244, 245, 248, 249, 250, 254, 255, 256, 258], "processor": [183, 185, 187, 188], "produc": [14, 147, 148, 159, 163, 178, 183, 191, 194, 202, 203, 204, 206, 210, 216, 219, 236, 239, 244, 251, 256], "product": [183, 184, 202, 209], "profil": [165, 185, 219, 221, 253, 254, 255], "progress": [202, 213], "project": 183, "promot": [202, 219], "prone": [191, 222], "pronounc": 220, "propag": [202, 215, 222], "propagate_encod": [14, 172, 178, 180, 191, 223, 224], "proper": [201, 202, 252], "properli": [150, 151, 202], "properti": [155, 162, 172], "provid": [0, 2, 5, 9, 10, 12, 14, 19, 155, 158, 162, 164, 169, 170, 172, 176, 177, 178, 179, 180, 183, 185, 186, 187, 189, 191, 198, 200, 202, 206, 207, 209, 210, 211, 213, 216, 219, 221, 222, 223, 224, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259], "proxi": 163, "prune": [18, 183, 202, 209, 210, 212, 214, 215, 217, 235, 242], "psnr": [202, 221], "psnr_eval_fn": 221, "pt_model": [189, 193, 194, 200, 221, 223, 236, 237, 238, 240], "pth": [172, 180, 223, 224], "ptq": [14, 178, 183, 184, 185, 190, 191, 194, 195, 196, 201, 202, 206, 211, 223, 224, 246, 254, 256, 258, 259], "public": [0, 158, 202, 223], "publish": 202, "pure": 226, "purpos": [183, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "put": [177, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "py310": 187, "pylint": 201, "pypi": [188, 202], "pyproject": 186, "pytest": 186, "python": [183, 185, 187, 188, 202, 252, 254], "python3": [186, 187, 202, 235], "pythonpath": 235, "pytorch": [0, 14, 158, 160, 163, 164, 166, 171, 172, 173, 178, 180, 181, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 212, 214, 218, 219, 221, 222, 223, 224, 225, 235, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 259], "q": [19, 147, 148, 149, 150, 151, 153, 154, 155, 162, 166, 171, 198, 199, 256], "q_": 256, "q_modul": 162, "q_output": 19, "q_proj": 201, "qadd": 166, "qairt": [202, 253, 254], "qat": [12, 176, 183, 185, 189, 192, 198, 199, 202, 227, 235, 236, 240, 244, 254, 256, 258, 259], "qat2": 202, "qc": 202, "qc_op": 10, "qc_quantize_op": 202, "qcquantizeop": [2, 10, 202, 219], "qcquantizewrapp": [162, 202], "qdo": 183, "qdq": [10, 151, 155, 198, 202, 255, 258, 259], "qlinear": [19, 162, 166], "qmax": [150, 151, 153, 154, 171, 172, 175, 223, 224], "qmin": [150, 151, 153, 154, 171, 172, 175, 223, 224], "qmodul": [162, 172], "qmul": 166, "qmult": 19, "qnn": [185, 202, 253, 258], "qol": 202, "qsim": [13, 190], "qtype": [7, 10, 221, 223], "qtzr": [171, 198], "quad": [150, 151, 152, 153, 154], "qualcomm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 252, 254, 255, 256, 257, 258, 259, 260], "qualiti": [236, 244, 254], "quant": [2, 8, 9, 10, 12, 14, 170, 172, 173, 176, 178, 179, 180, 181, 189, 191, 200, 202, 206, 218, 219, 223, 224, 235, 236, 237, 244, 245, 246, 247, 248, 249, 250], "quant_analyz": [0, 5, 158, 162, 206, 239, 251], "quant_dequ": 147, "quant_schem": [9, 10, 14, 159, 170, 172, 178, 179, 180, 187, 189, 190, 191, 196, 201, 204, 206, 216, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "quant_sim": [167, 196, 222], "quant_sim_model": [180, 223], "quant_stats_visu": [159, 204], "quant_wrapp": 162, "quantanalyz": [9, 162, 170, 179, 202, 203, 221, 255], "quantit": 254, "quantiz": [0, 1, 2, 5, 8, 9, 10, 12, 13, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 154, 155, 156, 157, 158, 159, 161, 163, 165, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 185, 187, 189, 190, 192, 193, 194, 196, 199, 200, 201, 202, 204, 205, 209, 211, 213, 220, 221, 222, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 241, 242, 243, 246], "quantizablemultiheadattent": 202, "quantizaiton": [8, 218], "quantization_overrid": 253, "quantization_tf": 187, "quantizationdatatyp": [2, 14, 162, 165, 172, 177, 178, 180, 191, 219, 223, 224, 236, 244], "quantizationmixin": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 166, 198, 199], "quantizationmod": 187, "quantizationsim": [236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 255], "quantizationsimmodel": [1, 2, 7, 8, 9, 10, 11, 13, 14, 19, 159, 161, 162, 165, 166, 167, 168, 170, 171, 172, 173, 178, 179, 180, 181, 188, 189, 190, 191, 192, 196, 198, 200, 201, 202, 204, 205, 206, 207, 216, 218, 219, 221, 222, 223, 224, 225, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 253, 256], "quantizationsimmodelonnxexport": 172, "quantizationsimmodelv1": 162, "quantizationsimmodelv2": 162, "quantized_": [189, 190, 196], "quantized_dlc": 253, "quantized_linear": 19, "quantized_mobilenet_v2": [189, 200, 223], "quantized_model": [172, 223, 224], "quantized_repr": [19, 147, 148, 149], "quantizedadd": 166, "quantizedconv2d": [162, 166, 172, 175, 207, 223, 224], "quantizedequant": [147, 162, 166, 171, 172, 175, 187, 198, 207, 223, 224], "quantizedlinear": [19, 162, 166, 172, 207], "quantizedmaskedadd": 19, "quantizedmultipli": [19, 166], "quantizedoptlearnedpositionalembed": [198, 199], "quantizedrelu": 162, "quantizedsoftmax": 166, "quantizedtensor": [19, 147, 149, 150, 171, 202], "quantizelinear": [10, 168, 172, 202, 255], "quantizer_arg": 216, "quantizer_group": [2, 165, 177, 219], "quantizer_info": [2, 219], "quantizer_paramet": 172, "quantizer_state_dict": 172, "quantizerbas": [19, 166, 171, 199], "quantizergroup": [2, 14, 165, 177, 178, 191, 219], "quantschem": [9, 10, 12, 14, 159, 170, 172, 176, 178, 179, 180, 188, 189, 190, 191, 196, 201, 202, 204, 206, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "quantsim": [0, 5, 6, 9, 12, 158, 159, 160, 162, 165, 169, 170, 176, 179, 183, 188, 189, 193, 198, 199, 202, 203, 204, 205, 206, 207, 218, 219, 220, 222, 224, 235, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 255, 258, 259], "quantsim_config": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "quantsimmodel": [190, 196, 201], "quatiz": 258, "question": 254, "quic": [183, 187, 235], "quick": [171, 184, 185, 202, 223, 241, 243], "quickli": [184, 221, 236, 239, 244, 251], "qwa": 202, "qwen": [196, 201], "qwen2": [174, 196, 201, 202], "qwen2forcausallm": [174, 201], "qwen3": [174, 201, 202], "r": [2, 9, 169, 170, 177, 179, 186, 206, 219], "r1": [174, 201, 202], "r1_fusion_pair": 201, "r1_placement": 201, "r2": 201, "r3": 201, "r4": 201, "radic": 259, "rais": [19, 155, 174, 191, 201, 252], "rand": [190, 244, 245, 247, 248, 249, 250, 251], "randint": [196, 201], "randn": [19, 148, 149, 150, 151, 166, 172, 187, 188, 189, 191, 192, 193, 194, 200, 221, 222, 223, 224, 236, 237, 238, 239, 240, 246], "random": [187, 188, 206, 208, 239, 246], "random_split": [221, 223], "randperm": 244, "rang": [9, 153, 154, 159, 163, 170, 171, 179, 180, 183, 192, 194, 195, 196, 198, 199, 201, 202, 203, 204, 210, 219, 223, 224, 227, 236, 237, 238, 240, 244, 245, 246, 247, 248, 249, 254, 255, 256], "rank": [18, 195, 197, 212, 214, 221], "rank_select_schem": [18, 214], "rankselectschem": [18, 214], "rapidli": [185, 220], "rare": [202, 255], "rate": [202, 211, 224, 241, 242, 243, 247, 249, 250, 254], "rather": [163, 200, 225, 247], "ratio": [18, 208, 209, 212, 214, 221, 241, 242, 243], "raw": [196, 198, 199, 201], "rceil": [153, 154], "re": [2, 171, 173, 177, 181, 200, 202, 219, 235, 236, 244, 259], "reach": [191, 246, 254], "read": [206, 218], "reader": [236, 239, 244, 247, 251], "readi": [171, 172, 180, 201, 223, 224, 227, 236, 238, 239, 240, 241, 242, 243, 244, 245, 248, 249, 250, 251, 255, 258], "readili": [236, 239, 244, 247, 251], "real": [147, 148, 188, 213, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "realiz": [165, 222], "realli": [236, 239, 247, 251], "reason": [164, 223, 224, 237, 238, 240, 245, 248, 249, 250, 259], "reassess": 255, "rebin": 202, "recalcul": 247, "recalibr": 221, "receiv": 163, "recent": 258, "recip": [202, 228], "recogn": [164, 256], "recommend": [1, 2, 9, 10, 12, 168, 171, 176, 185, 187, 188, 189, 190, 192, 193, 194, 196, 206, 209, 219, 223, 227, 236, 237, 238, 240, 245, 246, 254, 255, 258, 259], "recomput": [202, 237], "reconfigur": 202, "reconstruct": [12, 176, 189, 241, 243], "record": [9, 170, 179, 206], "recov": [202, 221, 227, 238, 241, 242, 243, 248, 252, 258, 259], "recoveri": 259, "recurr": 202, "recurs": 202, "redefin": 164, "redesign": 202, "reduc": [1, 11, 166, 169, 174, 183, 184, 189, 192, 194, 195, 200, 201, 202, 211, 215, 217, 220, 224, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256], "reduct": [209, 241, 242, 243, 259], "redund": [193, 211], "reestim": [15, 192], "reestimate_bn_stat": [15, 192, 247], "refactor": 202, "refer": [160, 162, 169, 180, 181, 205, 207, 216, 221, 223, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254], "referenc": 202, "reflect": [249, 250, 256], "reflectionpad1d": 117, "reflectionpad2d": 118, "reflectionpad3d": 119, "regard": [2, 177, 219, 237, 238, 239, 240, 245, 248, 249, 250, 251], "regardless": [190, 196], "regist": [19, 155, 166, 190, 198, 199, 201, 202, 236], "regress": 208, "regular": [12, 19, 171, 172, 176, 180, 189, 223, 224, 236, 237], "rel": [18, 208, 209, 212, 214, 219, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 255], "relat": [162, 256], "relationship": 207, "releas": [186, 187, 196, 235], "release_tag": 235, "relev": [241, 242, 243, 258], "reli": [162, 200, 224, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "reliabl": 187, "reload": 248, "relu": [115, 162, 163, 164, 194, 202, 215, 225, 226], "relu1": [164, 226], "relu2": 226, "relu6": [116, 193, 194, 202], "remain": [13, 162, 173, 181, 189, 190, 194, 196, 200, 201, 202, 221, 224], "remov": [14, 15, 163, 175, 178, 183, 191, 192, 193, 199, 202, 208, 215, 216, 223, 252, 256], "remove_activation_quant": 175, "remove_all_quant": [175, 199, 252], "remove_column": [196, 198, 199, 201], "remove_input_quant": 175, "remove_output_quant": 175, "remove_param_quant": 175, "renam": 202, "render": 202, "reorgan": 202, "repackag": 202, "repeat": [169, 208, 255, 259], "repeatedli": 258, "replac": [10, 162, 163, 166, 169, 198, 202, 207, 221, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256], "replace_lora_layers_with_quantizable_lay": [169, 198, 199, 202], "replicationpad1d": 120, "replicationpad2d": 121, "replicationpad3d": 122, "repo": 239, "report": [202, 221, 222, 254, 255, 259], "repositori": 235, "repres": [2, 9, 10, 14, 147, 148, 149, 165, 166, 168, 170, 172, 177, 178, 179, 180, 183, 188, 191, 200, 202, 206, 210, 216, 219, 221, 222, 223, 224, 226, 239, 251, 253, 254, 256], "represent": [147, 148, 149, 155, 171, 183, 202, 253, 254, 256], "reproduc": 252, "requant": 193, "request": 222, "requir": [2, 18, 19, 160, 162, 163, 169, 171, 172, 180, 183, 186, 187, 189, 190, 193, 194, 196, 198, 200, 201, 202, 205, 206, 208, 209, 211, 212, 213, 214, 216, 219, 220, 221, 222, 223, 224, 226, 236, 239, 241, 242, 243, 244, 245, 248, 249, 250, 251, 255, 256, 258, 259], "requires_grad": [148, 149, 155, 163, 174, 194, 201, 202, 223], "requires_grad_": [162, 198, 199], "rerun": 164, "rescal": 194, "research": 183, "resid": 202, "residu": 208, "resiz": [189, 191, 200, 202, 221, 223, 237, 244, 252], "resnet": [172, 209, 223, 224, 236, 237, 238, 239, 240], "resnet18": [172, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "resnet18_after_adaround": 237, "resnet18_after_cle_bc": [245, 246, 248], "resnet18_after_qat": [247, 249, 250], "resnet18_mixed_precis": [236, 244], "resnet18_quantsim_analysi": 239, "resolv": [164, 202], "resort": 227, "resourc": [183, 236, 244, 247, 252, 259], "respecit": [236, 244], "respect": [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 166, 171, 181, 195, 202, 206], "respond": [239, 251], "respons": [2, 177, 183, 211, 219, 236, 244, 259], "rest": [202, 227], "restor": [180, 202, 227, 241, 242, 243, 256, 258], "restrict": [207, 218], "result": [2, 9, 14, 19, 147, 148, 149, 170, 171, 172, 177, 178, 179, 180, 189, 190, 191, 192, 194, 196, 198, 199, 200, 201, 203, 206, 208, 209, 211, 219, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256], "results_dir": [2, 9, 14, 170, 177, 178, 179, 191, 206, 219, 236, 239, 244, 251], "retain": [183, 219], "retest": 259, "retrain": [183, 226, 258], "retriev": 19, "retrieve_context": 253, "return": [2, 3, 6, 9, 10, 12, 13, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 156, 157, 159, 160, 161, 163, 164, 165, 166, 167, 170, 171, 172, 173, 176, 177, 178, 179, 180, 181, 189, 190, 191, 192, 193, 196, 198, 199, 200, 201, 204, 205, 206, 207, 208, 210, 212, 214, 218, 219, 221, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256], "return_dict": [196, 198, 199, 201], "reus": [163, 164, 202, 226, 236, 244], "reveal": 227, "revert": 227, "revis": 216, "revisit": 209, "rework": 216, "rewrit": 164, "rfloor": [150, 151, 152, 153, 154, 155, 256], "rgb": [191, 226], "rgb_output": 226, "right": [150, 151, 152, 153, 154, 155, 166, 171, 207, 215, 219, 236, 244, 246, 256], "rm": [174, 201], "rmsnorm": [201, 202], "rmsnorm_fusion_pair": 201, "rmsnorm_linear_pair": 201, "rmsnormal": 202, "rnn": [112, 202], "rnncell": 113, "robust": [168, 200, 202, 258], "root": [186, 189, 200, 202, 244, 246, 252], "rotat": [174, 195, 201, 202, 254], "rough": [236, 244], "roughli": [14, 165, 177, 178, 191, 219], "round": [1, 12, 18, 176, 178, 180, 183, 191, 202, 206, 212, 214, 235, 255, 256], "round_nearest": 187, "rounding_mod": [178, 180, 202], "roundingmod": 187, "routin": [223, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "rrelu": 114, "rtype": [10, 138, 147, 148, 149, 165, 177, 219], "rule": [202, 207, 225, 258], "run": [2, 9, 10, 13, 14, 15, 18, 150, 151, 162, 163, 164, 165, 166, 168, 170, 171, 172, 177, 178, 179, 180, 183, 185, 188, 189, 190, 191, 192, 200, 202, 207, 208, 211, 212, 213, 214, 216, 218, 219, 220, 221, 222, 223, 228, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258, 259], "run_forward_pass": [10, 172, 223, 224], "run_infer": [14, 178, 191, 246], "runnabl": 258, "runtim": [11, 19, 171, 172, 180, 183, 184, 185, 193, 200, 202, 205, 206, 207, 209, 211, 216, 218, 219, 221, 223, 224, 236, 237, 238, 240, 244, 245, 248, 249, 250, 253, 254, 255, 257, 258, 259], "runtimeerror": [155, 172, 174, 201, 223, 224, 252], "s2": 190, "s3": 190, "s_1": 207, "s_2": 207, "s_n": 207, "safe": 149, "safetensor": [167, 196], "sai": [163, 209, 222, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "sake": [238, 248], "same": [9, 10, 14, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 160, 162, 163, 164, 165, 166, 168, 169, 170, 172, 176, 177, 178, 179, 180, 181, 191, 194, 195, 197, 201, 202, 205, 206, 207, 218, 222, 223, 224, 225, 236, 238, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 254], "sampl": [1, 2, 9, 10, 11, 13, 14, 159, 165, 166, 170, 173, 177, 178, 179, 180, 181, 182, 188, 189, 190, 191, 193, 194, 196, 198, 199, 200, 201, 202, 204, 206, 208, 219, 221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 256], "sandeep": 211, "saniti": [171, 202], "satisfactori": [189, 190, 192, 196, 200, 227, 259], "satisfi": [163, 165, 191, 207, 222, 246], "saurabh": 211, "save": [2, 6, 9, 10, 12, 14, 18, 159, 160, 167, 170, 172, 176, 177, 178, 179, 180, 189, 191, 196, 202, 203, 204, 205, 206, 208, 212, 214, 219, 221, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 253, 256, 258], "save_checkpoint": 180, "save_model_as_external_data": 202, "save_path": [159, 204], "saved_eval_scores_dict": [18, 208, 212, 214], "scalar": [9, 170, 179, 202, 206], "scale": [19, 148, 149, 150, 151, 152, 153, 154, 155, 167, 171, 183, 187, 194, 195, 196, 202, 206, 207, 216, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 253, 254, 256, 258], "scale_": [150, 151, 152, 153, 154, 171], "scenario": [215, 237, 238, 239, 240, 245, 248, 249, 250, 251, 252], "scene": 258, "schedul": [202, 241, 242, 243, 247, 249, 250], "schema": 202, "scheme": [9, 10, 12, 14, 18, 160, 170, 172, 176, 178, 179, 180, 183, 189, 191, 202, 205, 206, 207, 208, 211, 212, 214, 223, 224, 236, 239, 241, 242, 243, 244, 247, 251], "scope": 163, "score": [2, 9, 14, 18, 170, 177, 178, 179, 191, 206, 208, 210, 211, 212, 213, 214, 219, 236, 237, 238, 240, 246, 247], "script": [247, 254], "sdk": [184, 236, 244, 255, 258], "search": [2, 14, 173, 177, 178, 181, 191, 195, 200, 202, 219, 224, 225, 236, 241, 242, 243, 244, 247, 249, 250, 255, 258], "searcher": 211, "sec": [236, 244], "second": [10, 12, 166, 169, 172, 173, 176, 181, 189, 193, 200, 207, 219, 223, 224, 239, 251], "secondari": 202, "section": [2, 164, 177, 198, 199, 202, 207, 217, 218, 219, 225, 228, 236, 244, 246, 254, 255, 256, 257, 259], "see": [0, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 150, 151, 158, 168, 172, 180, 184, 187, 188, 194, 202, 207, 208, 211, 212, 213, 214, 218, 223, 225, 227, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 256, 259], "seem": 211, "seen": [206, 239, 251], "select": [14, 18, 178, 183, 187, 191, 198, 199, 206, 209, 212, 213, 214, 215, 218, 220, 221, 224, 225, 236, 244, 253, 254, 255, 256], "select_param": [18, 214], "self": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 163, 166, 191, 196, 201, 226, 246], "selu": 123, "sens": [166, 219], "sensit": [2, 7, 9, 170, 177, 179, 192, 202, 203, 210, 217, 220, 224, 236, 238, 239, 244, 248, 255, 256, 258, 259], "separ": [12, 162, 163, 164, 172, 176, 180, 189, 192, 202, 206, 223, 224, 227], "separableconv2d": 202, "seq": 202, "seq_length": [196, 198, 199, 201], "seq_ms": [0, 158, 162, 200], "seqms": [195, 200, 202, 254], "seqmseparam": [173, 181, 200], "sequenc": [172, 191, 195, 202, 223, 224, 225, 246, 254], "sequenti": [11, 19, 162, 166, 173, 175, 181, 193, 194, 202, 224, 225, 227], "sequentialms": [229, 230, 231, 232, 233, 234, 254], "seri": [14, 165, 172, 178, 180, 191, 222, 223, 224, 246, 253], "serial": [223, 253], "serializetostr": [221, 236, 237, 238, 240], "serv": [166, 191, 201, 213], "servic": 222, "sess": [10, 168, 223, 236, 237, 238, 240], "sess_opt": 168, "session": [10, 188, 189, 200, 202, 221, 223, 236, 237, 238, 239, 240], "sessionopt": 168, "set": [1, 2, 7, 8, 9, 10, 11, 14, 18, 19, 155, 157, 159, 161, 162, 163, 164, 165, 166, 168, 170, 172, 177, 178, 179, 180, 183, 188, 189, 191, 197, 198, 199, 200, 202, 204, 206, 207, 208, 209, 210, 211, 212, 214, 216, 218, 219, 220, 221, 223, 224, 225, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256, 258, 259], "set_activation_quantizers_to_float": [161, 207], "set_adapt": 169, "set_adaround_param": [14, 178, 191, 246], "set_and_freeze_param_encod": 245, "set_backend": 156, "set_blockwise_quantization_for_weight": [161, 207, 218], "set_default_kernel": 19, "set_descript": 252, "set_export_param": [14, 178, 191], "set_extra_st": 155, "set_grouped_blockwise_quantization_for_weight": [161, 218], "set_kernel": 19, "set_lpbq_for_param": [0, 5, 218], "set_mixed_precision_param": [14, 178, 191], "set_model_input_precis": [165, 222], "set_model_output_precis": [165, 222], "set_model_preparer_param": [14, 178, 191], "set_precis": [165, 222], "set_quant": 202, "set_quant_scheme_candid": [14, 178, 191], "set_quantizers_to_candid": [2, 165, 177, 219], "set_rang": [147, 148], "set_transform": 191, "settabl": 256, "setup": [162, 186, 187, 188, 193, 219], "sever": [164, 166, 200, 202, 206, 209, 226, 255], "sgd": [192, 224], "sh": 186, "shall": 216, "shape": [8, 10, 16, 17, 18, 147, 148, 149, 150, 151, 155, 161, 162, 163, 166, 171, 172, 175, 189, 193, 194, 198, 200, 206, 207, 208, 212, 214, 218, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "share": [164, 166, 201, 202, 207, 254], "sharp": 211, "sharpli": [241, 242, 243, 259], "shift": 202, "shift_label": [198, 199], "shift_logit": [198, 199], "should": [9, 13, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 149, 160, 162, 163, 166, 170, 172, 179, 180, 181, 187, 188, 190, 201, 205, 206, 208, 209, 212, 214, 216, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 259], "shouldn": 171, "show": [164, 169, 186, 201, 203, 207, 210, 219, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255], "showcas": [235, 239, 251], "shown": [162, 169, 186, 187, 188, 191, 206, 215, 219, 226, 227, 239, 251], "shuffl": [189, 190, 196, 200, 201, 221, 223, 237, 252], "side": 215, "sigmoid": [125, 163], "sign": [153, 154, 162, 168, 171, 172, 256], "signal": 221, "signatur": [8, 10, 18, 19, 153, 154, 172, 191, 202, 208, 212, 214, 218, 223, 224, 241, 242, 243], "signific": [219, 227, 252], "significantli": [202, 224, 255], "silu": 124, "sim": [1, 2, 7, 8, 9, 10, 11, 13, 19, 159, 161, 162, 165, 166, 168, 169, 170, 172, 173, 175, 177, 179, 180, 181, 188, 189, 190, 191, 192, 196, 199, 200, 201, 202, 204, 206, 207, 218, 219, 221, 222, 223, 224, 237, 246, 252], "sim1": 162, "sim2": [162, 172], "sim_model": [245, 247, 248, 249, 250, 251], "similar": [169, 171, 183, 187, 201, 218, 254, 255, 256], "similarli": [227, 236, 239, 244, 247, 251, 255, 259], "simpl": [163, 239, 251], "simpler": 162, "simpli": [9, 163, 170, 179, 180, 206, 221, 223, 236, 244, 247], "simplic": 169, "simplif": [194, 255], "simplifi": [1, 2, 9, 10, 162, 189, 193, 194, 206, 219, 221, 223, 253], "simuat": [237, 238, 245, 248, 249, 250], "simul": [10, 155, 161, 166, 169, 172, 180, 183, 185, 189, 190, 192, 193, 196, 200, 201, 202, 203, 204, 205, 207, 216, 219, 220, 223, 224, 225, 226, 228, 235, 241, 242, 243, 251, 252, 254, 255, 259], "sinc": [172, 201, 207, 209, 219, 223, 224, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 256], "singl": [2, 6, 14, 160, 165, 172, 177, 178, 191, 202, 205, 207, 210, 211, 216, 218, 219, 239, 251, 256], "singular": [211, 212, 214, 242, 243], "situat": 206, "six": 225, "size": [8, 9, 149, 150, 151, 152, 153, 154, 155, 161, 166, 171, 172, 183, 190, 196, 202, 206, 207, 212, 214, 216, 217, 218, 223, 245, 252, 255, 258, 259], "skbuild_build_target": 186, "skew": [237, 238, 240, 245, 248, 249, 250], "skip": [12, 14, 173, 176, 178, 181, 186, 189, 191, 198, 200, 202, 208, 238, 248, 255], "skipped_optim": 193, "slack": 254, "slice": 202, "slight": 254, "slightli": [197, 236, 244], "slim": 202, "slow": [211, 254], "slower": 202, "small": [10, 172, 192, 197, 200, 202, 207, 221, 223, 224, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 255], "smaller": [2, 12, 176, 177, 189, 194, 211, 212, 214, 219, 220, 227, 236, 244, 258], "smoothl1loss": 126, "snapdragon": [236, 244], "snippet": [163, 202, 236, 244], "snpe": 202, "so": [9, 10, 12, 13, 162, 163, 164, 166, 172, 176, 180, 183, 187, 189, 190, 193, 194, 198, 199, 201, 202, 206, 210, 223, 224, 236, 239, 241, 242, 243, 244, 245, 247, 249, 250, 251, 253, 254, 259], "softmarginloss": 127, "softmax": [128, 166, 244], "softmax2d": 129, "softmin": 130, "softplu": [131, 163], "softshrink": 132, "softsign": 133, "softwar": [183, 184, 202], "sole": 206, "solid": 224, "solut": [210, 219, 224, 227, 244], "some": [18, 162, 163, 166, 171, 194, 198, 199, 202, 208, 210, 212, 214, 215, 219, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 255, 256, 258, 259], "someth": [180, 211], "sometim": [206, 208, 211, 236, 237, 238, 240, 244, 245, 248, 249, 250, 259], "somewher": 259, "soon": 191, "sort": [13, 190, 221], "sourc": [2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 163, 164, 165, 166, 168, 170, 172, 173, 175, 176, 177, 178, 179, 180, 181, 183, 189, 191, 192, 193, 194, 200, 202, 204, 205, 206, 207, 208, 212, 214, 218, 219, 221, 222, 223, 224, 227], "space": [18, 208, 212, 214, 219, 256], "spars": 202, "spatial": [18, 202, 208, 209, 210, 214, 217, 235], "spatial_svd": [18, 208, 212, 214, 242, 243], "spatialsvdparamet": [18, 208, 212, 214, 242, 243], "spconv": 202, "special": [14, 163, 178, 183, 191], "specif": [9, 14, 18, 19, 161, 164, 166, 170, 172, 178, 179, 180, 186, 191, 195, 197, 201, 202, 206, 207, 208, 212, 214, 218, 223, 224, 225, 236, 237, 244, 245, 247, 252, 253, 254, 255, 256, 259], "specifi": [2, 8, 9, 12, 18, 150, 151, 152, 153, 154, 155, 165, 170, 172, 176, 177, 179, 180, 189, 191, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 222, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "speed": [18, 193, 202, 208, 211, 212, 214, 217, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254], "speedup": [173, 181, 200, 236, 244], "spinquant": [0, 158, 202, 229, 230, 231, 232, 233, 234, 254], "spinquant_optim": 201, "split": [173, 181, 189, 191, 192, 196, 198, 199, 200, 201, 221, 223, 224, 237, 252, 256], "sqnr": [2, 14, 165, 173, 177, 178, 181, 191, 200, 219, 236, 244, 256], "sqnr_metric": [7, 221], "sqrt": 163, "squar": [203, 254, 256], "squeez": 244, "ss": [229, 230, 231, 232, 233, 234, 254], "ssvd": 209, "ssvd_comp_stat": 243, "ssvd_compressed_model": 243, "ssvd_cp_compressed_model": 243, "ssvd_cp_finetuned_model": 243, "ssvd_finetuned_model": 243, "stabil": 202, "stabl": [163, 224, 247], "stack": [182, 202, 239], "stand": 202, "standalon": 202, "standard": [155, 163, 166, 183, 240, 249, 250, 252], "start": [2, 12, 153, 154, 163, 164, 169, 176, 177, 184, 186, 189, 202, 209, 219, 224, 225, 228, 235, 236, 239, 241, 242, 243, 244, 247, 249, 250, 251, 255, 256, 259], "start_beta": [12, 176, 189], "stat": [9, 15, 159, 192, 202, 204, 206], "statatist": 247, "state": [155, 172, 181, 182, 185, 202, 219, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251], "state_dict": [155, 172, 202], "stateless": 226, "statement": 163, "static": [18, 163, 164, 202, 208, 212, 214, 254], "staticgridperchannelquant": 162, "staticgridquant": 162, "staticgridquantwrapp": 162, "staticmethod": [163, 236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251], "statist": [9, 10, 15, 18, 150, 151, 155, 159, 166, 170, 172, 179, 192, 195, 203, 204, 208, 212, 214, 223, 224, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250], "std": [189, 191, 200, 221, 223, 237, 244, 246, 252], "step": [1, 9, 153, 154, 157, 163, 165, 169, 170, 173, 179, 181, 186, 187, 188, 194, 197, 198, 199, 208, 209, 210, 211, 218, 220, 227, 235, 236, 238, 239, 240, 244, 247, 251, 252, 253, 254, 256, 257, 258], "still": [162, 207, 227, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 259], "stochast": 180, "stop": [12, 176, 189, 191, 255], "stopiter": [191, 201], "storag": [217, 218, 256], "store": [12, 148, 165, 172, 176, 180, 189, 222, 223, 224, 236, 244], "str": [1, 2, 6, 7, 8, 9, 10, 11, 12, 14, 155, 159, 160, 163, 165, 167, 168, 170, 172, 173, 176, 177, 178, 179, 180, 181, 189, 191, 196, 200, 204, 205, 206, 218, 219, 221, 222, 223, 224, 226, 237, 252], "str_idx": 191, "straightforward": [187, 259], "strateg": 219, "strategi": 259, "stream": [189, 191, 192], "streamlin": 255, "strict": [8, 155, 165, 172, 218, 222, 223, 225, 256], "strict_symmetr": [187, 225], "strict_valid": [14, 178, 191], "strictli": [155, 201, 221, 223], "stride": [149, 163, 172, 175, 193, 194, 202, 223, 224], "strike": 211, "string": [10, 165, 172, 180, 216, 222, 223, 225], "strong": 254, "strongli": [163, 171, 237, 245], "structur": [14, 163, 166, 178, 183, 191, 211, 222, 239, 243, 251, 252], "studi": 228, "sub": [155, 239, 251], "subbackward0": 171, "subclass": [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 171, 202], "subdirectori": [236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "subfold": [236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "subgraph": 202, "subject": 254, "sublay": 164, "submit": 253, "submodul": 226, "subpackag": [19, 202], "subsequ": [147, 174, 183, 189, 193, 194, 201, 202, 223, 225, 236, 238, 244], "subset": [9, 10, 170, 172, 179, 192, 206, 207, 215, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255], "subsetrandomsampl": 246, "subsidiari": 184, "substanti": 216, "success": 246, "successfulli": [172, 223, 224, 252], "sudo": 235, "suffer": 254, "suffic": [237, 238, 240, 245, 248, 249, 250], "suffici": [206, 223, 256, 258], "suggest": [180, 210, 211, 241, 243, 255, 259], "suit": [183, 187, 191, 198, 246], "suitabl": [183, 255], "sum": [189, 191, 200, 221, 223, 224, 237, 244, 246, 252], "summar": 254, "summari": [184, 202, 211], "sun": 211, "super": [19, 163, 191, 201, 202], "supergroup": [202, 256], "suppli": [207, 218], "support": [1, 2, 7, 9, 10, 11, 12, 18, 19, 163, 164, 165, 170, 172, 173, 174, 176, 177, 179, 180, 181, 184, 187, 188, 189, 190, 191, 192, 193, 196, 200, 201, 202, 204, 206, 207, 208, 209, 211, 212, 214, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 235, 236, 244, 247, 249, 250, 255, 256, 259], "supported_kernel": [2, 177, 219, 236, 244], "supported_kernel_op": [165, 177, 219], "supported_module_dict": 201, "suppos": [2, 177, 194, 219, 236, 244], "suscept": 224, "svd": [18, 202, 208, 209, 210, 217, 235], "swap": 202, "sweep": [11, 200], "switch": [202, 207], "sy": 239, "symbol": 163, "symbolic_trac": [14, 163, 178, 191], "symfp": [173, 181, 200], "symmetr": [19, 147, 148, 149, 150, 151, 161, 162, 166, 171, 172, 175, 198, 202, 207, 216, 218, 223, 224, 225, 256], "symmetri": [162, 173, 181, 200, 202], "symqt": [173, 181, 200], "syncbatchnorm": 202, "syntax": 216, "system": 186, "systemat": 183, "t": [2, 10, 14, 19, 163, 171, 172, 173, 177, 178, 181, 186, 191, 196, 198, 199, 200, 201, 202, 206, 219, 223, 224, 236, 239, 244, 247, 251, 258], "tabl": [159, 201, 204, 213, 216, 235, 254], "tag": 235, "take": [1, 2, 10, 13, 14, 18, 19, 161, 164, 165, 168, 169, 171, 172, 177, 178, 189, 190, 191, 196, 201, 202, 207, 208, 210, 211, 212, 214, 215, 218, 219, 221, 222, 223, 224, 227, 228, 236, 239, 241, 242, 243, 244, 247, 251, 254, 255, 258], "taken": [13, 190, 215], "tanh": [134, 202], "tanhshrink": 135, "tap": [170, 179, 206], "tar": [18, 214, 221, 223, 239], "target": [10, 18, 30, 31, 37, 51, 53, 70, 76, 77, 81, 82, 93, 94, 102, 103, 104, 105, 106, 111, 126, 127, 169, 172, 177, 180, 183, 190, 191, 193, 196, 200, 202, 203, 205, 208, 209, 210, 211, 212, 214, 216, 218, 219, 223, 224, 227, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252, 254, 255, 256, 259], "target_comp_ratio": [18, 208, 212, 214, 241, 242, 243], "target_data": [244, 245, 247, 248, 249, 250, 251], "target_length": 37, "target_modul": 169, "task": [183, 213, 216, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 255], "taxonomi": 211, "tbd": 204, "techniqu": [14, 178, 183, 185, 189, 190, 191, 193, 194, 196, 197, 198, 200, 201, 202, 206, 208, 209, 212, 214, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 254, 256, 258, 259], "technologi": [183, 184], "tempfil": 202, "temporari": [10, 189, 193, 194, 223], "temporarili": [157, 162, 175, 199, 252], "tend": 224, "tensor": [1, 2, 10, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 160, 163, 164, 165, 166, 168, 170, 171, 172, 176, 177, 178, 179, 180, 189, 190, 191, 193, 194, 196, 198, 199, 202, 205, 206, 207, 208, 212, 214, 216, 217, 219, 222, 223, 224, 225, 226, 227, 236, 238, 239, 244, 248, 249, 250, 251, 256], "tensor_nam": 216, "tensor_quant": 162, "tensorflow": [183, 184, 193, 202, 205, 207, 216, 218, 219, 221, 235, 251, 253, 254, 255, 256, 258, 259], "tensorquantizationsimforpython": 187, "term": [172, 184, 207, 212, 214, 217, 223, 224], "termin": 235, "test": [9, 170, 179, 196, 198, 199, 202, 206, 252, 258, 259], "test_data_load": 252, "test_dataload": [196, 198, 199], "test_dataset": [196, 198, 199], "text": [150, 151, 152, 153, 154, 196, 198, 199, 201], "tf": [2, 9, 170, 179, 202, 206, 219, 223, 236, 239, 244, 247, 251], "tf_enhanc": [10, 223, 236, 244, 247], "tfe": 202, "tfencod": 162, "tfenhanc": 202, "tflite": [236, 244], "tfoplambda": 202, "than": [12, 18, 162, 163, 164, 176, 177, 180, 189, 200, 201, 202, 207, 208, 209, 212, 214, 218, 219, 222, 224, 225, 236, 237, 244, 245, 247, 254, 255, 258], "thei": [163, 164, 168, 174, 197, 198, 199, 201, 213, 218, 224, 227, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 255, 256, 258], "them": [14, 162, 163, 164, 166, 178, 183, 191, 194, 211, 217, 221, 222, 223, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "theme": 202, "therebi": [219, 236, 244], "therefor": [2, 177, 193, 209, 219, 236, 244], "theta_": [150, 151], "thi": [2, 6, 8, 9, 10, 11, 12, 13, 14, 18, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 174, 176, 177, 178, 179, 180, 181, 184, 186, 187, 188, 189, 190, 191, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250, 252, 253, 254, 255, 256, 257, 258, 259], "thing": [237, 238, 240, 241, 242, 243, 245, 248, 249, 250, 253], "those": [9, 170, 179, 198, 206, 221, 225, 255, 258], "though": [19, 171, 207, 225, 236, 244, 259], "thousand": 254, "three": [163, 209, 236, 241, 242, 243, 244, 258], "threshold": [136, 159, 191, 204, 254, 255], "through": [2, 10, 149, 162, 163, 165, 166, 170, 177, 179, 187, 189, 194, 198, 199, 201, 206, 218, 219, 221, 222, 223, 224, 228, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 252, 254, 255, 256, 259], "throughout": [13, 183, 190, 224, 225], "throughput": 254, "throw": [8, 19, 171, 202, 218], "thrown": [202, 223], "thu": [147, 148, 149, 219], "ti": [174, 201], "tie": 202, "tie_word_embed": 201, "tijmen": 211, "till": [2, 177, 219, 236, 244, 255], "time": [18, 149, 164, 169, 185, 191, 202, 206, 207, 208, 211, 212, 213, 214, 218, 221, 222, 224, 229, 230, 231, 232, 233, 234, 239, 246, 251, 254, 255, 258], "tmp": [9, 14, 170, 178, 179, 186, 188, 189, 191, 193, 194, 206, 251], "tmpdir": 169, "to_arrai": [193, 194], "to_list": [2, 165, 177, 219], "to_onnx_qdq": [10, 202], "todo": [165, 222, 252], "toggl": 186, "token": [196, 198, 199, 201, 227, 254], "tokenized_dummy_text": [198, 199], "toler": [191, 209, 246], "tolist": 244, "toml": 186, "too": [207, 241, 243], "tool": [10, 170, 179, 183, 185, 202, 206, 215, 253, 257, 259], "toolchain": 255, "toolkit": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 253, 254, 255, 256, 257, 258, 259, 260], "tooltip": 202, "top": [1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 161, 165, 167, 170, 173, 174, 176, 177, 178, 179, 180, 181, 189, 190, 191, 192, 193, 194, 196, 200, 201, 202, 206, 207, 208, 212, 213, 214, 218, 219, 221, 222, 223, 224, 236, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 254], "top1": [246, 252], "top1_accuraci": [244, 252], "top5": 252, "top5_accuraci": 252, "topk": [244, 246, 252], "torch": [10, 14, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 157, 161, 162, 163, 164, 165, 166, 168, 171, 172, 174, 177, 178, 180, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 207, 218, 219, 221, 222, 223, 224, 226, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 259], "torch_builtin": 157, "torch_lpbq": 254, "torch_pcq": 254, "torchscript": [14, 160, 172, 178, 180, 183, 191, 205, 223, 224], "torchvis": [172, 188, 189, 191, 192, 193, 194, 200, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "total": [198, 199, 210, 219, 223, 252, 256], "total_length": [196, 198, 199, 201], "total_sampl": [189, 200, 221, 223, 237], "totensor": [189, 191, 200, 221, 223, 237, 244, 246, 252], "toward": 227, "tpu": 183, "tqdm": [189, 198, 199, 200, 221, 223, 224, 237, 244, 246, 252], "trace": [14, 159, 163, 178, 191, 204, 226], "traceabl": [163, 198, 199, 226], "traceback": 163, "traceerror": 163, "tracer": 163, "track": [206, 239, 251], "track_running_stat": [193, 194], "trade": [12, 176, 189, 219, 236, 244, 254], "tradeoff": [183, 219, 236, 244, 259], "train": [9, 10, 14, 15, 18, 167, 170, 172, 178, 179, 183, 185, 188, 189, 190, 191, 192, 194, 196, 197, 200, 201, 202, 203, 205, 206, 208, 211, 212, 214, 218, 221, 227, 235, 236, 244, 246, 254, 256, 259], "train_data_load": 252, "train_dataload": [172, 196, 198, 199, 201, 223, 224], "train_dataset": [196, 198, 199, 201], "train_flag": [18, 208, 212, 214], "train_load": 247, "train_model": [18, 208, 212, 214], "train_one_epoch": [198, 199], "trainabl": [190, 196, 197, 198, 199, 250], "trainer": [18, 208, 212, 214, 235, 241, 242, 243, 247, 249, 250], "training_range_learning_with_tf_init": [172, 190, 196, 201, 223, 224, 247, 250], "trainingextens": 186, "trainingmod": [236, 240], "transact": 211, "transform": [163, 174, 189, 191, 196, 198, 199, 200, 201, 202, 207, 221, 223, 237, 244, 245, 246, 249, 250, 251, 254], "transpos": [171, 202], "trap": 200, "travers": 222, "treat": 224, "tri": [185, 211, 241, 242, 243], "tripletmarginloss": 137, "tripletmarginwithdistanceloss": 138, "triton": [156, 157, 202], "trivial": [202, 254], "true": [2, 8, 10, 14, 18, 19, 148, 149, 150, 151, 155, 161, 162, 163, 164, 165, 166, 168, 171, 172, 175, 177, 178, 180, 187, 189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 202, 207, 208, 212, 214, 216, 218, 219, 222, 223, 224, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "true_quant": 198, "truli": 163, "trust_remote_cod": [196, 201], "try": [18, 188, 191, 193, 194, 208, 209, 211, 212, 214, 227, 236, 238, 240, 241, 242, 243, 244, 255, 258], "tune": [18, 169, 180, 184, 208, 209, 212, 214, 224, 237, 238, 240, 245, 246, 248, 249, 250, 252, 254, 255, 258, 259], "tuner": 198, "tupl": [2, 3, 6, 9, 12, 13, 14, 16, 17, 18, 86, 150, 151, 152, 153, 154, 155, 160, 161, 164, 165, 170, 171, 172, 176, 177, 178, 179, 180, 188, 189, 190, 191, 193, 194, 196, 201, 202, 205, 206, 207, 208, 212, 214, 218, 219, 222, 223, 224, 226, 236, 239, 244, 251], "turn": [185, 202, 225], "tutori": [182, 254], "tweak": [169, 255], "twice": 169, "two": [13, 18, 162, 163, 164, 165, 177, 183, 185, 190, 194, 201, 206, 210, 211, 212, 214, 219, 222, 224, 237, 238, 239, 240, 241, 242, 243, 245, 246, 248, 249, 250, 251, 252, 253, 254, 256, 258], "txt": [165, 186, 222, 253], "ty": 202, "type": [1, 2, 3, 8, 9, 10, 12, 14, 15, 16, 18, 19, 147, 148, 149, 150, 151, 155, 156, 157, 159, 161, 162, 163, 164, 165, 166, 170, 172, 173, 176, 177, 178, 179, 180, 181, 183, 189, 190, 191, 192, 193, 200, 201, 202, 204, 206, 207, 208, 211, 212, 213, 214, 218, 219, 223, 224, 225, 236, 241, 242, 243, 244, 246, 249, 250, 252, 253, 258], "typeerror": 163, "typic": [19, 166, 183, 201, 206, 209, 219, 223, 236, 237, 239, 241, 243, 244, 245, 246, 247, 249, 250, 251, 255, 256, 258, 259], "ubuntu": [185, 186, 187, 188], "ubuntu22": 202, "uint": 202, "uint16": 216, "uint32": 216, "uint8": [147, 216], "unaccept": 259, "unattain": 219, "uncalibr": 202, "unchang": [181, 194, 223], "uncompress": 209, "under": [1, 2, 10, 159, 162, 177, 189, 192, 204, 206, 219, 225, 227, 236, 244, 254], "undergo": 183, "underli": [19, 171, 227], "understand": [162, 171, 235, 236, 239, 244, 247, 251, 254], "undo": [15, 192], "uneven": 227, "unexpected_kei": 155, "unflatten": 139, "unfold": 140, "unid": [14, 178, 191], "uniniti": [13, 189, 190, 196, 200, 201], "unintuit": [14, 178, 191], "union": [6, 8, 9, 12, 14, 16, 17, 18, 160, 161, 164, 165, 170, 172, 176, 177, 178, 179, 180, 189, 191, 193, 194, 205, 206, 207, 208, 212, 214, 218, 222, 223, 224, 255], "unit": 183, "unknown": [202, 209], "unkown": [19, 166], "unlabel": [9, 14, 178, 189, 191, 200, 206, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 248, 249, 250, 251], "unlabeled_data": [191, 200], "unlabeled_data_load": [191, 239], "unlabeled_dataset_iter": [9, 206], "unlabeled_imagenet_data_load": 246, "unlabeled_imagenet_dataset": 246, "unlabeleddatasetwrapp": 246, "unlabelled_data_load": 191, "unless": [14, 19, 155, 172, 178, 180, 191, 215, 223, 224, 259], "unlik": [193, 247], "unmodifi": [197, 210], "unnecessari": [193, 202, 215, 236, 244], "unpack": 202, "unpin": 202, "unquant": 254, "unrol": [163, 202], "unsign": [168, 172, 225, 256], "unsigned_symmetr": [187, 225], "unsigned_zero": 155, "unsimplifi": [193, 194, 236, 238, 240], "unsqueez": [198, 199, 252], "unti": [174, 201, 202], "until": [14, 150, 151, 172, 178, 191, 192, 202, 223, 224, 246], "untouch": 223, "up": [12, 18, 165, 176, 180, 183, 186, 189, 202, 208, 211, 212, 214, 215, 219, 222, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 258], "up_proj": 201, "updat": [1, 13, 155, 167, 189, 190, 192, 196, 199, 201, 202, 213, 216, 223, 224, 235, 237, 239, 240, 245, 246, 248, 249, 250, 256], "update_lora_weight": [167, 196], "updatestat": 187, "upgrad": [0, 158, 202], "upon": [15, 19, 166, 192, 202, 253], "upsampl": 141, "upsamplingbilinear2d": 142, "upsamplingnearest2d": 143, "upstream": [202, 208, 215], "upto": [236, 239, 244, 247, 251], "url": [18, 208, 212, 213, 214, 235], "us": [1, 2, 6, 8, 9, 10, 11, 12, 14, 15, 18, 19, 147, 148, 150, 151, 155, 158, 159, 160, 161, 163, 164, 165, 166, 167, 169, 170, 171, 172, 176, 177, 178, 179, 180, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 209, 213, 216, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 235, 236, 237, 238, 239, 240, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259], "usabl": 216, "usag": [13, 164, 169, 171, 183, 190, 196, 202, 207, 216, 219, 224, 253, 254, 255], "use_all_amp_candid": [2, 177, 219, 236, 244], "use_cach": [196, 198, 199, 201], "use_cuda": [18, 187, 202, 208, 212, 214, 239, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "use_embedded_encod": [172, 180, 223, 224], "use_fast": [196, 201], "use_monotonic_fit": [18, 208, 212, 214], "use_strict_symmetr": 162, "use_symmetric_encod": [162, 202], "user": [2, 10, 12, 13, 14, 18, 19, 162, 163, 165, 171, 172, 176, 177, 178, 180, 184, 186, 187, 189, 190, 191, 201, 202, 206, 208, 211, 212, 214, 216, 217, 219, 222, 223, 224, 226, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255], "user_onnx_lib": [10, 223], "userflow": [173, 181, 200], "usual": [192, 211, 224, 256, 259], "util": [0, 6, 9, 158, 162, 164, 170, 179, 188, 189, 190, 191, 192, 198, 199, 200, 202, 205, 206, 207, 218, 221, 222, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "v": [2, 12, 176, 177, 186, 189, 207, 219, 236, 239, 244, 251, 254, 256, 259], "v1": [0, 162, 196, 198, 199, 201, 202, 244, 245, 246, 247, 248, 249, 250, 251], "v2": [0, 19, 147, 148, 149, 150, 151, 153, 154, 156, 158, 159, 161, 165, 172, 190, 198, 202, 204, 207, 218, 222, 223, 224, 252], "v73": 202, "v81": 252, "v_proj": 201, "val": [189, 200, 221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "val_images_len": 239, "val_transform": 246, "valid": [2, 9, 164, 170, 177, 179, 182, 188, 189, 191, 202, 206, 207, 216, 219, 223, 236, 239, 244, 246, 247, 251], "validate_for_missing_modul": 164, "validate_for_reused_modul": 164, "validate_model": 164, "validation_check": 164, "validationerror": 202, "valu": [1, 2, 9, 10, 12, 18, 19, 148, 149, 150, 151, 153, 154, 155, 159, 160, 161, 163, 165, 170, 172, 176, 177, 179, 180, 183, 187, 188, 189, 194, 196, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 216, 218, 219, 222, 223, 224, 225, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 251, 256], "value_qtzr": 19, "var": [15, 70, 192], "vari": [2, 177, 209, 219], "variabl": [18, 150, 151, 163, 208, 210, 212, 214, 235, 252], "varianc": 247, "variant": [183, 185, 186, 187, 188, 202, 254], "varieti": 194, "variou": [2, 18, 165, 177, 185, 202, 208, 211, 212, 214, 217, 219, 227, 239, 246, 251, 255, 256], "vector": [202, 237, 245], "vedaldi": 211, "venic": 211, "venv": 186, "ver": 202, "ver_cuda": 186, "ver_onnxruntim": 186, "ver_python": 186, "ver_torch": 186, "veri": [13, 190, 206, 209, 211, 219, 237, 238, 239, 240, 245, 246, 248, 249, 250, 251, 255], "verifi": [163, 245, 248, 249, 250], "versa": [210, 227, 236, 244, 256], "version": [10, 19, 39, 40, 41, 42, 43, 44, 73, 74, 89, 117, 118, 119, 120, 121, 122, 136, 138, 139, 144, 145, 146, 159, 162, 163, 166, 168, 172, 180, 183, 185, 186, 188, 198, 199, 202, 204, 223, 224, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 256, 259], "via": [14, 178, 183, 185, 186, 187, 191, 202, 209, 221, 252, 253, 256], "vice": [210, 227, 236, 244, 256], "view": [159, 171, 184, 187, 198, 199, 203, 204, 213], "view_a": 246, "viewabl": 235, "virtual": 186, "virtual_env": 186, "visibl": 254, "vision": [174, 201, 211], "visit": 185, "visual": [18, 185, 202, 206, 208, 211, 212, 214, 219, 239, 251], "visualization_tool": [0, 158, 204], "visualization_url": [18, 208, 212, 214], "visualize_stat": [159, 204], "visualizecompress": 213, "vit_b_16": 252, "vl": [174, 201, 202], "vocab_s": [196, 198, 199, 201], "vol": 211, "volum": 215, "vram": 254, "w": [2, 9, 170, 177, 179, 191, 202, 206, 214, 215, 219], "w1616": 255, "w16a16": [7, 202, 219, 221, 255], "w4a16": 202, "w4a8": [191, 202, 258, 259], "w4fp16": 202, "w8a16": [191, 219, 223, 255, 258, 259], "w8a8": [191, 219, 224, 255, 258, 259], "w8a8_accuraci": 221, "w8a8_mp_accuraci": 221, "w_1": 194, "w_2": 194, "wa": [18, 147, 171, 202, 208, 211, 212, 214, 216, 219, 222, 236, 241, 243, 244, 245, 247, 255], "wai": [162, 171, 173, 180, 181, 187, 200, 223, 236, 237, 238, 239, 240, 244, 245, 248, 249, 250, 251, 255, 258], "walk": [218, 228], "want": [2, 6, 19, 160, 163, 177, 180, 186, 198, 199, 205, 219, 236, 239, 244, 251, 254, 259], "warm": [12, 176, 189], "warn": [13, 164, 190, 196], "wasn": 202, "wast": 202, "we": [2, 6, 160, 162, 163, 164, 166, 169, 171, 177, 185, 186, 189, 193, 194, 196, 198, 199, 201, 205, 207, 209, 211, 219, 223, 227, 236, 237, 239, 241, 243, 244, 245, 246, 247, 251, 252, 254, 255, 258, 259], "websit": [184, 209], "websocket": 213, "weight": [1, 2, 3, 8, 9, 10, 11, 12, 13, 16, 18, 19, 159, 161, 162, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 179, 183, 185, 188, 189, 190, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 207, 209, 212, 217, 218, 219, 221, 222, 223, 224, 225, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 252, 254, 255, 256, 258, 259], "weight_decai": 192, "weight_q": 171, "weight_qdq": 171, "weight_svd": [18, 208, 212, 214], "weights_pdf": [206, 239, 251], "weightsvdparamet": [18, 208, 212, 214], "well": [147, 164, 201, 206, 211, 224, 236, 237, 238, 239, 240, 244, 245, 246, 248, 249, 250, 251, 255], "were": [202, 209, 215, 216, 223, 224, 225, 237, 246, 252], "weren": 163, "wget": [221, 223, 239], "what": [184, 201, 213, 255, 258], "wheel": 202, "when": [2, 9, 10, 14, 18, 19, 155, 163, 166, 168, 170, 172, 173, 177, 178, 179, 180, 181, 190, 191, 192, 194, 196, 200, 201, 202, 206, 208, 211, 212, 213, 214, 215, 216, 219, 222, 223, 224, 225, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255, 256], "whenev": [202, 224, 226], "where": [2, 12, 18, 150, 151, 152, 153, 154, 155, 163, 171, 172, 176, 177, 180, 183, 189, 192, 201, 202, 206, 208, 210, 212, 213, 214, 215, 216, 219, 223, 224, 226, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256], "wherea": [155, 171, 177, 236, 244], "wherein": [6, 160, 205], "whether": [13, 14, 155, 163, 164, 165, 178, 190, 191, 210, 222, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "which": [2, 6, 8, 12, 14, 18, 19, 147, 148, 149, 150, 151, 153, 154, 155, 160, 161, 163, 164, 165, 166, 169, 171, 172, 176, 177, 178, 180, 183, 185, 189, 190, 191, 193, 194, 195, 196, 197, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 222, 223, 225, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 253, 254, 255, 256, 258], "while": [8, 18, 166, 197, 201, 202, 208, 210, 212, 214, 216, 218, 221, 223, 224, 227, 236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 252, 255, 256, 258, 259], "whl": [186, 187], "who": 216, "whole": 256, "whose": [159, 160, 161, 163, 204, 205, 207, 218, 219, 225, 253], "why": [227, 236, 244, 254], "wide": [183, 193, 194, 255], "wider": 254, "width": [9, 170, 179, 193, 194, 206, 212, 214, 215, 216, 217, 218, 219, 220, 227, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 256, 258, 259], "wikitext": [196, 198, 199, 201, 254], "wildcard": 171, "window": 202, "wise": [9, 155, 170, 173, 179, 181, 183, 192, 200, 202, 206, 227, 254, 255], "wish": [189, 193, 194, 200, 222], "within": [19, 147, 148, 149, 166, 183, 186, 202, 206, 209, 222, 223, 254, 256, 258], "without": [14, 147, 149, 159, 169, 172, 178, 180, 191, 200, 201, 202, 204, 215, 219, 223, 224, 237, 245, 246, 248, 249, 250, 258, 259], "won": [14, 163, 173, 178, 181, 191, 200], "word": 149, "work": [162, 164, 180, 185, 187, 194, 197, 202, 207, 211, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 254, 255, 259], "workaround": 163, "workflow": [184, 195, 197, 202, 209, 218, 228, 252, 253, 258], "workspac": [186, 235, 239], "world": [188, 252], "wors": 211, "worth": 171, "would": [162, 165, 198, 202, 221, 222, 223, 225, 236, 239, 244, 247, 251, 255], "wq": 202, "wrap": [19, 162, 163, 202, 239, 251], "wrap_linear": 162, "wrapped_module_nam": [170, 179, 206], "wrapper": [170, 179, 202, 206, 244, 245, 248, 249, 250], "write": [165, 222, 223, 238, 240, 245, 248, 249, 250], "written": [10, 172, 223, 224, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "wrong": 202, "wsl2": 202, "www": 235, "x": [0, 19, 147, 148, 149, 155, 158, 163, 166, 168, 172, 187, 193, 194, 202, 206, 209, 216, 221, 223, 226, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 256], "x1": [52, 108], "x2": [52, 108, 226], "x86": [185, 187, 188], "x_": 256, "x_c": 155, "x_dq": 148, "x_q": [148, 149], "x_qdq": 147, "xiangyu": 211, "xx": 216, "y": [163, 206, 239, 251], "y_zero_point": 202, "ybelkada": [198, 199], "ye": 211, "yet": [190, 196, 207, 236, 244], "yield": [2, 9, 12, 14, 15, 165, 172, 173, 176, 177, 178, 181, 189, 191, 192, 200, 206, 209, 219, 223, 227, 247, 255, 256], "yihui": 211, "you": [9, 14, 18, 19, 162, 163, 170, 171, 178, 179, 180, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 200, 201, 202, 206, 207, 208, 209, 211, 212, 213, 214, 218, 219, 221, 222, 223, 225, 227, 228, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259], "your": [9, 14, 19, 162, 163, 164, 170, 178, 179, 186, 187, 188, 189, 191, 192, 198, 200, 203, 205, 206, 209, 211, 213, 217, 221, 223, 228, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 256, 258, 259], "yy": 216, "zero": [12, 176, 189, 202, 256], "zero_grad": [192, 198, 199, 224, 252], "zero_point": 202, "zero_point_shift": 202, "zeropad1d": 144, "zeropad2d": 145, "zeropad3d": 146, "zhang": 211, "zisserman": 211, "zlib": 202, "zou": 211, "zz": 216, "\u00aa": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00b2": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00b3": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00b5": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00b9": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00ba": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00bc": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00bd": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u00be": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u03c9": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u210e": 212, "\u215b": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u215c": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u215d": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\u215e": [184, 193, 202, 205, 207, 216, 218, 219, 221, 253, 254, 255, 256, 258, 259], "\ud835\udc58": [212, 214], "\ud835\udc5a": 212, "\ud835\udc5b": 212, "\ud835\udc64": 212}, "titles": ["AIMET API", "aimet_onnx.apply_adaround", "aimet_onnx.mixed_precision", "aimet_onnx.batch_norm_fold", "aimet_onnx.cross_layer_equalization", "aimet_onnx API", "aimet_onnx.layer_output_utils", "aimet_onnx.lite_mp", "aimet_onnx.quantsim.set_lpbq_for_params", "aimet_onnx.quant_analyzer", "aimet_onnx.quantsim", "aimet_onnx.apply_seq_mse", "aimet_torch.adaround", "aimet_torch.experimental.adascale", "aimet_torch.auto_quant", "aimet_torch.bn_reestimation", "aimet_torch.batch_norm_fold", "aimet_torch.cross_layer_equalization", "aimet_torch.compress", "QuantizationMixin", "QuantizedAdaptiveAvgPool1d", "QuantizedAdaptiveAvgPool2d", "QuantizedAdaptiveAvgPool3d", "QuantizedAdaptiveMaxPool1d", "QuantizedAdaptiveMaxPool2d", "QuantizedAdaptiveMaxPool3d", "QuantizedAlphaDropout", "QuantizedAvgPool1d", "QuantizedAvgPool2d", "QuantizedAvgPool3d", "QuantizedBCELoss", "QuantizedBCEWithLogitsLoss", "QuantizedBatchNorm1d", "QuantizedBatchNorm2d", "QuantizedBatchNorm3d", "QuantizedBilinear", "QuantizedCELU", "QuantizedCTCLoss", "QuantizedChannelShuffle", "QuantizedCircularPad1d", "QuantizedCircularPad2d", "QuantizedCircularPad3d", "QuantizedConstantPad1d", "QuantizedConstantPad2d", "QuantizedConstantPad3d", "QuantizedConv1d", "QuantizedConv2d", "QuantizedConv3d", "QuantizedConvTranspose1d", "QuantizedConvTranspose2d", "QuantizedConvTranspose3d", "QuantizedCosineEmbeddingLoss", "QuantizedCosineSimilarity", "QuantizedCrossEntropyLoss", "QuantizedDropout", "QuantizedDropout1d", "QuantizedDropout2d", "QuantizedDropout3d", "QuantizedELU", "QuantizedEmbedding", "QuantizedEmbeddingBag", "QuantizedFeatureAlphaDropout", "QuantizedFlatten", "QuantizedFold", "QuantizedFractionalMaxPool2d", "QuantizedFractionalMaxPool3d", "QuantizedGELU", "QuantizedGLU", "QuantizedGRU", "QuantizedGRUCell", "QuantizedGaussianNLLLoss", "QuantizedGroupNorm", "QuantizedHardshrink", "QuantizedHardsigmoid", "QuantizedHardswish", "QuantizedHardtanh", "QuantizedHingeEmbeddingLoss", "QuantizedHuberLoss", "QuantizedInstanceNorm1d", "QuantizedInstanceNorm2d", "QuantizedInstanceNorm3d", "QuantizedKLDivLoss", "QuantizedL1Loss", "QuantizedLPPool1d", "QuantizedLPPool2d", "QuantizedLSTM", "QuantizedLSTMCell", "QuantizedLayerNorm", "QuantizedLeakyReLU", "QuantizedLinear", "QuantizedLocalResponseNorm", "QuantizedLogSigmoid", "QuantizedLogSoftmax", "QuantizedMSELoss", "QuantizedMarginRankingLoss", "QuantizedMaxPool1d", "QuantizedMaxPool2d", "QuantizedMaxPool3d", "QuantizedMaxUnpool1d", "QuantizedMaxUnpool2d", "QuantizedMaxUnpool3d", "QuantizedMish", "QuantizedMultiLabelMarginLoss", "QuantizedMultiLabelSoftMarginLoss", "QuantizedMultiMarginLoss", "QuantizedNLLLoss", "QuantizedNLLLoss2d", "QuantizedPReLU", "QuantizedPairwiseDistance", "QuantizedPixelShuffle", "QuantizedPixelUnshuffle", "QuantizedPoissonNLLLoss", "QuantizedRNN", "QuantizedRNNCell", "QuantizedRReLU", "QuantizedReLU", "QuantizedReLU6", "QuantizedReflectionPad1d", "QuantizedReflectionPad2d", "QuantizedReflectionPad3d", "QuantizedReplicationPad1d", "QuantizedReplicationPad2d", "QuantizedReplicationPad3d", "QuantizedSELU", "QuantizedSiLU", "QuantizedSigmoid", "QuantizedSmoothL1Loss", "QuantizedSoftMarginLoss", "QuantizedSoftmax", "QuantizedSoftmax2d", "QuantizedSoftmin", "QuantizedSoftplus", "QuantizedSoftshrink", "QuantizedSoftsign", "QuantizedTanh", "QuantizedTanhshrink", "QuantizedThreshold", "QuantizedTripletMarginLoss", "QuantizedTripletMarginWithDistanceLoss", "QuantizedUnflatten", "QuantizedUnfold", "QuantizedUpsample", "QuantizedUpsamplingBilinear2d", "QuantizedUpsamplingNearest2d", "QuantizedZeroPad1d", "QuantizedZeroPad2d", "QuantizedZeroPad3d", "DequantizedTensor", "QuantizedTensor", "QuantizedTensorBase", "Quantize", "QuantizeDequantize", "dequantize", "quantize", "quantize_dequantize", "FloatQuantizeDequantize", "get_backend", "set_backend", "aimet_torch API", "aimet_torch.visualization_tools", "aimet_torch.layer_output_utils", "aimet_torch.quantsim.config_utils", "Migration guide", "aimet_torch.model_preparer", "aimet_torch.model_validator", "aimet_torch.mixed_precision", "aimet_torch.nn", "aimet_torch.experimental.omniquant", "aimet_torch.onnx.export", "aimet_torch.peft", "aimet_torch.quant_analyzer", "aimet_torch.quantization", "aimet_torch.quantsim", "aimet_torch.seq_mse", "aimet_torch.experimental.spinquant", "aimet_torch.utils", "aimet_torch.v1.adaround", "aimet_torch.v1.mixed_precision", "aimet_torch.v1.auto_quant", "aimet_torch.v1.quant_analyzer", "aimet_torch.v1.quantsim", "aimet_torch.v1.seq_mse", "External resources", "Glossary", "AIMET Documentation", "What is AIMET?", "Building from source", "Installation", "Quick Start", "Adaptive rounding", "AdaScale", "Automatic quantization", "Batch norm re-estimation", "Batch norm folding", "Cross-layer equalization", "Post Training Quantization Techniques", "OmniQuant", "Quantized LoRa", "QW-LoRa", "QWA-LoRa", "Sequential MSE", "SpinQuant", "Release notes", "Analysis tools", "Interactive visualization", "Layer output generation", "Quantization analyzer", "Blockwise Quantization", "Channel pruning", "Compression features Guidebook", "Greedy compression ratio selection", "Compression", "Spatial SVD", "AIMET visualization", "Weight SVD", "Winnowing", "Encoding Format Specification", "Techniques", "Low-Power Blockwise Quantization (LPBQ)", "Automatic mixed precision", "Mixed precision", "Lite mixed precision", "Manual mixed precision", "Post Training Quantization", "Quantization-aware training", "Runtime configuration", "PyTorch model guidelines", "Quantization debugging guidelines", "Tutorials", "meta-llama/Llama-3.2-1B-Instruct", "meta-llama/Llama-3.2-3B-Instruct", "microsoft/Phi-3.5-mini-instruct", "Qwen/Qwen2.5-0.5B-Instruct", "Qwen/Qwen2.5-1.5B-Instruct", "Qwen/Qwen3-4B", "Example Notebooks", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "Cross-Layer Equalization", "Quant Analyzer", "Quantization simulation", "Model compression using channel pruning", "Model compression using spatial SVD", "Model compression using spatial SVD and channel pruning", "Automatic Mixed-Precision (AMP)", "Adaptive Rounding (AdaRound)", "AutoQuant", "Quantization-Aware Training with BatchNorm Re-estimation", "Cross-Layer Equalization", "Quantization-aware training", "Quantization-aware training with range learning", "Quant Analyzer", "Quantization-Aware Training (QAT)", "On-target inference", "Quantization recipes for LLMs", "Quantization workflow", "Quantization simulation guide", "Quantization user guide", "AIMET features", "Quantization workflow", "AIMET documentation versions"], "titleterms": {"0": [202, 216, 219, 232, 254], "1": [162, 189, 190, 191, 192, 193, 196, 200, 201, 202, 205, 206, 216, 219, 221, 222, 223, 224, 225, 227, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 259], "10": [186, 202], "11": 202, "12": 202, "13": 202, "14": 202, "15": 202, "16": 202, "17": 202, "18": 202, "19": 202, "1b": [229, 254], "2": [162, 189, 190, 191, 192, 193, 196, 200, 201, 202, 205, 206, 216, 219, 221, 222, 223, 224, 225, 227, 229, 230, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 259], "20": 202, "21": 202, "22": 202, "23": 202, "24": 202, "25": 202, "26": 202, "27": 202, "28": 202, "29": 202, "3": [186, 189, 190, 191, 192, 193, 196, 200, 201, 202, 205, 206, 216, 219, 221, 223, 224, 225, 227, 229, 230, 231, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 259], "30": 202, "31": 202, "32": 202, "33": 202, "34": 202, "35": 202, "3b": [230, 254], "4": [189, 190, 191, 192, 196, 200, 201, 202, 205, 206, 221, 223, 225, 227, 236, 237, 238, 244, 245, 247, 248, 249, 250, 252, 254, 255, 259], "4b": [234, 254], "5": [190, 191, 196, 200, 201, 202, 206, 221, 223, 225, 227, 231, 232, 233, 247, 254, 255], "5b": [232, 233, 254], "6": [191, 202, 206, 216, 221, 227, 254], "7": [191, 202, 221, 227], "8": [202, 227], "9": 202, "For": [237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250], "On": [253, 257], "accuraci": [236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 252, 255, 258, 259], "activ": [206, 227], "adapt": [189, 195, 237, 245], "adaround": [12, 176, 237, 245], "adascal": [13, 190, 195], "adjust": 221, "advanc": 255, "affin": [162, 171], "ai": 253, "aimet": [0, 184, 185, 186, 188, 213, 256, 257, 258, 260], "aimet_onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "aimet_torch": [12, 13, 14, 15, 16, 17, 18, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181], "algorithm": [219, 236, 244], "also": 256, "altern": 187, "amp": [236, 244, 255], "an": [236, 237, 238, 240], "analysi": [203, 206, 217, 219, 221, 227, 239, 251], "analyz": [203, 206, 239, 251], "api": [0, 5, 158, 163, 164, 166, 169, 171, 184, 189, 190, 191, 192, 193, 194, 196, 200, 201, 204, 205, 206, 207, 208, 212, 214, 218, 219, 221, 222, 223, 224, 236, 244, 258], "appli": [218, 221, 222, 237, 238, 239, 245, 248, 251], "apply_adaround": 1, "apply_seq_ms": 11, "arg": 216, "auto_qu": [14, 178], "automat": [191, 195, 219, 220, 236, 244, 255], "autoqu": 246, "awar": [217, 224, 247, 249, 250, 252, 255, 258], "base": [198, 221, 222], "baselin": [236, 237, 238, 240, 241, 242, 243, 244, 245, 248, 249, 250, 255], "batch": [192, 193, 195, 236, 238, 240, 244, 245, 248, 249, 250], "batch_norm_fold": [3, 16], "batchnorm": 247, "befor": 252, "block": 171, "blockwis": [207, 217, 218], "bn_reestim": 15, "bokeh": 213, "brows": 235, "build": [186, 187], "calibr": [199, 206, 223], "call": [236, 244, 256], "callback": [199, 206, 223, 236, 244], "case": [211, 219], "channel": [171, 208, 211, 241, 243], "check": 227, "choos": 187, "cle": [238, 248], "code": [162, 163, 208, 212, 214, 235], "compil": [186, 253], "compress": [18, 208, 209, 210, 211, 212, 213, 214, 217, 241, 242, 243], "comput": [166, 221, 223, 224, 236, 244], "conclus": 252, "confid": 227, "config_util": 161, "configur": [166, 225, 256], "constant": 246, "contact": 254, "contain": 186, "context": [189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 204, 205, 206, 208, 212, 214, 219, 222], "convers": 253, "convert": [219, 236, 237, 238, 240], "cp": 211, "creat": [186, 199, 206, 219, 221, 223, 236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 252], "cross": [194, 195, 238, 248], "cross_layer_equ": [4, 17], "cuda": 186, "data": [216, 252], "dataset": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252], "debug": [227, 257], "default": 225, "defin": [236, 244, 246], "depend": 186, "deploi": [255, 259], "deploy": 258, "dequant": 152, "dequantizedtensor": 147, "descript": 206, "design": 213, "desir": 186, "detail": 255, "determin": [237, 238, 240, 245, 248, 249, 250, 256], "dictionari": 216, "direct": 253, "disabl": [239, 251], "docker": 186, "document": [184, 186, 260], "download": 235, "enabl": [206, 239, 251], "encod": [166, 206, 216, 221, 223, 236, 239, 244, 251, 256], "engin": 253, "environ": 186, "equal": [194, 195, 238, 248], "error": 206, "estim": [192, 195, 247], "evalu": [206, 221, 223, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 254], "exampl": [162, 163, 184, 208, 212, 214, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "execut": [192, 194, 207, 253], "experiment": [13, 167, 174], "explor": 210, "export": [168, 223, 247, 256, 258], "extern": 182, "faq": [211, 254], "featur": [209, 257, 258], "file": 225, "find": [219, 255], "fine": [211, 241, 242, 243], "fix": 227, "float": 162, "floatquantizedequant": 155, "flow": [169, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "fold": [193, 195, 236, 238, 240, 244, 245, 247, 248, 249, 250], "format": 216, "fp16": 259, "fp32": [227, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "from": [162, 186, 187], "front": 219, "function": [236, 244, 246], "gener": [203, 205, 259], "get": [185, 241, 242, 243, 244, 245, 248, 249, 250, 252], "get_backend": 156, "glossari": [183, 184], "granular": 256, "greedi": 210, "group": 219, "guid": [162, 256, 257], "guidebook": 209, "guidelin": [226, 227, 257, 259], "helper": 246, "histogram": 206, "how": [162, 210, 215, 225, 256], "hub": 253, "i": [185, 236, 239, 244, 247, 251], "imagenet": 252, "import": [205, 206], "improv": 258, "individu": 227, "infer": [253, 257, 258], "inform": [237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250], "initi": [224, 252], "input": [205, 222], "instal": [186, 187, 188], "instanti": [237, 238, 240, 241, 242, 243, 245, 248, 249, 250, 252], "instruct": [229, 230, 231, 232, 233, 254], "interact": [203, 204], "layer": [194, 195, 203, 205, 206, 210, 211, 222, 227, 236, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251], "layer_output_util": [6, 160], "leaf": 222, "learn": 250, "learnedgrid": 162, "level": 216, "librari": 206, "limit": 163, "list": 219, "lite": [220, 221, 255], "lite_mp": 7, "llama": [229, 230, 254], "llm": 254, "load": [205, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "loader": 252, "local": 186, "lora": [195, 197, 198, 199], "loss": [206, 239, 251], "low": [217, 218], "lpbq": 218, "manual": [220, 222], "max": [206, 239, 251, 256], "mean": 206, "meta": [229, 230, 254], "microsoft": [231, 254], "migrat": 162, "min": [206, 239, 251, 256], "mini": [231, 254], "mix": [217, 219, 220, 221, 222, 236, 244, 255, 258], "mixed_precis": [2, 165, 177], "mmp": 222, "model": [188, 198, 205, 206, 211, 221, 222, 223, 226, 227, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 259], "model_input": 225, "model_output": 225, "model_prepar": 163, "model_valid": 164, "modifi": 225, "modul": [162, 166], "more": [237, 238, 240, 241, 242, 243, 245, 246, 248, 249, 250], "move": 162, "mse": [195, 200, 239, 251], "new": 186, "next": [237, 241, 242, 243, 245, 246, 248, 249, 250, 255, 259], "nn": 166, "nois": 256, "non": 222, "norm": [192, 193, 195, 238, 248], "normal": [236, 240, 244, 245, 249, 250], "note": [184, 202, 211], "notebook": [184, 235, 236, 239, 244, 247, 251], "nvidia": 186, "obtain": 205, "old": 187, "omniqu": [167, 195, 196], "onnx": [168, 236, 237, 238, 240], "option": [211, 222], "output": [203, 205, 222], "overal": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251], "overhead": 219, "overview": [184, 210, 211, 213, 215, 225, 254, 256], "packag": [186, 187], "param": 225, "paramet": [224, 236, 244, 256], "pareto": 219, "path": 258, "pdf": [239, 251], "peft": 169, "per": [171, 206, 210, 211, 227, 239, 251], "perform": [219, 221, 227, 247, 249, 250, 254, 255], "phase": 219, "phi": [231, 254], "pip": 186, "pipelin": [236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251], "platform": [185, 188], "post": [184, 195, 217, 223, 252, 255, 258], "power": [217, 218], "precis": [217, 219, 220, 221, 222, 236, 244, 255, 258, 259], "prepar": [206, 252], "prerequisit": [187, 189, 190, 191, 192, 196, 200, 201, 206, 221, 222, 223], "pretrain": 246, "procedur": [190, 191, 193, 196, 200, 201, 208, 219, 259], "process": 162, "profil": 222, "prune": [208, 211, 241, 243], "ptq": [198, 255], "pypi": 187, "python": 186, "pytorch": [226, 236, 237, 238, 240], "qat": [224, 247, 249, 250, 252, 255], "qualcomm": 253, "quant": [239, 251], "quant_analyz": [9, 170, 179], "quantanalyz": [206, 239, 251], "quantiz": [150, 153, 162, 166, 171, 184, 188, 191, 195, 197, 198, 203, 206, 207, 216, 217, 218, 219, 223, 224, 227, 236, 237, 238, 239, 240, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259], "quantizationmixin": 19, "quantizationsimmodel": [199, 252], "quantize_dequant": 154, "quantizedadaptiveavgpool1d": 20, "quantizedadaptiveavgpool2d": 21, "quantizedadaptiveavgpool3d": 22, "quantizedadaptivemaxpool1d": 23, "quantizedadaptivemaxpool2d": 24, "quantizedadaptivemaxpool3d": 25, "quantizedalphadropout": 26, "quantizedavgpool1d": 27, "quantizedavgpool2d": 28, "quantizedavgpool3d": 29, "quantizedbatchnorm1d": 32, "quantizedbatchnorm2d": 33, "quantizedbatchnorm3d": 34, "quantizedbceloss": 30, "quantizedbcewithlogitsloss": 31, "quantizedbilinear": 35, "quantizedcelu": 36, "quantizedchannelshuffl": 38, "quantizedcircularpad1d": 39, "quantizedcircularpad2d": 40, "quantizedcircularpad3d": 41, "quantizedconstantpad1d": 42, "quantizedconstantpad2d": 43, "quantizedconstantpad3d": 44, "quantizedconv1d": 45, "quantizedconv2d": 46, "quantizedconv3d": 47, "quantizedconvtranspose1d": 48, "quantizedconvtranspose2d": 49, "quantizedconvtranspose3d": 50, "quantizedcosineembeddingloss": 51, "quantizedcosinesimilar": 52, "quantizedcrossentropyloss": 53, "quantizedctcloss": 37, "quantizeddropout": 54, "quantizeddropout1d": 55, "quantizeddropout2d": 56, "quantizeddropout3d": 57, "quantizedelu": 58, "quantizedembed": 59, "quantizedembeddingbag": 60, "quantizedequant": 151, "quantizedfeaturealphadropout": 61, "quantizedflatten": 62, "quantizedfold": 63, "quantizedfractionalmaxpool2d": 64, "quantizedfractionalmaxpool3d": 65, "quantizedgaussiannllloss": 70, "quantizedgelu": 66, "quantizedglu": 67, "quantizedgroupnorm": 71, "quantizedgru": 68, "quantizedgrucel": 69, "quantizedhardshrink": 72, "quantizedhardsigmoid": 73, "quantizedhardswish": 74, "quantizedhardtanh": 75, "quantizedhingeembeddingloss": 76, "quantizedhuberloss": 77, "quantizedinstancenorm1d": 78, "quantizedinstancenorm2d": 79, "quantizedinstancenorm3d": 80, "quantizedkldivloss": 81, "quantizedl1loss": 82, "quantizedlayernorm": 87, "quantizedleakyrelu": 88, "quantizedlinear": 89, "quantizedlocalresponsenorm": 90, "quantizedlogsigmoid": 91, "quantizedlogsoftmax": 92, "quantizedlppool1d": 83, "quantizedlppool2d": 84, "quantizedlstm": 85, "quantizedlstmcel": 86, "quantizedmarginrankingloss": 94, "quantizedmaxpool1d": 95, "quantizedmaxpool2d": 96, "quantizedmaxpool3d": 97, "quantizedmaxunpool1d": 98, "quantizedmaxunpool2d": 99, "quantizedmaxunpool3d": 100, "quantizedmish": 101, "quantizedmseloss": 93, "quantizedmultilabelmarginloss": 102, "quantizedmultilabelsoftmarginloss": 103, "quantizedmultimarginloss": 104, "quantizednllloss": 105, "quantizednllloss2d": 106, "quantizedpairwisedist": 108, "quantizedpixelshuffl": 109, "quantizedpixelunshuffl": 110, "quantizedpoissonnllloss": 111, "quantizedprelu": 107, "quantizedreflectionpad1d": 117, "quantizedreflectionpad2d": 118, "quantizedreflectionpad3d": 119, "quantizedrelu": 115, "quantizedrelu6": 116, "quantizedreplicationpad1d": 120, "quantizedreplicationpad2d": 121, "quantizedreplicationpad3d": 122, "quantizedrnn": 112, "quantizedrnncel": 113, "quantizedrrelu": 114, "quantizedselu": 123, "quantizedsigmoid": 125, "quantizedsilu": 124, "quantizedsmoothl1loss": 126, "quantizedsoftmarginloss": 127, "quantizedsoftmax": 128, "quantizedsoftmax2d": 129, "quantizedsoftmin": 130, "quantizedsoftplu": 131, "quantizedsoftshrink": 132, "quantizedsoftsign": 133, "quantizedtanh": 134, "quantizedtanhshrink": 135, "quantizedtensor": 148, "quantizedtensorbas": 149, "quantizedthreshold": 136, "quantizedtripletmarginloss": 137, "quantizedtripletmarginwithdistanceloss": 138, "quantizedunflatten": 139, "quantizedunfold": 140, "quantizedupsampl": 141, "quantizedupsamplingbilinear2d": 142, "quantizedupsamplingnearest2d": 143, "quantizedzeropad1d": 144, "quantizedzeropad2d": 145, "quantizedzeropad3d": 146, "quantsim": [8, 10, 161, 172, 180, 221, 223, 256], "quantwrapp": 162, "quick": [188, 254], "quickli": 188, "qw": [197, 198], "qwa": [197, 199], "qwen": [232, 233, 234, 254], "qwen2": [232, 233, 254], "qwen3": [234, 254], "random": 252, "rang": [206, 239, 250, 251], "rank": 211, "ratio": [210, 211, 213], "re": [192, 195, 247], "recip": 254, "recommend": 224, "recomput": 221, "reconstruct": 208, "reduc": [219, 259], "reestim": 247, "refer": [166, 171, 184, 211], "relat": 235, "releas": [184, 202], "requir": 254, "resourc": 182, "restor": 259, "round": [189, 195, 211, 237, 245], "run": [186, 199, 206, 224, 235, 236, 244, 246, 252], "runtim": [225, 256], "scheme": 256, "score": [241, 242, 243, 244, 245, 248, 249, 250], "sdk": 253, "seed": 252, "select": [208, 210, 211], "sensit": [206, 219, 221, 227], "seq_ms": [173, 181], "sequenti": [195, 200], "server": [213, 235], "session": 213, "set": [186, 222, 252], "set_backend": 157, "set_lpbq_for_param": 8, "setup": [189, 190, 192, 194, 196, 198, 199, 200, 201, 208, 212, 214, 222, 224], "signal": 256, "sim": [236, 238, 240, 244, 245, 247, 248, 249, 250], "simplifi": [236, 237, 238, 240], "simul": [236, 237, 238, 240, 244, 245, 247, 248, 249, 250, 256, 258], "small": 188, "sourc": [186, 187], "spatial": [211, 212, 242, 243], "specif": 216, "spinquant": [174, 195, 201], "squar": 206, "start": [185, 188, 213, 252, 254], "staticgrid": 162, "statist": [206, 239, 247, 251], "step": [189, 190, 191, 192, 193, 196, 200, 201, 205, 206, 219, 221, 222, 223, 224, 237, 241, 242, 243, 245, 246, 248, 249, 250, 255, 259], "structur": [216, 225], "summari": [247, 254], "supergroup": 225, "support": [185, 186, 258], "svd": [211, 212, 214, 242, 243], "system": 254, "target": [253, 257, 258], "techniqu": [184, 195, 211, 217, 248, 255], "terminologi": 169, "test": [186, 188], "tf": 256, "thi": [236, 239, 244, 247, 251], "tool": [159, 203, 217, 258], "top": 216, "tradeoff": 255, "train": [184, 195, 198, 199, 217, 223, 224, 237, 238, 239, 240, 241, 242, 243, 245, 247, 248, 249, 250, 251, 252, 255, 258], "transform": 252, "try": 259, "tune": [211, 241, 242, 243], "tutori": [184, 228], "type": [216, 222], "typic": 224, "u": 254, "unit": 186, "updat": 198, "us": [162, 208, 211, 212, 214, 219, 241, 242, 243, 255], "user": [169, 257], "util": 175, "uv": 186, "v": [162, 255], "v1": [158, 176, 177, 178, 179, 180, 181], "valid": [237, 238, 240, 241, 242, 243, 245, 248, 249, 250], "variabl": 186, "variant": 224, "verifi": [187, 188, 259], "version": [187, 216, 260], "vision": 252, "visual": [159, 203, 204, 213, 227], "visualization_tool": 159, "vit": 252, "w16a16": 259, "w4a8": 252, "w8a8": 221, "w8a8_mix": 221, "weight": [198, 206, 208, 211, 214, 227], "what": [185, 236, 239, 244, 247, 251], "wheel": 186, "winnow": [208, 215], "work": [210, 215, 256], "workflow": [189, 190, 191, 192, 193, 194, 196, 198, 199, 200, 201, 204, 205, 206, 208, 212, 214, 219, 221, 222, 223, 224, 227, 254, 255, 256, 257, 259], "wrapper": [239, 251], "x": 162}})